{
  "model_path": "models/begin_url/final_model",
  "evaluation_params": {
    "max_new_tokens": 200,
    "temperature": 0.7,
    "top_p": 0.9,
    "batch_size": 32,
    "num_samples": 1604
  },
  "average_scores": {
    "rouge-1": 0.17476306699197983,
    "rouge-2": 0.02419204083784411,
    "rouge-l": 0.16033426856653593
  },
  "evaluation_time": 231.32546591758728,
  "samples_per_second": 6.933953396084139,
  "individual_results": [
    {
      "paper_id": "econ.EM.econ/EM/2411.16978v2",
      "true_abstract": "We establish normal approximation in the Wasserstein metric and central limit\ntheorems for both non-degenerate and degenerate U-statistics with\ncross-sectionally dependent samples using Stein's method. For the\nnon-degenerate case, our results extend recent studies on the asymptotic\nproperties of sums of cross-sectionally dependent random variables. The\ndegenerate case is more challenging due to the additional dependence induced by\nthe nonlinearity of the U-statistic kernel. Through a specific implementation\nof Stein's method, we derive convergence rates under conditions on the mixing\nrate, the sparsity of the cross-sectional dependence structure, and the moments\nof the U-statistic kernel. Finally, we demonstrate the application of our\ntheoretical results with a nonparametric specification test for data with\ncross-sectional dependence.",
      "generated_abstract": "This paper introduces a novel method for identifying the number of\nmodels that best represent a data set. Our approach leverages the empirical\nstructure of the data and the estimated model parameters to identify the number\nof models that best fit the data. This approach is particularly useful when\nthere is uncertainty in the estimated model parameters, and it allows for\nflexibility in the number of models that can be selected from a large pool of\npossible models. We show that our method is efficient and provides a reliable\napproach for identifying the number of models that best represent a data set.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15584415584415584,
          "p": 0.22641509433962265,
          "f": 0.18461537978579892
        },
        "rouge-2": {
          "r": 0.02830188679245283,
          "p": 0.03896103896103896,
          "f": 0.03278688037146598
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.20754716981132076,
          "f": 0.16923076440118356
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17215v1",
      "true_abstract": "This paper presents an integrated modelling assessment that estimated the\nsensitivities of five endogenous factors in commercial rangelands, i.e. number\nof active farmers, profits, stocking rate, standing herbage biomass, and soil\nerosion, to the same percentage variation in 70 factors, including economic and\nclimate drivers. The assessment utilised a system dynamics model (107\nequations) which represents an area of extensive private farms, its farmers,\nthe main local markets on which they trade, and key ecosystem services\ninvolved. The assessment procedure consisted in analysing the behaviours of\n288,000 variants of this system during 300 years, each under a different\neconomic and climate scenario. Our key findings were as follows: 1) It is\nlikely that at least annual grasslands will suffer environmental degradation in\nthe future, and that such degradation will be primarily caused by climate\nchange, not by the increasing demand for livestock products; 2) Private farming\nsystems provide social and economic security to farmers against the effects of\nclimate change, especially in a scenario of rising prices of animal products.\nHowever, this research will remain incomplete until its methods and results can\nbe contrasted with other similar assessments.",
      "generated_abstract": "The United States and China are currently the world's largest energy importers\nand exporters, respectively. While China is the world's largest importer of\nnatural gas, the United States is the largest exporter of oil, natural gas,\nand LNG. In this paper, we use a dynamic factor-augmented VAR model to\nquantify the impact of the COVID-19 pandemic on global energy demand. Using\ndata from 1970 to 2024, we construct a unique global energy demand model, which\ncaptures the impact of the pandemic on global energy demand using a novel\ndynamic factor-augmented VAR model. Our results show that the pandemic had a\nsignificant impact on global energy demand. This is primarily due to the\nrestriction of travel and the decline in economic activity, which reduced\nenergy demand. Additionally, we find that the impact of the pan",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1366906474820144,
          "p": 0.2638888888888889,
          "f": 0.18009478223400208
        },
        "rouge-2": {
          "r": 0.005434782608695652,
          "p": 0.009345794392523364,
          "f": 0.006872847583758651
        },
        "rouge-l": {
          "r": 0.12949640287769784,
          "p": 0.25,
          "f": 0.17061610924822007
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.15264v1",
      "true_abstract": "Speech recognition systems often face challenges due to domain mismatch,\nparticularly in real-world applications where domain-specific data is\nunavailable because of data accessibility and confidentiality constraints.\nInspired by Retrieval-Augmented Generation (RAG) techniques for large language\nmodels (LLMs), this paper introduces a LLM-based retrieval-augmented speech\nrecognition method that incorporates domain-specific textual data at the\ninference stage to enhance recognition performance. Rather than relying on\ndomain-specific textual data during the training phase, our model is trained to\nlearn how to utilize textual information provided in prompts for LLM decoder to\nimprove speech recognition performance. Benefiting from the advantages of the\nRAG retrieval mechanism, our approach efficiently accesses locally available\ndomain-specific documents, ensuring a convenient and effective process for\nsolving domain mismatch problems. Experiments conducted on the CSJ database\ndemonstrate that the proposed method significantly improves speech recognition\naccuracy and achieves state-of-the-art results on the CSJ dataset, even without\nrelying on the full training data.",
      "generated_abstract": "Deep Learning models are increasingly utilized to generate visual\nclues in text-based conversations, such as the target object, location, and\nattribute. However, such models often fail to capture the nuances of\nconversational contexts, resulting in suboptimal predictions. To address this\nissue, we introduce Conversational AI (ConAi), a novel framework for\ngenerative text-based conversational AI. ConAi leverages the power of\ntransformer-based language models to generate target objects, locations, and\nattributes in real-time, without requiring labeled data. ConAi achieves this\nby integrating a text-to-text transformer with a visual-text matching\nmodule. The former encodes the text input as a visual prompt, while the latter\nretrieves the corresponding visual clue through a pre-trained vision-language\nmodel. ConAi achieves state-of-the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14953271028037382,
          "p": 0.19047619047619047,
          "f": 0.16753926208821043
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.009009009009009009,
          "f": 0.007905133414991584
        },
        "rouge-l": {
          "r": 0.14018691588785046,
          "p": 0.17857142857142858,
          "f": 0.15706805789972877
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.17466v1",
      "true_abstract": "For a hypergroup $(H,\\circ)$ we consider $\\gamma^{\\ast}$, as the smallest\nequivalence relation on $H$ such that the quotion\n$(H/\\gamma^{\\ast},\\tiny{\\otimes})$ is an abelian group. We study some more\nproperties of $\\gamma^{\\ast}$. Initially, it is investigated which\nsubhypergroup the congruence relation modulo is strongly regular on, and its\nquotient results in an abelian group? This is directly related to the\nfundamental relation $\\gamma^{\\ast}$, since such subhypergroups must contain\n$S_{\\gamma}$. Then, we examine the functor $\\gamma^{\\ast}$ from a categorical\nperspective and investigate properties such as continuity and cocontinuity\nconcerning it using the decomposition $\\gamma=\\delta\\tiny{\\ast}\\beta$. For this\npurpose, we define the reduced words on strongly regular hypergroups. This has\na direct application in studying how the functor $\\gamma^{\\ast}$ affects on the\nstalks of the sheaves of hypergroups.",
      "generated_abstract": "We study the theory of quasi-isometries and quasi-isometry classes of\ncompact metric spaces. We define the notion of quasi-isometry classes of\nspaces and show that the notion of quasi-isometry is the same as that of\nquasi-isometry classes of compact metric spaces. We then prove that the\nGromov-Hausdorff distance between two quasi-isometry classes of metric spaces\nis equivalent to the distance between their corresponding quasi-isometry\nclasses. We also prove that the distance between two quasi-isometry classes of\nmetric spaces is in particular equivalent to the distance between the corresponding\nquasi-isometry classes of the underlying metric spaces.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13414634146341464,
          "p": 0.3333333333333333,
          "f": 0.19130434373383753
        },
        "rouge-2": {
          "r": 0.0423728813559322,
          "p": 0.08771929824561403,
          "f": 0.05714285275036768
        },
        "rouge-l": {
          "r": 0.12195121951219512,
          "p": 0.30303030303030304,
          "f": 0.17391303938601144
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.05951v1",
      "true_abstract": "Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.",
      "generated_abstract": "We study the mean-variance (MV) portfolio selection problem in a stochastic\nportfolio optimization framework with a continuous-time agent. The goal is to\nmaximize the expected return subject to a given constraint on the variance. We\nintroduce a novel dynamic mean-variance portfolio selection approach, referred\nto as the MV-DMC algorithm, which is based on a deep neural network to estimate\nthe mean and variance of the stochastic portfolio. We establish the\nconvergence of the MV-DMC algorithm to the optimal mean-variance portfolio\nwhen the agent's stochastic portfolio is well-specified. Furthermore, we\nprovide an upper bound on the MV-DMC portfolio selection error, which is\nrelatively small for a wide range of scenarios. The MV-DMC algorithm is\nimplemented on a real-world stock trading platform. We confirm the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1171875,
          "p": 0.2112676056338028,
          "f": 0.15075376425443815
        },
        "rouge-2": {
          "r": 0.010810810810810811,
          "p": 0.01818181818181818,
          "f": 0.013559317357082911
        },
        "rouge-l": {
          "r": 0.109375,
          "p": 0.19718309859154928,
          "f": 0.14070351299815678
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.08613v4",
      "true_abstract": "As operators acting on the undetermined final settlement of a derivative\nsecurity, expectation is linear but price is non-linear. When the market of\nunderlying securities is incomplete, non-linearity emerges from the bid-offer\naround the mid price that accounts for the residual risks of the optimal\nfunding and hedging strategy. At the extremes, non-linearity also arises from\nthe embedded options on capital that are exercised upon default. In this essay,\nthese convexities are quantified in an entropic risk metric that evaluates the\nstrategic risks, which is realised as a cost with the introduction of bilateral\nmargin. Price is then adjusted for market incompleteness and the risk of\ndefault caused by the exhaustion of capital.\n  In the complete market theory, price is derived from a martingale condition.\nIn the incomplete market theory presented here, price is instead derived from a\nlog-martingale condition: \\begin{equation}\np=-\\frac{1}{\\alpha}\\log\\mathbb{E}\\exp[-\\alpha P] \\notag \\end{equation} for the\nprice $p$ and payoff $P$ of a funded and hedged derivative security, where the\nprice measure $\\mathbb{E}$ has minimum entropy relative to economic\nexpectations, and the parameter $\\alpha$ matches the risk aversion of the\ninvestor. This price principle is easily applied to standard models for market\nevolution, with applications considered here including model risk analysis,\ndeep hedging and decentralised finance.",
      "generated_abstract": "We consider a continuous-time market with a single agent that trades a\nmarket-dependent asset. The agent's goal is to maximize expected utility over\nthe trading horizon, subject to a risk-averse constraint on the probability of\ntrading. We characterize the optimal portfolio policy and the optimal trading\nstrategy in terms of a new type of random walk, which we call a\n$T$-transformed random walk. We prove that the $T$-transformed random walk is\nthe unique solution to a nonlinear stochastic differential equation, which we\nshow is a linear stochastic differential equation in the case of a\ncontinuous-time market. We also show that, under appropriate conditions, the\n$T$-transformed random walk is the unique solution to a linear stochastic\ndifferential equation in the case of a discrete-time market. Finally, we\nshow that the $T$-transformed random walk is the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12121212121212122,
          "p": 0.24615384615384617,
          "f": 0.16243654380169562
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.04040404040404041,
          "f": 0.0274914044453899
        },
        "rouge-l": {
          "r": 0.11363636363636363,
          "p": 0.23076923076923078,
          "f": 0.15228425953773622
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2411.02531v3",
      "true_abstract": "The techniques suggested in Fr\\\"uhwirth-Schnatter et al. (2024) concern\nsparsity and factor selection and have enormous potential beyond standard\nfactor analysis applications. We show how these techniques can be applied to\nLatent Space (LS) models for network data. These models suffer from well-known\nidentification issues of the latent factors due to likelihood invariance to\nfactor translation, reflection, and rotation (see Hoff et al., 2002). A set of\nobservables can be instrumental in identifying the latent factors via auxiliary\nequations (see Liu et al., 2021). These, in turn, share many analogies with the\nequations used in factor modeling, and we argue that the factor loading\nrestrictions may be beneficial for achieving identification.",
      "generated_abstract": "We propose a novel method to estimate the causal effect of an exposure on a\ntarget outcome when the exposure and the target outcome are jointly exposed to\na confounder. This is a generalization of the well-known approach to\nunconfoundedness, where the exposure is assumed to be unconfounded. The\nconfounder is assumed to be confounded with both the exposure and the target\noutcome. We show that under mild conditions, the proposed estimator is\nidentifiable. We provide an analytical expression for the estimator, and\nprove consistency and asymptotic normality. We also provide a Monte Carlo\nsimulation study to illustrate the performance of the proposed estimator. We\nalso apply the proposed estimator to a real-world dataset on the effects of\nsmoking on cardiovascular disease.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.1791044776119403,
          "f": 0.162162157206903
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.019230769230769232,
          "f": 0.019047614048073876
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.16417910447761194,
          "f": 0.1486486436933895
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.13228v1",
      "true_abstract": "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
      "generated_abstract": "This paper examines the role of the internet in facilitating\nthe rise of local e-commerce platforms and the consequent evolution of\nconsumer-retailer relations in China. We utilize a unique dataset on the\ndevelopment of online shopping in China and examine the evolution of\nretailer-consumer relationships over time. We find that the rise of e-commerce\nplatforms (E-commerce) has fundamentally changed the consumer-retailer\nrelationship, with E-commerce enhancing the consumer's ability to identify\nproducts and make purchasing decisions, leading to more direct and aggressive\nretailer-consumer interactions. Our findings highlight the importance of\ntechnological innovation in transforming the consumer-retailer relationship and\nthe consequent evolution of retailer practices in China.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07482993197278912,
          "p": 0.16666666666666666,
          "f": 0.10328638069959681
        },
        "rouge-2": {
          "r": 0.005,
          "p": 0.010752688172043012,
          "f": 0.006825934233365981
        },
        "rouge-l": {
          "r": 0.07482993197278912,
          "p": 0.16666666666666666,
          "f": 0.10328638069959681
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2405.10453v1",
      "true_abstract": "Team and player evaluation in professional sport is extremely important given\nthe financial implications of success/failure. It is especially critical to\nidentify and retain elite shooters in the National Basketball Association\n(NBA), one of the premier basketball leagues worldwide because the ultimate\ngoal of the game is to score more points than one's opponent. To this end we\npropose two novel basketball metrics: \"expected points\" for team-based\ncomparisons and \"expected points above average (EPAA)\" as a player-evaluation\ntool. Both metrics leverage posterior samples from Bayesian hierarchical\nmodeling framework to cluster teams and players based on their shooting\npropensities and abilities. We illustrate the concepts for the top 100 shot\ntakers over the last decade and offer our metric as an additional metric for\nevaluating players.",
      "generated_abstract": "In a recent paper, we introduced a new method for constructing a graphical\nmodel for the distribution of a random variable by applying a graphical\nmodification of the multivariate Cram\\'er-Rao bound. We showed that this\nmethod leads to a graphical model for the joint distribution of two random\nvariables. In this paper, we show that the same method can be used to construct\na graphical model for the joint distribution of two random variables, as well.\nIn particular, we show that the method is applicable even when the marginal\ndistributions of the two random variables are unknown. We further show that the\nmethod can be used to construct a graphical model for the joint distribution of\na random variable and its predictor, which is a special case of our method.\nFinally, we show that the method can be used to construct a graphical model for\nthe joint distribution of two random variables and a predictor, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1326530612244898,
          "p": 0.22033898305084745,
          "f": 0.16560509084993322
        },
        "rouge-2": {
          "r": 0.016260162601626018,
          "p": 0.021505376344086023,
          "f": 0.018518513614970436
        },
        "rouge-l": {
          "r": 0.1326530612244898,
          "p": 0.22033898305084745,
          "f": 0.16560509084993322
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.13850v1",
      "true_abstract": "We provide a formal framework accounting for a widespread idea in the theory\nof economic design: analytically established incompatibilities between given\naxioms should be qualified by the likelihood of their violation. We define the\ndegree to which rules satisfy an axiom, as well as several axioms, on the basis\nof a probability measure over the inputs of the rules. Armed with this notion\nof degree, we propose and characterize i) a criterion to evaluate and compare\nrules given a set of axioms, allowing the importance of each combination of\naxioms to differ, and ii) a criterion to measure the compatibility between\ngiven axioms, building on a analogy with cooperative game theory.",
      "generated_abstract": "We study a two-stage game where an agent can either accept a fixed\n(or stochastic) payoff, or take a risk-neutral (or stochastic) payoff. The\nagent's utility depends on the payoff. We show that the game is strategic if\nand only if the payoff is a linear combination of the utility and the payoff.\nMoreover, if the payoff is a linear combination of the utility and the payoff,\nthen the game is strategic if and only if the utility function is monotone.\nFinally, we show that the game is strategic if and only if the payoff is a\nlinear combination of the utility and the payoff. We illustrate our results\nthrough examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14084507042253522,
          "p": 0.21739130434782608,
          "f": 0.17094016616845656
        },
        "rouge-2": {
          "r": 0.028037383177570093,
          "p": 0.046153846153846156,
          "f": 0.03488371622836732
        },
        "rouge-l": {
          "r": 0.14084507042253522,
          "p": 0.21739130434782608,
          "f": 0.17094016616845656
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.18816v1",
      "true_abstract": "This paper presents a novel approach to evaluating blue-chip art as a viable\nasset class for portfolio diversification. We present the Arte-Blue Chip Index,\nan index that tracks 100 top-performing artists based on 81,891 public\ntransactions from 157 artists across 584 auction houses over the period 1990 to\n2024. By comparing blue-chip art price trends with stock market fluctuations,\nour index provides insights into the risk and return profile of blue-chip art\ninvestments. Our analysis demonstrates that a 20% allocation of blue-chip art\nin a diversified portfolio enhances risk-adjusted returns by around 20%, while\nmaintaining volatility levels similar to the S&P 500.",
      "generated_abstract": "We introduce a novel approach to the pricing of interest rate derivatives\nunder a stochastic volatility framework, focusing on the effect of volatility\nstructures on the value of the option. Our framework extends the classical\nSDE-based approach by incorporating the volatility structure into the price\nfunction. We derive the Black-Scholes formula under a Gaussian and a\nDirichlet-valued volatility, and show that the resulting formulas are\nequivalent to the standard Black-Scholes formula when the volatility is Gaussian.\nWe then extend the framework to the case of a non-Gaussian volatility,\nproviding a generalization of the Black-Scholes formula. We show that the\nequivalent formula is also the Black-Scholes formula when the volatility is\nDirichlet-valued.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18072289156626506,
          "p": 0.26785714285714285,
          "f": 0.21582733331815135
        },
        "rouge-2": {
          "r": 0.05154639175257732,
          "p": 0.053763440860215055,
          "f": 0.052631573949584974
        },
        "rouge-l": {
          "r": 0.1686746987951807,
          "p": 0.25,
          "f": 0.20143884410951826
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.02011v1",
      "true_abstract": "Regression models are essential for a wide range of real-world applications.\nHowever, in practice, target values are not always precisely known; instead,\nthey may be represented as intervals of acceptable values. This challenge has\nled to the development of Interval Regression models. In this study, we provide\na comprehensive review of existing Interval Regression models and introduce\nalternative models for comparative analysis. Experiments are conducted on both\nreal-world and synthetic datasets to offer a broad perspective on model\nperformance. The results demonstrate that no single model is universally\noptimal, highlighting the importance of selecting the most suitable model for\neach specific scenario.",
      "generated_abstract": "In this paper, we consider the problem of generating synthetic samples from a\ndistribution with a known mean function and covariance function. The key\nquestion is whether such a distribution can be generated by an optimal design\nof the mean and covariance functions. We show that, under some regularity\nconditions, the answer is affirmative. The key idea is to consider a\nparametric family of distributions and show that the corresponding mean and\ncovariance functions can be chosen such that the samples generated by this\nfamily are close to the target distribution. The proof uses the theory of\nBayesian optimization and the notion of posterior mean and posterior covariance\nfunctions. We provide a detailed discussion of the proofs, along with an\nexample illustrating the use of the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2077922077922078,
          "p": 0.22535211267605634,
          "f": 0.21621621122443405
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.02654867256637168,
          "f": 0.028436013982615815
        },
        "rouge-l": {
          "r": 0.16883116883116883,
          "p": 0.18309859154929578,
          "f": 0.1756756706838935
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.09665v1",
      "true_abstract": "Optimizing accelerator control is a critical challenge in experimental\nparticle physics, requiring significant manual effort and resource expenditure.\nTraditional tuning methods are often time-consuming and reliant on expert\ninput, highlighting the need for more efficient approaches. This study aims to\ncreate a simulation-based framework integrated with Reinforcement Learning (RL)\nto address these challenges. Using \\texttt{Elegant} as the simulation backend,\nwe developed a Python wrapper that simplifies the interaction between RL\nalgorithms and accelerator simulations, enabling seamless input management,\nsimulation execution, and output analysis.\n  The proposed RL framework acts as a co-pilot for physicists, offering\nintelligent suggestions to enhance beamline performance, reduce tuning time,\nand improve operational efficiency. As a proof of concept, we demonstrate the\napplication of our RL approach to an accelerator control problem and highlight\nthe improvements in efficiency and performance achieved through our\nmethodology. We discuss how the integration of simulation tools with a\nPython-based RL framework provides a powerful resource for the accelerator\nphysics community, showcasing the potential of machine learning in optimizing\ncomplex physical systems.",
      "generated_abstract": "The current status of superconducting qubit designs in the area of quantum\ncryptography is reviewed. A brief overview of the various types of qubit\ndesigns is given, and their advantages and disadvantages are discussed.\nExamples of state-of-the-art qubit designs that use superconducting circuits\nas qubits are presented. These qubits are shown to be very robust against\nenvironmental noise, and their performance is compared to the performance of\nalternative qubit designs. The current status of quantum key distribution\nprotocols using superconducting qubits is also reviewed.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08943089430894309,
          "p": 0.21568627450980393,
          "f": 0.1264367774653192
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08943089430894309,
          "p": 0.21568627450980393,
          "f": 0.1264367774653192
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2408.00942v2",
      "true_abstract": "Composition is a powerful principle for systems biology, focused on the\ninterfaces, interconnections, and orchestration of distributed processes to\nenable integrative multiscale simulations. Whereas traditional models focus on\nthe structure or dynamics of specific subsystems in controlled conditions,\ncompositional systems biology aims to connect these models, asking critical\nquestions about the space between models: What variables should a submodel\nexpose through its interface? How do coupled models connect and translate\nacross scales? How do domain-specific models connect across biological and\nphysical disciplines to drive the synthesis of new knowledge? This approach\nrequires robust software to integrate diverse datasets and submodels, providing\nresearchers with tools to flexibly recombine, iteratively refine, and\ncollaboratively expand their models. This article offers a comprehensive\nframework to support this vision, including: a conceptual and graphical\nframework to define interfaces and composition patterns; standardized schemas\nthat facilitate modular data and model assembly; biological templates that\nintegrate detailed submodels that connect molecular processes to the emergence\nof the cellular interface; and user-friendly software interfaces that empower\nresearch communities to construct and improve multiscale models of cellular\nsystems. By addressing these needs, compositional systems biology will foster a\nunified and scalable approach to understanding complex cellular systems.",
      "generated_abstract": "The integration of epigenetic mechanisms into models of gene regulation is\nan emerging field of biological research, yet the lack of theoretical\nframeworks to guide experimentally testable predictions remains a challenge.\nHere, we propose a novel methodology to explore the interplay between\nepigenetic mechanisms and gene regulation in complex systems. We use the\nwell-established co-evolution model of H-body motors to examine the\nco-evolution of epigenetic and transcriptional regulation in a bacterial\npathogen. We integrate molecular data from a live-cell imaging experiment and\na cell-cycle model to demonstrate the importance of epigenetic dynamics in\nregulating transcriptional responses to stress conditions. We illustrate the\nmethodology by comparing the evolutionary dynamics of the H-body motors to\nobserved transcriptional dynamics. This approach provides a framework for\nstudying the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15671641791044777,
          "p": 0.2727272727272727,
          "f": 0.1990521280663059
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.034482758620689655,
          "f": 0.026229503483150535
        },
        "rouge-l": {
          "r": 0.12686567164179105,
          "p": 0.22077922077922077,
          "f": 0.16113743612317796
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10196v1",
      "true_abstract": "In this paper, we present an error estimate for the filtered Lie splitting\nscheme applied to the Zakharov system, characterized by solutions exhibiting\nvery low regularity across all dimensions. Our findings are derived from the\napplication of multilinear estimates established within the framework of\ndiscrete Bourgain spaces. Specifically, we demonstrate that when the solution\n$(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in\n$H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for\n$s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this\nrepresents the first explicit error estimate for the splitting method based on\nthe original Zakharov system, as well as the first instance where low\nregularity error estimates for coupled equations have been considered within\nthe Bourgain framework. Furthermore, numerical experiments confirm the validity\nof our theoretical results.",
      "generated_abstract": "We study the stability of a discrete-time, time-delayed Markov chain with\nalmost surely positive inter-jump intensities and a continuous-time,\ntime-delayed Markov chain with almost surely positive inter-jump intensities.\nWe show that the discrete-time chain with almost surely positive inter-jump\nintensities has a stable fixed point if and only if the continuous-time chain\nwith almost surely positive inter-jump intensities has a stable fixed point.\nWe also show that if the discrete-time chain with almost surely positive\ninter-jump intensities has a stable fixed point, then the discrete-time chain\nwith almost surely positive inter-jump intensities has a stable fixed point if\nand only if the continuous-time chain with almost surely positive inter-jump\nintensities has a stable fixed point. We also show that if the discrete-time\nchain with almost surely positive inter",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.033707865168539325,
          "p": 0.09375,
          "f": 0.0495867729690598
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.033707865168539325,
          "p": 0.09375,
          "f": 0.0495867729690598
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.09435v1",
      "true_abstract": "In this paper, we consider the dynamic oscillation in the Cournot oligopoly\nmodel, which involves multiple firms producing homogeneous products. To explore\nthe oscillation under the updates of best response strategies, we focus on the\nlinear price functions. In this setting, we establish the existence of\noscillations. In particular, we show that for the scenario of different costs\namong firms, the best response converges to either a unique equilibrium or a\ntwo-period oscillation. We further characterize the oscillations and propose\nlinear-time algorithms for finding all types of two-period oscillations. To the\nbest of our knowledge, our work is the first step toward fully analyzing the\nperiodic oscillation in the Cournot oligopoly model.",
      "generated_abstract": "We consider a setting in which the government imposes a tax on each unit of\nproduct produced, with the tax being the same for all units. The government\nwants to maximize its revenue, and it is not allowed to increase the tax\nlevel. We show that the optimal tax level depends on the distribution of\noutput. In particular, we show that the optimal tax level is the same for all\ndistributed equally, and different for all distributed in a way that is\nasymptotically optimal. We show that the optimal tax level is the same for\ndifferent amounts of output, and different for different amounts of output. We\nalso show that the optimal tax level is different for different levels of\noutput.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25675675675675674,
          "p": 0.3584905660377358,
          "f": 0.29921259356190716
        },
        "rouge-2": {
          "r": 0.057692307692307696,
          "p": 0.07228915662650602,
          "f": 0.06417111805770864
        },
        "rouge-l": {
          "r": 0.25675675675675674,
          "p": 0.3584905660377358,
          "f": 0.29921259356190716
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08732v1",
      "true_abstract": "Background: Circadian desynchrony characterized by the misalignment between\nan individual's internal biological rhythms and external environmental cues,\nsignificantly affects various physiological processes and health outcomes.\nQuantifying circadian desynchrony often requires prolonged and frequent\nmonitoring, and currently, an easy tool for this purpose is missing.\nAdditionally, its association with the incidence of delirium has not been\nclearly explored. Methods: A prospective observational study was carried out in\nintensive care units (ICU) of a tertiary hospital. Circadian transcriptomics of\nblood monocytes from 86 individuals were collected on two consecutive days,\nalthough a second sample could not be obtained from all participants. Using two\npublic datasets comprised of healthy volunteers, we replicated a model for\ndetermining internal circadian time. We developed an approach to quantify\ncircadian desynchrony by comparing internal circadian time and external blood\ncollection time. We applied the model and quantified circadian desynchrony\nindex among ICU patients, and investigated its association with the incidence\nof delirium. Results: The replicated model for determining internal circadian\ntime achieved comparable high accuracy. The quantified circadian desynchrony\nindex was significantly higher among critically ill ICU patients compared to\nhealthy subjects, with values of 10.03 hours vs 2.50-2.95 hours (p < 0.001).\nMost ICU patients had a circadian desynchrony index greater than 9 hours.\nAdditionally, the index was lower in patients whose blood samples were drawn\nafter 3pm, with values of 5.00 hours compared to 10.01-10.90 hours in other\ngroups (p < 0.001)...",
      "generated_abstract": "The emergence of antibiotic resistance is a global challenge that demands\na thorough understanding of the underlying mechanisms. One of the most\nrelevant mechanisms is the interaction of bacteria and the environment, which\ncan be affected by a variety of factors, including nutrients, temperature, and\nlight. This study focuses on the effect of environmental factors on\nantibiotic-resistant bacteria, and the relationship between these factors and\nantibiotic resistance. We examined the effects of different environmental factors\non the antibiotic-resistant Escherichia coli bacteria, which were cultured\nunder different conditions. We found that temperature and light were the most\nimportant factors affecting antibiotic resistance. The results show that\ntemperature and light can promote resistance to antibiotics, and antibiotics can\nreduce resistance to temperature and light. This study provides insights",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09803921568627451,
          "p": 0.21739130434782608,
          "f": 0.13513513085098625
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0915032679738562,
          "p": 0.2028985507246377,
          "f": 0.12612612184197725
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09392v1",
      "true_abstract": "The muon anomalous magnetic moment, $a_\\mu=\\frac{g-2}{2}$, is a low-energy\nobservable which can be both measured and computed to high precision, making it\na sensitive test of the Standard Model and a probe for new physics. This\nanomaly was measured with a precision of $0.20$~parts per million (ppm) by the\nFermilab's Muon g-2 (E989) experiment. The final goal of the E989 experiment is\nto reach a precision of $0.14$~ppm. The experiment is based on the measurement\nof the muon spin anomalous precession frequency, $\\omega_a$, based on the\narrival time distribution of high-energy decay positrons observed by 24\nelectromagnetic calorimeters, placed around the inner circumference of a $14$~m\ndiameter storage ring, and on the precise knowledge of the storage ring\nmagnetic field and of the beam time and space distribution. Achieving this\nlevel of precision requires strict control over systematics, which is ensured\nthrough several diagnostic devices. At the accelerator level, these devices\nmonitor the quality of the injected beam (e.g., verifying that it has the\ncorrect momentum), while at the detector level, they track both the magnetic\nfield and the gain of the calorimeters. In this work the devices and techniques\nused by the E989 experiment will be presented.",
      "generated_abstract": "The discovery of the Higgs boson at the LHC provides a new window into the\ncomplexity of nature. The discovery of a new particle of fundamental symmetry\nprovides a window into the underlying theory of nature, offering new insight\ninto the fundamental laws of nature. The Higgs boson is a test of the\nStandard Model (SM) and the most precise determination of the Higgs boson mass\nyields an uncertainty of 2.5\\%. However, it is not yet clear how to test the\npredictions of the SM in new physics beyond the SM. In this work, we present a\nnew method for testing the Higgs boson properties in a framework that\nincorporates the Higgs boson and the SM. Our approach is based on the\npredictions of the SM and the Higgs boson. It is based on a modified\nphenomenological approach that incorporates the Hig",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168,
          "p": 0.29577464788732394,
          "f": 0.21428570966524377
        },
        "rouge-2": {
          "r": 0.07103825136612021,
          "p": 0.12037037037037036,
          "f": 0.08934707436992974
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.28169014084507044,
          "f": 0.20408162803259072
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.08726v1",
      "true_abstract": "Traditional single-modality sensing faces limitations in accuracy and\ncapability, and its decoupled implementation with communication systems\nincreases latency in bandwidth-constrained environments. Additionally,\nsingle-task-oriented sensing systems fail to address users' diverse demands. To\novercome these challenges, we propose a semantic-driven integrated multimodal\nsensing and communication (SIMAC) framework. This framework leverages a joint\nsource-channel coding architecture to achieve simultaneous sensing decoding and\ntransmission of sensing results. Specifically, SIMAC first introduces a\nmultimodal semantic fusion (MSF) network, which employs two extractors to\nextract semantic information from radar signals and images, respectively. MSF\nthen applies cross-attention mechanisms to fuse these unimodal features and\ngenerate multimodal semantic representations. Secondly, we present a large\nlanguage model (LLM)-based semantic encoder (LSE), where relevant communication\nparameters and multimodal semantics are mapped into a unified latent space and\ninput to the LLM, enabling channel-adaptive semantic encoding. Thirdly, a\ntask-oriented sensing semantic decoder (SSD) is proposed, in which different\ndecoded heads are designed according to the specific needs of tasks.\nSimultaneously, a multi-task learning strategy is introduced to train the SIMAC\nframework, achieving diverse sensing services. Finally, experimental\nsimulations demonstrate that the proposed framework achieves diverse sensing\nservices and higher accuracy.",
      "generated_abstract": "Deep learning-based speech enhancement (SE) systems have shown significant\nprogress, but their performance remains limited by the large number of\nparameters and computational overhead. To address these challenges, this\nstudy introduces a novel hybrid model that combines a convolutional neural\nnetwork (CNN) with an autoencoder (AE) to enhance speech quality while\nreducing the model size and computational requirements. The proposed system\nutilizes a multi-layer AE to capture the temporal features of the speech,\nwhile a CNN is employed to enhance the speech energy and eliminate\nhigh-frequency noise. The efficiency of the proposed model is validated via\nexperiments, where the computational complexity of the hybrid model is\ncompared with state-of-the-art SE systems. Results show that the hybrid model\nsignificantly improves speech quality and reduces computational requirements\ncompared to the state-of-the-art.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.24096385542168675,
          "f": 0.18348623381659804
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.05084745762711865,
          "f": 0.039473679460700024
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.24096385542168675,
          "f": 0.18348623381659804
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.08929v1",
      "true_abstract": "Accurate and efficient 3D mapping of large-scale outdoor environments from\nLiDAR measurements is a fundamental challenge in robotics, particularly towards\nensuring smooth and artifact-free surface reconstructions. Although the\nstate-of-the-art methods focus on memory-efficient neural representations for\nhigh-fidelity surface generation, they often fail to produce artifact-free\nmanifolds, with artifacts arising due to noisy and sparse inputs. To address\nthis issue, we frame surface mapping as a physics-informed energy optimization\nproblem, enforcing surface smoothness by optimizing an energy functional that\npenalizes sharp surface ridges. Specifically, we propose a deep learning based\napproach that learns the signed distance field (SDF) of the surface manifold\nfrom raw LiDAR point clouds using a physics-informed loss function that\noptimizes the $L_2$-Hessian energy of the surface. Our learning framework\nincludes a hierarchical octree based input feature encoding and a multi-scale\nneural network to iteratively refine the signed distance field at different\nscales of resolution. Lastly, we introduce a test-time refinement strategy to\ncorrect topological inconsistencies and edge distortions that can arise in the\ngenerated mesh. We propose a \\texttt{CUDA}-accelerated least-squares\noptimization that locally adjusts vertex positions to enforce\nfeature-preserving smoothing. We evaluate our approach on large-scale outdoor\ndatasets and demonstrate that our approach outperforms current state-of-the-art\nmethods in terms of improved accuracy and smoothness. Our code is available at\n\\href{https://github.com/HrishikeshVish/HessianForge/}{https://github.com/HrishikeshVish/HessianForge/}",
      "generated_abstract": "In this paper, we propose a novel framework that allows users to interact\nwith an artificially intelligent (AI) agent in a way that is equivalent to\nhuman-to-human interaction, while still maintaining the ability to make\ninformed decisions. Our approach consists of a user interface that allows the\nuser to interact with an AI agent, and a policy that guides the agent's\ndecision-making process. Our framework is based on a hierarchical\nmulti-agent architecture, where the user interface is composed of two\ncomponents: a visual interface and a behavior-oriented interface. The visual\ninterface allows the user to interact with the agent through a\nmulti-modal-based user interface. The behavior-oriented interface is responsible\nfor guiding the agent's decision-making process. We evaluated our framework\nusing a two-agent task, where a user is tasked with selecting an AI agent to\nserve as",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16551724137931034,
          "p": 0.32432432432432434,
          "f": 0.21917807771731207
        },
        "rouge-2": {
          "r": 0.024509803921568627,
          "p": 0.04424778761061947,
          "f": 0.03154573673695695
        },
        "rouge-l": {
          "r": 0.15172413793103448,
          "p": 0.2972972972972973,
          "f": 0.20091323753466367
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08062v1",
      "true_abstract": "Orthogonal frequency division multiplexing (OFDM), which has been the\ndominating waveform for contemporary wireless communications, is also regarded\nas a competitive candidate for future integrated sensing and communication\n(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum\nsensing range should be limited by the cyclic prefix (CP) length since\ninter-symbol interference (ISI) and inter-carrier interference (ICI) should be\navoided. However, in this paper, we provide rigorous analysis to reveal that\nthe random data embedded in OFDM-ISAC signal can actually act as a free ``mask\"\nfor ISI, which makes ISI/ICI random and hence greatly attenuated after radar\nsignal processing. The derived signal-to-interference-plus-noise ratio (SINR)\nin the range profile demonstrates that the maximum sensing range of OFDM-ISAC\ncan greatly exceed the ISI-free distance that is limited by the CP length,\nwhich is validated by simulation results. To further mitigate power degradation\nfor long-range targets, a novel sliding window sensing method is proposed,\nwhich iteratively detects and cancels short-range targets before shifting the\ndetection window. The shifted detection window can effectively compensate the\npower degradation due to insufficient CP length for long-range targets. Such\nresults provide valuable guidance for the CP length design in OFDM-ISAC\nsystems.",
      "generated_abstract": "This paper presents a novel approach for nonlinear estimation and control\nof an uncertain nonlinear system, with a goal of developing a controller that\nis robust to disturbances. A key feature of the proposed method is the\nintroduction of a multi-step-ahead predictive model to enhance the estimation\nand control performance. The approach is based on the use of the Kalman filter\nwith a novel multi-step-ahead predictive model that captures the uncertainty\nin the state-space model. This is achieved by introducing a new prediction\nerror term that is based on the current state of the system. A controller is\ndeveloped to minimize the mean squared error (MSE) between the actual state\nand the predicted state at each time step. This is achieved by combining the\nKalman filter with a nonlinear extension of the Linear Quadratic Gaussian (LQG)\ncontrol method. The nonlinear extension of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.1891891891891892,
          "f": 0.13999999533800017
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.03361344537815126,
          "f": 0.02684563278658705
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.1891891891891892,
          "f": 0.13999999533800017
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2501.13135v1",
      "true_abstract": "The complexity of human biology and its intricate systems holds immense\npotential for advancing human health, disease treatment, and scientific\ndiscovery. However, traditional manual methods for studying biological\ninteractions are often constrained by the sheer volume and complexity of\nbiological data. Artificial Intelligence (AI), with its proven ability to\nanalyze vast datasets, offers a transformative approach to addressing these\nchallenges. This paper explores the intersection of AI and microscopy in life\nsciences, emphasizing their potential applications and associated challenges.\nWe provide a detailed review of how various biological systems can benefit from\nAI, highlighting the types of data and labeling requirements unique to this\ndomain. Particular attention is given to microscopy data, exploring the\nspecific AI techniques required to process and interpret this information. By\naddressing challenges such as data heterogeneity and annotation scarcity, we\noutline potential solutions and emerging trends in the field. Written primarily\nfrom an AI perspective, this paper aims to serve as a valuable resource for\nresearchers working at the intersection of AI, microscopy, and biology. It\nsummarizes current advancements, key insights, and open problems, fostering an\nunderstanding that encourages interdisciplinary collaborations. By offering a\ncomprehensive yet concise synthesis of the field, this paper aspires to\ncatalyze innovation, promote cross-disciplinary engagement, and accelerate the\nadoption of AI in life science research.",
      "generated_abstract": "The aim of this study was to investigate the distribution of the\ndistribution of the distribution of the distribution of the distribution of\nthe distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of the distribution of the distribution\nof the distribution of the distribution of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.034013605442176874,
          "p": 0.5,
          "f": 0.06369426632317743
        },
        "rouge-2": {
          "r": 0.004807692307692308,
          "p": 0.09090909090909091,
          "f": 0.009132419137215755
        },
        "rouge-l": {
          "r": 0.034013605442176874,
          "p": 0.5,
          "f": 0.06369426632317743
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18182v1",
      "true_abstract": "Blind source separation (BSS) refers to the process of recovering multiple\nsource signals from observations recorded by an array of sensors. Common\napproaches to BSS, including independent vector analysis (IVA), and independent\nlow-rank matrix analysis (ILRMA), typically rely on second-order models to\ncapture the statistical independence of source signals for separation. However,\nthese methods generally do not account for the implicit structural information\nacross frequency bands, which may lead to model mismatches between the assumed\nsource distributions and the distributions of the separated source signals\nestimated from the observed mixtures. To tackle these limitations, this paper\nshows that conventional approaches such as IVA and ILRMA can easily be\nleveraged by the Sinkhorn divergence, incorporating an optimal transport (OT)\nframework to adaptively correct source variance estimates. This allows for the\nrecovery of the source distribution while modeling the inter-band signal\ndependence and reallocating source power across bands. As a result, enhanced\nversions of these algorithms are developed, integrating a Sinkhorn iterative\nscheme into their standard implementations. Extensive simulations demonstrate\nthat the proposed methods consistently enhance BSS performance.",
      "generated_abstract": "This paper presents a novel neural network architecture for the audio\nsignature identification task, which is based on the convolutional neural\nnetwork architecture. The proposed network is trained to identify the\ndifferent types of audio signals, such as speech, music, and noise, by\nlearning to discriminate the different signals based on their spectral\nsignatures. The proposed architecture is designed to handle the complexities\nof audio data and the non-stationary nature of the signals. The proposed\narchitecture is capable of learning and generalizing the features of the audio\nsignature, thus, reducing the need for manual feature extraction and\nsegmentation. The proposed architecture is trained using the Kaldi toolkit to\nidentify the different audio signals. It is evaluated using the\nMUSDB18b dataset, and the results show that the proposed architecture\nsignificantly outperforms the state-of-the-art methods in terms of both\naccuracy and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14173228346456693,
          "p": 0.23684210526315788,
          "f": 0.17733989679341902
        },
        "rouge-2": {
          "r": 0.03508771929824561,
          "p": 0.05128205128205128,
          "f": 0.04166666184244847
        },
        "rouge-l": {
          "r": 0.14173228346456693,
          "p": 0.23684210526315788,
          "f": 0.17733989679341902
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10098v1",
      "true_abstract": "The linear response of CsI(Tl) crystals to $\\gamma$-rays plays a crucial role\nin their calibration, as any deviation from linearity can introduce systematic\nerrors not negligible in the measurement of $\\gamma$ energy spectra,\nparticularly at high energies. In this study, the responses of CsI(Tl) crystals\nto high-energy photons up to 20 MeV are investigated using quasi monochromatic\n$\\gamma$ beam provided by the Shanghai Laser Electron Gamma Source. The spectra\nare folded using a detector filter implemented by Geant4. Both quadratic and\nlinear fits to six energy points are used to assess the linearity of the\nCsI(Tl) detector. The results demonstrate that the difference between the\nlinear and non-linear fits is at the level of 4\\%. Applying these findings to\nthe $\\gamma$ hodoscope of the Compact Spectrometer for Heavy Ion Experiment\n(CSHINE), the potential systematic uncertainties caused by CsI(Tl)\nnon-linearity are evaluated. This work provides a comprehensive calibration\nmethodology for employing CsI(Tl) crystal to detect high energy $\\gamma$-rays.",
      "generated_abstract": "We present a novel experimental technique to measure the energy deposited\nin a silicon pixel detector as a function of the pixel position in a\ntime-of-flight geometry, where the energy depositions are associated with the\nposition of a track in the detector. Our technique is based on the assumption\nthat the energy depositions in the detector are proportional to the track\nvelocity, and we propose a method to measure the track velocity in the detector\nusing a set of tracks. This technique is used to measure the energy deposited\nin a silicon pixel detector in a time-of-flight geometry. Our technique\nsignificantly improves the precision of the energy measurements in the\ndetector. We demonstrate that the proposed technique is a viable tool for\nexperimental studies of the energy depositions in silicon pixel detectors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1619047619047619,
          "p": 0.2982456140350877,
          "f": 0.2098765386488341
        },
        "rouge-2": {
          "r": 0.046052631578947366,
          "p": 0.07291666666666667,
          "f": 0.05645160815816898
        },
        "rouge-l": {
          "r": 0.1523809523809524,
          "p": 0.2807017543859649,
          "f": 0.19753085963648845
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2411.10726v2",
      "true_abstract": "We study an optimal execution problem in the infinite horizon setup. Our\nfinancial market is given by the Black-Scholes model with a linear price\nimpact. The main novelty of the current note is that we study the constrained\ncase where the number of shares and the selling rate are non-negative\nprocesses. For this case we give a complete characterization of the value and\nthe optimal control via a solution of a non-linear ordinary differential\nequation (ODE). Furthermore, we provide an example where the non-linear ODE can\nbe solved explicitly. Our approach is purely probabilistic.",
      "generated_abstract": "In this paper, we study the dynamic hedging problem for a portfolio with\ndynamic trading costs. We propose a novel method to solve the dynamic hedging\nproblem by combining the optimal control theory and the dynamic programming\nprinciple. The optimal trading strategies of the portfolio are obtained by\nsolving the optimal control problem with the dynamic trading costs. The\ndynamic trading costs are represented by the hedging strategies of the portfolio.\nWe derive the optimal trading strategies and hedging strategies for the portfolio\nwith dynamic trading costs. Simulation results demonstrate that the proposed\nmethod outperforms the classical method in terms of the number of hedging\nstrategies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2647058823529412,
          "p": 0.3829787234042553,
          "f": 0.3130434734275993
        },
        "rouge-2": {
          "r": 0.1,
          "p": 0.1125,
          "f": 0.10588234795847774
        },
        "rouge-l": {
          "r": 0.23529411764705882,
          "p": 0.3404255319148936,
          "f": 0.27826086473194717
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.16298v1",
      "true_abstract": "Speech foundation models have demonstrated exceptional capabilities in\nspeech-related tasks. Nevertheless, these models often struggle with non-verbal\naudio data, such as vocalizations, baby crying, etc., which are critical for\nvarious real-world applications. Audio foundation models well handle non-speech\ndata but also fail to capture the nuanced features of non-verbal human sounds.\nIn this work, we aim to overcome the above shortcoming and propose a novel\nfoundation model, termed voc2vec, specifically designed for non-verbal human\ndata leveraging exclusively open-source non-verbal audio datasets. We employ a\ncollection of 10 datasets covering around 125 hours of non-verbal audio.\nExperimental results prove that voc2vec is effective in non-verbal vocalization\nclassification, and it outperforms conventional speech and audio foundation\nmodels. Moreover, voc2vec consistently outperforms strong baselines, namely\nOpenSmile and emotion2vec, on six different benchmark datasets. To the best of\nthe authors' knowledge, voc2vec is the first universal representation model for\nvocalization tasks.",
      "generated_abstract": "The development of low-cost, high-performance, and energy-efficient wireless\ncommunications systems is a critical challenge in the global shift toward\nInternet of Things (IoT) technologies. In this paper, we present a\nframework-based approach to designing energy-efficient wireless networks with\nfading channels. This framework is a novel concept that integrates power\nallocation and channel estimation in a unified manner, making it possible to\nobtain a tight trade-off between power consumption and performance, while\nmaintaining robustness to channel state information (CSI) uncertainty. To\nfacilitate the design of energy-efficient wireless networks, we propose a\nframework that includes three major components: (1) the power allocation\ncomponent, which uses the channel estimation results to balance the system\ncapacity and energy efficiency, (2) the channel estimation component, which\nidentifies the fading parameters, such as the path loss and shadow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1926605504587156,
          "p": 0.23076923076923078,
          "f": 0.20999999504050013
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.032520325203252036,
          "f": 0.03041824597146202
        },
        "rouge-l": {
          "r": 0.1743119266055046,
          "p": 0.2087912087912088,
          "f": 0.18999999504050016
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10116v1",
      "true_abstract": "The knowledge transfer from 3D printing technology paved the way for\nunlocking the innovative potential of 3D Food Printing (3DFP) technology.\nHowever, this technology-oriented approach neglects userderived issues that\ncould be addressed with advancements in 3DFP technology. To explore potential\nnew features and application areas for 3DFP technology, we created the Mobile\nFood Printer (MFP) prototype. We collected insights from novice chefs for MFP\nin the restaurant context through four online focus group sessions (N=12). Our\nresults revealed how MFP can be applied in the current kitchen routines\n(preparation, serving, and eating) and introduce novel dining experiences. We\ndiscuss our learnings under two themes: 1) dealing with the kitchen rush and 2)\nstreamlining workflows in the kitchen. The opportunities we present in this\nstudy act as a starting point for HCI and HFI researchers and encourage them to\nimplement mobility in 3DFP with a useroriented lens. We further provide a\nground for future research to uncover potentials for advancing 3DFP technology.",
      "generated_abstract": "The rapid development of large language models (LLMs) has led to\nthe emergence of large language models for healthcare (LLM4H), which\ndemonstrate promise for healthcare applications. However, the application of\nLLM4H in healthcare remains limited by the limited availability of large\nlanguage models for specific medical specialties. We introduce MUSH, a\nmulti-lingual large language model for healthcare. MUSH is designed to\nsupport medical specialties with limited language models, including neurology,\npsychiatry, and radiology. By integrating large language models and\nmultilingual language models, MUSH improves the comprehensiveness,\nrelevance, and accuracy of medical knowledge. Through experiments on\nmulti-lingual and multi-specialty datasets, we demonstrate that MUSH significantly\nimproves the performance of medical knowledge retrieval tasks. These results\ndem",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14655172413793102,
          "p": 0.24285714285714285,
          "f": 0.182795694230547
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.22857142857142856,
          "f": 0.17204300605850398
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2503.04876v1",
      "true_abstract": "Given two populations from which independent binary observations are taken\nwith parameters $p_1$ and $p_2$ respectively, estimators are proposed for the\nrelative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their\nlogarithms. The estimators guarantee that the relative mean-square error, or\nthe mean-square error for the logarithmic versions, is less than a target value\nfor any $p_1, p_2 \\in (0,1)$, and the ratio of average sample sizes from the\ntwo populations is close to a prescribed value. The estimators can also be used\nwith group sampling, whereby samples are taken in batches of fixed size from\nthe two populations. The efficiency of the estimators with respect to the\nCram\\'er-Rao bound is good, and in particular it is close to $1$ for small\nvalues of the target error.",
      "generated_abstract": "In the context of a finite mixture model, the number of components $K$\nis usually fixed. The aim of this paper is to investigate the asymptotic\ndistribution of the number of components $K$ under various conditions. We\nprovide a new perspective on the finite mixture model based on the use of\nmultivariate random fields. We first develop a new method for constructing the\nmultivariate random fields, which can be interpreted as a random graph model.\nThen we study the asymptotic distribution of $K$ under various conditions. We\nshow that when the number of samples is large enough, the asymptotic\ndistribution of $K$ is the same as the one of the number of vertices in the\nmultivariate random field. We also discuss the application of the finite mixture\nmodel to the analysis of brain imaging data and discuss the effect of\nincorporating covariate information in the finite mixture model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2077922077922078,
          "p": 0.21621621621621623,
          "f": 0.21192052480329823
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.018018018018018018,
          "f": 0.017777772778668075
        },
        "rouge-l": {
          "r": 0.16883116883116883,
          "p": 0.17567567567567569,
          "f": 0.17218542546554988
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.15376v1",
      "true_abstract": "Land use is a critical factor in the siting of renewable energy facilities\nand is often scrutinized due to perceived conflicts with other land demands.\nMeanwhile, substantial areas are devoted to activities such as golf, which are\naccessible to only a select few and have a significant land and environmental\nfootprint. Our study shows that in countries such as the United States and the\nUnited Kingdom, far more land is allocated to golf courses than to renewable\nenergy facilities. Areas equivalent to those currently used for golf could\nsupport the installation of up to 842 GW of solar and 659 GW of wind capacity\nin the top ten countries with the most golf courses. In many of these\ncountries, this potential exceeds both current installed capacity and\nmedium-term projections. These findings underscore the untapped potential of\nrethinking land use priorities to accelerate the transition to renewable\nenergy.",
      "generated_abstract": "This paper studies the impact of global economic integration on the\norganization and functioning of public sector labor markets. We propose a\nmodel in which a country's government directly employs its citizens by\norganizing its labor market. The model incorporates a trade deficit and\nintroduces a \"growth shock\" in the external demand for labor. We show that\nthe government's employment structure has important implications for its\neconomic performance. The model's predictions are consistent with the empirical\nevidence from the United States during the Great Recession. Our findings\nhighlight the importance of government employment for the economic health of\ncountries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.21428571428571427,
          "f": 0.18181817693296617
        },
        "rouge-2": {
          "r": 0.029197080291970802,
          "p": 0.04081632653061224,
          "f": 0.034042548329199424
        },
        "rouge-l": {
          "r": 0.1368421052631579,
          "p": 0.18571428571428572,
          "f": 0.15757575269054194
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08361v1",
      "true_abstract": "This technical report analyzes non-contrast CT image segmentation in computer\nvision. It revisits a proposed method, examines the background of non-contrast\nCT imaging, and highlights the significance of segmentation. The study reviews\nrepresentative methods, including convolutional-based and CNN-Transformer\nhybrid approaches, discussing their contributions, advantages, and limitations.\nThe nnUNet stands out as the state-of-the-art method across various\nsegmentation tasks. The report explores the relationship between the proposed\nmethod and existing approaches, emphasizing the role of global context modeling\nin semantic labeling and mask generation. Future directions include addressing\nthe long-tail problem, utilizing pre-trained models for medical imaging, and\nexploring self-supervised or contrastive pre-training techniques. This report\noffers insights into non-contrast CT image segmentation and potential\nadvancements in the field.",
      "generated_abstract": "Deep Learning (DL) has been a driving force in the advancement of medical\nimage analysis. However, the limited training data and the complex\nmulti-modality nature of medical images pose significant challenges for\nlearning-based image analysis. To address these limitations, we propose a novel\nmethod based on the Loss-Aware Network Architecture (LNA) that integrates\ncontrastive learning (CL) and self-supervised learning (SSL) techniques to\nenhance image classification performance. The LNA employs two auxiliary\nclassifiers, one for feature extraction and the other for classification, to\ncapture semantic and spatial information from multiple modalities. The\nextraction component extracts features from the input image, while the\nclassification component generates labels for the extracted features. These\nlabels are used as auxiliary input to the second auxiliary classifier,\nenhancing its ability to classify the input image. Our experiments",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.1595744680851064,
          "f": 0.16853932085847761
        },
        "rouge-2": {
          "r": 0.008928571428571428,
          "p": 0.007936507936507936,
          "f": 0.008403356361841805
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.14893617021276595,
          "f": 0.15730336580229787
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.07692v1",
      "true_abstract": "This paper narrowly replicates Chen and Kung's 2019 paper ($The$ $Quarterly$\n$Journal$ $of$ $Economics$ 134(1): 185-226). Inspecting the data reveals that\nnearly one-third of the transactions (388,903 out of 1,208,621) are perfect\nduplicates of other rows, excluding the transaction number. Replicating the\nanalysis on the data sans-duplicates yields a slightly smaller but still\nstatistically significant princeling effect, robust across the regression\nresults. Further analysis also reveals that coefficients interpreted as the\neffect of logarithm of area actually reflect the effect of scaled values of\narea; this paper also reinterprets and contextualizes these results in light of\nthe true scaled values.",
      "generated_abstract": "This study explores the impact of the COVID-19 pandemic on the financial\nstability of large corporations. We use a dataset containing 27,479 companies\nfrom the S&P 500 Index from 1987 to 2019 to analyze the correlation between the\nCOVID-19 outbreak and the financial performance of companies. The results\nindicate that the financial performance of companies was negatively affected by\nthe COVID-19 pandemic. The study also finds that the impact of the pandemic on\nthe financial performance of companies depends on the type of industry,\nsegment, and country. Furthermore, the study identifies the key factors that\ninfluenced the financial performance of companies during the COVID-19 pandemic\nand provides recommendations for future research on this topic. This study\ncontributes to the existing literature on the impact of the COVID-19 pandemic on\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1506849315068493,
          "p": 0.171875,
          "f": 0.1605839366274177
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.021052631578947368,
          "f": 0.02127659074524789
        },
        "rouge-l": {
          "r": 0.1095890410958904,
          "p": 0.125,
          "f": 0.11678831618946156
        }
      }
    },
    {
      "paper_id": "gr-qc.physics/ins-det/2503.10332v1",
      "true_abstract": "The double-pass interferometer scheme was proposed in Ref.\\,[Light Sci. Appl.\n{\\bf 7}, 11 (2018)] as the method of implementation of the quantum speed meter\nconcept in future laser gravitational-wave (GW) detectors. Later it was shown\nin Ref.\\,[Phys. Rev. D {\\bf 110}, 062006 (2024)] that it allows to implement\nthe new type of the optical spring that does not require detuning of the\ninterferometer. Here we show that both these regimes can coexist, combining the\nspeed meter type broadband sensitivity gain with the additional lows-frequency\nminimum in the quantum noise originated from the optical spring. We show that\nthe location of this minimum can be varied without affecting the core optics of\nthe interferometer, allowing to tune the quantum noise shape in real time to\nfollow the ``chirp'' GW signals.",
      "generated_abstract": "In this work, we investigate the experimental prospects for the search for\nthe Higgs boson in the muon g-2 measurement. We first present the current\nexperimental status of the muon g-2 experiment and then calculate the\ncontribution of the Higgs boson to the muon g-2 using the 1-loop effective\nHiggs potential. We also discuss the experimental conditions under which the\nHiggs contribution can be detected. In addition, we calculate the sensitivity\nof the muon g-2 experiment for the Higgs boson in the 2-loop effective\nHiggs potential and show that the experimental limit can be achieved in the\nnear future.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.2653061224489796,
          "f": 0.18978101730299973
        },
        "rouge-2": {
          "r": 0.04201680672268908,
          "p": 0.0684931506849315,
          "f": 0.05208332862033463
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.24489795918367346,
          "f": 0.17518247715701435
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.06611v1",
      "true_abstract": "Performance and reliability analyses of autonomous vehicles (AVs) can benefit\nfrom tools that ``amplify'' small datasets to synthesize larger volumes of\nplausible samples of the AV's behavior. We consider a specific instance of this\ndata synthesis problem that addresses minimizing the AV's exposure to adverse\nenvironmental conditions during travel to a fixed goal location. The\nenvironment is characterized by a threat field, which is a strictly positive\nscalar field with higher intensities corresponding to hazardous and unfavorable\nconditions for the AV. We address the problem of synthesizing datasets of\nminimum exposure paths that resemble a training dataset of such paths. The main\ncontribution of this paper is an inverse reinforcement learning (IRL) model to\nsolve this problem. We consider time-invariant (static) as well as time-varying\n(dynamic) threat fields. We find that the proposed IRL model provides excellent\nperformance in synthesizing paths from initial conditions not seen in the\ntraining dataset, when the threat field is the same as that used for training.\nFurthermore, we evaluate model performance on unseen threat fields and find low\nerror in that case as well. Finally, we demonstrate the model's ability to\nsynthesize distinct datasets when trained on different datasets with distinct\ncharacteristics.",
      "generated_abstract": "This paper investigates the problem of estimating the number of concurrent\nusers in an underground pedestrian tunnel using wireless sensor networks. The\ngoal is to predict the number of simultaneous users in order to decide whether\nto open a new entrance to the tunnel or to close the existing one. The\nunderground pedestrian tunnel is considered a nonlinear system, which means\nthat the expected value of the system does not exist. To address this issue, we\npropose a novel approach to estimate the expected value of the system and use\nit to predict the number of simultaneous users. To address the problem of\nestimating the number of simultaneous users, we propose a novel approach to\nestimate the expected value of the system and use it to predict the number of\nsimultaneous users. To address the problem of estimating the number of\nsimultaneous users, we propose a novel approach to estimate the expected value\nof the system and use",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.29508196721311475,
          "f": 0.1988950231555814
        },
        "rouge-2": {
          "r": 0.02617801047120419,
          "p": 0.058823529411764705,
          "f": 0.036231879795474146
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.2459016393442623,
          "f": 0.1657458518848632
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09892v1",
      "true_abstract": "As inverter-based resources (IBRs) penetrate power systems, the dynamics\nbecome more complex, exhibiting multiple timescales, including electromagnetic\ntransient (EMT) dynamics of power electronic controllers and electromechanical\ndynamics of synchronous generators. Consequently, the power system model\nbecomes highly stiff, posing a challenge for efficient simulation using\nexisting methods that focus on dynamics within a single timescale. This paper\nproposes a Heterogeneous Multiscale Method for highly efficient multi-timescale\nsimulation of a power system represented by its EMT model. The new method\nalternates between the microscopic EMT model of the system and an automatically\nreduced macroscopic model, varying the step size accordingly to achieve\nsignificant acceleration while maintaining accuracy in both fast and slow\ndynamics of interests. It also incorporates a semi-analytical solution method\nto enable a more adaptive variable-step mechanism. The new simulation method is\nillustrated using a two-area system and is then tested on a detailed EMT model\nof the IEEE 39-bus system.",
      "generated_abstract": "The increasing availability of large-scale multi-agent systems (MAS) has\nprompted researchers to investigate the challenges associated with MAS\nperformance in dynamic environments. This paper explores the impact of\nenvironmental variability on the performance of MAS, specifically, the\ninteraction of agents in a multi-lane highway. To address this, we propose a\ntwo-stage framework that uses the Simulated Annealed Importance Sampling\n(SAIS) algorithm to generate synthetic traffic flow data for each lane of a\nmulti-lane highway. The synthetic data is used to evaluate the performance of\na range of MAS, including a classic MAS, a hybrid MAS, and a multi-lane MAS.\nThis study also investigates how different agent types, such as vehicles,\ndriving assistants, and infrastructure, affect the performance of the MAS.\nThrough the analysis of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14705882352941177,
          "p": 0.18072289156626506,
          "f": 0.16216215721490154
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.02631578947368421,
          "f": 0.023437495059815492
        },
        "rouge-l": {
          "r": 0.13725490196078433,
          "p": 0.1686746987951807,
          "f": 0.15135134640409076
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.20467v1",
      "true_abstract": "A one-shot device is a unit that operates only once, after which it is either\ndestroyed or needs to be rebuilt. For this type of device, the operational\nstatus can only be assessed at a specific inspection time, determining whether\nfailure occurred before or after it. Consequently, lifetimes are subject to\nleft- or right-censoring. One-shot devices are usually highly reliables. To\nanalyze the reliability of such products, an accelerated life test (ALT) plan\nis typically employed by subjecting the devices to increased levels of stress\nfactors, thus allowing life characteristics observed under high-stress\nconditions to be extrapolated to normal operating conditions. By accelerating\nthe degradation process, ALT significantly reduces both the time required for\ntesting and the associated experimental costs.\n  Recently, robust inferential methods have gained considerable interest in\nstatistical analysis. Among them, weighted minimum density power divergence\nestimators (WMDPDEs) are widely recognized for their robust statistical\nproperties with small loss of efficiency. In this work, robust WMDPDE and\nassociated statistical tests are developed under a log-logistic lifetime\ndistribution with multiple stresses. Explicit expressions for the estimating\nequations and asymptotic distribution of the estimators are obtained. Further,\na Monte Carlo simulation study is presented to evaluate the performance of the\nWMDPDE in practical applications.",
      "generated_abstract": "This paper deals with the problem of computing the mean of the sum of\nnumerical random variables with a prescribed distribution. We consider the\ncase when the sum is a sum of independent random variables with a prescribed\ndistribution and we derive the asymptotic distribution of the mean of the sum\nwhen the distribution of the sum is known. We then extend this result to the\ncase where the sum is a sum of random variables with known distributions and\nthe distribution of the sum is unknown. We obtain a closed-form asymptotic\ndistribution of the mean of the sum of independent random variables with a\nprescribed distribution. In addition, we obtain asymptotic approximations of the\nmean and the variance of the sum of independent random variables with a\nprescribed distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0738255033557047,
          "p": 0.2682926829268293,
          "f": 0.1157894702997231
        },
        "rouge-2": {
          "r": 0.024875621890547265,
          "p": 0.07042253521126761,
          "f": 0.036764702024492196
        },
        "rouge-l": {
          "r": 0.0738255033557047,
          "p": 0.2682926829268293,
          "f": 0.1157894702997231
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.gr-qc/2503.10346v1",
      "true_abstract": "The Hubble tension has emerged as a critical crisis in cosmology, with the\ncause remaining unclear. Determining the Hubble constant ($H_0$) independently\nof cosmological models and distance ladders will help resolve this crisis. In\nthis letter, we for the first time use 47 gravitational-wave (GW) standard\nsirens from the third Gravitational-Wave Transient Catalog to calibrate\ndistances in the strong lensing system, RXJ1131-1231, and constrain $H_0$\nthrough the distance-sum rule, with minimal cosmological assumptions. We assume\nthat light propagation over long distances is described by the\nFriedmann-Lemaitre-Robertson-Walker metric and that geometrical optics holds,\nbut we do not need to assume the universe's contents or the theory of gravity\non cosmological scales. Fixing $\\Omega_K=0$, we obtain\n$H_0=73.22^{+5.95}_{-5.43}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ and\n$H_0=70.40^{+8.03}_{-5.60}$ ${\\rm km}~{\\rm s}^{-1}~{\\rm Mpc}^{-1}$ by using the\ndeflector galaxy's mass model and kinematic measurements to break mass-sheet\ntransform, respectively. When $\\Omega_K$ is not fixed, the central value of\n$H_0$ increases further. We find that our results are still dominated by\nstatistical errors, and at the same time, we notice the great potential of\nusing GW dark sirens to provide calibration, owing to their higher redshifts.\nWhen using 42 binary black holes and RXJ1131-1231, we obtain a $8.46 \\%$ $H_0$\nconstraint precision, which is better than that from the bright siren GW170817\nusing the Hubble law by about $40\\%$. In the future, as the redshift range of\nGW dark sirens increases, more and more SGLTDs can be included, and we can\nachieve high-precision, model-independent measurements of $H_0$ without the\nneed for GW bright sirens.",
      "generated_abstract": "We present a novel approach to computing the gravitational waveform and\ngravitational-wave polarization response of an advanced black-hole binary\nsystem that undergoes gravitational-wave (GW) inspiral, merger, and\npost-merger phases. The approach is based on the use of a fully\nhigh-dimensional multi-moment waveform model and is motivated by recent\nobservations of the first GW event of the gravitational-wave burst GW170817\nand its electromagnetic counterpart, an optical transient known as GRB 170817A.\nThis approach is applicable to systems of general mass ratios, including\nbinary neutron stars and black holes. The waveform model and the gravitational\nwaveform polarization response are constructed by integrating the Einstein\nequation using the numerical relativity code, REGeSs. The gravitational-wave",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14450867052023122,
          "p": 0.3333333333333333,
          "f": 0.20161289900656876
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.04854368932038835,
          "f": 0.028328607765089806
        },
        "rouge-l": {
          "r": 0.10982658959537572,
          "p": 0.25333333333333335,
          "f": 0.15322580223237525
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.05951v1",
      "true_abstract": "Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.",
      "generated_abstract": "We introduce a novel, end-to-end deep learning model, DeepFactor, which\npredicts the volatility of asset prices in real time. Our model combines a\nvariational autoencoder (VAE) with a large language model (LLM) to capture\nhigh-level contextual information. This approach integrates market data,\nlanguage models, and deep learning techniques to generate more accurate\npredictions than traditional deep learning models. Our results demonstrate that\nDeepFactor outperforms existing volatility forecasting methods, such as\nautoencoders, LLMs, and neural networks. By combining the power of deep\nlearning with the advantages of LLMs, DeepFactor offers a novel approach for\npredicting volatility, offering insights into market dynamics and providing\nactionable insights for investors and traders.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.109375,
          "p": 0.175,
          "f": 0.13461537988165698
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09375,
          "p": 0.15,
          "f": 0.11538461065088776
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.11113v1",
      "true_abstract": "We consider an economic environment with one buyer and one seller. For a\nbundle $(t,q)\\in [0,\\infty[\\times [0,1]=\\mathbb{Z}$, $q$ refers to the winning\nprobability of an object, and $t$ denotes the payment that the buyer makes. We\nconsider continuous and monotone preferences on $\\mathbb{Z}$ as the primitives\nof the buyer. These preferences can incorporate both quasilinear and\nnon-quasilinear preferences, and multidimensional pay-off relevant parameters.\nWe define rich single-crossing subsets of this class and characterize\nstrategy-proof mechanisms by using monotonicity of the mechanisms and\ncontinuity of the indirect preference correspondences. We also provide a\ncomputationally tractable optimization program to compute the optimal mechanism\nfor mechanisms with finite range. We do not use revenue equivalence and virtual\nvaluations as tools in our proofs. Our proof techniques bring out the geometric\ninteraction between the single-crossing property and the positions of bundles\n$(t,q)$s in the space $\\mathbb{Z}$. We also provide an extension of our\nanalysis to an $n-$buyer environment, and to the situation where $q$ is a\nqualitative variable.",
      "generated_abstract": "We introduce a novel approach to the problem of estimating a distribution\nunder a latent variable model, called Latent Distribution Estimation (LDE),\nwhich is inspired by the recently proposed Latent Variable Regression (LVR)\nframework. LDE extends the LVR by incorporating a structured prior for the\nlatent variable, which allows us to make more accurate inferences by\neffectively capturing the prior knowledge of the model. We also provide a\ncomprehensive theoretical analysis of LDE, demonstrating its superior\nperformance over standard methods in terms of estimation accuracy and\nconvergence rate. In addition, we propose a practical LDE algorithm that is\ncomputationally efficient and practical, and we provide a number of practical\nexperiments to illustrate the effectiveness of our method.\n  Our work contributes to the development of robust statistical models for\ncomplex systems, with applications in a variety of fields, including",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17592592592592593,
          "p": 0.19387755102040816,
          "f": 0.1844660144292583
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.044444444444444446,
          "f": 0.041237108428101354
        },
        "rouge-l": {
          "r": 0.1574074074074074,
          "p": 0.17346938775510204,
          "f": 0.16504853870110298
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.07868v1",
      "true_abstract": "This paper studies the ubiquitous problem of liquidating large quantities of\nhighly correlated stocks, a task frequently encountered by institutional\ninvestors and proprietary trading firms. Traditional methods in this setting\nsuffer from the curse of dimensionality, making them impractical for\nhigh-dimensional problems. In this work, we propose a novel method based on\nstochastic optimal control to optimally tackle this complex multidimensional\nproblem. The proposed method minimizes the overall execution shortfall of\nhighly correlated stocks using a reinforcement learning approach. We rigorously\nestablish the convergence of our optimal trading strategy and present an\nimplementation of our algorithm using intra-day market data.",
      "generated_abstract": "In this paper, we propose a novel reinforcement learning (RL) algorithm\nfor the pricing of options, leveraging the use of a deep learning\nreinforcement agent to improve the efficiency of option pricing. The agent\ntrains using the Heston model to price the options. The Heston model is a\npopular stochastic model that describes the behavior of financial markets.\nThe agent learns the parameters of the Heston model to improve the performance\nof option pricing. We evaluate the performance of the proposed algorithm using\nthree different datasets: a European call option with different strikes and\ntimes to maturity, a European put option with different strikes and times to\nmaturity, and a put option with a strike equal to 2000. The results show that\nthe proposed algorithm achieves a higher return on investment and a lower\nvolatility compared to other baseline methods. The agent also ach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26582278481012656,
          "p": 0.28378378378378377,
          "f": 0.27450979892690847
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.05128205128205128,
          "f": 0.05633802321761599
        },
        "rouge-l": {
          "r": 0.21518987341772153,
          "p": 0.22972972972972974,
          "f": 0.22222221722756216
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09908v1",
      "true_abstract": "We present a work optimal algorithm for parallel fully batch-dynamic maximal\nmatching against an oblivious adversary. In particular it processes batches of\nupdates (either insertions or deletions of edges) in constant expected\namortized work per edge update, and in $O(\\log^3 m)$ depth per batch whp, where\n$m$ is the maximum number of edges in the graph over time. This greatly\nimproves on the recent result by Ghaffari and Trygub (2024) that requires\n$O(\\log^9 m)$ amortized work per update and $O(\\log^4 m )$ depth per batch,\nboth whp. The algorithm can also be used for hyperedge maximal matching. For\nhypergraphs with rank $r$ (maximum cardinality of any edge) the algorithm\nsupports batches of insertions and deletions with $O(r^3)$ expected amortized\nwork per edge update, and $O(\\log^3 m)$ depth per batch whp. This is a factor\nof $O(r)$ work off of the best sequential algorithm, Assadi and Solomon (2021),\nwhich uses $O(r^2)$ work per update. Ghaffari and Trygub's parallel\nbatch-dynamic algorithm on hypergraphs requires $O(r^8 \\log^9 m)$ amortized\nwork per edge update whp. We leverage ideas from the prior algorithms but\nintroduce substantial new ideas. Furthermore, our algorithm is relatively\nsimple, perhaps even simpler than the sequential hyperedge algorithm. We also\npresent the first work-efficient algorithm for maximal matching on hypergraphs.\nFor a hypergraph with total cardinality $m'$ (i.e., sum over the cardinality of\neach edge), the algorithm runs in $O(m')$ work in expectation and $O(\\log^2 m)$\ndepth whp. The algorithm also has some properties that allow us to use it as a\nsubroutine in the dynamic algorithm to select random edges in the graph to add\nto the matching.",
      "generated_abstract": "The problem of deciding whether a given number is a square number is a classic\nproblem in number theory and combinatorics. Given a positive integer $n$, the\nsquare number $n^2$ is the smallest number $k$ for which $k$ is a multiple of\n$n$. Squares have many interesting properties, including being the only\nnon-trivial divisor of the number $n!$, being the smallest positive integer\ndivisible by $n$, and being the smallest positive integer divisible by a\npositive integer $q$ that is not a multiple of $n$.\n  This paper presents a polynomial-time algorithm that decides whether a\npositive integer $n$ is a square number. The algorithm is based on a simple\ncombinatorial observation that is reminiscent of the work of Sierpi\\'nski.\n  We also present a polynomial-time algorithm for deciding whether a given",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12837837837837837,
          "p": 0.31666666666666665,
          "f": 0.18269230358727817
        },
        "rouge-2": {
          "r": 0.03829787234042553,
          "p": 0.09183673469387756,
          "f": 0.054054049900351134
        },
        "rouge-l": {
          "r": 0.12837837837837837,
          "p": 0.31666666666666665,
          "f": 0.18269230358727817
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/class-ph/2503.03342v1",
      "true_abstract": "For linear electromagnetic systems possessing time-reversal symmetry, we\npresent an approach to bound ratios of internal fields excited from different\nports, using only the scattering matrix (S matrix), improving upon previous\nrelated bounds by Sounas and Al\\`u (2017). By reciprocity, emitted-wave\namplitudes from internal dipole sources are bounded in a similar way. When\napplied to coupled-resonant systems, our method constrains ratios of resonant\ncoupling/decay coefficients. We also obtain a relation for the relative phase\nof fields excited from the two ports and the ratio of field intensities in a\ntwo-port system. In addition, although lossy systems do not have time-reversal\nsymmetry, we can still approximately bound loss-induced non-unitarity of the S\nmatrix using only the lossless S matrix. We show numerical validations of the\nnear-tightness of our bounds in various scattering systems.",
      "generated_abstract": "This study investigates the optical properties of the single-crystalline\nsurface of a silicon nitride (Si3N4) thin film grown on sapphire substrates.\nThe film was deposited on sapphire substrates using molecular beam epitaxy (MBE)\nand atomic layer deposition (ALD) techniques. The surface of the films was\nexamined using optical microscopy, energy dispersive spectroscopy (EDS), and\nelectron microscopy (EM). The optical properties of the single-crystalline\nsurface of the films were analyzed using reflectance spectroscopy (RS),\ntransmittance spectroscopy (TS), and Fourier transform infrared spectroscopy\n(FTIR). The results showed that the surface of the films exhibited a\nsignificantly larger refractive index than the bulk of the films, with the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.053763440860215055,
          "p": 0.07692307692307693,
          "f": 0.06329113439753281
        },
        "rouge-2": {
          "r": 0.008264462809917356,
          "p": 0.011363636363636364,
          "f": 0.009569373115086845
        },
        "rouge-l": {
          "r": 0.053763440860215055,
          "p": 0.07692307692307693,
          "f": 0.06329113439753281
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.18901v1",
      "true_abstract": "We introduce sliced optimal transport dataset distance (s-OTDD), a\nmodel-agnostic, embedding-agnostic approach for dataset comparison that\nrequires no training, is robust to variations in the number of classes, and can\nhandle disjoint label sets. The core innovation is Moment Transform Projection\n(MTP), which maps a label, represented as a distribution over features, to a\nreal number. Using MTP, we derive a data point projection that transforms\ndatasets into one-dimensional distributions. The s-OTDD is defined as the\nexpected Wasserstein distance between the projected distributions, with respect\nto random projection parameters. Leveraging the closed form solution of\none-dimensional optimal transport, s-OTDD achieves (near-)linear computational\ncomplexity in the number of data points and feature dimensions and is\nindependent of the number of classes. With its geometrically meaningful\nprojection, s-OTDD strongly correlates with the optimal transport dataset\ndistance while being more efficient than existing dataset discrepancy measures.\nMoreover, it correlates well with the performance gap in transfer learning and\nclassification accuracy in data augmentation.",
      "generated_abstract": "We study the problem of predicting the next state of an agent in a continuous\ntime Markov chain using a single forward pass of a neural network. We assume\nthat the agent's state at the beginning of the chain is not known, but we\nknow that it will evolve according to a Markov chain. We show that, under\nsuitable assumptions, the next state can be predicted by sampling from a\nsufficiently large set of forward passes of the network. This is a generalization\nof the prediction error bound for Markov chains by Vovk and Shkunov, who show\nthat if the number of forward passes is bounded by a polynomial in the number of\nstates, then the prediction error is bounded by a polynomial in the number of\nstates and the transition probability matrix. We also provide an example of a\nMarkov chain with a large enough set of forward passes to enable prediction,\nusing a simple example with a single state",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13513513513513514,
          "p": 0.18292682926829268,
          "f": 0.15544040962066114
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.03787878787878788,
          "f": 0.03546098792817333
        },
        "rouge-l": {
          "r": 0.12612612612612611,
          "p": 0.17073170731707318,
          "f": 0.145077715320143
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/str-el/2503.10085v1",
      "true_abstract": "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
      "generated_abstract": "In this work, we present a novel approach to the study of the spin dynamics of\na spin-$\\frac{1}{2}$ particle in a spin-$\\frac{1}{2}$ Heisenberg model on a\nlattice with an antiferromagnetic nearest-neighbor interaction. In particular,\nwe consider the case of a square lattice with an antiferromagnetic interaction\nbetween neighboring spins and with nearest-neighbor interactions between spins\non the same site. We introduce a method to calculate the spin-spin correlation\nfunction for an arbitrary spin-$\\frac{1}{2}$ particle, which can be used to\nstudy the spin dynamics of the particle in the ground state. This method is\napplicable to a wide range of spin-$\\frac{1}{2}$ particles, including the\n$J_1-J_2$ Heisenberg model and a family of spin-$1/2$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17757009345794392,
          "p": 0.30158730158730157,
          "f": 0.22352940709965408
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.06060606060606061,
          "f": 0.05020920016806475
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.2698412698412698,
          "f": 0.19999999533494822
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/IT/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "In this paper, we propose a novel approach to modeling and analyzing\nthe dynamic interaction of multiple agents in complex networks. The proposed\napproach is based on the concept of a network of networks, which is used to\nformulate the problem as a combinatorial optimization problem. We introduce a\nnovel approach to modeling the network of networks, which is based on the\nconcept of a graph-weighted multilayer network. We present a novel method for\nderiving the objective function of the problem, which takes into account both\nthe dynamics of the individual agents and the interactions between them. We\npresent a numerical example to illustrate the performance of the proposed\nmethod. Finally, we show that the proposed approach can be used to model and\nanalyze the dynamic interaction of multiple agents in complex networks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20161290322580644,
          "p": 0.373134328358209,
          "f": 0.26178010015734227
        },
        "rouge-2": {
          "r": 0.05027932960893855,
          "p": 0.08823529411764706,
          "f": 0.0640569348772182
        },
        "rouge-l": {
          "r": 0.20161290322580644,
          "p": 0.373134328358209,
          "f": 0.26178010015734227
        }
      }
    },
    {
      "paper_id": "math.ST.stat/CO/2502.17738v1",
      "true_abstract": "Motivated by learning dynamical structures from static snapshot data, this\npaper presents a distribution-on-scalar regression approach for estimating the\ndensity evolution of a stochastic process from its noisy temporal point clouds.\nWe propose an entropy-regularized nonparametric maximum likelihood estimator\n(E-NPMLE), which leverages the entropic optimal transport as a smoothing\nregularizer for the density flow. We show that the E-NPMLE has almost\ndimension-free statistical rates of convergence to the ground truth\ndistributions, which exhibit a striking phase transition phenomenon in terms of\nthe number of snapshots and per-snapshot sample size. To efficiently compute\nthe E-NPMLE, we design a novel particle-based and grid-free coordinate KL\ndivergence gradient descent (CKLGD) algorithm and prove its polynomial\niteration complexity. Moreover, we provide numerical evidence on synthetic data\nto support our theoretical findings. This work contributes to the theoretical\nunderstanding and practical computation of estimating density evolution from\nnoisy observations in arbitrary dimensions.",
      "generated_abstract": "This paper studies a linear regression model with a binary outcome variable and\nan explanatory variable that is either a continuous or a discrete variable. The\nmain contribution of the paper is to develop a robust methodology for estimating\nthe model parameters, particularly when the outcome variable is discrete. The\nestimator is based on a bootstrap procedure that uses a random sample from the\nfull dataset to estimate the parameters. The proposed methodology is\nextensively validated by simulations and a simulation study that involves a\nreal-world data set. It is shown that the proposed methodology is highly\nrobust to outliers and misspecification of the model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.2857142857142857,
          "f": 0.20454544994899285
        },
        "rouge-2": {
          "r": 0.027972027972027972,
          "p": 0.04040404040404041,
          "f": 0.03305784640495939
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.2698412698412698,
          "f": 0.19318181358535652
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/CO/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "We consider the problem of estimating the mean and variance of a Gaussian\ndistribution given an unknown mean and covariance. We propose a novel\nparametric distribution called the Gaussian Gauss-Markov Distribution (GGS),\nwhich is a mixture of two Gaussian distributions. We show that the GGS is a\nspecial case of the Gauss-Markov Distribution (GMD), and that the two are\nidentical if and only if the covariance is the identity matrix. We propose a\nsemi-parametric estimator for the GGS, and show that the estimator is\nconsistent and asymptotically normal. We also propose a non-parametric\nestimator for the GGS, and show that it is asymptotically normal. Finally, we\nestablish the consistency and asymptotic normality of our non-parametric\nestimator for the GGS. Theoretical guarantees are established",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0641025641025641,
          "p": 0.16666666666666666,
          "f": 0.09259258858024708
        },
        "rouge-2": {
          "r": 0.0049504950495049506,
          "p": 0.00980392156862745,
          "f": 0.006578942909455931
        },
        "rouge-l": {
          "r": 0.057692307692307696,
          "p": 0.15,
          "f": 0.08333332932098786
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.19906v1",
      "true_abstract": "Single-channel speech enhancement is a challenging ill-posed problem focused\non estimating clean speech from degraded signals. Existing studies have\ndemonstrated the competitive performance of combining convolutional neural\nnetworks (CNNs) with Transformers in speech enhancement tasks. However,\nexisting frameworks have not sufficiently addressed computational efficiency\nand have overlooked the natural multi-scale distribution of the spectrum.\nAdditionally, the potential of CNNs in speech enhancement has yet to be fully\nrealized. To address these issues, this study proposes a Deep Separable Dilated\nDense Block (DSDDB) and a Group Prime Kernel Feedforward Channel Attention\n(GPFCA) module. Specifically, the DSDDB introduces higher parameter and\ncomputational efficiency to the Encoder/Decoder of existing frameworks. The\nGPFCA module replaces the position of the Conformer, extracting deep temporal\nand frequency features of the spectrum with linear complexity. The GPFCA\nleverages the proposed Group Prime Kernel Feedforward Network (GPFN) to\nintegrate multi-granularity long-range, medium-range, and short-range receptive\nfields, while utilizing the properties of prime numbers to avoid periodic\noverlap effects. Experimental results demonstrate that PrimeK-Net, proposed in\nthis study, achieves state-of-the-art (SOTA) performance on the\nVoiceBank+Demand dataset, reaching a PESQ score of 3.61 with only 1.41M\nparameters.",
      "generated_abstract": "This paper presents a novel approach to enhance the quality of speech by\nusing an augmented reality (AR) system. The proposed method utilizes\nspeech-based language modeling (SBLM) and a speaker recognition (SR) system to\nrecognize the user's speech input, which is then used to generate an audio\nrepresentation using a pre-trained speaker-independent AR system. The generated\naudio is then used as input to the SBLM, which predicts the speaker's identity.\nThe AR system then uses the generated audio to enhance the speech of the\npredicted speaker. The proposed system was evaluated using an artificial\nspeech-speaker-recognition dataset and an in-field user study. The results\ndemonstrate that the proposed system can enhance the quality of speech by\nincreasing the likelihood of recognizing the correct speaker.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09701492537313433,
          "p": 0.18840579710144928,
          "f": 0.12807881324662104
        },
        "rouge-2": {
          "r": 0.03409090909090909,
          "p": 0.056074766355140186,
          "f": 0.04240282215235602
        },
        "rouge-l": {
          "r": 0.08955223880597014,
          "p": 0.17391304347826086,
          "f": 0.11822659649785258
        }
      }
    },
    {
      "paper_id": "quant-ph.math-ph/2503.10123v1",
      "true_abstract": "Based on the generalized Bloch representation, we study the separability and\ngenuine multipartite entanglement of arbitrary dimensional multipartite quantum\nstates. Some sufficient and some necessary criteria are presented. For certain\nstates, these criteria together are both sufficient and necessary. Detailed\nexamples show that our criteria are better than some existing ones in\nidentifying entanglement. Based on these criteria, the largest separable ball\naround the maximally mixed state for arbitrary multi-qubit systems is found,\nand it is proved that its radius is the constant 1. Furthermore, the criteria\nin this paper can be implemented experimentally.",
      "generated_abstract": "We study the dynamics of a system of $N$ coupled quantum oscillators\nwith a continuous-time master equation and a continuous-time Lindblad\nmaster equation. We derive the master equation for the reduced density\noperator of the system and identify the master equation for the reduced density\noperator of the system as a special case of the master equation for the\nreduced density operator of a system of $N$ coupled oscillators. We show that\nthe master equation for the reduced density operator of the system is\ncharacterized by a Hamiltonian and a dissipation operator that can be\nconstructed from the Hamiltonian and the dissipation operator of the system of\n$N$ coupled oscillators. We show that the master equation for the reduced\ndensity operator of the system is equivalent to the master equation for the\nreduced density operator of a system of $N$ coupled oscillators, which is\nequivalent to the master equation for the reduced",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16176470588235295,
          "p": 0.2682926829268293,
          "f": 0.2018348576921136
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.04411764705882353,
          "f": 0.03797467864124402
        },
        "rouge-l": {
          "r": 0.16176470588235295,
          "p": 0.2682926829268293,
          "f": 0.2018348576921136
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.08878v1",
      "true_abstract": "Ion measurements made with the Hot Plasma Composition Analyzers of the\nMagnetospheric Multiscale Mission (MMS-HPCAs) during the Mother's Day Storm\n(Gannon Storm) of 10-13 May 2024 yield the first observations of atomic and\nmolecular nitrogen ions in the Earth's dayside outer magnetosphere. A\npopulation of ions identified as doubly charged nitrogen and oxygen was also\nmeasured. These observations were made within a highly compressed magnetosphere\nat a geocentric distance of ~6 Earth Radii during the early recovery phase of\nthe storm. From the ion composition measurements and accompanying magnetic\nfield data, we determine the reconnection rate at the magnetopause; we compare\nthis result to a model reconnection rate that assumes the presence of only\natomic oxygen and hydrogen. The heavy ion-laden-mass density in the\nmagnetosphere was greater than the shocked solar wind mass density in the\nmagnetosheath. Despite these conditions, magnetic reconnection still occurred\nat the magnetopause.",
      "generated_abstract": "The proposed method uses the concept of the generalized Fourier transform to\nanalyze the optical properties of aqueous solutions of silica and graphite,\nusing the dielectric constant as a parameter. The use of the generalized\nFourier transform method is a more general method for analyzing the\ndielectric properties of aqueous solutions than the conventional Fourier\ntransform method. This method is more accurate than the conventional Fourier\ntransform method for analyzing the dielectric properties of aqueous solutions.\nThe results of the study show that the dielectric constant of silica is\n0.9666, and the dielectric constant of graphite is 1.0000. The dielectric\nconstants of silica and graphite are different, and the difference between the\ndielectric constants of silica and graphite is not negligible. The dielectric\nconstants of silica and graphite have",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0891089108910891,
          "p": 0.17307692307692307,
          "f": 0.1176470543363665
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.024096385542168676,
          "f": 0.018018013336175226
        },
        "rouge-l": {
          "r": 0.07920792079207921,
          "p": 0.15384615384615385,
          "f": 0.10457515891152994
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.02942v1",
      "true_abstract": "Semantic information refers to the meaning conveyed through words, phrases,\nand contextual relationships within a given linguistic structure. Humans can\nleverage semantic information, such as familiar linguistic patterns and\ncontextual cues, to reconstruct incomplete or masked speech signals in noisy\nenvironments. However, existing speech enhancement (SE) approaches often\noverlook the rich semantic information embedded in speech, which is crucial for\nimproving intelligibility, speaker consistency, and overall quality of enhanced\nspeech signals. To enrich the SE model with semantic information, we employ\nlanguage models as an efficient semantic learner and propose a comprehensive\nframework tailored for language model-based speech enhancement, called\n\\textit{GenSE}. Specifically, we approach SE as a conditional language modeling\ntask rather than a continuous signal regression problem defined in existing\nworks. This is achieved by tokenizing speech signals into semantic tokens using\na pre-trained self-supervised model and into acoustic tokens using a\ncustom-designed single-quantizer neural codec model. To improve the stability\nof language model predictions, we propose a hierarchical modeling method that\ndecouples the generation of clean semantic tokens and clean acoustic tokens\ninto two distinct stages. Moreover, we introduce a token chain prompting\nmechanism during the acoustic token generation stage to ensure timbre\nconsistency throughout the speech enhancement process. Experimental results on\nbenchmark datasets demonstrate that our proposed approach outperforms\nstate-of-the-art SE systems in terms of speech quality and generalization\ncapability.",
      "generated_abstract": "This paper presents a novel approach for learning time-varying and\nreliability-aware channel models for wireless networks. Traditional methods\nfocus on stationary channel models, which are insufficient for dynamic\nenvironments. In contrast, we propose a generative adversarial network (GAN)\nbased approach for learning time-varying channel models, which accounts for\nchanging environmental conditions. Specifically, we first design a channel\nmodel based on the GAN-in-Network (GIN) architecture to capture the complex\nchannel dynamics. Then, we leverage a deep generative model to learn the\nstationary channel model. The proposed framework is evaluated on both\nsimulated and real datasets. Simulation results demonstrate that the proposed\nmethod outperforms existing channel models in terms of channel estimation\naccuracy and reliability. Additionally, we present a detailed analysis of the\nperformance of the proposed model across different environments. This work",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.3411764705882353,
          "f": 0.25217390838374293
        },
        "rouge-2": {
          "r": 0.02830188679245283,
          "p": 0.05,
          "f": 0.03614457369719902
        },
        "rouge-l": {
          "r": 0.18620689655172415,
          "p": 0.3176470588235294,
          "f": 0.2347826040359169
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08826v1",
      "true_abstract": "Integrating BD-RIS into wireless communications systems has attracted\nsignificant interest due to its transformative potential in enhancing system\nperformance. This survey provides a comprehensive analysis of BD-RIS\ntechnology, examining its modeling, structural characteristics, and network\nintegration while highlighting its advantages over traditional diagonal RIS.\nSpecifically, we review various BD-RIS modeling approaches, including multiport\nnetwork theory, graph theory, and matrix theory, and emphasize their\napplication in diverse wireless scenarios. The survey also covers BD-RIS's\nstructural diversity, including different scattering matrix types, transmission\nmodes, intercell architectures, and circuit topologies, showing their\nflexibility in improving network performance. We delve into the potential\napplications of BD-RIS, such as enhancing wireless coverage, improving PLS,\nenabling multi-cell interference cancellation, improving precise sensing and\nlocalization, and optimizing channel manipulation. Further, we explore BD-RIS\narchitectural development, providing insights into new configurations focusing\non channel estimation, optimization, performance analysis, and circuit\ncomplexity perspectives. Additionally, we investigate the integration of BD-RIS\nwith emerging wireless technologies, such as millimeter-wave and terahertz\ncommunications, integrated sensing and communications, mobile edge computing,\nand other cutting-edge technologies. These integrations are pivotal in\nadvancing the capabilities and efficiency of future wireless networks. Finally,\nthe survey identifies key challenges, including channel state information\nestimation, interference modeling, and phase-shift designs, and outlines future\nresearch directions. The survey aims to provide valuable insights into BD-RIS's\npotential in shaping the future of wireless communications systems.",
      "generated_abstract": "This paper presents a new framework for managing the digital twin of a\ndevice in the field, leveraging the capabilities of a distributed cloud\nenvironment. The framework addresses the challenges of maintaining the digital\ntwin in the field, while maintaining the integrity of the original physical\ndevice. The framework consists of three components: a cloud-based system to\nmanage and store the digital twin, a local system to process data and execute\nmachine learning models, and a local system to implement the digital twin\nitself. The framework is designed to address the limitations of existing\nsolutions by providing a scalable, reliable, and secure solution for\nmanaging and implementing digital twins in the field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08843537414965986,
          "p": 0.20967741935483872,
          "f": 0.12440190970261683
        },
        "rouge-2": {
          "r": 0.004672897196261682,
          "p": 0.010638297872340425,
          "f": 0.006493502252490542
        },
        "rouge-l": {
          "r": 0.08843537414965986,
          "p": 0.20967741935483872,
          "f": 0.12440190970261683
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2502.10161v1",
      "true_abstract": "Reasoning about fairness through correlation-based notions is rife with\npitfalls. The 1973 University of California, Berkeley graduate school\nadmissions case from Bickel et. al. (1975) is a classic example of one such\npitfall, namely Simpson's paradox. The discrepancy in admission rates among\nmales and female applicants, in the aggregate data over all departments,\nvanishes when admission rates per department are examined. We reason about the\nBerkeley graduate school admissions case through a causal lens. In the process,\nwe introduce a statistical test for causal hypothesis testing based on Pearl's\ninstrumental-variable inequalities (Pearl 1995). We compare different causal\nnotions of fairness that are based on graphical, counterfactual and\ninterventional queries on the causal model, and develop statistical tests for\nthese notions that use only observational data. We study the logical relations\nbetween notions, and show that while notions may not be equivalent, their\ncorresponding statistical tests coincide for the case at hand. We believe that\na thorough case-based causal analysis helps develop a more principled\nunderstanding of both causal hypothesis testing and fairness.",
      "generated_abstract": "The purpose of this study is to develop a simple and effective method for\ndetecting the presence of a hidden variable in the data. Our approach is based\non the use of a novel Bayesian nonparametric methodology, known as the\nLaplace approximation, which is computationally efficient and can be applied to\nlarge datasets. The proposed method is applied to the detection of a hidden\nvariable in the data obtained from the analysis of the temperature record from\nthe Eiffel Tower. The results of this study indicate that the presence of a\nhidden variable is evident in the data, providing evidence that the temperature\nrecord at the Eiffel Tower is not a random sample from a normal distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17543859649122806,
          "p": 0.3125,
          "f": 0.2247190965181165
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.0425531914893617,
          "f": 0.03124999535278389
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.296875,
          "f": 0.21348314146193673
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.17726v1",
      "true_abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
      "generated_abstract": "In this paper, we present a method for training large-scale speech-to-text\ntransformer models using an end-to-end approach. The proposed approach\nintegrates the LM-based pre-training with the fine-tuning of the transformer\nmodel. It is achieved by introducing a multi-head attention (MHA) module, which\nis designed to enhance the representation of the source text. The proposed\nmethod is validated using the large-scale TIMIT speech dataset. The results\ndemonstrate that the proposed method significantly outperforms the baseline\nmethod in terms of both speech quality and language modeling (LM) performance.\nFurthermore, the proposed method can be applied to various languages and\nspeech conditions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1032258064516129,
          "p": 0.2318840579710145,
          "f": 0.14285713859414872
        },
        "rouge-2": {
          "r": 0.004761904761904762,
          "p": 0.010752688172043012,
          "f": 0.006600655811524489
        },
        "rouge-l": {
          "r": 0.1032258064516129,
          "p": 0.2318840579710145,
          "f": 0.14285713859414872
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.00850v1",
      "true_abstract": "In this paper we develop the theory of the depth of a simple algebraic\nextension of valued fields $(L/K,v)$. This is defined as the minimal number of\naugmentations appearing in some Mac Lane-Vaqui\\'e chain for the valuation on\n$K[x]$ determined by the choice of some generator of the extension. In the\ndefectless and unibranched case, this concept leads to a generalization of a\nclassical result of Ore about the existence of $p$-regular generators for\nnumber fields. Also, we find what valuation-theoretic conditions characterize\nthe extensions having depth one.",
      "generated_abstract": "In this paper, we present a new construction of the infinite dimensional\ngeneralized Riemann--Hilbert problem for the $A_\\infty$-algebra of the\nassociated $A_\\infty$-algebra of the symmetric monoidal category\n$\\mathcal{C}=\\mathcal{C}(k[[\\hbar",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11475409836065574,
          "p": 0.3181818181818182,
          "f": 0.1686746948991146
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.125,
          "f": 0.05504586812557888
        },
        "rouge-l": {
          "r": 0.09836065573770492,
          "p": 0.2727272727272727,
          "f": 0.14457830935694596
        }
      }
    },
    {
      "paper_id": "quant-ph.quant-ph/2503.10400v1",
      "true_abstract": "We elucidate the requirements for quantum operations that achieve\nenvironment-assisted invariance (envariance), a symmetry of entanglement. While\nenvariance has traditionally been studied within the framework of local unitary\noperations, we extend the analysis to consider non-unitary local operations.\nFirst, we investigate the conditions imposed on operators acting on pure\nbipartite entanglement to attain envariance. We show that the local operations\nmust take a direct-sum form in their Kraus operator representations,\nestablishing decoherence-free subspaces. Furthermore, we prove that the unitary\noperation on the system's subspace uniquely determines the corresponding\nunitary operator on the environment's subspace. As an immediate consequence, we\ndemonstrate that environment-assisted shortcuts to adiabaticity cannot be\nachieved through non-unitary operations. In addition, we identify the\nrequirements that local operations must satisfy to ensure that the eternal\nblack hole states remain static in AdS/CFT.",
      "generated_abstract": "We study the quantum dynamics of a system consisting of $N$ identical\nspin-1/2 particles. The system is in a mixed state, but the time evolution of\nthe state can be described by a $N$-particle reduced density matrix.\n  We study the evolution of the reduced density matrix under the action of\nlocal unitaries, showing that the reduced density matrix evolves according to a\ntime-dependent Lindblad equation. We analyze the effect of a local unitary\noperation on the reduced density matrix and find a general expression for the\ncorresponding change in the reduced density matrix. We derive a closed\nexpression for the evolution of the reduced density matrix under a continuous\ntime-dependent local unitaries, and show that the resulting dynamics is\nequivalent to the evolution of a system of $N$ identical spin-1/2 particles in\na mixed state. We also analyze the effect of the local unitaries on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.25806451612903225,
          "f": 0.21333332848355566
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.08823529411764706,
          "f": 0.07894736347645462
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.25806451612903225,
          "f": 0.21333332848355566
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.q-bio/OT/2412.18030v1",
      "true_abstract": "The 2024 Nobel Prize in Physics was awarded to John Hopfield and Geoffrey\nHinton, \"for foundational discoveries and inventions that enable machine\nlearning with artificial neural networks.\" As noted by the Nobel committee,\ntheir work moved the boundaries of physics. This is a brief reflection on\nHopfield's work, its implications for the emergence of biological physics as a\npart of physics, the path from his early papers to the modern revolution in\nartificial intelligence, and prospects for the future.",
      "generated_abstract": "This paper presents a novel methodology for quantifying the evolution of\nthe evolution of the evolution of the evolution of the evolution of the\nevolution of the evolution of the evolution of the evolution of the evolution\nof the evolution of the evolution of the evolution of the evolution of the\nevolution of the evolution of the evolution of the evolution of the evolution\nof the evolution of the evolution of the evolution of the evolution of the\nevolution of the evolution of the evolution of the evolution of the evolution\nof the evolution of the evolution of the evolution of the evolution of the\nevolution of the evolution of the evolution of the evolution of the evolution\nof the evolution of the evolution of the evolution of the evolution of the\nevolution of the evolution of the evolution of the evolution of the evolution\nof the evolution of the evolution of the evolution of the evolution of the\nevolution of the evolution of the evolution of the evolution of the evolution\nof the evolution of the evolution of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.078125,
          "p": 0.45454545454545453,
          "f": 0.13333333083022222
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.078125,
          "p": 0.45454545454545453,
          "f": 0.13333333083022222
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.19898v1",
      "true_abstract": "This study utilizes a simulated dataset to establish Python code for Double\nMachine Learning (DML) using Anaconda's Jupyter Notebook and the DML software\npackage from GitHub. The research focuses on causal inference experiments for\nboth binary and continuous treatment variables. The findings reveal that the\nDML model demonstrates relatively stable performance in calculating the Average\nTreatment Effect (ATE) and its robustness metrics. However, the study also\nhighlights that the computation of Conditional Average Treatment Effect (CATE)\nremains a significant challenge for future DML modeling, particularly in the\ncontext of continuous treatment variables. This underscores the need for\nfurther research and development in this area to enhance the model's\napplicability and accuracy.",
      "generated_abstract": "This study examines the relationship between employment growth and\ntemperature, using daily temperature and employment data from the U.S. Bureau of\nLabor Statistics from 1976 to 2021. We find that employment growth is negatively\nassociated with temperature, especially in the winter. Additionally, we find\nthat the relationship between employment growth and temperature is\ntime-varying. The results are robust to various specification changes, such as\nseasonal adjustment, and temperature and employment data. These findings\nhighlight the importance of considering seasonal and non-seasonal dynamics\nwhen evaluating the impacts of economic conditions on employment growth. They\nalso underscore the need for further research on the relationship between\nemployment growth and temperature in the U.S. and other countries.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22784810126582278,
          "p": 0.2571428571428571,
          "f": 0.24161073327327606
        },
        "rouge-2": {
          "r": 0.0673076923076923,
          "p": 0.07446808510638298,
          "f": 0.07070706571982487
        },
        "rouge-l": {
          "r": 0.22784810126582278,
          "p": 0.2571428571428571,
          "f": 0.24161073327327606
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.15239v1",
      "true_abstract": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters from Wattpad, we demonstrate that our framework\ncomplements existing feature engineering techniques by amplifying their\nmarginal explanatory power on average by 31%. The results reveal that different\ntypes of engagement-continuing to read, commenting, and voting-are driven by\ndistinct combinations of current and anticipated content features. Our\nframework provides a novel way to study and explore how audience\nforward-looking beliefs shape their engagement with narrative media, with\nimplications for marketing strategy in content-focused industries.",
      "generated_abstract": "This paper proposes a method for the automatic evaluation of textual summaries.\nWe introduce a novel framework that combines two key components: (1) a\ndifferentiable metric for evaluating textual summaries and (2) a sequence-to-\ntextual-summary (seq2seq) model that generates summaries from textual data.\nThe framework is built on the assumption that summaries are well-formed by\nfollowing the sequence-to-summary model's generated summary. To evaluate the\nquality of summaries, we propose a metric that quantifies the similarity between\nthe generated summary and the reference summary. We also propose a seq2seq\nmodel that can generate summaries from textual data, and the model is\ntrained using the generated summaries as the ground-truth. We evaluate our\nframework on the Summarunner dataset, and our framework demonstrates that the\ngenerated summaries from the seq2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15602836879432624,
          "p": 0.30985915492957744,
          "f": 0.20754716535644369
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.035398230088495575,
          "f": 0.025889962998293683
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.29577464788732394,
          "f": 0.19811320309229272
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.06619v1",
      "true_abstract": "We study the problem of synthetic generation of samples of environmental\nfeatures for autonomous vehicle navigation. These features are described by a\nspatiotemporally varying scalar field that we refer to as a threat field. The\nthreat field is known to have some underlying dynamics subject to process\nnoise. Some \"real-world\" data of observations of various threat fields are also\navailable. The assumption is that the volume of ``real-world'' data is\nrelatively small. The objective is to synthesize samples that are statistically\nsimilar to the data. The proposed solution is a generative artificial\nintelligence model that we refer to as a split variational recurrent neural\nnetwork (S-VRNN). The S-VRNN merges the capabilities of a variational\nautoencoder, which is a widely used generative model, and a recurrent neural\nnetwork, which is used to learn temporal dependencies in data. The main\ninnovation in this work is that we split the latent space of the S-VRNN into\ntwo subspaces. The latent variables in one subspace are learned using the\n``real-world'' data, whereas those in the other subspace are learned using the\ndata as well as the known underlying system dynamics. Through numerical\nexperiments we demonstrate that the proposed S-VRNN can synthesize data that\nare statistically similar to the training data even in the case of very small\nvolume of ``real-world'' training data.",
      "generated_abstract": "Recent advances in Large Language Models (LLMs) have shown strong performance\nin various domains, including music generation. However, existing music\ngeneration models often lack explicit control over the audio, which limits\ntheir expressiveness and creativity. In this paper, we propose MUSIC, a\nmusic-generating LLM that leverages music-specific attention mechanisms to\ngenerate music. MUSIC adopts a three-stage framework, including a music\ngenerator, an audio generator, and an audio-text generator. The music generator\nmodels the relationship between musical notes and audio signals by leveraging\nmusic-specific attention mechanisms, while the audio generator generates audio\nsignals using the generated music signals. The audio-text generator generates\naudio-text pairs based on the generated audio signals and music-specific\nattention. We evaluated MUSIC on music generation tasks and found that MUSIC\noutperforms existing music",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14414414414414414,
          "p": 0.20512820512820512,
          "f": 0.1693121644646008
        },
        "rouge-2": {
          "r": 0.005263157894736842,
          "p": 0.008695652173913044,
          "f": 0.006557372351521774
        },
        "rouge-l": {
          "r": 0.14414414414414414,
          "p": 0.20512820512820512,
          "f": 0.1693121644646008
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.08615v1",
      "true_abstract": "Let $H$ be a multiplicative monoid and $\\mathcal{P}_{{\\rm fin},1}(H)$ be the\nmonoid obtained by endowing the family of all non-empty finite subsets of $H$\ncontaining the identity $1_H$ with the operation of setwise multiplication\ninduced by $H$. We study fundamental aspects of the arithmetic of these\nmonoids, taking into account the possible presence of nontrivial idempotents.\nWe consider for this purpose minimal factorizations into irreducibles, a\nconcept recently introduced in the abstract context of a new general theory of\nfactorization. Among other results, we provide necessary and sufficient\nconditions on $H$ for $\\mathcal{P}_{{\\rm fin},1}(H)$ to admit unique minimal\nfactorizations. Our results generalize and shed new light on recent\ndevelopments on the topic.",
      "generated_abstract": "In this article we study the effect of the number of edges on the spectral\nproperties of random directed graphs. Specifically, we consider the\ndeterministic random graph model and focus on the case of large graphs. We\nprove that for all positive integers $k$ and $N$, there exists a constant\n$C_{k,N}>0$ such that for every $N$-vertex graph $G$ with $N\\geq C_{k,N}$ edges,\nthe number of eigenvectors of the adjacency matrix associated with $G$ is\n$O(N^{k-1})$. Moreover, the constant $C_{k,N}$ does not depend on the graph\nstructure. We also consider the random graph model and study the spectral gap of\nthe adjacency matrix associated with the graph $G$ with high probability,\nprovided that $N\\geq C_{k,N}$ edges are available. We show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.19402985074626866,
          "f": 0.17931033985636163
        },
        "rouge-2": {
          "r": 0.027522935779816515,
          "p": 0.030612244897959183,
          "f": 0.028985502260497016
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.1791044776119403,
          "f": 0.16551723640808577
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.07828v1",
      "true_abstract": "To trade tokens in cryptoeconomic systems, automated market makers (AMMs)\ntypically rely on liquidity providers (LPs) that deposit tokens in exchange for\nrewards. To profit from such rewards, LPs must use effective liquidity\nprovisioning strategies. However, LPs lack guidance for developing such\nstrategies, which often leads them to financial losses. We developed a\nmeasurement model based on impermanent loss to analyze the influences of key\nparameters (i.e., liquidity pool type, position duration, position range size,\nand position size) of liquidity provisioning strategies on LPs' returns. To\nreveal the influences of those key parameters on LPs' profits, we used the\nmeasurement model to analyze 700 days of historical liquidity provision data of\nUniswap v3. By uncovering the influences of key parameters of liquidity\nprovisioning strategies on profitability, this work supports LPs in developing\nmore profitable strategies.",
      "generated_abstract": "We propose a novel method for modeling the daily price movements of\nStock Index Futures (SIFs), based on a deep learning approach. Our approach\nemploys a Convolutional Neural Network (CNN) to model the market data and\nintegrates the forecasting results with the historical market data. We\ninvestigate the effects of several key parameters in our model, such as the\nnumber of layers and the learning rate, and present a comprehensive analysis of\nthe model's performance. Our results demonstrate that the proposed model\nachieves superior performance in terms of both accuracy and robustness,\nespecially in the presence of outlier data and extreme price movements. The\nmodel's ability to forecast market trends in real-time is also demonstrated.\nThis study offers a novel approach for forecasting SIF prices and provides\nvaluable insights into the dynamics of financial markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2111111111111111,
          "p": 0.2159090909090909,
          "f": 0.21348314106804708
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.015384615384615385,
          "f": 0.016064252037871908
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.20454545454545456,
          "f": 0.20224718601186734
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.18024v1",
      "true_abstract": "Systems Biology Graphical Notation (SBGN) is a standardised notational system\nthat visualises biochemical processes as networks. These visualizations lack a\nformal framework, so that the analysis of such networks through modelling and\nsimulation is an entirely separate task, determined by a chosen modelling\nframework (e.g. differential equations, Petri nets, stochastic processes,\ngraphs). A second research gap is the lack of a mathematical framework to\ncompose network representations. The complexity of molecular and cellular\nprocesses forces experimental studies to focus on subsystems. To study the\nfunctioning of biological systems across levels of structural and functional\norganisation, we require tools to compose and organise networks with different\nlevels of detail and abstraction.\n  We address these challenges by introducing a category-theoretic formalism for\nbiochemical processes visualised using SBGN Process Description (SBGN-PD)\nlanguage. Using the theory of structured cospans, we construct a symmetric\nmonoidal double category and demonstrate its horizontal 1-morphisms as SBGN\nProcess Descriptions. We obtain organisational principles such as\n'compositionality' (building a large SBGN-PD from smaller ones) and\n'zooming-out' (abstracting away details in biochemical processes) defined in\ncategory-theoretic terms. We also formally investigate how a particular portion\nof a biochemical network influences the remaining portion of the network and\nvice versa. Throughout the paper, we illustrate our findings using standard\nSBGN-PD examples.",
      "generated_abstract": "A key feature of the neuronal soma is that it is surrounded by a network of\nneurites, which can be thought of as a network of molecular projections from the\nsoma. These neurites serve to transport the chemical signal of the soma to\nneighbouring cells. In this work, we introduce a general framework for\nconstructing network models of neurite projections that are based on a\nspatially localized representation of the neurite. The network model is\ndefined by a network of neurites, each representing a distinct neurite. The\nnetwork model is constructed by introducing a set of constraints on the\nconnectivity of the neurites. We then characterize the geometry of the network\nmodel by investigating the structure of the connectivity graph. We then\nderive the set of constraints that can be used to define the network model,\nand we show that the network model is equivalent to a graph. We then show",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14864864864864866,
          "p": 0.2972972972972973,
          "f": 0.1981981937537539
        },
        "rouge-2": {
          "r": 0.03431372549019608,
          "p": 0.05737704918032787,
          "f": 0.042944780592420254
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.2972972972972973,
          "f": 0.1981981937537539
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2501.13639v2",
      "true_abstract": "Droplet formation has emerged as an essential concept for the spatiotemporal\norganisation of biomolecules in cells. However, classical descriptions of\ndroplet dynamics based on passive liquid-liquid phase separation cannot capture\nthe complex situations inside cells. This review discusses three general\naspects that are crucial in cells: (i) biomolecules are diverse and\nindividually complex, implying that cellular droplets posses complex internal\nbehaviour, e.g., in terms of their material properties; (ii) the cellular\nenvironment contains many solid-like structures that droplets can wet; (iii)\ncells are alive and use fuel to drive processes out of equilibrium. We\nillustrate how these principles control droplet nucleation, growth, position,\nand count to unveil possible regulatory mechanisms in biological cells and\nother applications of phase separation.",
      "generated_abstract": "Quantum-based technologies are transforming biological research, but their\napplications have yet to meet the demands of the life sciences. Here, we\npropose a novel quantum algorithm for simulating the dynamics of the\nspatiotemporal evolution of a biomolecule. We demonstrate its feasibility by\nsimulating the motion of a 3D DNA molecule and demonstrate that it outperforms\nthe standard Monte Carlo method. The algorithm is based on the use of a\nquantum-classical hybrid algorithm that combines a quantum Monte Carlo method\nwith a classical simulation of the dynamics. The algorithm can be applied to\nany biomolecule with a discrete configuration space, such as proteins,\nmolecules, and cells, and can be applied to both continuous and discrete\nconfigurations. The algorithm also scales to larger systems, making it a\npotential tool for simulating biomolecular dynamics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20212765957446807,
          "p": 0.23170731707317074,
          "f": 0.21590908593233482
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.017391304347826087,
          "f": 0.01709401209547958
        },
        "rouge-l": {
          "r": 0.18085106382978725,
          "p": 0.2073170731707317,
          "f": 0.19318181320506211
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.06873v1",
      "true_abstract": "We analyze over 44,000 NBER and CEPR working papers from 1980 to 2023 using a\ncustom language model to construct knowledge graphs that map economic concepts\nand their relationships. We distinguish between general claims and those\ndocumented via causal inference methods (e.g., DiD, IV, RDD, RCTs). We document\na substantial rise in the share of causal claims-from roughly 4% in 1990 to\nnearly 28% in 2020-reflecting the growing influence of the \"credibility\nrevolution.\" We find that causal narrative complexity (e.g., the depth of\ncausal chains) strongly predicts both publication in top-5 journals and higher\ncitation counts, whereas non-causal complexity tends to be uncorrelated or\nnegatively associated with these outcomes. Novelty is also pivotal for top-5\npublication, but only when grounded in credible causal methods: introducing\ngenuinely new causal edges or paths markedly increases both the likelihood of\nacceptance at leading outlets and long-run citations, while non-causal novelty\nexhibits weak or even negative effects. Papers engaging with central, widely\nrecognized concepts tend to attract more citations, highlighting a divergence\nbetween factors driving publication success and long-term academic impact.\nFinally, bridging underexplored concept pairs is rewarded primarily when\ngrounded in causal methods, yet such gap filling exhibits no consistent link\nwith future citations. Overall, our findings suggest that methodological rigor\nand causal innovation are key drivers of academic recognition, but sustained\nimpact may require balancing novel contributions with conceptual integration\ninto established economic discourse.",
      "generated_abstract": "This paper explores the role of the supply chain in the COVID-19 pandemic.\nIts analysis focuses on the supply chain management strategies of three\nmanufacturing companies: one in the United States, one in China, and one in\nthe United Kingdom. Through a detailed analysis of their supply chains, this\npaper explores the impact of COVID-19 on the three companies.\n  The United States manufactures the vaccines, while China and the UK\nmanufacture the other essential items. The paper identifies the most\nimportant supply chain management strategies of these manufacturers and\nassesses their impact on the pandemic. The paper also examines the impact of\nthese strategies on the companies' profitability. Finally, the paper examines\nthe impact of these strategies on the supply chain efficiency.\n  The results of the analysis show that the supply chain management strategies\nof the three companies differ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06818181818181818,
          "p": 0.19672131147540983,
          "f": 0.10126581896206106
        },
        "rouge-2": {
          "r": 0.008658008658008658,
          "p": 0.02,
          "f": 0.012084587928187595
        },
        "rouge-l": {
          "r": 0.06818181818181818,
          "p": 0.19672131147540983,
          "f": 0.10126581896206106
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10126v1",
      "true_abstract": "For a regularized least squares estimation of discrete-valued signals, we\npropose an LiGME regularizer, as a nonconvex regularizer, of designated\nisolated minimizers. The proposed regularizer is designed as a Generalized\nMoreau Enhancement (GME) of the so-called SOAV convex regularizer. Every\ncandidate vector in the discrete-valued set is aimed to be assigned to an\nisolated local minimizer of the proposed regularizer while the overall\nconvexity of the regularized least squares model is maintained. Moreover, a\nglobal minimizer of the proposed model can be approximated iteratively by using\na variant of cLiGME algorithm. To enhance the accuracy of the proposed\nestimation, we also propose a pair of simple modifications, called respectively\nan iterative reweighting and a generalized superiorization. Numerical\nexperiments demonstrate the effectiveness of the proposed model and algorithms\nin a scenario of MIMO signal detection.",
      "generated_abstract": "In recent years, the development of wireless networks has significantly\nprolonged the lifetime of radio frequency (RF) components. This has led to\nincreased reliance on digital signal processing (DSP) components to improve\nsystem performance, reduce power consumption, and enhance system robustness.\nHowever, this reliance on DSP has also led to increased energy consumption and\npower consumption. This paper proposes a novel RF-DSP hybrid approach for\nenergy-efficient RF-DSP applications. The proposed approach consists of an RF\nsubsystem, an energy-efficient DSP subsystem, and an energy-efficient RF\nsubsystem. The RF subsystem consists of an analog RF front-end (AFE) and an\nanalog RF front-end (ARFE) for wireless communications. The energy-efficient\nDSP subsystem consists of an energy-efficient DSP (EEDS)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13414634146341464,
          "p": 0.171875,
          "f": 0.1506849265828487
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.010416666666666666,
          "f": 0.009302320638618092
        },
        "rouge-l": {
          "r": 0.12195121951219512,
          "p": 0.15625,
          "f": 0.13698629644586244
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.09828v1",
      "true_abstract": "Deep learning has significantly advanced medical imaging analysis, yet\nvariations in image resolution remain an overlooked challenge. Most methods\naddress this by resampling images, leading to either information loss or\ncomputational inefficiencies. While solutions exist for specific tasks, no\nunified approach has been proposed. We introduce a resolution-invariant\nautoencoder that adapts spatial resizing at each layer in the network via a\nlearned variable resizing process, replacing fixed spatial down/upsampling at\nthe traditional factor of 2. This ensures a consistent latent space resolution,\nregardless of input or output resolution. Our model enables various downstream\ntasks to be performed on an image latent whilst maintaining performance across\ndifferent resolutions, overcoming the shortfalls of traditional methods. We\ndemonstrate its effectiveness in uncertainty-aware super-resolution,\nclassification, and generative modelling tasks and show how our method\noutperforms conventional baselines with minimal performance loss across\nresolutions.",
      "generated_abstract": "The rapid development of large language models (LLMs) has enabled the\nconversion of textual content into 3D visual content, opening new perspectives\nfor visual media production. However, existing methods are often limited by\ntheir reliance on specific LLMs, which may lead to inconsistent and\ninefficient visual content generation. To address this challenge, we introduce\nFusion-LLM, a framework that integrates text and LLM-generated audio as\ninputs to a video-based video synthesis model. By leveraging the temporal\nconsistency of audio-text pairs, Fusion-LLM enables more efficient and\nflexible generation of video content. Specifically, we propose a novel audio\nalignment module that aligns audio and text sequences with the same semantic\nmeaning. We also introduce a novel audio-text fusion method that integrates\naudio and text representations in a unified manner. Extensive experiments on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19469026548672566,
          "p": 0.22916666666666666,
          "f": 0.21052631082255455
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.016129032258064516,
          "f": 0.015267170586797267
        },
        "rouge-l": {
          "r": 0.168141592920354,
          "p": 0.19791666666666666,
          "f": 0.1818181768512627
        }
      }
    },
    {
      "paper_id": "math.FA.math/OA/2502.19028v2",
      "true_abstract": "We give a new proof of the Weyl-von Neumann-Berg theorem. Our proof improves\nHalmos' proof in 1972 by observing the fact that every compact set in the\ncomplex plane is the continuous image of a compact set in the real line.",
      "generated_abstract": "In this paper, we study the asymptotic behavior of the local eigenvalue\ndistribution of the $d$-dimensional random subcritical Laplacian $\\Delta_d$ on\nthe unit ball $B_d$ under the Poisson boundary condition. We prove that the\nlocal eigenvalue distribution converges weakly to a Bernstein distribution as\n$d\\to\\infty$, where the variance of the Bernstein distribution is proportional\nto the Laplacian's central limit theorem. In addition, we prove that the local\neigenvalue distribution of the random subcritical Laplacian on $B_d$ converges\nweakly to a Gaussian distribution under the Dirichlet boundary condition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2413793103448276,
          "p": 0.15217391304347827,
          "f": 0.18666666192355566
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.015151515151515152,
          "f": 0.01941747112451801
        },
        "rouge-l": {
          "r": 0.20689655172413793,
          "p": 0.13043478260869565,
          "f": 0.15999999525688902
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.01461v1",
      "true_abstract": "Computational prediction of enzymatic reactions represents a crucial\nchallenge in sustainable chemical synthesis across various scientific domains,\nranging from drug discovery to materials science and green chemistry. These\nsyntheses rely on proteins that selectively catalyze complex molecular\ntransformations. These protein catalysts exhibit remarkable substrate\nadaptability, with the same protein often catalyzing different chemical\ntransformations depending on its molecular partners. Current approaches to\nprotein representation in reaction prediction either ignore protein structure\nentirely or rely on static embeddings, failing to capture how proteins\ndynamically adapt their behavior to different substrates. We present\nDocking-Aware Attention (DAA), a novel architecture that generates dynamic,\ncontext-dependent protein representations by incorporating molecular docking\ninformation into the attention mechanism. DAA combines physical interaction\nscores from docking predictions with learned attention patterns to focus on\nprotein regions most relevant to specific molecular interactions. We evaluate\nour method on enzymatic reaction prediction, where it outperforms previous\nstate-of-the-art methods, achieving 62.2\\% accuracy versus 56.79\\% on complex\nmolecules and 55.54\\% versus 49.45\\% on innovative reactions. Through detailed\nablation studies and visualizations, we demonstrate how DAA generates\ninterpretable attention patterns that adapt to different molecular contexts.\nOur approach represents a general framework for context-aware protein\nrepresentation in biocatalysis prediction, with potential applications across\nenzymatic synthesis planning. We open-source our implementation and pre-trained\nmodels to facilitate further research.",
      "generated_abstract": "This paper addresses the challenge of predicting the structure of\nbiomolecular networks using only experimental data. To address this challenge,\nwe propose an approach that leverages graph convolutional neural networks\n(GCNs) and integrates them with deep belief networks (DBNs). Our approach\nintegrates the GCN layers with the DBN layers to capture the structural\ninformation in the network. We evaluate our approach on a benchmark dataset\ncollected from the PubChem database and demonstrate that the proposed method\noutperforms existing methods. The performance of the proposed method is further\nvalidated using the experimental datasets from the Protein Data Bank and the\nGene Ontology databases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.32432432432432434,
          "f": 0.2114537400989735
        },
        "rouge-2": {
          "r": 0.02358490566037736,
          "p": 0.05102040816326531,
          "f": 0.03225806019230028
        },
        "rouge-l": {
          "r": 0.1503267973856209,
          "p": 0.3108108108108108,
          "f": 0.20264316741174881
        }
      }
    },
    {
      "paper_id": "eess.SP.math/IT/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "In this paper, we consider a multi-agent system in which a multi-agent agent\nis able to predict the actions of its neighbor agents, and the neighbor\nagents can predict the actions of their neighbors, and so on. The multi-agent\nagent uses its own prediction to take an action and the neighbor agents\ninformally verify this action, and then the multi-agent agent updates its\nprediction. The multi-agent agent is assumed to have a private observation of\nthe environment and the neighbor agents are assumed to have a private\nobservation of the multi-agent agent. We first propose an agent's prediction\nin the multi-agent system, and then propose a neighbor's prediction in the\nmulti-agent system. The multi-agent system is also designed based on these two\nprediction methods, and a multi-agent system is designed based on the\nprediction methods. The multi-agent system is characterized by the multi",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14516129032258066,
          "p": 0.2903225806451613,
          "f": 0.19354838265232985
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.029411764705882353,
          "f": 0.021352308542699296
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.25806451612903225,
          "f": 0.17204300630824382
        }
      }
    },
    {
      "paper_id": "physics.optics.q-bio/CB/2412.16427v1",
      "true_abstract": "Compressed streak imaging (CSI), introduced in 2014, has proven to be a\npowerful imaging technology for recording ultrafast phenomena such as light\npropagation and fluorescence lifetimes at over 150 trillion frames per second.\nDespite these achievements, CSI has faced challenges in detecting subtle\nintensity fluctuations in slow-moving, continuously illuminated objects. This\nlimitation, largely attributable to high streak compression and motion blur,\nhas curtailed broader adoption of CSI in applications such as cellular\nfluorescence microscopy. To address these issues and expand the utility of CSI,\nwe present a novel encoding strategy, termed two-axis compressed streak imaging\n(TACSI) that results in significant improvements to the reconstructed image\nfidelity. TACSI introduces a second scanning axis which shuttles a conjugate\nimage of the object with respect to the coded aperture. The moving image\ndecreases the streak compression ratio and produces a flash and shutter\nphenomenon that reduces coded aperture motion blur, overcoming the limitations\nof current CSI technologies. We support this approach with an analytical model\ndescribing the two-axis streak compression ratio, along with both simulated and\nempirical measurements. As proof of concept, we demonstrate the ability of\nTACSI to measure rapid variations in cell membrane potentials using\nvoltage-sensitive dye, which were previously unattainable with conventional\nCSI. This method has broad implications for high-speed photography, including\nthe visualization of action potentials, muscle contractions, and enzymatic\nreactions that occur on microsecond and faster timescales using fluorescence\nmicroscopy.",
      "generated_abstract": "This study investigates the dynamics of the fluorescence decay of a single\nfluorophore\n  in a continuous flow, using a single-cell fluorescence microscopy technique\nin combination with a numerical simulation. The fluorophore, with an emission\nwavelength of 532 nm, is excited by a laser with a wavelength of 532 nm, and is\nmonitored with a camera in a flowing medium. The camera acquires a time series\nof fluorescence intensities at different flow velocities. The results show that\nthe fluorescence decay rate depends on the flow velocity and the distance from\nthe focal plane. The fluorescence decay rate is found to be proportional to the\nflow velocity and inversely proportional to the distance from the focal plane.\nThe results show that the fluorescence decay rate is controlled by the flow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11320754716981132,
          "p": 0.2857142857142857,
          "f": 0.16216215809715134
        },
        "rouge-2": {
          "r": 0.02242152466367713,
          "p": 0.05154639175257732,
          "f": 0.031249995775195886
        },
        "rouge-l": {
          "r": 0.11320754716981132,
          "p": 0.2857142857142857,
          "f": 0.16216215809715134
        }
      }
    },
    {
      "paper_id": "cs.IT.stat/OT/2402.08135v1",
      "true_abstract": "Since its introduction in 2011, the partial information decomposition (PID)\nhas triggered an explosion of interest in the field of multivariate information\ntheory and the study of emergent, higher-order (\"synergistic\") interactions in\ncomplex systems. Despite its power, however, the PID has a number of\nlimitations that restrict its general applicability: it scales poorly with\nsystem size and the standard approach to decomposition hinges on a definition\nof \"redundancy\", leaving synergy only vaguely defined as \"that information not\nredundant.\" Other heuristic measures, such as the O-information, have been\nintroduced, although these measures typically only provided a summary statistic\nof redundancy/synergy dominance, rather than direct insight into the synergy\nitself. To address this issue, we present an alternative decomposition that is\nsynergy-first, scales much more gracefully than the PID, and has a\nstraightforward interpretation. Our approach defines synergy as that\ninformation in a set that would be lost following the minimally invasive\nperturbation on any single element. By generalizing this idea to sets of\nelements, we construct a totally ordered \"backbone\" of partial synergy atoms\nthat sweeps systems scales. Our approach starts with entropy, but can be\ngeneralized to the Kullback-Leibler divergence, and by extension, to the total\ncorrelation and the single-target mutual information. Finally, we show that\nthis approach can be used to decompose higher-order interactions beyond just\ninformation theory: we demonstrate this by showing how synergistic combinations\nof pairwise edges in a complex network supports signal communicability and\nglobal integration. We conclude by discussing how this perspective on\nsynergistic structure (information-based or otherwise) can deepen our\nunderstanding of part-whole relationships in complex systems.",
      "generated_abstract": "This paper studies the design of an online learning algorithm to learn\nthe causal effects of a stochastic signal on a continuous response using\nlimited data. We consider a setting where the signal is generated from a\ncontinuous-time stochastic process, which is governed by a stochastic differential\nequation with state-dependent mean and noisy observations of the signal are\navailable at each time step. We propose a learning algorithm that is based on a\nrandomized policy that first samples a state of the stochastic process from its\ndistribution and then applies a gradient descent step to estimate the\nsignificance of each time step. We show that the learned causal effects of the\nsignal on the response converge to the optimal solution of the linear regression\nproblem with the causal effect as a constraint. We also show that the algorithm\nachieves the optimal convergence rate, which is the worst-case rate of the\nconcentration error of the estimated causal effects",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09467455621301775,
          "p": 0.18604651162790697,
          "f": 0.12549019160815086
        },
        "rouge-2": {
          "r": 0.015748031496062992,
          "p": 0.029850746268656716,
          "f": 0.02061855217929741
        },
        "rouge-l": {
          "r": 0.09467455621301775,
          "p": 0.18604651162790697,
          "f": 0.12549019160815086
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.01319v1",
      "true_abstract": "This paper addresses the estimation of the systemic risk measure known as\nCoVaR, which quantifies the risk of a financial portfolio conditional on\nanother portfolio being at risk. We identify two principal challenges:\nconditioning on a zero-probability event and the repricing of portfolios. To\ntackle these issues, we propose a decoupled approach utilizing smoothing\ntechniques and develop a model-independent theoretical framework grounded in a\nfunctional perspective. We demonstrate that the rate of convergence of the\ndecoupled estimator can achieve approximately $O_{\\rm P}(\\Gamma^{-1/2})$, where\n$\\Gamma$ represents the computational budget. Additionally, we establish the\nsmoothness of the portfolio loss functions, highlighting its crucial role in\nenhancing sample efficiency. Our numerical results confirm the effectiveness of\nthe decoupled estimators and provide practical insights for the selection of\nappropriate smoothing techniques.",
      "generated_abstract": "We investigate the behavior of the value-at-risk (VaR) of a portfolio of\nportfolios. We find that the VaR is an upper bound of the risk measure\nunderlying the portfolio. We derive the asymptotic distribution of the\nportfolio VaR. We provide some theoretical results on the asymptotic behavior of\nthe portfolio's VaR. We also discuss the conditions under which the\nportfolio's VaR is an upper bound of the risk measure underlying the portfolio.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.4117647058823529,
          "f": 0.22399999603968004
        },
        "rouge-2": {
          "r": 0.05785123966942149,
          "p": 0.13725490196078433,
          "f": 0.08139534466535987
        },
        "rouge-l": {
          "r": 0.13186813186813187,
          "p": 0.35294117647058826,
          "f": 0.19199999603968007
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.19370v1",
      "true_abstract": "In this project, we propose a Variational Inference algorithm to approximate\nposterior distributions. Building on prior methods, we develop the\nGradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This\nmethod introduces a novel loss function that combines a weighted gradient and\nthe Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The\nlearning rate is determined through a suboptimal minimization of this loss\nfunction within a gradient descent framework.\n  The G-SVGD method is compared against the standard Stein Variational Gradient\nDescent (SVGD) approach, employing the ADAM optimizer for learning rate\nadaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess\nperformance in two wave prospection models representing low-contrast and\nhigh-contrast subsurface scenarios. To achieve robust numerical approximations\nin the forward model solver, a five-point operator is employed, while the\nadjoint method improves accuracy in computing gradients of the log posterior.\n  Our findings demonstrate that G-SVGD accelerates convergence and offers\nimproved performance in scenarios where gradient evaluation is computationally\nexpensive. The abstract highlights the algorithm's applicability to wave\nprospection models and its potential for broader applications in Bayesian\ninference. Finally, we discuss the benefits and limitations of G-SVGD,\nemphasizing its contribution to advancing computational efficiency in\nuncertainty quantification.",
      "generated_abstract": "The standardized mean difference (SMD) is the most widely used measure of\ndifference between treatment groups in clinical trials. However, the\ncomputation of SMD is highly sensitive to the choice of statistical\nrepresentation of the difference between treatment groups. In this paper, we\npropose a new SMD based on the difference between the mean and variance of\ntreatment groups. We derive the asymptotic distribution of the proposed SMD\nand show that the asymptotic distribution of the SMD is identical to the\nstandardized mean difference SMD under certain conditions. We also propose a\nnew estimation method based on the difference between the mean and variance of\ntreatment groups. The proposed estimator is shown to be consistent and\nasymptotically normal under some conditions. Numerical studies demonstrate the\neffectiveness of the proposed estimator in comparison with existing\nestimators.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2537313432835821,
          "f": 0.16748768030672923
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.038834951456310676,
          "f": 0.027210879801703756
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.23880597014925373,
          "f": 0.15763546355796076
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.09505v1",
      "true_abstract": "Photoacoustic (PA) imaging of deep tissue tends to employ Q-switched lasers\nwith high pulse energy to generate high optical fluence and therefore high PA\nsignal. Compared to Q-switched lasers, pulsed laser diodes (PLDs) typically\ngenerate low pulse energy. In PA imaging applications with strong acoustic\nattenuation, such as through human skull bone, the broadband PA waves generated\nby nanoseconds laser pulses are significantly reduced in bandwidth during their\npropagation to a detector. As high-frequency PA signal components are not\ntransmitted through skull, we propose to not generate them by increasing\nexcitation pulse duration. Because PLDs are mainly limited in their peak power\noutput, an increase in pulse duration linearly increases pulse energy and\ntherefore PA signal amplitude. Here we show that the optimal pulse duration for\ndeep PA sensing through thick skull bone is far higher than in typical PA\napplications. Counterintuitively, this makes PLD excitation well-suited for\ntranscranial photoacoustics. We show this in PA sensing experiments on ex vivo\nhuman skull bone.",
      "generated_abstract": "Accurate, high-resolution, and reproducible 3D image reconstruction is\nrequired for clinical applications in medical imaging, such as brain\nmagnetic resonance imaging (MRI). Current methods in 3D image reconstruction\ntypically use iterative optimization to determine a reconstruction solution\nwith minimal distortion. This approach can be computationally expensive,\nparticularly for large datasets and/or with high-resolution imaging. To\naddress these limitations, we introduce a novel iterative optimization method\nbased on the Kaczmarz method. The method leverages the structuredness of the\nreconstruction problem, allowing it to be solved more efficiently than existing\nmethods. The Kaczmarz method is also more scalable to larger datasets and\naccommodates different reconstruction settings, including slice-by-slice\nreconstruction and non-uniform acquisition (NUA). We demonstrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16822429906542055,
          "p": 0.21686746987951808,
          "f": 0.18947367929030481
        },
        "rouge-2": {
          "r": 0.006578947368421052,
          "p": 0.009259259259259259,
          "f": 0.007692302835506024
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.20481927710843373,
          "f": 0.17894736350083112
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.16794v1",
      "true_abstract": "Auditory foundation models, including auditory large language models (LLMs),\nprocess all sound inputs equally, independent of listener perception. However,\nhuman auditory perception is inherently selective: listeners focus on specific\nspeakers while ignoring others in complex auditory scenes. Existing models do\nnot incorporate this selectivity, limiting their ability to generate\nperception-aligned responses. To address this, we introduce Intention-Informed\nAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM\n(AAD-LLM), a prototype system that integrates brain signals to infer listener\nattention. AAD-LLM extends an auditory LLM by incorporating intracranial\nelectroencephalography (iEEG) recordings to decode which speaker a listener is\nattending to and refine responses accordingly. The model first predicts the\nattended speaker from neural activity, then conditions response generation on\nthis inferred attentional state. We evaluate AAD-LLM on speaker description,\nspeech transcription and extraction, and question answering in multitalker\nscenarios, with both objective and subjective ratings showing improved\nalignment with listener intention. By taking a first step toward\nintention-aware auditory AI, this work explores a new paradigm where listener\nperception informs machine listening, paving the way for future\nlistener-centered auditory systems. Demo and code available:\nhttps://aad-llm.github.io.",
      "generated_abstract": "In this work, we introduce an innovative approach for speech enhancement\n(SE) that leverages both the temporal and spectral dimensions. The temporal\ndimension is represented using the proposed spectral-temporal (ST) feature\ndescriptions, which are computed from the ST-sparse spectrogram. The\ntemporal dimension is then addressed through a novel Temporal Attention (TA)\nmechanism that enhances the representation of important temporal features.\nAdditionally, we propose an ST-attention based Adaptive Interpolation (STAI)\nmethod that interpolates between the ST-sparse spectrogram and the reference\nST-denoised spectrogram. The ST-sparse spectrogram is used for the temporal\ndimension, while the reference ST-denoised spectrogram is used for the\nspectral dimension. The proposed method is evaluated using three\nstate-of-the-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1232876712328767,
          "p": 0.2647058823529412,
          "f": 0.16822429472967082
        },
        "rouge-2": {
          "r": 0.00546448087431694,
          "p": 0.010752688172043012,
          "f": 0.007246372343260475
        },
        "rouge-l": {
          "r": 0.11643835616438356,
          "p": 0.25,
          "f": 0.15887850033714745
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/pop-ph/2501.10701v1",
      "true_abstract": "It is found from textbooks that there are the different versions of the\nschematic diagram related to the Nernst equation, and consequently, it leads to\nsome discussion related to the Nernst equation and the discovery of other\nmeaningful schematic diagrams never appearing in literature. It is also found\nthat through the introduction of a new function, the schematic diagram of the\nNernst equation in the isothermal process of any thermodynamic system can be\ngenerated in a unified way and that the Nernst equation can be re-obtained from\nthe experimental data of low-temperature chemical reactions without any\nartificial additional assumptions. The results obtained here show clearly that\nthe centenary progress from the Nernst theorem to the Nernst statement is\ncompleted.",
      "generated_abstract": "We present a novel approach to studying the dynamics of a large-scale\nsystem of interacting agents in a periodic environment. We use a new method\nbased on the concept of \"spatial chaos\" to identify a set of critical\nparameters that characterize the system's phase diagram. The proposed method\nallows us to analyze the behavior of the system even if its dimensions are\nhigher than the dimension of the phase space of the dynamics. This approach\noffers a simple and direct way to study the phase diagram of a large-scale\nsystem in a systematic and system-independent way, regardless of the\ndynamical system's complexity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18571428571428572,
          "p": 0.20967741935483872,
          "f": 0.19696969198806258
        },
        "rouge-2": {
          "r": 0.04950495049504951,
          "p": 0.054945054945054944,
          "f": 0.05208332834689718
        },
        "rouge-l": {
          "r": 0.15714285714285714,
          "p": 0.1774193548387097,
          "f": 0.16666666168503227
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.08252v1",
      "true_abstract": "Environmental and mental conditions are known risk factors for dermatitis and\nsymptoms of skin inflammation, but their interplay is difficult to quantify;\nepidemiological studies rarely include both, along with possible confounding\nfactors. Infodemiology leverages large online data sets to address this issue,\nbut fusing them produces strong patterns of spatial and temporal correlation,\nmissingness, and heterogeneity.\n  In this paper, we design a causal network that correctly models these complex\nstructures in large-scale infodemiological data from Google, EPA, NOAA and US\nCensus (434 US counties, 134 weeks). Our model successfully captures known\ncausal relationships between weather patterns, pollutants, mental conditions,\nand dermatitis. Key findings reveal that anxiety accounts for 57.4% of\nexplained variance in dermatitis, followed by NO2 (33.9%), while environmental\nfactors show significant mediation effects through mental conditions. The model\npredicts that reducing PM2.5 emissions by 25% could decrease dermatitis\nprevalence by 18%. Through statistical validation and causal inference, we\nprovide unprecedented insights into the complex interplay between environmental\nand mental health factors affecting dermatitis, offering valuable guidance for\npublic health policies and environmental regulations.",
      "generated_abstract": "This paper presents a novel method to detect and remove outliers in\nquantitative data, while preserving the underlying underlying distribution.\nOutliers are defined as extreme values outside of a specified range, which\ncan be defined as a set of percentiles. In this work, the method is applied to\na dataset of 1,289 blood samples, which was collected from 100 patients for\ntheir liver enzyme levels. The dataset was divided into three groups based on\nthe median liver enzyme levels. The three groups were then used to assess the\nperformance of the proposed method, and the results were compared with those\nof a traditional outlier removal method. The proposed method was found to\noutperform traditional methods, with a mean absolute error of 11.55% and a\nmean relative error of 30.27%, compared to a mean relative error of 28.69% for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1259259259259259,
          "p": 0.19318181818181818,
          "f": 0.15246636293510843
        },
        "rouge-2": {
          "r": 0.005714285714285714,
          "p": 0.007874015748031496,
          "f": 0.006622511682605227
        },
        "rouge-l": {
          "r": 0.11851851851851852,
          "p": 0.18181818181818182,
          "f": 0.1434977530696376
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.12940v1",
      "true_abstract": "Global discrete optimization is notoriously difficult due to the lack of\ngradient information and the curse of dimensionality, making exhaustive search\ninfeasible. Tensor cross approximation is an efficient technique to approximate\nmultivariate tensors (and discretized functions) by tensor product\ndecompositions based on a small number of tensor elements, evaluated on\nadaptively selected fibers of the tensor, that intersect on submatrices of\n(nearly) maximum volume. The submatrices of maximum volume are empirically\nknown to contain large elements, hence the entries selected for cross\ninterpolation can also be good candidates for the globally maximal element\nwithin the tensor. In this paper we consider evolution of epidemics on\nnetworks, and infer the contact network from observations of network nodal\nstates over time. By numerical experiments we demonstrate that the contact\nnetwork can be inferred accurately by finding the global maximum of the\nlikelihood using tensor cross interpolation. The proposed tensor product\napproach is flexible and can be applied to global discrete optimization for\nother problems, e.g. discrete hyperparameter tuning.",
      "generated_abstract": "We introduce a novel framework for Bayesian nonparametric inference in\nestimating the functional principal component analysis (FPCA) model. The\nframework builds on the functional version of the principal component analysis\n(FPCA), where the model is parameterized as a functional linear model. This\nframework allows for efficient Bayesian inference for both parametric and\nnonparametric functional models. We derive the posterior distribution of the\nmodel parameters, including the eigenvalues and eigenvectors of the covariance\nmatrix. We also provide a Bayesian algorithm for the estimation of the\neigenvalues and eigenvectors. Additionally, we propose a Bayesian approach to\nestimate the FPCA model parameters under sparsity constraints, allowing for\nefficient inference under structural assumptions. We apply our framework to\nestimate the functional principal component analysis model and demonstrate the\napplicability of our Bayesian approach in estimating the functional\nprincipal component analysis model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.208955223880597,
          "f": 0.1590909043756458
        },
        "rouge-2": {
          "r": 0.012738853503184714,
          "p": 0.01818181818181818,
          "f": 0.014981268563174005
        },
        "rouge-l": {
          "r": 0.11926605504587157,
          "p": 0.19402985074626866,
          "f": 0.14772726801200944
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.18488v1",
      "true_abstract": "Aquaculture has been the fastest growing food production sector globally due\nto its potential to improve food security, stimulate economic growth, and\nreduce poverty. Its rapid development has been linked to sustainability\nchallenges, many of which are still unresolved and poorly understood.\nSmall-scale producers account for an increasing fraction of aquacultural\noutput. At the same time, many of these producers experience poverty, food\ninsecurity, and rely on unimproved production practices. We develop a stylized\nmathematical model to explore the effects of ecological, social, and economic\nfactors on the dynamics of a small-scale pond aquaculture system. Using\nanalytical and numerical methods, we explore the stability, asymptotic\ndynamics, and bifurcations of the model. Depending on the characteristics of\nthe system, the model exhibits one of three distinct configurations:\nmonostability with a global poverty trap in a nutrient-dominated or\nfish-dominated system; bistability with poverty trap and well-being attractors;\nmultistability with poverty trap and two well-being attractors with different\ncharacteristics. The model results show that intensification can be sustainable\nonly if it takes into account the local social-ecological context. In addition,\nthe heterogeneity of small-scale aquaculture producers matters, as the effects\nof intensification can be unevenly distributed among them. Finally, more is not\nalways better because too high nutrient input or productivity can lead to a\nsuboptimal attractor or system collapse.",
      "generated_abstract": "We propose a novel, simple, and scalable method for estimating the\ntime-varying\n  probability of occurrence of a random event using only its past events. The\ntime-varying\n  probability of occurrence is defined as the probability of the event occurring\nin the future, given that it has already occurred in the past. The method\nallows for\n  the estimation of the time-varying probability of occurrence in any\nnon-stationary\n  stochastic process. This includes a wide range of processes such as\nstationary,\n  stochastic, non-stationary, and stochastic autoregressive processes. We\ninvestigate\n  the properties of the proposed method and its implications for statistical\ninference. We\n  demonstrate its potential for estimating the time-varying probability of\noccurrence of\n  non-stationary processes using data from a real-world study. We discuss",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1095890410958904,
          "p": 0.25,
          "f": 0.15238094814331077
        },
        "rouge-2": {
          "r": 0.019801980198019802,
          "p": 0.04,
          "f": 0.026490061795536025
        },
        "rouge-l": {
          "r": 0.10273972602739725,
          "p": 0.234375,
          "f": 0.14285713861950128
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2407.00420v1",
      "true_abstract": "The potential health risks of cement dust exposure are increasingly raising\nconcern worldwide as the cement industry expands in response to rising cement\ndemand. This necessitates the need to determine the nature of the risks in\norder to develop appropriate measures. This study determined the effects of\ncement dust exposure on the weight and blood glucose levels of people residing\nor working around a cement company in Sokoto, Nigeria. Demographic information\nwas obtained using questionnaires from 72 participants, which included age,\ngender, educational level, exposure hours, occupation, and lifestyle. The blood\nglucose levels and body mass index (BMI) were measured using a Fine Test\nglucometer and a mechanical scale, respectively. The results showed that most\nof the people living or working around the cement company were middle-aged men\n(31-40; 42.06%) with a primary (33.33%) or secondary (45.83%) school education.\nIt showed that 30 (41.69%) of the participants were overweight while 5 (6.94%)\nwere obese. Additionally, 52.78% of the participants were diabetic while 31.94%\nwere prediabetic. Participants that were exposed for long hours (> 15 hours per\nday) were the most diabetic (20% of the participants), followed by smokers\n(15%), and artisans (7%). It can be concluded that exposure to cement dust from\nthe company increased the risk of overweight, obesity, and hyperglycemia among\nthe participants. These health risks were worsened by daily long hours of\nexposure, smoking, and artisanal pollutant exposure. Human settlements and\nartisans should not be located near the cement company, and the company should\nminimize pollutant emissions.",
      "generated_abstract": "Increasing evidence suggests that the human brain is capable of processing\ninformation in a manner that is fundamentally different from what has been\nassumed in standard theories of consciousness. Here, we present a theory of\nintelligence based on the principle of information processing that is not\naffected by the assumption of a single, unified computational model. We define\nintelligence as a computational task that is executed through a distributed\nset of computational tasks, each of which is implemented by a set of\ninformation processing units that have been optimized for that task. We demonstrate\nthat this distributed model of intelligence can be constructed using\nwell-established information-processing principles, and that it is capable of\nsimulating many of the computational tasks that have been previously\nexemplified in artificial intelligence. Additionally, we show that this model\ncan be used to analyze the computational tasks that are currently considered\nto be the most complex in artificial intelligence.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13291139240506328,
          "p": 0.25,
          "f": 0.17355371447578727
        },
        "rouge-2": {
          "r": 0.017167381974248927,
          "p": 0.02962962962962963,
          "f": 0.021739125789373627
        },
        "rouge-l": {
          "r": 0.11392405063291139,
          "p": 0.21428571428571427,
          "f": 0.14876032604603526
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.21129v1",
      "true_abstract": "We propose Microscopic Propagator Imaging (MPI) as a novel method to retrieve\nthe indices of the microscopic propagator which is the probability density\nfunction of water displacements due to diffusion within the nervous tissue\nmicrostructures. Unlike the Ensemble Average Propagator indices or the\nDiffusion Tensor Imaging metrics, MPI indices are independent from the\nmesoscopic organization of the tissue such as the presence of multiple axonal\nbundle directions and orientation dispersion. As a consequence, MPI indices are\nmore specific to the volumes, sizes, and types of microstructures, like axons\nand cells, that are present in the tissue. Thus, changes in MPI indices can be\nmore directly linked to alterations in the presence and integrity of\nmicrostructures themselves. The methodology behind MPI is rooted on zonal\nmodeling of spherical harmonics, signal simulation, and machine learning\nregression, and is demonstrated on both synthetic and Human Diffusion MRI data.",
      "generated_abstract": "The study of gene regulatory networks (GRNs) is crucial for understanding\nthe mechanisms of biological processes, particularly in developmental\nbiology. Despite the advancement of computational tools for analyzing GRNs,\ncurrent methods primarily focus on the graph structure and ignore the dynamics\nof the underlying gene expression. In this study, we propose a novel approach\nthat integrates the dynamics of gene expression and graph structure to\nunderstand the transcriptional regulation of genes. Our approach combines a\ntopological neural network (TNN) model and a graph neural network (GNN) to\ncapture the complex relationship between gene expression and GRN structures.\nThe proposed approach is applied to the human genome to study the\nregulation of the human retinoic acid receptor gene (RARalpha) and its\nneuroblastoma cell lineage-specific ortholog (RARalpha-Nb) using a\nmulti",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12631578947368421,
          "p": 0.14634146341463414,
          "f": 0.13559321536595506
        },
        "rouge-2": {
          "r": 0.021897810218978103,
          "p": 0.02586206896551724,
          "f": 0.02371541005421217
        },
        "rouge-l": {
          "r": 0.11578947368421053,
          "p": 0.13414634146341464,
          "f": 0.12429378033770648
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.02674v1",
      "true_abstract": "We address functional uncertainty quantification for ill-posed inverse\nproblems where it is possible to evaluate a possibly rank-deficient forward\nmodel, the observation noise distribution is known, and there are known\nparameter constraints. We present four constraint-aware confidence intervals\nextending the work of Batlle et al. (2023) by making the intervals both\ncomputationally feasible and less conservative. Our approach first shrinks the\npotentially unbounded constraint set compact in a data-adaptive way, obtains\nsamples of the relevant test statistic inside this set to estimate a quantile\nfunction, and then uses these computed quantities to produce the intervals. Our\ndata-adaptive bounding approach is based on the approach by Berger and Boos\n(1994), and involves defining a subset of the constraint set where the true\nparameter exists with high probability. This probabilistic guarantee is then\nincorporated into the final coverage guarantee in the form of an uncertainty\nbudget. We then propose custom sampling algorithms to efficiently sample from\nthis subset, even when the parameter space is high-dimensional.\nOptimization-based interval methods formulate confidence interval computation\nas two endpoint optimizations, where the optimization constraints can be set to\nachieve different types of interval calibration while seamlessly incorporating\nparameter constraints. However, choosing valid optimization constraints has\nbeen elusive. We show that all four proposed intervals achieve nominal coverage\nfor a particular functional both theoretically and in practice, with numerical\nexamples demonstrating superior performance of our intervals over the OSB\ninterval in terms of both coverage and expected length. In particular, we show\nthe superior performance in a realistic unfolding simulation from high-energy\nphysics that is severely ill-posed and involves a rank-deficient forward model.",
      "generated_abstract": "In this paper, we present a new method for the detection of outliers in\nthe log-rank test statistic. The proposed method, called the log-rank outlier\ntest statistic (LOGS), is based on the log-rank test statistic but uses a\ndifferent test statistic. This approach allows us to detect outliers in the\nlog-rank test statistic. To this end, we propose two algorithms for\ndetecting outliers in the log-rank test statistic. We show that LOGS can be\nused to detect outliers in the log-rank test statistic by using the\nlog-rank test statistic. We show that LOGS is equivalent to the log-rank test\nstatistic under the null hypothesis of normality. We also show that the\nlog-rank outlier test statistic has a similar equivalence to the log-rank test\nstatistic under the null hypothesis of normality. The advantage of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.5,
          "f": 0.2631578908587258
        },
        "rouge-2": {
          "r": 0.031496062992125984,
          "p": 0.09523809523809523,
          "f": 0.047337274371345835
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.5,
          "f": 0.2631578908587258
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.09207v1",
      "true_abstract": "In this paper, we study the $p$-Selmer groups in the family of $p$-twists of\nan elliptic curve $E$ over a number field $K$. We prove that if $E/K$ is an\nelliptic curve over a number field $K$, and if $d$ is congruent to the\ndimension of the Selmer group of $E/K$ modulo $2$ and is greater than that\ndimension, then there exist infinitely many characters $\\chi \\in\n\\text{Hom}(G_K, \\mu_p)$ such that $\\text{dim}_{\\mathbb{F}_p}(\\text{Sel}_p(E/K,\n\\chi)) = d$ under certain conditions.",
      "generated_abstract": "We prove that the class of non-elementary $2$-groups is closed under\n$\\mathrm{H}_{2}(\\mathrm{G})=\\mathrm{H}_{2}(\\mathrm{N}_{2}(\\mathrm{G}))$ for\n$\\mathrm{G}\\in\\mathrm{GL}(n)$, $\\mathrm{N}_{2}(\\mathrm{G})$ the subgroup of\n$2$-groups of $\\mathrm{G}$, and $\\mathrm{H}_{2}(\\mathrm{G})$ the\n$2$-groups of $\\mathrm{G}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13793103448275862,
          "p": 0.4,
          "f": 0.20512820131492446
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.08,
          "f": 0.04123710957593829
        },
        "rouge-l": {
          "r": 0.10344827586206896,
          "p": 0.3,
          "f": 0.15384615003287322
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/PM/2411.07949v2",
      "true_abstract": "We consider a simplified model for optimizing a single-asset portfolio in the\npresence of transaction costs given a signal with a certain autocorrelation and\ncross-correlation structure. In our setup, the portfolio manager is given two\none-parameter controls to influence the construction of the portfolio. The\nfirst is a linear filtering parameter that may increase or decrease the level\nof autocorrelation in the signal. The second is a numerical threshold that\ndetermines a symmetric \"no-trade\" zone. Portfolio positions are constrained to\na single unit long or a single unit short. These constraints allow us to focus\non the interplay between the signal filtering mechanism and the hysteresis\nintroduced by the \"no-trade\" zone. We then formulate an optimization problem\nwhere we aim to minimize the frequency of trades subject to a fixed return\nlevel of the portfolio. We show that maintaining a no-trade zone while removing\nautocorrelation entirely from the signal yields a locally optimal solution. For\nany given \"no-trade\" zone threshold, this locally optimal solution also\nachieves the maximum attainable return level, and we derive a quantitative\nlower bound for the amount of improvement in terms of the given threshold and\nthe amount of autocorrelation removed.",
      "generated_abstract": "In this paper, we derive a closed-form solution for the optimal\nportfolio in a portfolio selection problem with correlated assets. The\nproblem is formulated as a constrained quadratic programming problem, which\ncan be solved efficiently by a Newton algorithm. The solution is presented in\nterms of the optimal portfolio weights and the optimal allocation of assets.\nWe compare the performance of our proposed approach with that of a linear\nregression model, and show that our approach is superior to the linear\nregression model in terms of both accuracy and computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23684210526315788,
          "p": 0.46551724137931033,
          "f": 0.31395348390210925
        },
        "rouge-2": {
          "r": 0.05747126436781609,
          "p": 0.11904761904761904,
          "f": 0.07751937545339847
        },
        "rouge-l": {
          "r": 0.20175438596491227,
          "p": 0.39655172413793105,
          "f": 0.26744185599513254
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.06093v1",
      "true_abstract": "Bayesian Optimization (BO) is a well-established method for addressing\nblack-box optimization problems. In many real-world scenarios, optimization\noften involves multiple functions, emphasizing the importance of leveraging\ndata and learned functions from prior tasks to enhance efficiency in the\ncurrent task. To expedite convergence to the global optimum, recent studies\nhave introduced meta-learning strategies, collectively referred to as meta-BO,\nto incorporate knowledge from historical tasks. However, in practical settings,\nthe underlying functions are often heterogeneous, which can adversely affect\noptimization performance for the current task. Additionally, when the number of\nhistorical tasks is large, meta-BO methods face significant scalability\nchallenges. In this work, we propose a scalable and robust meta-BO method\ndesigned to address key challenges in heterogeneous and large-scale meta-tasks.\nOur approach (1) effectively partitions transferred meta-functions into highly\nhomogeneous clusters, (2) learns the geometry-based surrogate prototype that\ncapture the structural patterns within each cluster, and (3) adaptively\nsynthesizes meta-priors during the online phase using statistical\ndistance-based weighting policies. Experimental results on real-world\nhyperparameter optimization (HPO) tasks, combined with theoretical guarantees,\ndemonstrate the robustness and effectiveness of our method in overcoming these\nchallenges.",
      "generated_abstract": "This paper studies the design of efficient randomized algorithms for the\nprediction of the values of a stochastic process in the time interval $[0,T",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.04285714285714286,
          "p": 0.3157894736842105,
          "f": 0.07547169600886047
        },
        "rouge-2": {
          "r": 0.0111731843575419,
          "p": 0.08695652173913043,
          "f": 0.01980197818008059
        },
        "rouge-l": {
          "r": 0.04285714285714286,
          "p": 0.3157894736842105,
          "f": 0.07547169600886047
        }
      }
    },
    {
      "paper_id": "math.KT.math/KT/2503.06267v1",
      "true_abstract": "We present the fundamental properties of the K-theory groups of complex\nvector bundles endowed with actions of magnetic groups. In this work we show\nthat the magnetic equivariant K-theory groups define an equivariant cohomology\ntheory, we determine its coefficients, we show Bott's, Thom's and the degree\nshift isomorphism, we present the Atiyah-Hirzeburh spectral sequence, and we\nexplicitly calculate two magnetic equivariant K-theory groups in order to\nshowcase its applicability. These magnetic equivariant K-theory groups are\nrelevant in condensed matter physics since they provide topological invariants\nof gapped Hamiltonians in magnetic crystals.",
      "generated_abstract": "In this article, we study the local-in-time existence of global-in-time\nclass of solutions of a system of nonlinear fluid-structure interactons with\nanisotropic elasticity. The system considers a fluid moving in a\nthree-dimensional curved spatial domain coupled via a structure residing in a\nnon-flat spatial domain. The interaction relies on the transport of the structure\nin the fluid domain. The dynamics are modeled through a Lagrangian framework\nand the evolutions of the system are formulated through a set of integro-differential\nequations. We prove the existence of global-in-time solutions of the system\nwhen both structures and fluids are well-prepared initial data. In addition,\nfor specific geometries of the initial data sets, we provide explicit local-in-\ntime solutions for the system when the viscosities of the fluids are\nof order $\\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.16901408450704225,
          "f": 0.1832061019054835
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.02702702702702703,
          "f": 0.031249995122071078
        },
        "rouge-l": {
          "r": 0.18333333333333332,
          "p": 0.15492957746478872,
          "f": 0.16793892633296442
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "This paper studies the effectiveness of a tax on non-renewable resources in\nmeasuring a country's environmental performance. The tax is designed to raise\nrevenues to improve the environmental performance of the country. However, the\ntax's effectiveness is limited by its regressive nature, which makes it\ninaccessible to the poorest households. To address this, we introduce a\nregression discontinuity (RD) design. We find that the regressive nature of\nthe tax is reduced by 61% when the RD is applied. The results suggest that\nusing a RD to raise revenues and improve environmental performance are not\nincompatible. The tax's effectiveness is enhanced by the RD, which reduces\nregressivity while improving its accessibility. This study provides policy\ninsights on how to design regressive taxes to achieve the dual goals of\nenvironmental protection and pover",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.1875,
          "f": 0.15228425913576762
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.01639344262295082,
          "f": 0.014084502141441895
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.1875,
          "f": 0.15228425913576762
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.07900v1",
      "true_abstract": "Cost-effective localization methods for Autonomous Underwater Vehicle (AUV)\nnavigation are key for ocean monitoring and data collection at high resolution\nin time and space. Algorithmic solutions suitable for real-time processing that\nhandle nonlinear measurement models and different forms of measurement\nuncertainty will accelerate the development of field-ready technology. This\npaper details a Bayesian estimation method for landmark-aided navigation using\na Side-scan Sonar (SSS) sensor. The method bounds navigation filter error in\nthe GPS-denied undersea environment and captures the highly nonlinear nature of\nslant range measurements while remaining computationally tractable. Combining a\nnovel measurement model with the chosen statistical framework facilitates the\nefficient use of SSS data and, in the future, could be used in real time. The\nproposed filter has two primary steps: a prediction step using an unscented\ntransform and an update step utilizing particles. The update step performs\nprobabilistic association of sonar detections with known landmarks. We evaluate\nalgorithm performance and tractability using synthetic data and real data\ncollected field experiments. Field experiments were performed using two\ndifferent marine robotic platforms with two different SSS and at two different\nsites. Finally, we discuss the computational requirements of the proposed\nmethod and how it extends to real-time applications.",
      "generated_abstract": "This paper addresses the challenge of autonomous navigation in unstructured\nenvironments. We propose a deep reinforcement learning (DRL)-based approach that\ncan navigate autonomously to a goal location within a confined area while\npreserving safety. The proposed framework integrates two key components: a\ntemporal-action recurrent neural network to learn safe motion plans and a\npolicy network to select the optimal action at each time step. Our approach\nemploys a hierarchical architecture that combines a 2D-3D navigation module\nwith a 3D-4D navigation module, enabling seamless navigation across different\ndimensions. The 2D-3D navigation module integrates a depth sensor to identify\nthe 2D coordinates of obstacles and the 3D coordinates of the goal location,\nwhile the 3D-4D navigation module leverages a 3D object detector and a\n4D-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17037037037037037,
          "p": 0.27710843373493976,
          "f": 0.21100916959641453
        },
        "rouge-2": {
          "r": 0.015463917525773196,
          "p": 0.02608695652173913,
          "f": 0.01941747105497541
        },
        "rouge-l": {
          "r": 0.17037037037037037,
          "p": 0.27710843373493976,
          "f": 0.21100916959641453
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.16998v1",
      "true_abstract": "We provide a counterexample to the conduct parameter identification result\nestablished in the foundational work of Lau (1982), which generalizes the\nidentification theorem of Bresnahan (1982) by relaxing the linearity\nassumptions. We identify a separable demand function that still permits\nidentification and validate this case both theoretically and through numerical\nsimulations.",
      "generated_abstract": "In this paper, we propose a novel methodology to address the challenges of\nnon-stationary and time-varying data, which are common in many applications. Our\napproach consists of two main steps. First, we introduce a novel estimation\nmethod for the heteroskedasticity-in-sample error component. Second, we\nintroduce an efficient algorithm to estimate the parameters of the structural\nmodel for the ex-post error component. Theoretically, we establish the\nasymptotic properties of the proposed estimators, and we prove that the proposed\nalgorithms are efficient under mild assumptions. Additionally, we conduct a\nsimulation study to evaluate the performance of the proposed methodology in\ncomparison to alternative methods. The results demonstrate that our methodology\nyields superior performance, and it outperforms state-of-the-art methods in\nmost cases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2619047619047619,
          "p": 0.13924050632911392,
          "f": 0.18181817728570462
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.23809523809523808,
          "p": 0.12658227848101267,
          "f": 0.16528925166586994
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.07374v1",
      "true_abstract": "Recent statistical postprocessing methods for wind speed forecasts have\nincorporated linear models and neural networks to produce more skillful\nprobabilistic forecasts in the low-to-medium wind speed range. At the same\ntime, these methods struggle in the high-to-extreme wind speed range. In this\nwork, we aim to increase the performance in this range by training using a\nweighted version of the continuous ranked probability score (wCRPS). We develop\nan approach using shifted Gaussian cdf weight functions, whose parameters are\ntuned using a multi-objective hyperparameter tuning algorithm that balances\nperformance on low and high wind speed ranges. We explore this approach for\nboth linear models and convolutional neural network models combined with\nvarious parametric distributions, namely the truncated normal, log-normal, and\ngeneralized extreme value distributions, as well as adaptive mixtures. We apply\nthese methods to forecasts from KNMI's deterministic Harmonie-Arome numerical\nweather prediction model to obtain probabilistic wind speed forecasts in the\nNetherlands for 48 hours ahead. For linear models we observe that even with a\ntuned weight function, training using the wCRPS produces a strong body-tail\ntrade-off, where increased performance on extremes comes at the price of lower\nperformance for the bulk of the distribution. For the best models using\nconvolutional neural networks, we find that using a tuned weight function the\nperformance on extremes can be increased without a significant deterioration in\nperformance on the bulk. The best-performing weight function is shown to be\nmodel-specific. Finally, the choice of distribution has no significant impact\non the performance of our models.",
      "generated_abstract": "We consider the problem of estimating a mean function of a stochastic\nprocess, where the process has a high-dimensional state space and the\nstate-space model is unknown. We propose a novel estimator for the mean,\nbased on a recently proposed estimator for the inverse of the state-space\ncovariance matrix. The estimator can be efficiently computed using a\nconjugate gradient method. We demonstrate the effectiveness of the proposed\nestimator on synthetic and real-world data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.3333333333333333,
          "f": 0.16666666291666676
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.05970149253731343,
          "f": 0.027874560880914397
        },
        "rouge-l": {
          "r": 0.09722222222222222,
          "p": 0.2916666666666667,
          "f": 0.14583332958333342
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2411.13813v3",
      "true_abstract": "I examine the value of information from sell-side analysts by analyzing a\nlarge corpus of their written reports. Using embeddings from state-of-the-art\nlarge language models, I show that textual information in analyst reports\nexplains 10.19% of contemporaneous stock returns out-of-sample, a value that is\neconomically more significant than quantitative forecasts. I then perform a\nShapley value decomposition to assess how much each topic within the reports\ncontributes to explaining stock returns. The results show that analysts' income\nstatement analyses account for more than half of the reports' explanatory\npower. Expressing these findings in economic terms, I estimate that early\nacquisition of analysts' reports can yield significant profits. Analysts'\ninformation value peeks in the first week following earnings announcements,\nhighlighting their vital role in interpreting new financial data.",
      "generated_abstract": "This paper provides a comprehensive overview of the development of AI-based\nreinforcement learning (RL) models for trading, focusing on their integration\nwith advanced financial applications. By reviewing the existing literature, we\ndiscuss how AI-based trading models are being applied in the financial sector,\nhighlighting their advantages and limitations. Additionally, we examine the\nchallenges and opportunities that AI-based trading models face, including\nsafety, scalability, and efficiency. By providing a comprehensive overview of\nthe current state of the art, this paper aims to provide a foundation for future\nresearch and development in this area.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13186813186813187,
          "p": 0.19047619047619047,
          "f": 0.15584415100944526
        },
        "rouge-2": {
          "r": 0.032,
          "p": 0.047619047619047616,
          "f": 0.038277507154140855
        },
        "rouge-l": {
          "r": 0.12087912087912088,
          "p": 0.1746031746031746,
          "f": 0.14285713802243227
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.06413v1",
      "true_abstract": "Despite a plethora of anomaly detection models developed over the years,\ntheir ability to generalize to unseen anomalies remains an issue, particularly\nin critical systems. This paper aims to address this challenge by introducing\nSwift Hydra, a new framework for training an anomaly detection method based on\ngenerative AI and reinforcement learning (RL). Through featuring an RL policy\nthat operates on the latent variables of a generative model, the framework\nsynthesizes novel and diverse anomaly samples that are capable of bypassing a\ndetection model. These generated synthetic samples are, in turn, used to\naugment the detection model, further improving its ability to handle\nchallenging anomalies. Swift Hydra also incorporates Mamba models structured as\na Mixture of Experts (MoE) to enable scalable adaptation of the number of Mamba\nexperts based on data complexity, effectively capturing diverse feature\ndistributions without increasing the model's inference time. Empirical\nevaluations on ADBench benchmark demonstrate that Swift Hydra outperforms other\nstate-of-the-art anomaly detection models while maintaining a relatively short\ninference time. From these results, our research highlights a new and\nauspicious paradigm of integrating RL and generative AI for advancing anomaly\ndetection.",
      "generated_abstract": "We study the problem of learning sparse representations from data, where the\ninstrumental variable (IV) is sparse in the sense that its support is\nnon-zero with probability one. We show that the problem is equivalent to\nlearning a sparse kernel with a specific structure, and propose a\ncorresponding kernelized Sinkhorn algorithm. We demonstrate the effectiveness\nof the algorithm on synthetic and real-world datasets. Theoretical analysis\nshows that our algorithm is guaranteed to recover the true kernel when the\ndata is generated by a specific kernel distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13008130081300814,
          "p": 0.2909090909090909,
          "f": 0.17977527662858234
        },
        "rouge-2": {
          "r": 0.005714285714285714,
          "p": 0.012048192771084338,
          "f": 0.007751933620277681
        },
        "rouge-l": {
          "r": 0.10569105691056911,
          "p": 0.23636363636363636,
          "f": 0.14606741146004307
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.10718v1",
      "true_abstract": "The escalating challenges of managing vast sensor-generated data,\nparticularly in audio applications, necessitate innovative solutions. Current\nsystems face significant computational and storage demands, especially in\nreal-time applications like gunshot detection systems (GSDS), and the\nproliferation of edge sensors exacerbates these issues. This paper proposes a\ngroundbreaking approach with a near-sensor model tailored for intelligent\naudio-sensing frameworks. Utilizing a Fast Fourier Transform (FFT) module,\nconvolutional neural network (CNN) layers, and HyperDimensional Computing\n(HDC), our model excels in low-energy, rapid inference, and online learning. It\nis highly adaptable for efficient ASIC design implementation, offering superior\nenergy efficiency compared to conventional embedded CPUs or GPUs, and is\ncompatible with the trend of shrinking microphone sensor sizes. Comprehensive\nevaluations at both software and hardware levels underscore the model's\nefficacy. Software assessments through detailed ROC curve analysis revealed a\ndelicate balance between energy conservation and quality loss, achieving up to\n82.1% energy savings with only 1.39% quality loss. Hardware evaluations\nhighlight the model's commendable energy efficiency when implemented via ASIC\ndesign, especially with the Google Edge TPU, showcasing its superiority over\nprevalent embedded CPUs and GPUs.",
      "generated_abstract": "This paper introduces a novel audio-based approach to enhance human\ncommunication and social interaction by enhancing the quality of voice in\nvideo-based conversations. We propose the use of multimodal audio embedding\nmethods to enhance voice quality, with the objective of enhancing the\nperceived quality of the audio in video-based conversations. Our approach\nintroduces an embedding method that leverages a pre-trained audio model,\ncombined with a text-based speaker embeddings, to improve the quality of the\naudio in video-based conversations. Our approach has two main components: (1)\naudio embedding method and (2) speaker embeddings. Our audio embedding method\nuses pre-trained audio models to extract speaker-specific audio embeddings,\nwhich are combined with the text-based speaker embeddings. The resulting audio\nembedding model is then used to enhance the audio",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10273972602739725,
          "p": 0.23076923076923078,
          "f": 0.14218009052357325
        },
        "rouge-2": {
          "r": 0.016853932584269662,
          "p": 0.030612244897959183,
          "f": 0.021739125854863386
        },
        "rouge-l": {
          "r": 0.10273972602739725,
          "p": 0.23076923076923078,
          "f": 0.14218009052357325
        }
      }
    },
    {
      "paper_id": "cs.CE.q-fin/CP/2412.18174v1",
      "true_abstract": "Recent advancements have underscored the potential of large language model\n(LLM)-based agents in financial decision-making. Despite this progress, the\nfield currently encounters two main challenges: (1) the lack of a comprehensive\nLLM agent framework adaptable to a variety of financial tasks, and (2) the\nabsence of standardized benchmarks and consistent datasets for assessing agent\nperformance. To tackle these issues, we introduce \\textsc{InvestorBench}, the\nfirst benchmark specifically designed for evaluating LLM-based agents in\ndiverse financial decision-making contexts. InvestorBench enhances the\nversatility of LLM-enabled agents by providing a comprehensive suite of tasks\napplicable to different financial products, including single equities like\nstocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we\nassess the reasoning and decision-making capabilities of our agent framework\nusing thirteen different LLMs as backbone models, across various market\nenvironments and tasks. Furthermore, we have curated a diverse collection of\nopen-source, multi-modal datasets and developed a comprehensive suite of\nenvironments for financial decision-making. This establishes a highly\naccessible platform for evaluating financial agents' performance across various\nscenarios.",
      "generated_abstract": "This paper presents a novel framework for the design and evaluation of\nstrategies for the multi-agent systems (MAS) in the context of commodity\ntrading. The proposed framework leverages the notion of a MAS to design and\nevaluate the strategies, while maintaining the properties of the MAS. The\nframework also provides the methods for the evaluation of the strategies. The\nproposed framework has been applied to the commodity trading case study to\nidentify the best strategies for the trading of the commodities. The results\ndemonstrate the effectiveness of the proposed framework in designing and\nevaluating the strategies for the commodity trading.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09259259259259259,
          "p": 0.2127659574468085,
          "f": 0.12903225383891795
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.012345679012345678,
          "f": 0.008510633780355497
        },
        "rouge-l": {
          "r": 0.09259259259259259,
          "p": 0.2127659574468085,
          "f": 0.12903225383891795
        }
      }
    },
    {
      "paper_id": "econ.TH.cs/GT/2503.06582v1",
      "true_abstract": "The steady rise of e-commerce marketplaces underscores the need to study a\nmarket structure that captures the key features of this setting. To this end,\nwe consider a price-quantity Stackelberg duopoly in which the leader is the\nmarketplace operator and the follower is an independent seller. The objective\nof the marketplace operator is to maximize a weighted sum of profit and a term\ncapturing positive customer experience, whereas the independent seller solely\nseeks to maximize their own profit. Furthermore, the independent seller is\nrequired to share a fraction of their revenue with the marketplace operator for\nthe privilege of selling on the platform. We derive the subgame-perfect Nash\nequilibrium of this game and find that the equilibrium strategies depend on the\nassumed rationing rule. We then consider practical implications for marketplace\noperators. Finally, we show that, under intensity rationing, consumer surplus\nand total welfare in the duopoly marketplace is always at least as high as\nunder an independent seller monopoly, demonstrating that it is socially\nbeneficial for the operator to join the market as a seller.",
      "generated_abstract": "We study a model of two-sided auctions with a single buyer. The buyer is\nbuying a product with a discrete set of items. We analyze the conditions\nunder which a single item can be sold at the highest possible price. We show\nthat, under some conditions, the optimal price is the smallest price among all\npossible prices for a single item. In particular, the optimal price is the\nsum of the optimal prices for each of the items.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1485148514851485,
          "p": 0.3409090909090909,
          "f": 0.20689654749678962
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.09090909090909091,
          "f": 0.05263157483379533
        },
        "rouge-l": {
          "r": 0.12871287128712872,
          "p": 0.29545454545454547,
          "f": 0.17931034060023793
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2410.20915v1",
      "true_abstract": "In the literature on stochastic frontier models until the early 2000s, the\njoint consideration of spatial and temporal dimensions was often inadequately\naddressed, if not completely neglected. However, from an evolutionary economics\nperspective, the production process of the decision-making units constantly\nchanges over both dimensions: it is not stable over time due to managerial\nenhancements and/or internal or external shocks, and is influenced by the\nnearest territorial neighbours. This paper proposes an extension of the Fusco\nand Vidoli [2013] SEM-like approach, which globally accounts for spatial and\ntemporal effects in the term of inefficiency. In particular, coherently with\nthe stochastic panel frontier literature, two different versions of the model\nare proposed: the time-invariant and the time-varying spatial stochastic\nfrontier models. In order to evaluate the inferential properties of the\nproposed estimators, we first run Monte Carlo experiments and we then present\nthe results of an application to a set of commonly referenced data,\ndemonstrating robustness and stability of estimates across all scenarios.",
      "generated_abstract": "This paper proposes a novel method for constructing semiparametric prediction\ninfluence functions (PIFs) for dynamic time-series models. The proposed\nmethod is based on the multivariate bootstrap and employs a hierarchical\nregression model to incorporate the information across time series. The\nproposed method provides a simple and practical solution for constructing\nsemiparametric PIFs for dynamic time-series models and can be used to\nestimate the prediction influence function for the target time series. The\nproposed method is implemented in a Python package, which can be accessed at\nhttps://github.com/zhaoyuanqing/DTS_PIF_Python.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14655172413793102,
          "p": 0.3090909090909091,
          "f": 0.19883040499298935
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.027777777777777776,
          "f": 0.017699110702483638
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.2545454545454545,
          "f": 0.1637426856947438
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.10169v1",
      "true_abstract": "As part of work to connect phylogenetics with machine learning, there has\nbeen considerable recent interest in vector encodings of phylogenetic trees. We\npresent a simple new ``ordered leaf attachment'' (OLA) method for uniquely\nencoding a binary, rooted phylogenetic tree topology as an integer vector. OLA\nencoding and decoding take linear time in the number of leaf nodes, and the set\nof vectors corresponding to trees is a simply-described subset of integer\nsequences. The OLA encoding is unique compared to other existing encodings in\nhaving these properties. The integer vector encoding induces a distance on the\nset of trees, and we investigate this distance in relation to the NNI and SPR\ndistances.",
      "generated_abstract": "The discovery of the first protein-coding genes in 1975 by D.A. Page and\nF.M. Nickles was the beginning of a revolution in biology, paving the way for\ngenome sequencing, proteomics, and systems biology. Since then, genomic and\nproteomic data have been collected in ever-increasing numbers, with the\nsurpassing of 100 billion bases in the human genome (2021) and 3 billion\nproteins (2022) now being studied. These vast datasets have provided a\nfoundation for the development of computational methods to analyze the\nvarious types of data from genomics and proteomics, including the\ncomprehensive analysis of protein-coding genes and their expression levels in\ndifferent tissues.\n  However, there are still many challenges to overcome to enable a more\ncompre",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.12941176470588237,
          "f": 0.13664595774854388
        },
        "rouge-2": {
          "r": 0.009345794392523364,
          "p": 0.008620689655172414,
          "f": 0.008968604873617772
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.11764705882352941,
          "f": 0.12422359750009664
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.20550v1",
      "true_abstract": "Achaete-scute complex homolog 2 (ASCL2) codes a part of the basic\nhelix-loop-helix (BHLH) transcription factor family. WNTs have been found to\ndirectly affect the stemness of the tumor cells via regulation of ASCL2.\nSwitching off the ASCL2 literally blocks the stemness process of the tumor\ncells and vice versa. In colorectal cancer (CRC) cells treated with\nETC-1922159, ASCL2 was found to be down regulated along with other genes. A\nrecently developed search engine ranked combinations of ASCL2-X (X, a\nparticular gene/protein) at 2nd order level after drug administration. Some\nrankings confirm the already tested combinations, while others point to those\nthat are untested/unexplored. These rankings reveal which ASCL2-X combinations\nmight be working synergistically in CRC. In this research work, I cover\ncombinations of ASCL2 with WNT, transforming growth factor beta (TGFB),\ninterleukin (IL), leucine rich repeat containing G protein-coupled receptor\n(LGR), NOTCH, solute carrier family (SLC), SRY-box transcription factor (SOX),\nsmall nucleolar RNA host gene (SNHG), KIAA, F-box protein (FBXO), family with\nsequence similarity (FAM), B cell CLL/lymphoma (BCL), autophagy related (ATG)\nand Rho GTPase activating protein (ARHGAP) family.",
      "generated_abstract": "The molecular basis of cognition is not yet fully understood. This paper\naddresses this gap by proposing that the brain's ability to represent\ninformation is governed by the number of synaptic connections, an idea that\nhas been largely ignored in the literature. We show that this mechanism is\nfundamentally different from existing theories of memory and learning, which\noften assume that the brain's storage capacity is fixed. We provide an\nexperimental test of our theory and show that it is supported by both\nindividual and population-level data from brain imaging, behaviour, and\ncomputational models. We also show that the brain's storage capacity is\nnon-linearly related to the number of synapses, a phenomenon we call the\nmemory-to-capacity trade-off, and propose a theoretical framework for\nunderstanding this trade-off. The results of this study suggest that the\nbrain's storage",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07857142857142857,
          "p": 0.125,
          "f": 0.0964912233302557
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.11363636363636363,
          "f": 0.08771929350569432
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.stat/ML/2503.04454v1",
      "true_abstract": "By leveraging tools from the statistical mechanics of complex systems, in\nthese short notes we extend the architecture of a neural network for\nhetero-associative memory (called three-directional associative memories, TAM)\nto explore supervised and unsupervised learning protocols. In particular, by\nproviding entropic-heterogeneous datasets to its various layers, we predict and\nquantify a new emergent phenomenon -- that we term {\\em layer's\ncooperativeness} -- where the interplay of dataset entropies across network's\nlayers enhances their retrieval capabilities Beyond those they would have\nwithout reciprocal influence. Naively we would expect layers trained with less\ninformative datasets to develop smaller retrieval regions compared to those\npertaining to layers that experienced more information: this does not happen\nand all the retrieval regions settle to the same amplitude, allowing for\noptimal retrieval performance globally. This cooperative dynamics marks a\nsignificant advancement in understanding emergent computational capabilities\nwithin disordered systems.",
      "generated_abstract": "The stochastic block model (SBM) is a widely used model to study\nnetworks, where each node is associated with a random block structure. It has\nbeen extensively studied in the literature, and various types of models have\nbeen proposed. In this work, we focus on the random block model, which is\ncharacterized by a random block structure, but with blocks that are\nindistinguishable from each other. In this model, we introduce a novel method\nto estimate the block structure, which is based on the distribution of\nresidual eigenvectors. We apply this method to various datasets, including the\nLouvain-model, the Cora-dataset, the CiteSeer-dataset, and the PubMed-dataset,\nand show that our method provides a good estimate of the block structure in\nthese datasets. We also compare our method with the traditional Laplacian\neigenvalue estimation method, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.2222222222222222,
          "f": 0.18749999512207044
        },
        "rouge-2": {
          "r": 0.0070921985815602835,
          "p": 0.00819672131147541,
          "f": 0.007604557763741344
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.18518518518518517,
          "f": 0.15624999512207047
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.09806v1",
      "true_abstract": "Interdependencies between units in online two-sided marketplaces complicate\nestimating causal effects in experimental settings. We propose a novel\nexperimental design to mitigate the interference bias in estimating the total\naverage treatment effect (TATE) of item-side interventions in online two-sided\nmarketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the\nrecommender system as an instrument for experimentation. TSPR strategically\nprioritizes items based on their treatment status in the listings displayed to\nusers. We designed TSPR to provide users with a coherent platform experience by\nensuring access to all items and a consistent realization of their treatment by\nall users. We evaluate our experimental design through simulations using a\nsearch impression dataset from an online travel agency. Our methodology closely\nestimates the true simulated TATE, while a baseline item-side estimator\nsignificantly overestimates TATE.",
      "generated_abstract": "This paper provides a theoretical framework for estimating optimal\nincome tax rates in a heterogeneous-population, heterogeneous-preferences\nsetting. We establish that the optimal income tax rate is non-negative and\nsublinear in the tax rate parameter. We also show that the optimal tax rate is\nnon-negative and monotone in the pretax income parameter. We derive a\ncomputable formula for the optimal tax rate, which we apply to estimate\nsublinear-in-the-tax-rate optimal tax rates for the U.S. and several other\ncountries. We use this formula to estimate optimal tax rates for the U.S. and\nseveral other countries, and we compare the estimated optimal tax rates with\nthe theoretical optimal tax rates. We also estimate optimal tax rates for the\nU.S. and several other countries using a non-linear-in-the-tax-rate optimal\ntax rate formula. We compare the estimated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10989010989010989,
          "p": 0.18867924528301888,
          "f": 0.13888888423707577
        },
        "rouge-2": {
          "r": 0.016260162601626018,
          "p": 0.022988505747126436,
          "f": 0.01904761419455906
        },
        "rouge-l": {
          "r": 0.0989010989010989,
          "p": 0.16981132075471697,
          "f": 0.1249999953481869
        }
      }
    },
    {
      "paper_id": "math.ST.econ/EM/2412.18080v1",
      "true_abstract": "There are many nonparametric objects of interest that are a function of a\nconditional distribution. One important example is an average treatment effect\nconditional on a subset of covariates. Many of these objects have a conditional\ninfluence function that generalizes the classical influence function of a\nfunctional of a (unconditional) distribution. Conditional influence functions\nhave important uses analogous to those of the classical influence function.\nThey can be used to construct Neyman orthogonal estimating equations for\nconditional objects of interest that depend on high dimensional regressions.\nThey can be used to formulate local policy effects and describe the effect of\nlocal misspecification on conditional objects of interest. We derive\nconditional influence functions for functionals of conditional means and other\nfeatures of the conditional distribution of an outcome variable. We show how\nthese can be used for locally linear estimation of conditional objects of\ninterest. We give rate conditions for first step machine learners to have no\neffect on asymptotic distributions of locally linear estimators. We also give a\ngeneral construction of Neyman orthogonal estimating equations for conditional\nobjects of interest.",
      "generated_abstract": "This paper presents a novel method for estimating the parameters of the\nKalman filter using a large-scale Bayesian inference approach. We derive the\nlikelihood function of the Kalman filter and develop a Bayesian inference\nframework that is applicable to arbitrary state-space models. We show that the\nlikelihood function is a polynomial in the state variable. As a result, our\nmethod is able to estimate the parameters of the Kalman filter for any state\nspace model, even if the likelihood function is not analytically available. We\nalso demonstrate the performance of the proposed method through numerical\nsimulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1744186046511628,
          "p": 0.2631578947368421,
          "f": 0.20979020499584342
        },
        "rouge-2": {
          "r": 0.03546099290780142,
          "p": 0.0625,
          "f": 0.04524886415921098
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.22807017543859648,
          "f": 0.18181817702381547
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.01910v1",
      "true_abstract": "The development of therapeutic antibodies heavily relies on accurate\npredictions of how antigens will interact with antibodies. Existing\ncomputational methods in antibody design often overlook crucial conformational\nchanges that antigens undergo during the binding process, significantly\nimpacting the reliability of the resulting antibodies. To bridge this gap, we\nintroduce dyAb, a flexible framework that incorporates AlphaFold2-driven\npredictions to model pre-binding antigen structures and specifically addresses\nthe dynamic nature of antigen conformation changes. Our dyAb model leverages a\nunique combination of coarse-grained interface alignment and fine-grained flow\nmatching techniques to simulate the interaction dynamics and structural\nevolution of the antigen-antibody complex, providing a realistic representation\nof the binding process. Extensive experiments show that dyAb significantly\noutperforms existing models in antibody design involving changing antigen\nconformations. These results highlight dyAb's potential to streamline the\ndesign process for therapeutic antibodies, promising more efficient development\ncycles and improved outcomes in clinical applications.",
      "generated_abstract": "This paper presents a novel approach to understanding the role of\ncortical neurons in the medial temporal lobe (MTL), using a combined\ncomputational-pharmacological approach. We developed a novel computational\nmodel, which was trained on simulated MTL data from a large-scale,\nmulti-site, clinical cohort study. The model was then validated in\nsimulated data, and subsequently applied to real data from 35 MTL patients\ncollected from the MTL-Brain Project. The model was able to accurately\nreconstruct the distribution of spike trains in the MTL, and accurately\nreproduce the temporal dynamics of spike trains, including the emergence of\nspike trains that were not observed in the simulated data. These findings\ndemonstrate the potential of the model to capture the temporal dynamics of\nspike trains in MTLs, and provide a foundation for future",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14953271028037382,
          "p": 0.20512820512820512,
          "f": 0.17297296809583654
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.008849557522123894,
          "f": 0.007843132319572501
        },
        "rouge-l": {
          "r": 0.14018691588785046,
          "p": 0.19230769230769232,
          "f": 0.1621621572850257
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.00158v1",
      "true_abstract": "Systemic risk is receiving increasing attention in the insurance industry, as\nthese risks can have severe impacts on the entire financial system. In this\npaper, we propose a multi-dimensional L/'{e}vy process-based renewal risk model\nwith heterogeneous insurance claims, where every dimension indicates a business\nline of an insurer. We use the systemic expected shortfall (SES) and marginal\nexpected shortfall (MES) defined with a Value-at-Risk (VaR) target level as the\nmeasurement of systemic risks. Assuming that all the claim sizes are pairwise\nasymptotically independent (PAI), we derive asymptotic formulas for the tail\nprobabilities of discounted aggregate claims and total loss, which holds\nuniformly for all time horizons. We further obtain the asymptotics of the above\nsystemic risk measures. The main technical issues involve the treatment of\nuniform convergence in the dynamic time setting. Finally, we conduct a Monte\nCarlo numerical study and verify that our asymptotics are accurate and\nconvenient in computation.",
      "generated_abstract": "This paper studies the joint asymptotic distribution of the portfolio\ndistribution and the mean return of the mean-variance portfolio with\ndiscrete-time processes of dividends and the mean return, in the mean-variance\nportfolio optimization framework. It is shown that the joint asymptotic\ndistribution of the portfolio distribution and the mean return converges to\nthe same limiting distribution in the mean-variance portfolio optimization\nframework. The asymptotic limiting distribution is a Gaussian mixture, and the\nmean return is asymptotically distributed as a Gaussian variable. The\ndistribution of the portfolio distribution and the mean return can be\nsimplified further by the concentration of the joint distribution of the\ndiscrete-time dividend and the mean return.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12612612612612611,
          "p": 0.3181818181818182,
          "f": 0.18064515722455785
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.029411764705882353,
          "f": 0.01851851420439058
        },
        "rouge-l": {
          "r": 0.11711711711711711,
          "p": 0.29545454545454547,
          "f": 0.16774193141810625
        }
      }
    },
    {
      "paper_id": "cs.AI.q-bio/NC/2502.18725v1",
      "true_abstract": "Traditional psychological experiments utilizing naturalistic stimuli face\nchallenges in manual annotation and ecological validity. To address this, we\nintroduce a novel paradigm leveraging multimodal large language models (LLMs)\nas proxies to extract rich semantic information from naturalistic images\nthrough a Visual Question Answering (VQA) strategy for analyzing human visual\nsemantic representation. LLM-derived representations successfully predict\nestablished neural activity patterns measured by fMRI (e.g., faces, buildings),\nvalidating its feasibility and revealing hierarchical semantic organization\nacross cortical regions. A brain semantic network constructed from LLM-derived\nrepresentations identifies meaningful clusters reflecting functional and\ncontextual associations. This innovative methodology offers a powerful solution\nfor investigating brain semantic organization with naturalistic stimuli,\novercoming limitations of traditional annotation methods and paving the way for\nmore ecologically valid explorations of human cognition.",
      "generated_abstract": "In this paper, we present a novel framework for the development of\nresearch problems in neuroscience and cognitive neuroscience (NCN) that\nemphasize the use of deep learning and reinforcement learning (RL). The\nframework is based on the observation that, in NCN, the research problem is\ntypically multidisciplinary, involving data acquisition, analysis, and\nmodeling. While some researchers have successfully employed deep learning for\ntheir own research problems, the process of developing a research problem in\nNCN often requires expertise across multiple disciplines, and the need for\ndeep learning-based methods to address the multidisciplinary nature of NCN\nproblems is becoming increasingly evident. In order to bridge this gap, we\nintroduce the Deep NCN Framework (DNF), which allows researchers to identify\nresearch problems, design research methods, and develop RL models within a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14150943396226415,
          "p": 0.17647058823529413,
          "f": 0.1570680578876677
        },
        "rouge-2": {
          "r": 0.016260162601626018,
          "p": 0.016,
          "f": 0.016129027258391247
        },
        "rouge-l": {
          "r": 0.11320754716981132,
          "p": 0.1411764705882353,
          "f": 0.12565444532222272
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SY/2503.04637v1",
      "true_abstract": "Sensing is emerging as a vital future service in next-generation wireless\nnetworks, enabling applications such as object localization and activity\nrecognition. The IEEE 802.11bf standard extends Wi-Fi capabilities to\nincorporate these sensing functionalities. However, coexistence with legacy\nWi-Fi in densely populated networks poses challenges, as contention for\nchannels can impair both sensing and communication quality. This paper develops\nan analytical framework and a system-level simulation in ns-3 to evaluate the\ncoexistence of IEEE 802.11bf and legacy 802.11ax in terms of sensing delay and\ncommunication throughput. Forthis purpose, we have developed a dedicated ns-3\nmodule forIEEE 802.11bf, which is made publicly available as open-source. We\nprovide the first coexistence analysis between IEEE 802.11bfand IEEE 802.11ax,\nsupported by link-level simulation in ns-3to assess the impact on sensing delay\nand network performance. Key parameters, including sensing intervals, access\ncategories, network densities, and antenna configurations, are systematically\nanalyzed to understand their influence on the sensing delay and aggregated\nnetwork throughput. The evaluation is further extended to a realistic indoor\noffice environment modeled after the 3GPP TR 38.901 standard. Our findings\nreveal key trade-offs between sensing intervals and throughput and the need for\nbalanced sensing parameters to ensure effective coexistence in Wi-Fi networks.",
      "generated_abstract": "We present a novel method to optimize the trade-off between the robustness and\nthe delay of a distributed communication system in the presence of a\nsub-optimal receiver. The robustness of the communication system is measured\nthrough its average delay-Doppler spectra and the delay of the system is\ndetermined through its Doppler-delay spectra. The system is described by a\nnetwork of $M$ nodes and $N$ antennas. The robustness and delay are optimized\nwith respect to the system parameters such as the number of antennas and the\nnumber of nodes. The proposed method consists of two steps. First, we derive\nthe optimal system parameters under the constraint of robustness and delay.\nSecond, we solve a linear programming problem to find the optimal number of\nantennas and the number of nodes for the optimal system parameters. The\nproposed method is evaluated through simulations and experiments. The results\nshow that the proposed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15441176470588236,
          "p": 0.30434782608695654,
          "f": 0.20487804431457476
        },
        "rouge-2": {
          "r": 0.010309278350515464,
          "p": 0.017241379310344827,
          "f": 0.012903221122998578
        },
        "rouge-l": {
          "r": 0.15441176470588236,
          "p": 0.30434782608695654,
          "f": 0.20487804431457476
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.12219v1",
      "true_abstract": "Optimizing chemical properties is a challenging task due to the vastness and\ncomplexity of chemical space. Here, we present a generative energy-based\narchitecture for implicit chemical property optimization, designed to\nefficiently generate molecules that satisfy target properties without explicit\nconditional generation. We use Graph Energy Based Models and a training\napproach that does not require property labels. We validated our approach on\nwell-established chemical benchmarks, showing superior results to\nstate-of-the-art methods and demonstrating robustness and efficiency towards de\nnovo drug design.",
      "generated_abstract": "The evolution of proteins has been studied for many decades, but the\nextreme complexity of the proteome makes it difficult to predict evolutionary\ntrajectories. Here, we present a new approach, Protein Tree, to study protein\nevolution. Protein Tree is a graph-based framework that integrates\ncomputational modeling and evolutionary theory to predict protein evolutionary\ntrajectories. The model is based on a stochastic process that generates\nprotein trees based on evolutionary scenarios, such as random mutation and\nselection. This process can be iteratively repeated to explore different\nevolutionary scenarios, which can be used to explore the evolutionary\nlandscape of proteins. We demonstrate the effectiveness of Protein Tree by\nstudying a broad range of protein evolutionary scenarios, including random\nmutation, natural selection, and adaptive evolution. By integrating\ncomputational modeling with evolutionary theory, Protein Tree provides",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21212121212121213,
          "p": 0.17073170731707318,
          "f": 0.18918918424762612
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.04201680672268908,
          "f": 0.050251251473448104
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.17073170731707318,
          "f": 0.18918918424762612
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.06092v1",
      "true_abstract": "The popular choice of using a $direct$ forecasting scheme implies that the\nindividual predictions do not contain information on cross-horizon dependence.\nHowever, this dependence is needed if the forecaster has to construct, based on\n$direct$ density forecasts, predictive objects that are functions of several\nhorizons ($e.g.$ when constructing annual-average growth rates from\nquarter-on-quarter growth rates). To address this issue we propose to use\ncopulas to combine the individual $h$-step-ahead predictive distributions into\na joint predictive distribution. Our method is particularly appealing to\npractitioners for whom changing the $direct$ forecasting specification is too\ncostly. In a Monte Carlo study, we demonstrate that our approach leads to a\nbetter approximation of the true density than an approach that ignores the\npotential dependence. We show the superior performance of our method in several\nempirical examples, where we construct (i) quarterly forecasts using\nmonth-on-month $direct$ forecasts, (ii) annual-average forecasts using monthly\nyear-on-year $direct$ forecasts, and (iii) annual-average forecasts using\nquarter-on-quarter $direct$ forecasts.",
      "generated_abstract": "This paper introduces a new framework for the estimation of quantile-based\nmodels, where the dependence structure of the data is described by a quantile\nfunction. The proposed methodology is applicable to both discrete and continuous\nquantile functions. We establish the consistency and asymptotic normality of the\nestimator of the quantile function, which is based on the Cram\\'er-von Mises\n(CVM) estimator. We also derive the asymptotic distribution of the\nquantile-based estimator, which is based on the generalized Cram\\'er-von Mises\n(GCVM) estimator. The proposed estimators are applied to empirical data from\neight different quantile functions. The results suggest that the quantile\nfunction estimates can be used to quantify the uncertainty of the quantile\nfunction estimates. This uncertainty can be useful for analyzing the\nuncertainty of quantile function estimates, as well as for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17757009345794392,
          "p": 0.2676056338028169,
          "f": 0.21348314127193543
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.027522935779816515,
          "f": 0.02290075849979708
        },
        "rouge-l": {
          "r": 0.17757009345794392,
          "p": 0.2676056338028169,
          "f": 0.21348314127193543
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2411.13965v1",
      "true_abstract": "Universal power laws have been scrutinised in physics and beyond, and a\nlong-standing debate exists in econophysics regarding the strict universality\nof the nonlinear price impact, commonly referred to as the square-root law\n(SRL). The SRL posits that the average price impact $I$ follows a power law\nwith respect to transaction volume $Q$, such that $I(Q) \\propto Q^{\\delta}$\nwith $\\delta \\approx 1/2$. Some researchers argue that the exponent $\\delta$\nshould be system-specific, without universality. Conversely, others contend\nthat $\\delta$ should be exactly $1/2$ for all stocks across all countries,\nimplying universality. However, resolving this debate requires high-precision\nmeasurements of $\\delta$ with errors of around $0.1$ across hundreds of stocks,\nwhich has been extremely challenging due to the scarcity of large microscopic\ndatasets -- those that enable tracking the trading behaviour of all individual\naccounts. Here we conclusively support the universality hypothesis of the SRL\nby a complete survey of all trading accounts for all liquid stocks on the Tokyo\nStock Exchange (TSE) over eight years. Using this comprehensive microscopic\ndataset, we show that the exponent $\\delta$ is equal to $1/2$ within\nstatistical errors at both the individual stock level and the individual trader\nlevel. Additionally, we rejected two prominent models supporting the\nnonuniversality hypothesis: the Gabaix-Gopikrishnan-Plerou-Stanley and the\nFarmer-Gerig-Lillo-Waelbroeck models. Our work provides exceptionally\nhigh-precision evidence for the universality hypothesis in social science and\ncould prove useful in evaluating the price impact by large investors -- an\nimportant topic even among practitioners.",
      "generated_abstract": "We propose a novel method for pricing Asian options using a continuous\nmodel for the underlying asset, and we compare it with a discrete-time model.\nThe continuous model is derived as a numerical approximation of the\ntime-varying volatility, and we show that it approximates the logarithmic\nprice ratio well. We also propose a numerical method for solving the\ncontinua-model pricing problem. We show that the discrete-time model gives\nincorrect results for the pricing of European options, whereas the continuous-time\nmodel gives correct results. We use the model to price Asian options with\nmaturities of one and five years, and we find that the continuous-time model\noffers significant advantages over the discrete-time model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09554140127388536,
          "p": 0.25,
          "f": 0.13824884392533301
        },
        "rouge-2": {
          "r": 0.021929824561403508,
          "p": 0.05263157894736842,
          "f": 0.030959748169732845
        },
        "rouge-l": {
          "r": 0.08280254777070063,
          "p": 0.21666666666666667,
          "f": 0.11981566420183071
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.07369v1",
      "true_abstract": "Skeletonization extracts thin representations from images that compactly\nencode their geometry and topology. These representations have become an\nimportant topological prior for preserving connectivity in curvilinear\nstructures, aiding medical tasks like vessel segmentation. Existing compatible\nskeletonization algorithms face significant trade-offs: morphology-based\napproaches are computationally efficient but prone to frequent breakages, while\ntopology-preserving methods require substantial computational resources.\n  We propose a novel framework for training iterative skeletonization\nalgorithms with a learnable component. The framework leverages synthetic data,\ntask-specific augmentation, and a model distillation strategy to learn compact\nneural networks that produce thin, connected skeletons with a fully\ndifferentiable iterative algorithm.\n  Our method demonstrates a 100 times speedup over topology-constrained\nalgorithms while maintaining high accuracy and generalizing effectively to new\ndomains without fine-tuning. Benchmarking and downstream validation in 2D and\n3D tasks demonstrate its computational efficiency and real-world applicability",
      "generated_abstract": "We propose a novel deep learning-based framework for segmenting and\npredicting brain lesions in CT scans. The framework is based on a novel\ndetection module that simultaneously detects and labels lesion boundaries and\npredicts their sizes. It consists of two components: a convolutional neural\nnetwork (CNN) that performs 3D convolutions on the CT image and a neural\nnetwork that predicts lesion sizes. The detection module is trained on a\npre-trained lesion detector, and the prediction module is trained on a\npre-trained lesion size predictor. We demonstrate that our framework outperforms\nstate-of-the-art segmentation and prediction methods on the DRIVE-Brain\nbenchmark and the CINECA dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1415929203539823,
          "p": 0.26229508196721313,
          "f": 0.18390804142356995
        },
        "rouge-2": {
          "r": 0.04477611940298507,
          "p": 0.06666666666666667,
          "f": 0.05357142376434992
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.2459016393442623,
          "f": 0.17241378855000672
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/MS/2503.06010v1",
      "true_abstract": "In this paper, we propose the InfoFusion Controller, an advanced path\nplanning algorithm that integrates both global and local planning strategies to\nenhance autonomous driving in complex urban environments. The global planner\nutilizes the informed Theta-Rapidly-exploring Random Tree Star (Informed-TRRT*)\nalgorithm to generate an optimal reference path, while the local planner\ncombines Model Predictive Control (MPC) and Pure Pursuit algorithms. Mutual\nInformation (MI) is employed to fuse the outputs of the MPC and Pure Pursuit\ncontrollers, effectively balancing their strengths and compensating for their\nweaknesses. The proposed method addresses the challenges of navigating in\ndynamic environments with unpredictable obstacles by reducing uncertainty in\nlocal path planning and improving dynamic obstacle avoidance capabilities.\nExperimental results demonstrate that the InfoFusion Controller outperforms\ntraditional methods in terms of safety, stability, and efficiency across\nvarious scenarios, including complex maps generated using SLAM techniques.\n  The code for the InfoFusion Controller is available at https:\n//github.com/DrawingProcess/InfoFusionController.",
      "generated_abstract": "In this paper, we present a real-time, robust and accurate multi-robot\nassisted mapping and navigation system for indoor environments, where\ncooperative navigation among multiple robots is required. We consider the\nenvironment to be a graph and the robots to be nodes, where the robots are\nassigned to different tasks. In order to accomplish the navigation task, we\npropose to use a hybrid multi-robot approach that includes a localization\nmodule, a mapping module and a navigation module. The localization module\nutilizes the robot odometry and the visual information to estimate the robot\nposition and orientation, while the mapping module utilizes the robot odometry,\nthe visual information and the robot position to compute the robot position and\norientation. The navigation module utilizes the robot odometry, the robot\nposition and the robot orientation to compute the robot's next position. The\nlocalization module and the mapping module are trained",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.2153846153846154,
          "f": 0.1609195355496104
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.05504587155963303,
          "f": 0.047430825135215876
        },
        "rouge-l": {
          "r": 0.12844036697247707,
          "p": 0.2153846153846154,
          "f": 0.1609195355496104
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.08179v3",
      "true_abstract": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
      "generated_abstract": "This paper presents a novel approach to integrate and analyze\nneural activity and structural information from single-cell RNA-sequencing\n(scRNA-seq) data. We introduce the concept of the Integrated Neural\nSignatures (INS) to represent the complex biological signals derived from the\ncell-type-specific neural activity and cell-type-specific gene expression\npatterns, with the aim of improving the interpretation of scRNA-seq data. Our\napproach incorporates INS into a deep learning framework to enhance the\naccuracy of cell-type-specific gene expression prediction. Furthermore, we\nintroduce a novel approach to integrate the cell-type-specific neural activity\nand the gene expression patterns derived from scRNA-seq data. This novel\napproach employs a neural network to predict the cell-type-specific gene\nexpression patterns from the neural activity patterns. We demonstrate the\neffect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12582781456953643,
          "p": 0.31666666666666665,
          "f": 0.18009478265986847
        },
        "rouge-2": {
          "r": 0.03365384615384615,
          "p": 0.07291666666666667,
          "f": 0.046052627257618134
        },
        "rouge-l": {
          "r": 0.12582781456953643,
          "p": 0.31666666666666665,
          "f": 0.18009478265986847
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2409.04903v2",
      "true_abstract": "In this paper, we propose a semi-analytical approach to pricing options on\nSOFR futures where the underlying SOFR follows a time-dependent CEV model. By\ndefinition, these options change their type at the beginning of the reference\nperiod: before this time, this is an American option written on a SOFR forward\nprice as an underlying, and after this point, this is an arithmetic Asian\noption with an American style exercise written on the daily SOFR rates. We\ndevelop a new version of the GIT method and solve both problems\nsemi-analytically, obtaining the option price, the exercise boundary, and the\noption Greeks. This work is intended to address the concern that the transfer\nfrom LIBOR to SOFR has resulted in a situation in which the options of the key\nmoney market (i.e., futures on the reference rate) are options without any\npricing model available. Therefore, the trading in options on 3M SOFR futures\ncurrently ends before their reference quarter starts, to eliminate the final\nmetamorphosis into exotic options.",
      "generated_abstract": "This paper examines the impact of the ongoing COVID-19 pandemic on\neconomic activity in China and the United States. We find that the pandemic\nhas had a significant negative impact on both economies, with negative effects\non GDP growth in both countries. However, the impact is different between the\ntwo countries. In China, the pandemic led to a sharp decline in economic activity\nand GDP growth, while in the United States, the pandemic had a muted impact on\neconomic activity and GDP growth. The pandemic also led to significant\nuncertainty and volatility in both countries' economies, with significant\neconomic disruptions and market volatility. In China, the pandemic led to\nsignificant economic disruptions and market volatility, while in the United\nStates, the pandemic had a muted impact on economic disruptions and market",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2909090909090909,
          "f": 0.2012578571100828
        },
        "rouge-2": {
          "r": 0.019230769230769232,
          "p": 0.03333333333333333,
          "f": 0.024390239262344723
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.2909090909090909,
          "f": 0.2012578571100828
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.07573v1",
      "true_abstract": "We prove an inversion formula for the exterior $k$-plane transform. As a\nconsequence, we show that if $m < k$ then an $m$-current in $\\mathbf R^n$ can\nbe reconstructed from its projections onto $\\mathbf R^k$, which proves a\nconjecture of Solomon.",
      "generated_abstract": "We prove that for a compact K\\\"ahler manifold $X$ with K\\\"ahler metric\n$g$, the second Chern class $c_2(X)$ has a non-trivial intersection with the\nColeman class $\\mathcal{C}_X$, provided that $g$ is an even Euclidean metric.\nThis implies that the second Chern class is an isomorphism to $\\mathcal{C}_X$\nfor $X$ compact and simply connected, which was conjectured by Coleman in 1970\nand proved by Hitchin in 1983. This is in contrast to the case of a compact\nRiemannian manifold $M$, where the second Chern class is an isomorphism to the\nColeman class for any non-positive metric.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2631578947368421,
          "p": 0.18867924528301888,
          "f": 0.21978021491607302
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.012658227848101266,
          "f": 0.01680671822611516
        },
        "rouge-l": {
          "r": 0.2631578947368421,
          "p": 0.18867924528301888,
          "f": 0.21978021491607302
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10594v1",
      "true_abstract": "The structural analogies of ResNets and Multigrid (MG) methods such as common\nbuilding blocks like convolutions and poolings where already pointed out by He\net al.\\ in 2016. Multigrid methods are used in the context of scientific\ncomputing for solving large sparse linear systems arising from partial\ndifferential equations. MG methods particularly rely on two main concepts:\nsmoothing and residual restriction / coarsening. Exploiting these analogies, He\nand Xu developed the MgNet framework, which integrates MG schemes into the\ndesign of ResNets. In this work, we introduce a novel neural network building\nblock inspired by polynomial smoothers from MG theory. Our polynomial block\nfrom an MG perspective naturally extends the MgNet framework to Poly-Mgnet and\nat the same time reduces the number of weights in MgNet. We present a\ncomprehensive study of our polynomial block, analyzing the choice of initial\ncoefficients, the polynomial degree, the placement of activation functions, as\nwell as of batch normalizations. Our results demonstrate that constructing\n(quadratic) polynomial building blocks based on real and imaginary polynomial\nroots enhances Poly-MgNet's capacity in terms of accuracy. Furthermore, our\napproach achieves an improved trade-off of model accuracy and number of weights\ncompared to ResNet as well as compared to specific configurations of MgNet.",
      "generated_abstract": "We study the problem of generating a valid randomized solution for a\ngeneralized max-cut problem (GMC) using a polynomial number of rounds. We show\nthat this problem is NP-hard. We further propose a novel solution for GMC,\ncalled the ``GMC-solution'', that runs in polynomial time using only $O(n^4)$\nrounds. The GMC-solution is a randomized solution for GMC that is\n$(1/2+\\varepsilon)$-approximate for all $\\varepsilon > 0$. Moreover, we show that\nGMC-solution is also $(1/2-\\varepsilon)$-approximate for $\\varepsilon = 0$.\nAdditionally, we show that the GMC-solution is $(1/2+\\varepsilon)$-approximate for\nall $\\varepsilon > 0$ with high probability. Finally, we show that the GMC-solution\nis $(1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10869565217391304,
          "p": 0.2830188679245283,
          "f": 0.15706805881746672
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.024691358024691357,
          "f": 0.014440429074796896
        },
        "rouge-l": {
          "r": 0.10144927536231885,
          "p": 0.2641509433962264,
          "f": 0.14659685462898506
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.physics/ao-ph/2503.07466v1",
      "true_abstract": "Supercell thunderstorms are the most hazardous thunderstorm category and\nparticularly impactful to society. Their monitoring is challenging and often\nconfined to the radar networks of single countries. By exploiting\nkilometer-scale climate simulations, a first-of-its-kind characterization of\nsupercell occurrence in Europe is derived for the current and a warmer climate.\nDespite previous notions of supercells being uncommon in Europe, the model\nshows ~700 supercells per convective season. Occurrence peaks are co-located\nwith complex topography e.g. the Alps. The absolute frequency maximum lies\nalong the southern Alps with minima over the oceans and flat areas. Contrasting\na current-climate simulation with a pseudo-global-warming +3$^\\circ$C global\nwarming scenario, the future climate simulation shows an average increase of\nsupercell occurrence by 11 %. However, there is a spatial dipole of change with\nstrong increases in supercell frequencies in central and eastern Europe and a\ndecrease in frequency over the Iberian Peninsula and southwestern France.",
      "generated_abstract": "This study examines the role of aerosol size distributions in influencing\nthe atmospheric distribution of aerosol-radiation interactions. By using a\ntwo-dimensional radiative transfer model coupled with aerosol size\ndistributions, we investigate how the aerosol size distribution impacts\nradiative forcing and cloud water content in a mesoscale convective system. The\nresults show that aerosol size distributions can significantly influence\nradiative forcing and cloud water content, particularly in regions with\nlarge-scale atmospheric circulations. When considering the effect of\naerosol-radiation interactions, we find that the influence of aerosol size\ndistributions is particularly pronounced in regions where the aerosol size\ndistributions are steep, such as the western United States and western China.\nThese findings provide new insights into the impact of aerosol size distributions\non atmospheric dynamics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.17567567567567569,
          "f": 0.14606741087236477
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.17567567567567569,
          "f": 0.14606741087236477
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08028v1",
      "true_abstract": "Denoising diffusions provide a general strategy to sample from a probability\ndistribution $\\mu$ in $\\mathbb{R}^d$ by constructing a stochastic process\n$(\\hat{\\boldsymbol x}_t:t\\ge 0)$ in ${\\mathbb R}^d$ such that the distribution\nof $\\hat{\\boldsymbol x}_T$ at large times $T$ approximates $\\mu$. The drift\n${\\boldsymbol m}:{\\mathbb R}^d\\times{\\mathbb R}\\to{\\mathbb R}^d$ of this\ndiffusion process is learned from data (samples from $\\mu$) by minimizing the\nso-called score-matching objective. In order for the generating process to be\nefficient, it must be possible to evaluate (an approximation of) ${\\boldsymbol\nm}({\\boldsymbol y},t)$ in polynomial time.\n  Is every probability distribution $\\mu$, for which sampling is tractable,\nalso amenable to sampling via diffusions? We provide evidence to the contrary\nby constructing a probability distribution $\\mu$ for which sampling is easy,\nbut the drift of the diffusion process is intractable -- under a popular\nconjecture on information-computation gaps in statistical estimation. We\nfurther show that any polynomial-time computable drift can be modified in a way\nthat changes minimally the score matching objective and yet results in\nincorrect sampling.",
      "generated_abstract": "This paper develops a new method for estimating the optimal transport\ndistance between two probability measures in the case where the two measures\nhave the same mass but differ in their support. We prove that this optimal\ntransport distance can be approximated by a sum of $n$ Gaussians. We then\nintroduce a modified expectation of the optimal transport distance and prove\nthat this modified expectation can be approximated by a sum of $n$ Gaussians\nas well. This new approximation allows us to obtain a faster convergence rate\nof our method than that of the original method. We also demonstrate that our\nmethod can be used for the estimation of the optimal transport distance between\ntwo non-gaussian measures.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1743119266055046,
          "p": 0.3114754098360656,
          "f": 0.22352940716332192
        },
        "rouge-2": {
          "r": 0.01935483870967742,
          "p": 0.03296703296703297,
          "f": 0.02439023924086281
        },
        "rouge-l": {
          "r": 0.12844036697247707,
          "p": 0.22950819672131148,
          "f": 0.1647058777515572
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.09223v1",
      "true_abstract": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
      "generated_abstract": "The increasing prevalence of AI-based healthcare systems raises new challenges\nin data privacy, especially as AI-driven technologies increasingly capture,\nprocess, and analyze large-scale healthcare data. In this paper, we present\nthe first comprehensive survey of AI-based healthcare data privacy from a\npractical perspective. We focus on the key challenges and issues in the\ndevelopment, deployment, and use of AI-based healthcare data privacy\nsolutions. We also survey existing AI-based healthcare data privacy models\nand techniques, including data privacy models, data privacy technologies, and\ndata privacy frameworks. Additionally, we provide an overview of privacy\nregulations and their implications for AI-based healthcare data privacy,\nincluding the European General Data Protection Regulation (GDPR), the Health\nInsurance Portability and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1320754716981132,
          "p": 0.29577464788732394,
          "f": 0.18260869138412106
        },
        "rouge-2": {
          "r": 0.004672897196261682,
          "p": 0.010309278350515464,
          "f": 0.006430863874859411
        },
        "rouge-l": {
          "r": 0.11320754716981132,
          "p": 0.2535211267605634,
          "f": 0.156521734862382
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.10512v1",
      "true_abstract": "Blockchain technology has revolutionized financial markets by enabling\ndecentralized exchanges (DEXs) that operate without intermediaries. Uniswap V2,\na leading DEX, facilitates the rapid creation and trading of new tokens,\noffering high return potential but exposing investors to significant risks. In\nthis work, we analyze the financial impact of newly created tokens, assessing\ntheir market dynamics, profitability and liquidity manipulations. Our findings\nreveal that a significant portion of market liquidity is trapped in honeypots,\nreducing market efficiency and misleading investors. Applying a simple\nbuy-and-hold strategy, we are able to uncover some major risks associated with\ninvesting in newly created tokens, including the widespread presence of rug\npulls and sandwich attacks. We extract the optimal sandwich amount, revealing\nthat their proliferation in new tokens stems from higher profitability in\nlow-liquidity pools. Furthermore, we analyze the fundamental differences\nbetween token price evolution in swap time and physical time. Using clustering\ntechniques, we highlight these differences and identify typical patterns of\nhoneypot and sellable tokens. Our study provides insights into the risks and\nfinancial dynamics of decentralized markets and their challenges for investors.",
      "generated_abstract": "The study of financial markets and asset prices is a crucial area of\nstudy in mathematics, economics, finance, and other fields. This paper\nintroduces a novel framework for studying asset prices based on the\ngeneralized Ornstein-Uhlenbeck process (OUP). The OUP is a stochastic process\nthat describes the evolution of the price of a financial asset. The model is\ncaptured by a stochastic differential equation (SDE), which allows for the\nanalysis of the market dynamics. The proposed framework offers a more\ncompact and intuitive approach to analyzing asset prices than traditional\napproaches, such as the Black-Scholes model. The model is shown to exhibit\nfeatures such as jumps, volatility, and stochastic behavior, which are not\npresent in the Black-Scholes model. Additionally, the model is shown to be\nwell-posed and has a unique solution. The performance of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15447154471544716,
          "p": 0.24050632911392406,
          "f": 0.18811880711841988
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.016666666666666666,
          "f": 0.01360543734555218
        },
        "rouge-l": {
          "r": 0.13008130081300814,
          "p": 0.20253164556962025,
          "f": 0.1584158368213902
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.08653v1",
      "true_abstract": "National forest inventory (NFI) data are often costly to collect, which\ninhibits efforts to estimate parameters of interest for small spatial,\ntemporal, or biophysical domains. Traditionally, design-based estimators are\nused to estimate status of forest parameters of interest, but are unreliable\nfor small areas where data are sparse. Additionally, design-based estimates\nconstructed directly from the survey data are often unavailable when sample\nsizes are especially small. Traditional model-based small area estimation\napproaches, such as the Fay-Herriot (FH) model, rely on these direct estimates\nfor inference; hence, missing direct estimates preclude the use of such\napproaches. Here, we detail a Bayesian spatio-temporal small area estimation\nmodel that efficiently leverages sparse NFI data to estimate status and trends\nfor forest parameters. The proposed model bypasses the use of direct estimates\nand instead uses plot-level NFI measurements along with auxiliary data\nincluding remotely sensed tree canopy cover. We produce forest carbon estimates\nfrom the United States NFI over 14 years across the contiguous US (CONUS) and\nconduct a simulation study to assess our proposed model's accuracy, precision,\nand bias, compared to that of a design-based estimator. The proposed model\nprovides improved precision and accuracy over traditional estimation methods,\nand provides useful insights into county-level forest carbon dynamics across\nthe CONUS.",
      "generated_abstract": "We consider the problem of predicting the location of a target in a large-scale\ndistribution. In this paper, we propose a novel approach for this problem using\na multiscale model that incorporates both the local and global dependencies\nbetween the target and its surroundings. The multiscale model is a hierarchical\nmodel, with a hierarchical structure based on the spatial resolution and the\nspatial correlation. In this hierarchical structure, we introduce two\ndifferent multiscale models, one for the local and the other for the global\ndependencies. The local multiscale model is a multivariate autoregressive\nmodel with a flexible dependence structure. The global multiscale model is a\nmultivariate autoregressive model with a general dependence structure. In this\npaper, we propose a novel method for constructing the multiscale model. We\nexplain the algorithm and demonstrate its effectiveness in a simulation study",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11450381679389313,
          "p": 0.234375,
          "f": 0.15384614943642352
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.037383177570093455,
          "f": 0.027303749629699444
        },
        "rouge-l": {
          "r": 0.11450381679389313,
          "p": 0.234375,
          "f": 0.15384614943642352
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08930v1",
      "true_abstract": "We consider the problem of optimizing neural implicit surfaces for 3D\nreconstruction using acoustic images collected with drifting sensor poses. The\naccuracy of current state-of-the-art 3D acoustic modeling algorithms is highly\ndependent on accurate pose estimation; small errors in sensor pose can lead to\nsevere reconstruction artifacts. In this paper, we propose an algorithm that\njointly optimizes the neural scene representation and sonar poses. Our\nalgorithm does so by parameterizing the 6DoF poses as learnable parameters and\nbackpropagating gradients through the neural renderer and implicit\nrepresentation. We validated our algorithm on both real and simulated datasets.\nIt produces high-fidelity 3D reconstructions even under significant pose drift.",
      "generated_abstract": "This paper addresses the problem of estimating the position of a moving\ntarget in an unknown environment from time-invariant measurements. A key\nchallenge lies in the fact that the target may not be visible at all\ninstants, and may be occluded by the environment. This paper proposes an\nalgorithm that leverages the structure of the problem to enhance estimation\naccuracy. The proposed approach is based on the use of a set of target\nocclusion maps, which are generated at each measurement. The proposed method\nis demonstrated in the context of a tracking problem involving a moving target\nwith a nonlinear dynamics. The results demonstrate that the proposed method\nis able to effectively reduce the estimation error, particularly in the case\nwhen the target is occluded.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18292682926829268,
          "p": 0.20270270270270271,
          "f": 0.19230768732084172
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.036036036036036036,
          "f": 0.037209297330882234
        },
        "rouge-l": {
          "r": 0.15853658536585366,
          "p": 0.17567567567567569,
          "f": 0.16666666167981606
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/DS/2503.07545v1",
      "true_abstract": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
      "generated_abstract": "In this paper, we study a setting where a single agent is required to\nunderstand and react to multiple actions, each of which may be partially\nunderstood. We consider the problem of understanding and selecting the best\naction given partial information. We propose a two-player strategy-proof game\nto achieve this task. We show that the problem is NP-Hard and show how to\napproximate the solution within a factor of $O(\\log n)$. We then show that this\napproximation is tight in the sense that the optimal solution is optimal up to\na constant factor. Finally, we show that the problem can be solved in\n$\\tilde{O}(\\log n)$ time. This work provides a solution to the problem of\nunderstanding and selecting from multiple actions, which may be partially\nunderstood, within a factor of $O(\\log n)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15079365079365079,
          "p": 0.2714285714285714,
          "f": 0.19387754642857155
        },
        "rouge-2": {
          "r": 0.017045454545454544,
          "p": 0.02702702702702703,
          "f": 0.02090591860141666
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.24285714285714285,
          "f": 0.1734693831632654
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.06964v1",
      "true_abstract": "This paper seeks to clarify whether the processing of two types of messages\nthat promote responsible gambling, namely personalized messages and normative\nmessages, varies across gamblers severity habits and rational thinking",
      "generated_abstract": "In the context of the COVID-19 pandemic, the need for a rapid and effective\nresponse is critical to mitigate the negative impact on the global economy.\nThis paper investigates the impact of the COVID-19 pandemic on the global\neconomy through a stochastic general equilibrium model. The model takes into\naccount the impact of the pandemic on economic activity and financial\ninstitutions, and it includes the effects of the pandemic on the global\neconomy through changes in the exchange rate. The model is calibrated using\nglobal data on economic activity, financial institutions, and exchange rates\nfrom 2001 to 2023. The results show that the global economy is highly\nsensitive to changes in the exchange rate, with significant effects on\neconomic activity and financial institutions. The model also highlights the\nimportance of financial institutions and exchange rates in driving economic\nactivity and financial stability during",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.10144927536231885,
          "f": 0.14432989280051028
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.009433962264150943,
          "f": 0.014705878914360666
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.08695652173913043,
          "f": 0.12371133609947936
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/MN/2501.03235v1",
      "true_abstract": "Neural networks based on soft and biological matter constitute an interesting\npotential alternative to traditional implementations based on electric\ncircuits. DNA is a particularly promising system in this context due its\nnatural ability to store information. In recent years, researchers have started\nto construct neural networks that are based on DNA. In this chapter, I provide\na very basic introduction to the concept of DNA neural networks, aiming at an\naudience that is not familiar with biochemistry.",
      "generated_abstract": "The recent advancements in machine learning and deep learning techniques\nprovide significant potential to improve diagnostic and treatment outcomes in\ncancer research. However, there are few benchmark datasets in this field.\nCurrently, there are limited benchmark datasets available for the application\nof deep learning methods in cancer research. Therefore, we created the\nDifferential Diagnosis Cancer Benchmark Dataset (DDCB), a new benchmark\ndataset comprising of 1,320 cases of breast cancer, 1,320 cases of colorectal\ncancer, and 1,320 cases of lung cancer. This dataset includes 10-fold\ncross-validation, 5-fold cross-validation, and a balanced dataset, and is\navailable at https://www.kaggle.com/babakpouyandeh/ddcb-dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21666666666666667,
          "p": 0.19696969696969696,
          "f": 0.20634920136054433
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.011363636363636364,
          "f": 0.012345674049689543
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.18181818181818182,
          "f": 0.1904761854875285
        }
      }
    },
    {
      "paper_id": "cs.MA.econ/TH/2502.16719v2",
      "true_abstract": "Recent research on instant runoff voting (IRV) shows that it exhibits a\nstriking combinatorial property in one-dimensional preference spaces: there is\nan \"exclusion zone\" around the median voter such that if a candidate from the\nexclusion zone is on the ballot, then the winner must come from the exclusion\nzone. Thus, in one dimension, IRV cannot elect an extreme candidate as long as\na sufficiently moderate candidate is running. In this work, we examine the\nmathematical structure of exclusion zones as a broad phenomenon in more general\npreference spaces. We prove that with voters uniformly distributed over any\n$d$-dimensional hyperrectangle (for $d > 1$), IRV has no nontrivial exclusion\nzone. However, we also show that IRV exclusion zones are not solely a\none-dimensional phenomenon. For irregular higher-dimensional preference spaces\nwith fewer symmetries than hyperrectangles, IRV can exhibit nontrivial\nexclusion zones. As a further exploration, we study IRV exclusion zones in\ngraph voting, where nodes represent voters who prefer candidates closer to them\nin the graph. Here, we show that IRV exclusion zones present a surprising\ncomputational challenge: even checking whether a given set of positions is an\nIRV exclusion zone is NP-hard. We develop an efficient randomized approximation\nalgorithm for checking and finding exclusion zones. We also report on\ncomputational experiments with exclusion zones in two directions: (i) applying\nour approximation algorithm to a collection of real-world school friendship\nnetworks, we find that about 60% of these networks have probable nontrivial IRV\nexclusion zones; and (ii) performing an exhaustive computer search of small\ngraphs and trees, we also find nontrivial IRV exclusion zones in most graphs.\nWhile our focus is on IRV, the properties of exclusion zones we establish\nprovide a novel method for analyzing voting systems in metric spaces more\ngenerally.",
      "generated_abstract": "We study the problem of maximizing the expected number of collaborative\ncollisions in a system of $n$ agents, each of which has a finite number of\nagents that they can collaborate with. The agents can collaborate with one\nanother or with any number of agents in the system. We propose a simple\nheuristic based on a randomized algorithm to solve this problem. The algorithm\nis inspired by the work of Barak, Cohen, and Mossel (2019) and is simple and\nefficient. We provide theoretical guarantees for the expected number of\ncollisions that the proposed algorithm will cause, which is a function of the\nnumber of agents and the number of agents in the system. We also provide\nresults on the optimal number of agents in the system and the optimal number of\nagents that can collaborate with each other.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1329479768786127,
          "p": 0.3709677419354839,
          "f": 0.19574467696659126
        },
        "rouge-2": {
          "r": 0.015444015444015444,
          "p": 0.037383177570093455,
          "f": 0.021857919359641258
        },
        "rouge-l": {
          "r": 0.10982658959537572,
          "p": 0.3064516129032258,
          "f": 0.16170212377510196
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.05186v1",
      "true_abstract": "In an era where financial markets are heavily influenced by many static and\ndynamic factors, it has become increasingly critical to carefully integrate\ndiverse data sources with machine learning for accurate stock price prediction.\nThis paper explores a multimodal machine learning approach for stock price\nprediction by combining data from diverse sources, including traditional\nfinancial metrics, tweets, and news articles. We capture real-time market\ndynamics and investor mood through sentiment analysis on these textual data\nusing both ChatGPT-4o and FinBERT models. We look at how these integrated data\nstreams augment predictions made with a standard Long Short-Term Memory (LSTM\nmodel) to illustrate the extent of performance gains. Our study's results\nindicate that incorporating the mentioned data sources considerably increases\nthe forecast effectiveness of the reference model by up to 5%. We also provide\ninsights into the individual and combined predictive capacities of these\nmodalities, highlighting the substantial impact of incorporating sentiment\nanalysis from tweets and news articles. This research offers a systematic and\neffective framework for applying multimodal data analytics techniques in\nfinancial time series forecasting that provides a new view for investors to\nleverage data for decision-making.",
      "generated_abstract": "We investigate the impact of market microstructure on the performance of\nmarket-makers and liquidity providers (LPs) in crypto markets. We propose a\nmechanism that combines a model-based dynamic market maker (DMM) and a\nmodel-free liquidity provider (LP) in a one-for-one model, where the DMM and LP\noperate independently. We analyze the impact of the DMM's market-making\nstrategy on the performance of the LP, and the LP's liquidity-providing\nstrategy on the performance of the DMM. Our results show that, in a\none-for-one model, the DMM's market-making strategy can enhance the LP's\nliquidity, resulting in improved liquidity provision and lower transaction costs\nfor the LP. This result is in line with the existing literature on liquidity\nprovision, which suggests that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2835820895522388,
          "f": 0.1899999955445001
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.02040816326530612,
          "f": 0.014388484643653433
        },
        "rouge-l": {
          "r": 0.12030075187969924,
          "p": 0.23880597014925373,
          "f": 0.15999999554450012
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.05530v1",
      "true_abstract": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
      "generated_abstract": "This paper presents a novel approach to large-scale database system\nmanagement using a unified system architecture that integrates the\noptimization, query processing, and data management layers. The architecture\nenables data-driven decision-making, enabling the management of complex\ndatabase systems with high scalability and flexibility.\n  We present a multi-layered architecture for managing database systems that\nenables a data-driven approach to system management. This architecture integrates\nthe optimization, query processing, and data management layers, enabling data-driven\ndecision-making. The architecture provides a comprehensive framework for\nmanaging large-scale database systems, enabling the system to adapt to\nchanging requirements and scenarios, ensuring resiliency, and improving\nperformance and efficiency. The architecture provides a unified system\narchitecture that integrates the optimization, query processing, and data\nmanagement layers. It enables data-driven decision",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11403508771929824,
          "p": 0.22807017543859648,
          "f": 0.1520467791812867
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.21052631578947367,
          "f": 0.14035087274853814
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.05974v1",
      "true_abstract": "Contrast enhancement, a key aspect of image-to-image translation (I2IT),\nimproves visual quality by adjusting intensity differences between pixels.\nHowever, many existing methods struggle to preserve fine-grained details, often\nleading to the loss of low-level features. This paper introduces LapLoss, a\nnovel approach designed for I2IT contrast enhancement, based on the Laplacian\npyramid-centric networks, forming the core of our proposed methodology. The\nproposed approach employs a multiple discriminator architecture, each operating\nat a different resolution to capture high-level features, in addition to\nmaintaining low-level details and textures under mixed lighting conditions. The\nproposed methodology computes the loss at multiple scales, balancing\nreconstruction accuracy and perceptual quality to enhance overall image\ngeneration. The distinct blend of the loss calculation at each level of the\npyramid, combined with the architecture of the Laplacian pyramid enables\nLapLoss to exceed contemporary contrast enhancement techniques. This framework\nachieves state-of-the-art results, consistently performing well across\ndifferent lighting conditions in the SICE dataset.",
      "generated_abstract": "With the advancement of deep learning techniques, medical image segmentation\nhas been revolutionized, with state-of-the-art methods achieving higher\nperformance and efficiency. However, despite the significant progress in the\nsegmentation of medical images, there is still a gap between the segmentation\nresults of different medical institutions. To bridge this gap, this paper\nintroduces a framework that employs a multimodal learning approach to enhance\nthe segmentation accuracy of brain tumor. Our framework incorporates two\nmodules: (1) a pre-trained encoder network that extracts the global and local\nfeatures of medical images, and (2) a multimodal module that fuses these\nfeatures with the medical image and class information. Our approach achieves\nstate-of-the-art performance, with a mean intersection-over-union (mIoU) of\n70.2%, outperforming state",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1981981981981982,
          "p": 0.27848101265822783,
          "f": 0.23157894251024938
        },
        "rouge-2": {
          "r": 0.0410958904109589,
          "p": 0.05357142857142857,
          "f": 0.04651162299381099
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.24050632911392406,
          "f": 0.1999999951418284
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.06172v1",
      "true_abstract": "Noninvasive brain stimulation can activate neurons in the brain but requires\npower electronics with exceptionally high power in the mega-volt-ampere and\nhigh frequencies in the kilohertz range. Whereas oscillator circuits offered\nonly one or very few pulse shapes, modular power electronics solved a\nlong-standing problem for the first time and enabled arbitrary software-based\ndesign of the temporal shape of stimuli. However, synthesizing arbitrary\nstimuli with a high output quality requires a large number of modules. Systems\nwith few modules and pulse-width modulation may generate apparently smooth\ncurrent shapes in the highly inductive coil, but the stimulation effect of the\nneurons depends on the electric field and the electric field becomes a burst of\nultra-brief rectangular pulses. We propose an alternative solution that\nachieves high-resolution pulse shaping with fewer modules by implementing\nhigh-power wide-bandwidth voltage asymmetry. Rather than equal voltage steps,\nour system strategically assigns different voltages to each module to achieve a\nnear-exponential improvement in resolution. Compared to prior designs, our\nexperimental prototype achieved better output quality, although it uses only\nhalf the number of modules.",
      "generated_abstract": "In the last decade, the development of magnetic resonance imaging (MRI)\ntechnology has dramatically advanced the ability to image the brain. This\nadvance has resulted in a surge of research studies that focus on MRI\napplications for neurological disorders, including multiple sclerosis (MS),\nAlzheimer's disease (AD), and stroke. However, the complex biological and\nphysical properties of the brain, as well as the inherent variability of\nexperimental conditions, can complicate the interpretation of MRI data, leading\nto uncertainty in the results of clinical trials. This uncertainty is\nparticularly challenging for clinical trials involving the treatment of\nneurological disorders, given the high incidence of adverse events and the\ncomplicated pathophysiological mechanisms that may underlie these events.\nFurthermore, the limited number of existing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1322314049586777,
          "p": 0.19047619047619047,
          "f": 0.15609755613848914
        },
        "rouge-2": {
          "r": 0.030120481927710843,
          "p": 0.042735042735042736,
          "f": 0.03533568419583281
        },
        "rouge-l": {
          "r": 0.10743801652892562,
          "p": 0.15476190476190477,
          "f": 0.12682926345556236
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08835v1",
      "true_abstract": "Roll-to-roll (R2R) printing technologies are promising for high-volume\ncontinuous production of substrate-based electronic products. One of the major\nchallenges in R2R flexible electronics printing is achieving tight alignment\ntolerances, as specified by the device resolution (usually at the micro-meter\nlevel), for multi-layer printed electronics. The alignment of the printed\npatterns in different layers is known as registration. Conventional\nregistration control methods rely on real-time feedback controllers, such as\nPID control, to regulate the web tension and the web speed. However, those\nmethods may lose effectiveness in compensating for recurring disturbances and\nsupporting effective mitigation of registration errors. In this paper, we\npropose a Spatial-Terminal Iterative Learning Control (STILC) method integrated\nwith PID control to iteratively learn and reduce registration error\ncycle-by-cycle, converging it to zero. This approach enables unprecedented\nprecision in the creation, integration, and manipulation of multi-layer\nmicrostructures in R2R processes. We theoretically prove the convergence of the\nproposed STILC-PID hybrid approach and validate its effectiveness through a\nsimulated registration error scenario caused by axis mismatch between roller\nand motor, a common issue in R2R systems. The results demonstrate that the\nSTILC-PID hybrid control method can fully eliminate the registration error\nafter a feasible number of iterations. Additionally, we analyze the impact of\ndifferent learning gains on the convergence performance of STILC.",
      "generated_abstract": "This paper investigates a novel two-way adaptive distributed controller\nfor a two-dimensional (2D) oscillator with unknown disturbances. The\ncontroller is designed to be able to adapt to the unknown disturbances by\nadapting its gain and gain parameters. The controller is designed by minimizing\na modified version of the sum of squared errors (SSE), which is shown to be\nfeasible. Moreover, the controller is shown to be asymptotically stable in\nterms of the disturbance. The performance of the proposed controller is\nanalyzed using Lyapunov function analysis. Numerical simulations are also\npresented to validate the effectiveness of the proposed controller.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14685314685314685,
          "p": 0.3442622950819672,
          "f": 0.2058823487490389
        },
        "rouge-2": {
          "r": 0.014778325123152709,
          "p": 0.036585365853658534,
          "f": 0.021052627480210088
        },
        "rouge-l": {
          "r": 0.1258741258741259,
          "p": 0.29508196721311475,
          "f": 0.1764705840431566
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mtrl-sci/2503.10012v1",
      "true_abstract": "Epitaxial thin-film growth is a versatile and powerful technique for\nachieving a precise control of composition, stabilizing non-equilibrium phases,\ntailoring growth orientation, as well as forming heterointerfaces of various\nquantum materials. For synthesis of highly crystalline thin films, in-depth\nunderstanding of epitaxial relationship between the desired thin film and the\nsingle-crystalline substrates is necessary. In this study, we investigate\nepitaxial relationship in thin-film growth of triangular-lattice\nantiferromagnet CrSe on the (001) plane of Al2O3 and the lattice-matched (111)\nplane of yttria-stabilized zirconia (YSZ) substrates. Structural\ncharacterization using out-of-plane and in-plane x-ray diffraction shows that\nthe presence of 19.1o-twisted domains of CrSe significantly dominates the\naligned domain on the Al2O3 substrate while it reveals a single-domain\nformation on the YSZ substrate. The stability of the 19.1o-twisted domain\nrather than the aligned domain can be explained by rotational commensurate\nepitaxy, which is well reproduced by density functional theory calculations.\nThe single-domain CrSe thin film on the YSZ substrate exhibits a superior\nmetallic conductivity compared to the twisted-domain thin film on the Al2O3\nsubstrate, implying contribution of the grain boundary scattering mechanism to\nelectrical transport.",
      "generated_abstract": "This study examines the effect of temperature on the structural and magnetic\nproperties of a 12-atom-wide triangular layered perovskite, Sr12P2O7, under\nhigh-pressure conditions. The results reveal that the system exhibits a\nsecond-order phase transition under high pressures, with the lattice structure\ntransiting from a cubic to a tetragonal phase. These structural transitions are\naccompanied by a significant reduction in the lattice constant and a change in\nthe crystallographic orientation. Moreover, the magnetic properties show a\nclearly distinguishable transition from a paramagnetic to a ferromagnetic\nphase. These findings provide insight into the mechanism underlying the\nsecond-order phase transition in this perovskite system and reveal the\nsignificant impact of high-pressure conditions on the structural and magnetic\nproperties of perovskite systems",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092436974789916,
          "p": 0.19117647058823528,
          "f": 0.1390374285269812
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.029411764705882353,
          "f": 0.022471905390734487
        },
        "rouge-l": {
          "r": 0.09243697478991597,
          "p": 0.16176470588235295,
          "f": 0.11764705419543044
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2410.16101v1",
      "true_abstract": "Humans and other animals coactivate agonist and antagonist muscles in many\nmotor actions. Increases in muscle coactivation are thought to leverage\nviscoelastic properties of skeletal muscles to provide resistance against limb\nmotion. However, coactivation also emerges in scenarios where it seems\nparadoxical because the goal is not to resist limb motion but instead to\nrapidly mobilize the limb(s) or body to launch or correct movements. Here, we\npresent a new perspective on muscle coactivation: to prime the nervous system\nfor fast, task-dependent responses to sensory stimuli. We review distributed\nneural control mechanisms that may allow the healthy nervous system to leverage\nmuscle coactivation to produce fast and flexible responses to sensory feedback.",
      "generated_abstract": "The rapid development of computational methods has improved the accuracy\nand precision of phylogenetic analyses. However, the application of these\nmethods in the study of microbial communities remains limited by the\nunderestimation of the phylogenetic tree rooted at the species level. To\naddress this limitation, we propose a method for the rooting of phylogenetic\ntrees in microbial communities. Our method is based on the statistical\nanalysis of the pairwise dissimilarities between the species in a microbial\ncommunity and the species in a reference community. The pairwise dissimilarity\nmatrix is estimated by applying the Jaccard index to the gene content of the\ntwo microbial communities. The rooting of the phylogenetic tree is based on\nthe pairwise dissimilarities between the species in the root microbial\ncommunity and the species in the reference community. We demonstrate that the\nphy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1566265060240964,
          "p": 0.19402985074626866,
          "f": 0.17333332839022236
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1566265060240964,
          "p": 0.19402985074626866,
          "f": 0.17333332839022236
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.09938v1",
      "true_abstract": "Vision-and-language navigation (VLN) tasks require agents to navigate\nthree-dimensional environments guided by natural language instructions,\noffering substantial potential for diverse applications. However, the scarcity\nof training data impedes progress in this field. This paper introduces\nPanoGen++, a novel framework that addresses this limitation by generating\nvaried and pertinent panoramic environments for VLN tasks. PanoGen++\nincorporates pre-trained diffusion models with domain-specific fine-tuning,\nemploying parameter-efficient techniques such as low-rank adaptation to\nminimize computational costs. We investigate two settings for environment\ngeneration: masked image inpainting and recursive image outpainting. The former\nmaximizes novel environment creation by inpainting masked regions based on\ntextual descriptions, while the latter facilitates agents' learning of spatial\nrelationships within panoramas. Empirical evaluations on room-to-room (R2R),\nroom-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN)\ndatasets reveal significant performance enhancements: a 2.44% increase in\nsuccess rate on the R2R test leaderboard, a 0.63% improvement on the R4R\nvalidation unseen set, and a 0.75-meter enhancement in goal progress on the\nCVDN validation unseen set. PanoGen++ augments the diversity and relevance of\ntraining environments, resulting in improved generalization and efficacy in VLN\ntasks.",
      "generated_abstract": "The rapid advancement of deep learning and large language models (LLMs) has\nopened a new era of language understanding. However, existing methods are\nrestricted by their reliance on large-scale datasets, which often fail to\ncapture the diversity of human languages. To address this issue, we propose a\nnovel approach that leverages the multilingual capabilities of LLMs to\ngenerate natural language explanations for image-text pairs. Specifically, we\nconstruct a multilingual corpus by integrating 200 million image-text pairs\nfrom 25 languages, including 200 million image-text pairs from 15 languages,\nsuch as English, Spanish, and Mandarin. This multilingual corpus enables us to\ngenerate natural language explanations for image-text pairs in 15 languages. We\nalso propose a novel multilingual attention mechanism that utilizes the\nmultilingual cor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1678832116788321,
          "p": 0.2804878048780488,
          "f": 0.2100456574158171
        },
        "rouge-2": {
          "r": 0.022857142857142857,
          "p": 0.0380952380952381,
          "f": 0.02857142388392934
        },
        "rouge-l": {
          "r": 0.16058394160583941,
          "p": 0.2682926829268293,
          "f": 0.20091323732449293
        }
      }
    },
    {
      "paper_id": "math.MG.math/FA/2503.09542v2",
      "true_abstract": "In 1959, Marcus and Ree proved that any bistochastic matrix $A$ satisfies\n$\\Delta_n(A):= \\max_{\\sigma\\in S_n}\\sum_{i=1}^{n}A(i, \\sigma(i))-\\sum_{i,\nj=1}^n A(i, j)^2 \\geq 0$. Erd\\H{o}s asked to characterize the bistochastic\nmatrices satisfying $\\Delta_n(A)=0$. It was recently proved that there are only\nfinitely many such matrices for any $n$. However, a complete list of such\nmatrices was obtained in dimension $n=2, 3$ only recently, arXiv:2306.05518. In\nthis paper, we characterize all $4\\times 4$ bistochastic matrices satisfying\n$\\Delta_4(A)=0$. Furthermore, we show that for $n\\geq 3$, $\\Delta_n(A)=\\alpha$\nhas uncountably many solutions when $\\alpha\\notin \\{0, (n-1)/4\\}$. This answers\na question raised in arXiv:2410.06612. We extend the Marcus$\\unicode{x2013}$Ree\ninequality to infinite bistochastic arrays and bistochastic kernels. Our\ninvestigation into $4\\times 4$ Erd\\H{o}s matrices also raises several\nintriguing questions that are of independent interest. We propose several\nquestions and conjectures and present numerical evidence for them.",
      "generated_abstract": "We study the stability of the limiting behavior of a random walk in a\npolynomial space when the path is allowed to jump. We prove that the limiting\nbehavior is a supercritical $K$-stable law, where $K$ is the total number of\ndistinct positive jump locations. As a consequence, we obtain an upper bound on\nthe largest eigenvalue of the transition matrix of the random walk, and we\nestablish an explicit formula for this eigenvalue. In particular, our results\ngive a simple proof of the result of Deift and Pries, which is known to hold\nfor a wide range of models. We also discuss the role of the jump locations and\ntheir relation to the distribution of the random walk.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1485148514851485,
          "p": 0.20833333333333334,
          "f": 0.17341039976477676
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13861386138613863,
          "p": 0.19444444444444445,
          "f": 0.1618497061231583
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.10490v1",
      "true_abstract": "The effect of a constant electric field on two-photon absorption in a\nsemiconductor is calculated using an independent-particle theory. The\ntheoretical framework is an extension of a theory of the one-photon\nFranz-Keldysh effect [Wahlstrand and Sipe, Phys. Rev. B 82, 075206 (2010)]. The\ntheory includes the effect of the constant field, including field-induced\ncoupling between closely spaced bands, in the electronic wavefunctions and\ncalculates optical absorption perturbatively. Numerical calculations are\nperformed using a 14-band $\\mathbf{k} \\cdot\\mathbf{p}$ band structure model for\nGaAs. For all nonzero tensor elements, field-enabled two-photon absorption\n(TPA) below the band gap and Franz-Keldysh oscillations in the TPA spectrum are\npredicted, with a generally larger effect in tensor elements with more\ncomponents parallel to the constant electric field direction. Some tensor\nelements that are zero in the absence of a field become nonzero in the presence\nof the constant electric field and depend on its sign. Notably, these elements\nare linear in the electric field to lowest order and may be substantial away\nfrom band structure critical points at room temperature and/or with a\nnon-uniform field. Electric-field-induced changes in the carrier injection rate\ndue to interference between one- and two-photon absorption are also calculated.\nThe electric field enables this bichromatic coherent control process for\npolarization configurations where it is normally forbidden, and also modifies\nthe spectrum of the process for configurations where it is allowed by crystal\nsymmetry.",
      "generated_abstract": "We study the optical response of a semiconductor quantum dot (QD) suspended\nin a nanofluidic channel. We show that the QD optical response can be\ncharacterized by a single parameter: the optical thickness $T$, which\ncorresponds to the effective number of scattering sites per unit area. We\nanalyze the optical response in the near-field and far-field, and discuss the\ninfluence of the QD size and the channel geometry on the optical response. We\nfind that the near-field response of the QD is dominated by the direct\nscattering and the near-field response of the QD in the far-field is\ndominated by the diffraction and the backscattering effects. We also show that\nthe near-field response of the QD can be approximated by a single scattering\nsolution, which can be used to compute the near-field",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.2413793103448276,
          "f": 0.14659685440969283
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.0625,
          "f": 0.04040403602897711
        },
        "rouge-l": {
          "r": 0.09774436090225563,
          "p": 0.22413793103448276,
          "f": 0.13612565022121118
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2412.09157v1",
      "true_abstract": "This paper studies the robust reinsurance and investment games for\ncompetitive insurers. Model uncertainty is characterized by a class of\nequivalent probability measures. Each insurer is concerned with relative\nperformance under the worst-case scenario. Insurers' surplus processes are\napproximated by drifted Brownian motion with common and idiosyncratic insurance\nrisks. The insurers can purchase proportional reinsurance to divide the\ninsurance risk with the reinsurance premium calculated by the variance\nprinciple. We consider an incomplete market driven by the 4/2 stochastic\nvolatility mode. This paper formulates the robust mean-field game for a\nnon-linear system originating from the variance principle and the 4/2 model.\nFor the case of an exponential utility function, we derive closed-form\nsolutions for the $n$-insurer game and the corresponding mean-field game. We\nshow that relative concerns lead to new hedging terms in the investment and\nreinsurance strategies. Model uncertainty can significantly change the\ninsurers' hedging demands. The hedging demands in the investment-reinsurance\nstrategies exhibit highly non-linear dependence with the insurers' competitive\ncoefficients, risk aversion and ambiguity aversion coefficients. Finally,\nnumerical results demonstrate the herd effect of competition.",
      "generated_abstract": "We study the problem of minimizing the sum of the discounted returns of a\nportfolio of risky assets, subject to a fixed budget constraint. In this\nframework, we propose a two-stage algorithm that combines the ideas of\ndynamic programming and the BSDE approach. The first stage is to solve a\nlinear-quadratic problem to determine a risk-neutral measure and an\ninvestment policy that minimizes the sum of the discounted returns of the\nportfolio, subject to a given budget constraint. The second stage is to solve\na quadratic-BSDE problem with the risk-neutral measure and the investment\npolicy of the first stage, to determine the optimal investment portfolio.\nSpecifically, we derive the explicit form of the solution of the linear\nquadratic problem in terms of the solution of the quadratic-BSDE problem,\nand then prove the optimality of the solution of the linear",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.24242424242424243,
          "f": 0.17977527623279901
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.037383177570093455,
          "f": 0.029520290424150768
        },
        "rouge-l": {
          "r": 0.13392857142857142,
          "p": 0.22727272727272727,
          "f": 0.16853932117661924
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.14317v2",
      "true_abstract": "We develop a model that captures peer effect heterogeneity by modeling the\nendogenous spillover to be linear in ordered peer outcomes. Unlike the\ncanonical linear-in-means model, our approach accounts for the distribution of\npeer outcomes as well as the size of peer groups. Under a minimal condition,\nour model admits a unique equilibrium and is therefore tractable and\nidentified. Simulations show our estimator has good finite sample performance.\nFinally, we apply our model to educational data from Norway, finding that\nhigher-performing friends disproportionately drive GPA spillovers. Our\nframework provides new insights into the structure of peer effects beyond\naggregate measures.",
      "generated_abstract": "This paper develops a novel empirical framework to test for and identify\nsignificant interactions between the fixed effects and random effects terms\nin semiparametric regression models. The framework is based on the semiparametric\nEconometric Model with Indicator Variables (SEMIV) and is designed to handle\nnon-normal, non-linear, and possibly non-stationary covariate distributions. The\nframework is useful in studying the impact of the fixed effects on the\nestimation of the random effects term, as well as the reverse. The\nsignificance of interactions is examined through various statistical tests.\nNumerical examples are provided to illustrate the empirical applicability of\nthe proposed framework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.1875,
          "f": 0.16901407955564388
        },
        "rouge-2": {
          "r": 0.042105263157894736,
          "p": 0.044444444444444446,
          "f": 0.043243238246896125
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.1875,
          "f": 0.16901407955564388
        }
      }
    },
    {
      "paper_id": "math.AT.math/AT/2503.02839v1",
      "true_abstract": "We show that the $\\infty$-category of normed algebras in genuine $G$-spectra,\nas introduced by Bachmann-Hoyois, is modelled by strictly commutative algebras\nin $G$-symmetric spectra for any finite group $G$. We moreover provide an\nanalogous description of Schwede's ultra-commutative global ring spectra in\nhigher categorical terms.\n  Using these new descriptions, we exhibit the $\\infty$-category of\nultra-commutative global ring spectra as a partially lax limit of the\n$\\infty$-categories of genuine $G$-spectra for varying $G$, in analogy with the\nnon-multiplicative comparison of Nardin, Pol, and the second author.\n  Along the way, we establish various new results in parametrized higher\nalgebra, which we hope to be of independent interest.",
      "generated_abstract": "In this paper, we study the dynamics of the two-dimensional KdV equation on\nthe torus in the framework of the so-called \"non-autonomous setting\", where the\ndifferential operator $\\partial_x^2 + 2\\sin(2x)$ is replaced by a non-autonomous\none $\\partial_x^2 + \\lambda\\sin(2x)$. We prove that, for $\\lambda<0$, the\nsolution is a periodic orbit of period $2\\pi$. Our results are obtained by\nreducing the problem to that of the classical KdV equation on the torus and by\nusing a classical \"tube\" approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17567567567567569,
          "p": 0.24074074074074073,
          "f": 0.2031249951220704
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.014285714285714285,
          "f": 0.011904757043652776
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.2037037037037037,
          "f": 0.17187499512207047
        }
      }
    },
    {
      "paper_id": "math.NA.stat/CO/2503.05533v1",
      "true_abstract": "Multilevel sampling methods, such as multilevel and multifidelity Monte\nCarlo, multilevel stochastic collocation, or delayed acceptance Markov chain\nMonte Carlo, have become standard uncertainty quantification tools for a wide\nclass of forward and inverse problems. The underlying idea is to achieve faster\nconvergence by leveraging a hierarchy of models, such as partial differential\nequation (PDE) or stochastic differential equation (SDE) discretisations with\nincreasing accuracy. By optimally redistributing work among the levels,\nmultilevel methods can achieve significant performance improvement compared to\nsingle level methods working with one high-fidelity model. Intuitively,\napproximate solutions on coarser levels can tolerate large computational error\nwithout affecting the overall accuracy. We show how this can be used in\nhigh-performance computing applications to obtain a significant performance\ngain.\n  As a use case, we analyse the computational error in the standard multilevel\nMonte Carlo method and formulate an adaptive algorithm which determines a\nminimum required computational accuracy on each level of discretisation. We\nshow two examples of how the inexactness can be converted into actual gains\nusing an elliptic PDE with lognormal random coefficients. Using a low precision\nsparse direct solver combined with iterative refinement results in a simulated\ngain in memory references of up to $3.5\\times$ compared to the reference double\nprecision solver; while using a MINRES iterative solver, a practical speedup of\nup to $1.5\\times$ in terms of FLOPs is achieved. These results provide a step\nin the direction of energy-aware scientific computing, with significant\npotential for energy savings.",
      "generated_abstract": "This paper provides a comprehensive and general characterization of\ngeneralized multivariate normal distributions. Specifically, we demonstrate\nthat any multivariate normal distribution can be written as a generalized\nmultivariate normal distribution, and vice versa. We also show that any\nmultivariate normal distribution can be written as a sum of two generalized\nmultivariate normal distributions. Additionally, we prove that any multivariate\nnormal distribution can be written as the product of two generalized\nmultivariate normal distributions. We also demonstrate that any generalized\nmultivariate normal distribution can be written as a product of two generalized\nmultivariate normal distributions, and vice versa. Finally, we provide a\nconceptual discussion on the relationship between the multivariate normal\ndistribution and the generalized multivariate normal distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07926829268292683,
          "p": 0.3023255813953488,
          "f": 0.12560386144274086
        },
        "rouge-2": {
          "r": 0.008583690987124463,
          "p": 0.03076923076923077,
          "f": 0.013422815381064783
        },
        "rouge-l": {
          "r": 0.07317073170731707,
          "p": 0.27906976744186046,
          "f": 0.1159420256939486
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.05682v1",
      "true_abstract": "Multi-contrast magnetic resonance imaging (MRI) plays a vital role in brain\ntumor segmentation and diagnosis by leveraging complementary information from\ndifferent contrasts. Each contrast highlights specific tumor characteristics,\nenabling a comprehensive understanding of tumor morphology, edema, and\npathological heterogeneity. However, existing methods still face the challenges\nof multi-level specificity perception across different contrasts, especially\nwith limited annotations. These challenges include data heterogeneity,\ngranularity differences, and interference from redundant information. To\naddress these limitations, we propose a Task-oriented Uncertainty Collaborative\nLearning (TUCL) framework for multi-contrast MRI segmentation. TUCL introduces\na task-oriented prompt attention (TPA) module with intra-prompt and\ncross-prompt attention mechanisms to dynamically model feature interactions\nacross contrasts and tasks. Additionally, a cyclic process is designed to map\nthe predictions back to the prompt to ensure that the prompts are effectively\nutilized. In the decoding stage, the TUCL framework proposes a dual-path\nuncertainty refinement (DUR) strategy which ensures robust segmentation by\nrefining predictions iteratively. Extensive experimental results on limited\nlabeled data demonstrate that TUCL significantly improves segmentation accuracy\n(88.2\\% in Dice and 10.853 mm in HD95). It shows that TUCL has the potential to\nextract multi-contrast information and reduce the reliance on extensive\nannotations. The code is available at:\nhttps://github.com/Zhenxuan-Zhang/TUCL_BrainSeg.",
      "generated_abstract": "Brain imaging plays a crucial role in diagnosing and monitoring diseases in\nthe human body. In recent years, deep learning has been widely used in the\nfield of brain imaging, providing effective solutions for image analysis and\ninterpretation. However, the high computational costs and complex training\nprocedures often limit the practical applications of deep learning in brain\nimaging. To address these limitations, we propose a novel and efficient\nframework, named DeepBrainNet, which is based on the LightGBM algorithm and\ndeep learning models. DeepBrainNet is designed for scalable and efficient\nbrain imaging analysis, with a focus on computational efficiency. Specifically,\nthe framework utilizes a multi-stage training process, with each stage\noptimizing different components of the DeepBrainNet model. The first stage\nenables the model to achieve state-of-the-art performance, while the remaining\nstages refine the model's performance further",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19594594594594594,
          "p": 0.30526315789473685,
          "f": 0.23868312280986986
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.07751937984496124,
          "f": 0.06079026878909138
        },
        "rouge-l": {
          "r": 0.19594594594594594,
          "p": 0.30526315789473685,
          "f": 0.23868312280986986
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2409.10805v1",
      "true_abstract": "Dengue virus (DENV) is a mosquito-borne virus with a significant human health\nconcern. With 390 million infections annually and 96 million showing clinical\nsymptoms, severe dengue can lead to life-threatening conditions like dengue\nhemorrhagic fever (DHF) and dengue shock syndrome (DSS). The only FDA-approved\nvaccine, Dengvaxia, has limitations due to antibody-dependent enhancement\n(ADE), necessitating careful administration. The recent pre-approval of TAK-003\nby WHO in 2024 highlights ongoing efforts to improve vaccine options. This\nreview explores recent advancements in dengue vaccine development, emphasizing\npotential utility of mRNA-based vaccines. By examining current clinical trial\ndata and innovations, we aim to identify promising strategies to address the\nlimitations of existing vaccines and enhance global dengue prevention efforts.",
      "generated_abstract": "We present a novel method for simulating dynamic networks of protein\nnetworks, incorporating a number of key biological features such as intrinsic\ndynamics and regulatory interactions. The method is based on the\ndynamic-local-dynamics (DLD) approach, which models the dynamics of individual\nproteins by a linear stochastic model while accounting for the intrinsic\ndynamics of the protein network. We describe a general framework for\nsimulating dynamic protein networks, including both equilibrium and non-equilibrium\nstochastic processes. We demonstrate the utility of our method by applying it\nto a model of the TGF-beta signaling pathway, where we compute the time-evolving\nstochastic properties of the network, and subsequently, we analyze the\nstructural properties of the network using the recently proposed notion of\n``intrinsic'' networks. We find that the intrinsic network exhibits a rich\nhier",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.12658227848101267,
          "f": 0.11834319028745513
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.008403361344537815,
          "f": 0.008620684658519543
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.11392405063291139,
          "f": 0.10650887076082793
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.01698v1",
      "true_abstract": "Fractional vegetation coverage (FVC) and its spatio-temporal variations are\ncritical indicators of regional ecological changes, which are of great\nsignificance to study the laws of surface variation and analyze regional\necosystem. Under the development of RS and GIS technology, this analysis\nemploys Landsat satellite images in 1994, 2008, 2013 and 2016 to estimate FVC\nin Yalong River Basin based on the Dimidiate Pixel Model. With consideration of\nthe vegetation coverage condition and land surface law in the study area, the\nresearch further analyzes the Spatio-temporal variations as well as the\ninfluencing factors of FVC in terms of topography and land use types\nrespectively. The results show that since 1994, FVC in Yalong River Basin has\nexperienced a downward trend yet displaying an uptick from 2013. Moreover,\ndifferent land use types indicate the versatility of land covers in Yalong\nRiver Basin, with grassland and forest performing probably the most important\nfactors that can induce changes to the stability of FVC in whole basin.\nOverall, the research reflects the impact of human activities on vegetation in\nYalong River Basin, and provides available data and theoretical basis for\necological assessment, ecological restoration and environmental protection.",
      "generated_abstract": "The use of 3D cell culture systems in the study of cell-matrix interactions\nand tissue regeneration has recently expanded. In particular, the ability to\ncontrol the physical properties of the matrix has enabled researchers to study\ncell-matrix interactions in 3D cell cultures. The aim of this review is to\nprovide an overview of the different methods used to control the mechanical\nproperties of the matrix in 3D cell culture systems, with a focus on\nmechanical stimuli. We discuss the advantages and limitations of each method and\nhighlight the potential applications of these systems in regenerative medicine.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.25862068965517243,
          "f": 0.16853932144931208
        },
        "rouge-2": {
          "r": 0.022988505747126436,
          "p": 0.04819277108433735,
          "f": 0.03112840029614439
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.20689655172413793,
          "f": 0.13483145628077278
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2503.06273v1",
      "true_abstract": "We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR)\nframework, dubbed Zero-AVSR, which enables speech recognition in target\nlanguages without requiring any audio-visual speech data in those languages.\nSpecifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer),\nwhich learns language-agnostic speech representations by predicting Roman text.\nThen, by leveraging the strong multilingual modeling capabilities of Large\nLanguage Models (LLMs), we propose converting the predicted Roman text into\nlanguage-specific graphemes, forming the proposed Cascaded Zero-AVSR. Taking it\na step further, we explore a unified Zero-AVSR approach by directly integrating\nthe audio-visual speech representations encoded by the AV-Romanizer into the\nLLM. This is achieved through finetuning the adapter and the LLM using our\nproposed multi-task learning scheme. To capture the wide spectrum of phonetic\nand linguistic diversity, we also introduce a Multilingual Audio-Visual\nRomanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data\nacross 82 languages, along with transcriptions in both language-specific\ngraphemes and Roman text. Extensive analysis and experiments confirm that the\nproposed Zero-AVSR framework has the potential to expand language support\nbeyond the languages seen during the training of the AV-Romanizer.",
      "generated_abstract": "We present a novel deep learning approach to speech recognition, combining\nmodeling of acoustic features with a multimodal attention mechanism that\ncaptures contextual information. Our model, which employs a stacked residual\nnetwork, integrates multimodal features (e.g., audio and visual) through\nattention mechanisms, enabling simultaneous processing of speech and other\ncues. We demonstrate the effectiveness of our approach on the WSJ0-2M\ndataset, achieving state-of-the-art accuracy of 79.1% (F1-score of 77.4%) on\nthe test set. Our model outperforms other state-of-the-art approaches by\napproximately 2% in F1-score, and by more than 4% in accuracy. This is the\nfirst work to integrate visual and multimodal cues, enabling simultane",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1652892561983471,
          "p": 0.25,
          "f": 0.19900497033241762
        },
        "rouge-2": {
          "r": 0.011834319526627219,
          "p": 0.018867924528301886,
          "f": 0.01454544980786931
        },
        "rouge-l": {
          "r": 0.15702479338842976,
          "p": 0.2375,
          "f": 0.18905472157619874
        }
      }
    },
    {
      "paper_id": "nlin.AO.nlin/AO/2503.01812v1",
      "true_abstract": "A novel model for dynamical traps in intermittent human control is proposed.\nIt describes probabilistic, step-wise transitions between two modes of a\nsubject's behavior - active and passive phases in controlling an object's\ndynamics - using an original stochastic differential equation. This equation\ngoverns time variations of a special variable, denoted as $\\zeta$, between two\nlimit values, $\\zeta=0$ and $\\zeta=1$. The introduced trap function,\n$\\Omega(\\Delta)$, quantifies the subject's perception of the object's deviation\nfrom a desired state, thereby determining the relative priority of the two\naction modes. Notably, these transitions - referred to as the subject's action\npoints - occur before the trap function reaches its limit values,\n$\\Omega(\\Delta)=0$ or $\\Omega(\\Delta)=1$. This characteristic enables the\napplication of the proposed model to describe intermittent human control over\nreal objects.",
      "generated_abstract": "We study the nonlinear evolution of a non-relativistic ideal fluid system\nwith a strong nonlinear interaction between the mass density and the velocity\nfield. This problem is motivated by the study of the evolution of a\nnon-relativistic fluid system in a collisionless plasma. We find that the\nnonlinear evolution of the system can be analyzed by a set of coupled\nsecond-order ordinary differential equations, which can be solved numerically\nfor a wide range of initial conditions. We discuss the evolution of the\ninteraction between the mass density and the velocity field, and we show that\nthe system tends to a stable equilibrium state. Furthermore, we also show that\nthe system tends to a stationary equilibrium state when the initial conditions\nare chosen appropriately. We also discuss the evolution of the mass density in\nthe nonlinear stage of the system. In particular, we find that the mass\ndensity can be approximated by a sum",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.16176470588235295,
          "f": 0.14102563610782398
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.018518518518518517,
          "f": 0.01762114038619171
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16176470588235295,
          "f": 0.14102563610782398
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10415v1",
      "true_abstract": "This article focuses on NGC7538 IRS1, one of the most luminous and studied HC\nHII regions in the northern hemisphere. Our aim is to identify the young\nstellar objects (YSOs) embedded within the ionized gas and study their\nkinematic structures. This work expands on a recent survey called \"Protostellar\nOutflows at the EarliesT Stages\" (POETS), which has been devoted to studying\nyoung outflow emission on scales of 10-100 au near luminous YSOs, before they\nstart photoionizing the surrounding medium. We carried out multi-epoch Very\nLong Baseline Array observations of the 22 GHz water masers toward NGC7538 IRS1\nto measure the maser 3D velocities, which, following POETS' findings, are\nreliable tracers of the protostellar winds. Recently, we reobserved the water\nmasers in NGC7538 IRS1 with sensitive global very long baseline interferometry\n(VLBI) observations to map weaker maser emission. Our study confirms the\npresence of two embedded YSOs, IRS1a and IRS1b, at the center of the two linear\ndistributions of 6.7 GHz methanol masers observed in the southern and northern\ncores of the HC HII region, which have been previously interpreted in terms of\nedge-on rotating disks. The water masers trace an extended (~200 au) stationary\nshock front adjacent to the inner portion of the disk around IRS1a. This shock\nfront corresponds to the edge of the southern tip of the ionized core and might\nbe produced by the interaction of the disk wind ejected from IRS1a with the\ninfalling envelope. The water masers closer to IRS1b follow the same LSR\nvelocity (Vlsr) pattern of the 6.7~GHz masers rotating in the disk, but the\ndirection and amplitude of the water maser proper motions are inconsistent with\nrotation. We propose that these water masers are tracing a photo-evaporated\ndisk wind, where the maser Vlsr traces mainly the disk rotation and the proper\nmotions the poloidal velocity of the wind.",
      "generated_abstract": "The study of the gravitational potential of the solar system, and in particular\nthe Sun, has a long and rich history. This is partly due to the fact that the\nSun is the most important body in the solar system, and that the Sun is also\nthe most important body in the universe. In the 19th century, the French\nastronomer Jules Janssen (1847-1924) showed that the gravitational potential\nof the solar system is not a simple harmonic oscillator. Rather, it is a\nnonlinear function of the gravitational potential of the solar system, the\ngravitational potential of the Earth, and the gravitational potential of the\nSun. This result was first published in 1897, but it was not fully understood\nuntil the 1950s, when it was confirmed experimentally. In this review, we\nsummarize the history",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08648648648648649,
          "p": 0.24242424242424243,
          "f": 0.1274900359645086
        },
        "rouge-2": {
          "r": 0.018050541516245487,
          "p": 0.05154639175257732,
          "f": 0.02673796407260772
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.22727272727272727,
          "f": 0.1195219084744688
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.10139v1",
      "true_abstract": "The basic principle of any version of insurance is the paradigm that\nexchanging risk by sharing it in a pool is beneficial for the participants. In\ncase of independent risks with a finite mean this is the case for risk averse\ndecision makers. The situation may be very different in case of infinite mean\nmodels. In that case it is known that risk sharing may have a negative effect,\nwhich is sometimes called the nondiversification trap. This phenomenon is well\nknown for infinite mean stable distributions. In a series of recent papers\nsimilar results for infinite mean Pareto and Fr\\'echet distributions have been\nobtained. We further investigate this property by showing that many of these\nresults can be obtained as special cases of a simple result demonstrating that\nthis holds for any distribution that is more skewed than a Cauchy distribution.\nWe also relate this to the situation of deadly catastrophic risks, where we\nassume a positive probability for an infinite value. That case gives a very\nsimple intuition why this phenomenon can occur for such catastrophic risks. We\nalso mention several open problems and conjectures in this context.",
      "generated_abstract": "The purpose of this paper is to investigate the impact of macroeconomic\nvariables on the volatility of the value of an investment. To do this, we\nconsider the logarithmic and exponential price processes and the\ndetrended-fluctuation-analysis (DFA) technique. We use the stock market of the\nPhilippines as an example. The results show that the volatility of the value\nof an investment exhibits a positive correlation with the exchange rate, which\nis inversely correlated with the inflation rate. The logarithmic price process\nwith the exponential deterrent process and the DFA technique are the most\nappropriate for this type of model. The results also show that the logarithmic\nprice process with the exponential deterrent process and the DFA technique is\nthe most appropriate for this type of model. The results show that the logarithmic\nprice process with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1926605504587156,
          "p": 0.3387096774193548,
          "f": 0.24561403046544242
        },
        "rouge-2": {
          "r": 0.016483516483516484,
          "p": 0.03260869565217391,
          "f": 0.021897805758432365
        },
        "rouge-l": {
          "r": 0.1834862385321101,
          "p": 0.3225806451612903,
          "f": 0.23391812403269394
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.02763v1",
      "true_abstract": "The Index of Dissimilarity (ID), widely utilized in economic literature as a\nmeasure of segregation, is inadequate for cross-country or time series studies\ndue to its failure to account for structural variations across countries' labor\nmarkets or changes over time within a single country's labor market. Building\non the works of Karmel and MacLachlan (1988) and Blackburn et al. (1993), we\npropose a new measure - the standardized ID - that isolates structural\ndifferences from true differences in segregation across space or time. A key\nadvantage of our proposed measure lies in its ease of implementation and\ninterpretation, even when working with datasets encompassing a large number of\ncountries or time periods. Moreover, our measure can be consistently applied in\nthe case of lumpy sectors or occupations that account for a large fraction of\nthe workforce. We illustrate the new measure in an analysis of the\ncross-country relationship between economic development (as measured by GDP per\ncapita) and occupational and sectoral gender segregation. Comparing the crude\nID with the standardized ID, we show that the crude ID overestimates the\npositive correlation between income and segregation, especially between low-\nand middle-income countries. This suggests that analyses relying on the crude\nID risk overestimating the importance of income differentials in explaining\ncross-country variation in gender segregation.",
      "generated_abstract": "We propose a novel, simple, and interpretable framework for evaluating\nnon-market value, based on the notion of the \"Market-Value-of-Market-Value.\"\nThis concept allows us to quantify the value of a market's value, even when the\nmarket does not explicitly sell its own stock. Using this measure, we show that\na company's market capitalization, even if it is not publicly traded, is\ngenerally non-market-valued. Further, we show that there are two primary\nreasons why a company's market capitalization is non-market-valued. First, it\nis non-market-valued when the company's stock is illiquid, and second, when\nthe company's stock is illiquid, but its market capitalization is not. We\ndemonstrate that this non-market-valued condition is not necessarily caused by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1450381679389313,
          "p": 0.2753623188405797,
          "f": 0.1899999954805001
        },
        "rouge-2": {
          "r": 0.03,
          "p": 0.061855670103092786,
          "f": 0.04040403600539676
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.2753623188405797,
          "f": 0.1899999954805001
        }
      }
    },
    {
      "paper_id": "physics.med-ph.physics/med-ph/2503.06131v1",
      "true_abstract": "This study focuses on the rotation of the hips and shoulders during a\nbaseball bat swing, analyzing the time-series changes in rotational angles,\nrotational velocities, and axes using marker position data obtained from a\nmotion capture system with 12 infrared cameras. Previous studies have examined\nfactors such as ground reaction forces, muscle activation patterns, rotational\nenergy, angular velocity, and angles during a swing. However, to the best of\nour knowledge, the hip and shoulder rotational motions have not been adequately\nvisualized or compared. In particular, there is a lack of analysis regarding\nthe coordination and timing differences between hip and shoulder movements\nduring the swing. Therefore, this study aims to quantitatively compare the hip\nand shoulder rotational movements during the swing between skilled and\nunskilled players and visualizes the differences between them. Based on the\nobtained data, the study aims to improve the understanding of bat swing\nmechanics by visualizing the coordinated body movements during the swing.",
      "generated_abstract": "The emergence of large-scale, open-source computational models of the human\nphysiological system represents a paradigm shift in the study of complex biological\nsystems. The current literature on open-source modeling of the human body is\nlargely focused on the computational simulation of tissues and organs,\noften neglecting the heterogeneity of human anatomy. This lack of\ninter-tissue connectivity is critical for capturing the complex interactions\nbetween tissues and organs, yet current approaches lack a comprehensive\nunderstanding of the inherent anatomical variability of the human body.\n  In this paper, we introduce a novel framework for modeling the human body\nbased on anatomically grounded structural connectivity, named the Human\nStructural Connectome Model (HSCM). The HSCM is based on the recently\nintroduced Human Connectome Project (HCP) brain connectivity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16161616161616163,
          "p": 0.21052631578947367,
          "f": 0.18285713794351033
        },
        "rouge-2": {
          "r": 0.03597122302158273,
          "p": 0.045454545454545456,
          "f": 0.040160637638103215
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.19736842105263158,
          "f": 0.1714285665149389
        }
      }
    },
    {
      "paper_id": "nlin.CG.nlin/CG/2411.03601v3",
      "true_abstract": "A one-dimensional cellular automaton $\\tau : A^\\mathbb{Z} \\to A^\\mathbb{Z}$\nis a transformation of the full shift defined via a finite neighborhood $S\n\\subset \\mathbb{Z}$ and a local function $\\mu : A^S \\to A$. We study the family\nof cellular automata whose finite neighborhood $S$ is an interval containing\n$0$, and there exists a pattern $p \\in A^S$ satisfying that $\\mu(z) = z(0)$ if\nand only if $z \\neq p$; this means that these cellular automata have a unique\n\\emph{active transition}. Despite its simplicity, this family presents\ninteresting and subtle problems, as the behavior of the cellular automaton\ncompletely depends on the structure of $p$. We show that every cellular\nautomaton $\\tau$ with a unique active transition $p \\in A^S$ is either\nidempotent or strictly almost equicontinuous, and we completely characterize\neach one of these situations in terms of $p$. In essence, the idempotence of\n$\\tau$ depends on the existence of a certain subpattern of $p$ with a\ntranslational symmetry.",
      "generated_abstract": "This work is the first to examine the dynamics of a class of nonlinear\nsystems characterized by a linearization of the state variables around a\ntrajectory of the system. These systems are described by the ordinary differential\nequations\n  \\begin{equation}\n    \\frac{d}{dt}x_i = f_i(x_1,\\ldots,x_n), \\quad i=1,\\ldots,n,\n  \\end{equation}\nwhere the $f_i$ are given functions and $x_i$ are the state variables. The\ndynamics of these systems is characterized by the vector field\n  \\begin{equation}\n    V = \\begin{bmatrix}\n      f_1(x_1,\\ldots,x_n) \\\\ \\vdots \\\\ f_n(x_1,\\ldots,x_n)\n    \\end{bmatrix}.\n  \\end{",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06796116504854369,
          "p": 0.1346153846153846,
          "f": 0.09032257618647264
        },
        "rouge-2": {
          "r": 0.020689655172413793,
          "p": 0.041666666666666664,
          "f": 0.02764976515109757
        },
        "rouge-l": {
          "r": 0.06796116504854369,
          "p": 0.1346153846153846,
          "f": 0.09032257618647264
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.09962v1",
      "true_abstract": "We consider a two-sided matching problem in which the agents on one side have\ndichotomous preferences and the other side representing institutions has strict\npreferences (priorities). It captures several important applications in\nmatching market design in which the agents are only interested in getting\nmatched to an acceptable institution. These include centralized daycare\nassignment and healthcare rationing. We present a compelling new mechanism that\nsatisfies many prominent and desirable properties including individual\nrationality, maximum size, fairness, Pareto-efficiency on both sides,\nstrategyproofness on both sides, non-bossiness and having polynomial time\nrunning time. As a result, we answer an open problem whether there exists a\nmechanism that is agent-strategyproof, maximum, fair and non-bossy.",
      "generated_abstract": "This paper explores the problem of distributing a fixed amount of money\nbetween two agents, one of whom has a higher utility than the other. In\naddition to the standard notion of equal opportunity, we consider a more\nsophisticated model in which the utility function of the first agent can depend\non the agent's type, while that of the second agent can depend on neither of\ntheir types. The two agents are in a non-equilibrium situation, and their\nutility functions are not constant over time. We show that, under certain\nassumptions, the agents may simultaneously maximize their joint utility and\nmaximize their individual utility. In addition, we provide a mechanism that\nachieves this joint maximization and individual maximization simultaneously.\nOur mechanism is a simple yet effective combination of a mechanism that\ndistributes the money among the agents in a way that maximizes their joint\nutility, and a mechanism that dist",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.23595505617977527,
          "f": 0.2427745614781651
        },
        "rouge-2": {
          "r": 0.07692307692307693,
          "p": 0.058394160583941604,
          "f": 0.0663900365875247
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.20224719101123595,
          "f": 0.20809248055330962
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.05013v1",
      "true_abstract": "This study utilizes machine learning algorithms to analyze and organize\nknowledge in the field of algorithmic trading. By filtering a dataset of 136\nmillion research papers, we identified 14,342 relevant articles published\nbetween 1956 and Q1 2020. We compare traditional practices-such as\nkeyword-based algorithms and embedding techniques-with state-of-the-art topic\nmodeling methods that employ dimensionality reduction and clustering. This\ncomparison allows us to assess the popularity and evolution of different\napproaches and themes within algorithmic trading. We demonstrate the usefulness\nof Natural Language Processing (NLP) in the automatic extraction of knowledge,\nhighlighting the new possibilities created by the latest iterations of Large\nLanguage Models (LLMs) like ChatGPT. The rationale for focusing on this topic\nstems from our analysis, which reveals that research articles on algorithmic\ntrading are increasing at a faster rate than the overall number of\npublications. While stocks and main indices comprise more than half of all\nassets considered, certain asset classes, such as cryptocurrencies, exhibit a\nmuch stronger growth trend. Machine learning models have become the most\npopular methods in recent years. The study demonstrates the efficacy of LLMs in\nrefining datasets and addressing intricate questions about the analyzed\narticles, such as comparing the efficiency of different models. Our research\nshows that by decomposing tasks into smaller components and incorporating\nreasoning steps, we can effectively tackle complex questions supported by case\nanalyses. This approach contributes to a deeper understanding of algorithmic\ntrading methodologies and underscores the potential of advanced NLP techniques\nin literature reviews.",
      "generated_abstract": "In this paper, we propose a novel method to estimate the average number of\ntrade days for a given portfolio in a given time horizon. The method relies on\nthe empirical distribution of the realized daily returns, which is known to be\nthe same across different portfolios. This distribution is obtained by\ncomputing the empirical distribution of the realized daily returns over the\nportfolio's realized daily returns. The empirical distribution is then used to\nestimate the average number of trade days for the portfolio. We then develop a\nnovel empirical estimator of the mean return that incorporates the realized\nreturns of the portfolio as well as the realized daily returns of the\nportfolio's constituents. We show that the empirical estimator of the mean\nreturn is consistent and asymptotically normal. We further propose a method to\ncompute the covariance matrix of the mean return, which is needed for the\nempir",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10919540229885058,
          "p": 0.2753623188405797,
          "f": 0.15637859675659208
        },
        "rouge-2": {
          "r": 0.004201680672268907,
          "p": 0.009433962264150943,
          "f": 0.00581394922458398
        },
        "rouge-l": {
          "r": 0.09770114942528736,
          "p": 0.2463768115942029,
          "f": 0.13991769140679786
        }
      }
    },
    {
      "paper_id": "math.CO.math/AC/2503.07299v1",
      "true_abstract": "Let $\\varphi:V\\times V\\to W$ be a bilinear map of finite vector spaces $V$\nand $W$ over a finite field $\\mathbb{F}_q$. We present asymptotic bounds on the\nnumber of isomorphism classes of bilinear maps under the natural action of\n$\\mathrm{GL}(V)$ and $\\mathrm{GL}(W)$, when $\\dim(V)$ and $\\dim(W)$ are\nlinearly related.\n  As motivations and applications of the results, we present almost tight upper\nbounds on the number of $p$-groups of Frattini class $2$ as first studied by\nHigman (Proc. Lond. Math. Soc., 1960). Such bounds lead to answers for some\nopen questions by Blackburn, Neumann, and Venkataraman (Cambridge Tracts in\nMathematics, 2007). Further applications include sampling matrix spaces with\nthe trivial automorphism group, and asymptotic bounds on the number of\nisomorphism classes of finite cube-zero commutative algebras.",
      "generated_abstract": "We establish a connection between the geometry of the Hilbert Sch\\\"{u}tzenberger\ntower and the combinatorics of the Gale dual of the fan of a projective\nlinearization of a finite-dimensional algebra over a field. In particular, we\nshow that the Gale dual of the fan of the projective linearization of a\nfinite-dimensional algebra over a field is a fan with a unique $0$-dimensional\nfacet and a unique $1$-dimensional facet, and we prove that the Hilbert Sch\\\"{u}tzenberger\ntower of the algebra is a cone over this fan. Furthermore, we show that the\ncone over this fan has a unique $0$-dimensional facet and a unique $1$-dimensional\nfacet, and we prove that the Hilbert Sch\\\"{u}tzenberger series of the algebra\nis a sum of the characteristic polynomial of this cone and the characteristic\npol",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10112359550561797,
          "p": 0.20454545454545456,
          "f": 0.13533834143705142
        },
        "rouge-2": {
          "r": 0.018018018018018018,
          "p": 0.02666666666666667,
          "f": 0.021505371531391988
        },
        "rouge-l": {
          "r": 0.0898876404494382,
          "p": 0.18181818181818182,
          "f": 0.12030074745208903
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.08566v1",
      "true_abstract": "We provide a characterization of those relation algebras which are isomorphic\nto the algebras of compatible relations of some $\\Z_2$-set. We further prove\nthat this class is finitely axiomatizable in first-order logic in the language\nof relation algebras.",
      "generated_abstract": "We construct a functor $\\mathcal{F}: \\mathbf{Spc}(\\mathbb{F}_2) \\to\n(\\mathbf{Set}^{\\mathrm{op}})^\\mathrm{op}$ from the category of sets to the\ncategory of functors from $\\mathbf{Spc}(\\mathbb{F}_2)$ to $\\mathbf{Set}^{\\mathrm{op}}$.\nWe show that $\\mathcal{F}$ is a functor of groups, which we interpret as a\nconstruction of a functor $\\mathcal{F}$ from the category of groups to the\ncategory of functors from $\\mathbf{Spc}(\\mathbb{F}_2)$ to $\\mathbf{Set}^{\\mathrm{op}}$.\nWe then show that the functor $\\mathcal{F}$ induces a functor of abelian groups\n$\\mathcal{F} \\times \\mathcal{F} : \\mathbf{Spc}(\\mathbb{F}_2) \\to\n(\\mathbf{Set}^{\\mathrm{",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27586206896551724,
          "p": 0.2222222222222222,
          "f": 0.24615384121183442
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.018867924528301886,
          "f": 0.022471905294787042
        },
        "rouge-l": {
          "r": 0.27586206896551724,
          "p": 0.2222222222222222,
          "f": 0.24615384121183442
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2405.02170v1",
      "true_abstract": "We consider the Fourier-Laplace transforms of a broad class of polynomial\nOrnstein-Uhlenbeck (OU) volatility models, including the well-known\nStein-Stein, Sch\\\"obel-Zhu, one-factor Bergomi, and the recently introduced\nQuintic OU models motivated by the SPX-VIX joint calibration problem. We show\nthe connection between the joint Fourier-Laplace functional of the log-price\nand the integrated variance, and the solution of an infinite dimensional\nRiccati equation. Next, under some non-vanishing conditions of the\nFourier-Laplace transforms, we establish an existence result for such Riccati\nequation and we provide a discretized approximation of the joint characteristic\nfunctional that is exponentially entire. On the practical side, we develop a\nnumerical scheme to solve the stiff infinite dimensional Riccati equations and\ndemonstrate the efficiency and accuracy of the scheme for pricing SPX options\nand volatility swaps using Fourier and Laplace inversions, with specific\nexamples of the Quintic OU and the one-factor Bergomi models and their\ncalibration to real market data.",
      "generated_abstract": "We investigate the effects of trading fees on the efficiency of a market\ncompetition model. We introduce an alternative cost function, which we call\nthe fairness cost, to capture the fairness of trading fees. We derive a\nmathematical representation of the fairness cost, and analyze the impact of\ndifferent fairness cost choices on the market competition model. We also\nintroduce a novel trading strategy that minimizes the fairness cost while\nmaximizing the market efficiency. The effectiveness of the proposed strategy\nis evaluated through Monte Carlo simulations and empirical studies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.23529411764705882,
          "f": 0.16551723681902505
        },
        "rouge-2": {
          "r": 0.021897810218978103,
          "p": 0.04,
          "f": 0.028301882220096858
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.19607843137254902,
          "f": 0.13793102992247339
        }
      }
    },
    {
      "paper_id": "cs.CL.stat/AP/2502.16556v1",
      "true_abstract": "This study examines how Large Language Models (LLMs) perform when tackling\nquantitative management decision problems in a zero-shot setting. Drawing on\n900 responses generated by five leading models across 20 diverse managerial\nscenarios, our analysis explores whether these base models can deliver accurate\nnumerical decisions under varying presentation formats, scenario complexities,\nand repeated attempts. Contrary to prior findings, we observed no significant\neffects of text presentation format (direct, narrative, or tabular) or text\nlength on accuracy. However, scenario complexity -- particularly in terms of\nconstraints and irrelevant parameters -- strongly influenced performance, often\ndegrading accuracy. Surprisingly, the models handled tasks requiring multiple\nsolution steps more effectively than expected. Notably, only 28.8\\% of\nresponses were exactly correct, highlighting limitations in precision. We\nfurther found no significant ``learning effect'' across iterations: performance\nremained stable across repeated queries. Nonetheless, significant variations\nemerged among the five tested LLMs, with some showing superior binary accuracy.\nOverall, these findings underscore both the promise and the pitfalls of\nharnessing LLMs for complex quantitative decision-making, informing managers\nand researchers about optimal deployment strategies.",
      "generated_abstract": "Recent advances in generative models have revolutionized language understanding,\nchallenging the traditional assumption that textual information alone\nenables a strong model to understand a sentence. While existing models have\nmade significant progress, the lack of explicit semantic information in textual\ndata makes it difficult to fully understand the context of a sentence. To\naddress this limitation, we propose a novel framework, Semantic-Guided\nGenerative Model (SGGM), which uses semantic-aware textual features to\nenhance generative performance. Specifically, we introduce a semantic\nrepresentation network, which extracts contextual information from textual\ndata, and a semantic-aware attention network, which integrates contextual\ninformation with semantic representations to enhance the generation process.\nExperimental results on three large-scale benchmark datasets demonstrate that\nSGGM outperforms state-of-the-art models, achieving significant improvements\nin textual understanding and generation quality.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08275862068965517,
          "p": 0.13636363636363635,
          "f": 0.10300428714472566
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08275862068965517,
          "p": 0.13636363636363635,
          "f": 0.10300428714472566
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/GT/2503.04542v1",
      "true_abstract": "Professional networks are a key determinant of individuals' labor market\noutcomes. They may also play a role in either exacerbating or ameliorating\ninequality of opportunity across demographic groups. In a theoretical model of\nprofessional network formation, we show that inequality can increase even\nwithout exogenous in-group preferences, confirming and complementing existing\ntheoretical literature. Increased inequality emerges from the differential\nleverage privileged and unprivileged individuals have in forming connections\ndue to their asymmetric ex ante prospects. This is a formalization of a source\nof inequality in the labor market which has not been previously explored.\n  We next show how inequality-aware platforms may reduce inequality by\nsubsidizing connections, through link recommendations that reduce costs,\nbetween privileged and unprivileged individuals. Indeed, mixed-privilege\nconnections turn out to be welfare improving, over all possible equilibria,\ncompared to not recommending links or recommending some smaller fraction of\ncross-group links. Taken together, these two findings reveal a stark reality:\nprofessional networking platforms that fail to foster integration in the link\nformation process risk reducing the platform's utility to its users and\nexacerbating existing labor market inequality.",
      "generated_abstract": "This paper introduces a novel approach for efficient and accurate\nestimation of the nonlinear mean curvature flow (NMCF) in high-dimensional\nmanifolds. The proposed method is based on the construction of a projection\noperator that maps the NMCF trajectory onto a low-dimensional manifold. The\nproposed projection operator is constructed using a sequence of\nnon-convex-optimization-based iterative methods. Numerical experiments are\nperformed to compare the performance of the proposed method with other\nalternative approaches. The results indicate that the proposed method is able\nto estimate the NMCF trajectory with a high degree of accuracy while\ncomparing favorably with other alternative approaches in terms of computational\ncomplexity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.078125,
          "p": 0.14925373134328357,
          "f": 0.10256409805338612
        },
        "rouge-2": {
          "r": 0.005813953488372093,
          "p": 0.011111111111111112,
          "f": 0.007633583276035531
        },
        "rouge-l": {
          "r": 0.078125,
          "p": 0.14925373134328357,
          "f": 0.10256409805338612
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2412.08987v1",
      "true_abstract": "Computational efficiency is essential for enhancing the accuracy and\npracticality of pricing complex financial derivatives. In this paper, we\ndiscuss Isogeometric Analysis (IGA) for valuing financial derivatives, modeled\nby two nonlinear Black-Scholes PDEs: the Leland model for European call with\ntransaction costs and the AFV model for convertible bonds with default options.\nWe compare the solutions of IGA with finite difference methods (FDM) and finite\nelement methods (FEM). In particular, very accurate solutions can be\nnumerically calculated on far less mesh (knots) than FDM or FEM, by using\nnon-uniform knots and weighted cubic NURBS, which in turn reduces the\ncomputational time significantly.",
      "generated_abstract": "We consider the problem of pricing American-style options with a volatility\n$\\sigma$ that is stochastic and depends on the underlying asset price\n$S(t)$. The underlying stochastic volatility is modeled by an It\\^o process with\nquadratic covariance, which allows us to derive a stochastic differential\nequation (SDE) for the price process of the American option. We show that the\nprice process is given by the solution of a system of stochastic differential\nequations with quadratic covariance, and solve it numerically. We also derive\nthe price of the European option, which is a special case of the American\noption. We show that the European option price is given by the solution of a\nsystem of stochastic differential equations with the same covariance as the\nAmerican option. We also derive the European option price formula in closed\nform. The formula for the European option price is expressed in terms",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.25396825396825395,
          "f": 0.2206896502582641
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.019230769230769232,
          "f": 0.01960783813917851
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.19047619047619047,
          "f": 0.16551723646516064
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.17679v1",
      "true_abstract": "Adverse childhood experiences (ACEs) have been linked to a wide range of\nnegative health outcomes in adulthood. However, few studies have investigated\nwhat specific combinations of ACEs most substantially impact mental health. In\nthis article, we provide the protocol for our observational study of the\neffects of combinations of ACEs on adult depression. We use data from the 2023\nBehavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We\nwill evaluate the replicability of our findings by splitting the sample into\ntwo discrete subpopulations of individuals. We employ data turnover for this\nanalysis, enabling a single team of statisticians and domain experts to\ncollaboratively evaluate the strength of evidence, and also integrating both\nqualitative and quantitative insights from exploratory data analysis. We\noutline our analysis plan using this method and conclude with a brief\ndiscussion of several specifics for our study.",
      "generated_abstract": "We introduce a novel framework for the analysis of data collected from a\nrandomized trial (RT) and analyze the impact of a treatment shift on a\npopulation of interest. This population is defined by a distribution of the\nchange in the observed treatment effect. This approach, known as the\n\"distribution-based approach,\" is more flexible and general than existing\napproaches, as it allows for a broader class of distributions and thus\nimproves generalizability of results. We use a simulated data example and a\nreal data example to illustrate the approach. The results show that the\ndistribution-based approach can be used to analyze the impact of a treatment\nshift on the population of interest, providing valuable insights into the\nimpact of the treatment shift.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17,
          "p": 0.23943661971830985,
          "f": 0.1988304045005302
        },
        "rouge-2": {
          "r": 0.014598540145985401,
          "p": 0.018518518518518517,
          "f": 0.01632652568230053
        },
        "rouge-l": {
          "r": 0.17,
          "p": 0.23943661971830985,
          "f": 0.1988304045005302
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.17431v1",
      "true_abstract": "In this paper, we show exponential dimensional dependence for the Hermite\nmethod of moments as a statistical test for Gaussianity in the case of i.i.d.\nGaussian variables, by constructing a lower bound for the the\nKolmogorov-Smirnov distance and an upper bound for the convex distance.",
      "generated_abstract": "This paper is concerned with the problem of recovering the parameter vector of a\nnon-linear dynamical system from noisy data, under the assumption that the\nsystem is governed by a linear dynamical model. This model is typically\nincorrect and therefore the observed data is noisy. The main contribution of\nthis paper is a novel method for recovering the unknown parameter vector from\nnoisy data, which is based on the use of the empirical likelihood function and\nits derivatives, along with a novel regularization technique. This novel\nregularization technique is designed to prevent the use of a large number of\ndata points in the recovery of the unknown parameter vector. The effectiveness\nof the proposed method is illustrated through numerical simulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2571428571428571,
          "p": 0.13846153846153847,
          "f": 0.17999999545000012
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.009708737864077669,
          "f": 0.013698625981423665
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.09230769230769231,
          "f": 0.11999999545000017
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.00239v1",
      "true_abstract": "Bayesian posterior approximation has become more accessible to practitioners\nthan ever, thanks to modern black-box software. While these tools provide\nhighly accurate approximations with minimal user effort, certain posterior\ngeometries remain notoriously difficult for standard methods. As a result,\nresearch into alternative approximation techniques continues to flourish. In\nmany papers, authors validate their new approaches by testing them on posterior\nshapes deemed challenging or \"wild.\" However, these shapes are not always\ndirectly linked to real-world applications where they naturally occur. In this\nnote, we present examples of practical applications that give rise to some\ncommonly used benchmark posterior shapes.",
      "generated_abstract": "This paper studies the problem of estimating a parameter $\\beta$ in a\nHausdorff distance model, where the data are drawn from an unknown probability\ndistribution. We assume that the data are generated by a random process\n$\\{\\boldsymbol{X}_n\\}$ satisfying a certain set of linear moment conditions.\nThe parameters are then estimated by computing the empirical risk, which is\ndefined as the average of squared residuals over the training data. We propose\nan estimator of $\\beta$ that is based on the empirical risk and use a\nprojection method to address the problem of unknown parameter $\\beta$. We show\nthat this estimator is consistent and asymptotically normal in the sense that\nits asymptotic distribution is the same as the distribution of the true\nparameter. Moreover, we derive the asymptotic distribution of the projection\nestimator. Numerical simulations are conducted to illustrate the\nestimation and inference properties of the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12643678160919541,
          "p": 0.12941176470588237,
          "f": 0.1279069717448623
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12643678160919541,
          "p": 0.12941176470588237,
          "f": 0.1279069717448623
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10172v1",
      "true_abstract": "In this paper, for solving nonlinear systems we propose two\npseudoinverse-free greedy block methods with momentum by combining the\nresidual-based weighted nonlinear Kaczmarz and heavy ball methods. Without the\nfull column rank assumptions on Jacobi matrices of nonlinear systems, we\nprovide a thorough convergence analysis, and derive upper bounds for the\nconvergence rates of the new methods. Numerical experiments demonstrate that\nthe proposed methods with momentum are much more effective than the existing\nones.",
      "generated_abstract": "We prove that the $L^p$-Laplacian on a compact Riemannian manifold is\nfinite-dimensional if and only if it is the $L^p$-Laplacian on a\n$C^\\infty$-smooth compact Riemannian manifold. In particular, the Laplacian\non the sphere is finite-dimensional if and only if the Laplacian on the sphere\nis the Laplacian on a $C^\\infty$-smooth compact Riemannian manifold. We also\nestablish a characterization of the $L^p$-Laplacian on a compact Riemannian\nmanifold with nonnegative curvature in terms of the $L^p$-Laplacian on a\n$C^\\infty$-smooth compact Riemannian manifold.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14035087719298245,
          "p": 0.26666666666666666,
          "f": 0.18390804145858117
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.047619047619047616,
          "f": 0.035398225417809376
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.26666666666666666,
          "f": 0.18390804145858117
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.16800v1",
      "true_abstract": "This paper introduces a new solution concept for the Cooperative Game with\nPublic Externalities, called the w-value, which is characterized by three\nproperties (axioms), namely Pareto-optimality (PO), Market-equilbrium (ME) and\nFiscal-balance (FB). Additionally, the implementation mechanism for w-value is\nalso provided. The w-value exists and is unique. It belongs to the core. And,\nmore specifically, it belongs to the -core. Meanwhile, the computational cost\nof w-value is very low. Therefore, the w-value is a theoretically more\ncompelling solution concept than the existing cooperation game solutions when\nanalyzing cooperative games with public externalities. A numerical illustration\nshows the calculation steps of w-value. Meanwhile, the w-value well explains\nthe reason why the mandatory emission reduction mechanism must be transformed\ninto a \"nationally determined contribution\" mechanism in current international\nclimate negotiations.",
      "generated_abstract": "We study the problem of choosing a set of agents and a set of actions\nas a stochastic optimization problem. This problem arises in a wide variety of\napplications in economic and social sciences, such as the optimal response to\na demand shock, the problem of allocating resources to agents, and the problem\nof selecting a coalition of agents. We propose a novel approach to the\nconstrained stochastic optimization problem. This approach is based on\ncombining the stochastic programming approach with the concept of\nmulti-armed bandits. This approach allows to solve the stochastic optimization\nproblem in a distributed way, without relying on a centralized algorithm. We\npropose a distributed algorithm for the stochastic optimization problem, and we\nshow that the proposed algorithm achieves the optimal convergence rate for the\nproblem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11702127659574468,
          "p": 0.171875,
          "f": 0.13924050150937364
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.018867924528301886,
          "f": 0.017777772794470532
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.15625,
          "f": 0.12658227366127242
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.01820v1",
      "true_abstract": "We consider a stochastic volatility model where the dynamics of the\nvolatility are given by a possibly infinite linear combination of the elements\nof the time extended signature of a Brownian motion. First, we show that the\nmodel is remarkably universal, as it includes, but is not limited to, the\ncelebrated Stein-Stein, Bergomi, and Heston models, together with some\npath-dependent variants. Second, we derive the joint characteristic functional\nof the log-price and integrated variance provided that some infinite\ndimensional extended tensor algebra valued Riccati equation admits a solution.\nThis allows us to price and (quadratically) hedge certain European and\npath-dependent options using Fourier inversion techniques. We highlight the\nefficiency and accuracy of these Fourier techniques in a comprehensive\nnumerical study.",
      "generated_abstract": "The global financial crisis of 2007-2009 had a profound impact on the\nmarket dynamics and the way financial institutions operate. It was the first\nglobal crisis that involved the entire financial system. It was a time when\nseveral financial institutions were in trouble, and the crisis was not limited\nto only one country. The impact of this crisis on the financial sector was\nhigher than any other financial crisis in the past. This paper explores the\nimpact of this crisis on financial institutions through a detailed analysis of\ntheir financial statements and corporate governance practices. It examines the\nimpact of this crisis on the performance of the financial institutions and\ndiscusses the factors that contributed to the recovery of the financial\ninstitutions after the crisis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13793103448275862,
          "p": 0.1935483870967742,
          "f": 0.16107382064411527
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.0196078431372549,
          "f": 0.01834861887383352
        },
        "rouge-l": {
          "r": 0.11494252873563218,
          "p": 0.16129032258064516,
          "f": 0.1342281830602227
        }
      }
    },
    {
      "paper_id": "cs.CY.q-fin/EC/2502.12397v1",
      "true_abstract": "Access to digital information is a driver of economic development. But\nalthough 85% of sub-Saharan Africa's population is covered by mobile broadband\nsignal, only 37% use the internet, and those who do seldom use the web. We\ninvestigate whether AI can bridge this gap by analyzing how 469 teachers use an\nAI chatbot in Sierra Leone. The chatbot, accessible via a common messaging app,\nis compared against traditional web search. Teachers use AI more frequently\nthan web search for teaching assistance. Data cost is the most frequently cited\nreason for low internet usage across Africa. The average web search result\nconsumes 3,107 times more data than an AI response, making AI 87% less\nexpensive than web search. Additionally, only 2% of results for corresponding\nweb searches contain content from Sierra Leone. In blinded evaluations, an\nindependent sample of teachers rate AI responses as more relevant, helpful, and\ncorrect than web search results. These findings suggest that AI-driven\nsolutions can cost-effectively bridge information gaps in low-connectivity\nregions.",
      "generated_abstract": "The increasingly complex nature of financial markets has led to the\napparent emergence of a new class of models, known as multi-agent systems,\nwhich mimic the complex behaviour of real-world markets. While these models\nprovide valuable insights into the mechanisms that drive market behaviour,\nunderstanding their impact on portfolio performance remains a critical\narea of research. This study aims to investigate the effect of agent-based\nmulti-agent systems on portfolio performance by evaluating the performance of\ntraditional portfolio strategies alongside those that incorporate multi-agent\nsystems. The results show that the incorporation of multi-agent systems can\nimprove the performance of traditional portfolio strategies, with the combination\nof both models achieving the highest performance. This study provides\ninsights into the potential of multi-agent systems to enhance portfolio\nperformance, offering a more nuanced understanding of the complex",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11965811965811966,
          "p": 0.17721518987341772,
          "f": 0.14285713804508554
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11965811965811966,
          "p": 0.17721518987341772,
          "f": 0.14285713804508554
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.10491v1",
      "true_abstract": "While music remains a challenging domain for generative models like\nTransformers, recent progress has been made by exploiting suitable\nmusically-informed priors. One technique to leverage information about musical\nstructure in Transformers is inserting such knowledge into the positional\nencoding (PE) module. However, Transformers carry a quadratic cost in sequence\nlength. In this paper, we propose F-StrIPE, a structure-informed PE scheme that\nworks in linear complexity. Using existing kernel approximation techniques\nbased on random features, we show that F-StrIPE is a generalization of\nStochastic Positional Encoding (SPE). We illustrate the empirical merits of\nF-StrIPE using melody harmonization for symbolic music.",
      "generated_abstract": "Recent advances in speech synthesis and generation technologies have\ndemonstrated significant progress in human-like speech quality, but they\nstill face significant limitations in naturalness and expressiveness. To\naddress these challenges, this paper introduces SpeechJet, a novel framework\nthat integrates multi-modal language modeling (MLLM) with multi-modal\ngenerative audio synthesis (GAS) to generate speech with natural sounding\nvocals and expressive vocal inflections. To capture both speech and vocal\nfeatures, we propose an end-to-end SpeechJet framework that integrates a\nmulti-modal GAS model and a multi-modal MLLM model. The former generates\nsynthetic speech from text, while the latter synthesizes vocal inflections via\ntext-to-speech (TTS) and text-to-vocal-inflections (TVI). To address the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12941176470588237,
          "p": 0.14473684210526316,
          "f": 0.13664595774854388
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.02,
          "f": 0.02020201520253159
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.14473684210526316,
          "f": 0.13664595774854388
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/QM/2503.07104v1",
      "true_abstract": "Whole-brain tractography in diffusion MRI is often followed by a parcellation\nin which each streamline is classified as belonging to a specific white matter\nbundle, or discarded as a false positive. Efficient parcellation is important\nboth in large-scale studies, which have to process huge amounts of data, and in\nthe clinic, where computational resources are often limited. TractCloud is a\nstate-of-the-art approach that aims to maximize accuracy with a local-global\nrepresentation. We demonstrate that the local context does not contribute to\nthe accuracy of that approach, and is even detrimental when dealing with\npathological cases. Based on this observation, we propose PETParc, a new method\nfor Parallel Efficient Tractography Parcellation. PETParc is a\ntransformer-based architecture in which the whole-brain tractogram is randomly\npartitioned into sub-tractograms whose streamlines are classified in parallel,\nwhile serving as global context for each other. This leads to a speedup of up\nto two orders of magnitude relative to TractCloud, and permits inference even\non clinical workstations without a GPU. PETParc accounts for the lack of\nstreamline orientation either via a novel flip-invariant embedding, or by\nsimply using flips as part of data augmentation. Despite the speedup, results\nare often even better than those of prior methods. The code and pretrained\nmodel will be made public upon acceptance.",
      "generated_abstract": "In this paper, we introduce a novel framework for generating synthetic\ndata for high-dimensional biomedical imaging. We focus on the case of\nmulti-modality data, where multiple modalities of images are available for the\nsame patient, and the goal is to generate synthetic data that faithfully\nrepresents the data from each modality. To address this problem, we propose a\nvariational autoencoder (VAE) framework, where the encoder and decoder\ncomponents are jointly optimized through a generative adversarial network\n(GAN). In the encoder component, we train a VAE model to generate synthetic\ndata that faithfully replicates the observed data from each modality, while in\nthe decoder component, we train a GAN model to generate synthetic data that\nfits the observed data from each modality. The framework is implemented in the\nPython programming language, and the implementation code is available at",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16551724137931034,
          "p": 0.3037974683544304,
          "f": 0.21428570971978644
        },
        "rouge-2": {
          "r": 0.01932367149758454,
          "p": 0.03571428571428571,
          "f": 0.02507836534939794
        },
        "rouge-l": {
          "r": 0.15862068965517243,
          "p": 0.2911392405063291,
          "f": 0.20535713829121505
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.07973v1",
      "true_abstract": "The Satellite-Terrestrial Integrated Network (STIN) enhances end-to-end\ntransmission by simultaneously utilizing terrestrial and satellite networks,\noffering significant benefits in scenarios like emergency response and\ncross-continental communication. Low Earth Orbit (LEO) satellite networks offer\nreduced Round Trip Time (RTT) for long-distance data transmission and serve as\na crucial backup during terrestrial network failures. Meanwhile, terrestrial\nnetworks are characterized by ample bandwidth resources and generally more\nstable link conditions. Therefore, integrating Multipath TCP (MPTCP) into STIN\nis vital for optimizing resource utilization and ensuring efficient data\ntransfer by exploiting the complementary strengths of both networks. However,\nthe inherent challenges of STIN, such as heterogeneity, instability, and\nhandovers, pose difficulties for traditional multipath schedulers, which are\ntypically designed for terrestrial networks. We propose a novel multipath data\nscheduling approach for STIN, Adaptive Latency Compensation Scheduler (ALCS),\nto address these issues. ALCS refines transmission latency estimates by\nincorporating RTT, congestion window size, inflight and queuing packets, and\nsatellite trajectory information. It further employs adaptive mechanisms for\nlatency compensation and proactive handover management. It further employs\nadaptive mechanisms for latency compensation and proactive handover management.\nImplemented in the MPTCP Linux Kernel and evaluated in a simulated STIN\ntestbed, ALCS outperforms existing multipath schedulers, delivering faster data\ntransmission and achieving throughput gains of 9.8% to 44.0% compared to\nbenchmark algorithms.",
      "generated_abstract": "This paper presents a scalable and efficient framework for training and\nperforming inference on a large number of data points using the Neural Information\nProcessing System (NIPS) architecture. The proposed framework is composed of\nthree major components: (1) an encoder-decoder architecture for modeling\nmulti-modal data, (2) a multi-scale feature extractor to reduce the dimension\nof the input data, and (3) a graph-based inference network for generating\npredictions. The encoder-decoder architecture is composed of two encoder\nmodules and two decoder modules. Each encoder module consists of a multi-layer\nperceptron (MLP) network, which is trained to model the interaction between\nmultiple modalities. The decoder modules consist of a MLP network, which\nprocesses the output of the encoder modules, and a graph-based inference network\nto perform the inference. The inference network is a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0784313725490196,
          "p": 0.15789473684210525,
          "f": 0.10480348901508381
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0784313725490196,
          "p": 0.15789473684210525,
          "f": 0.10480348901508381
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.06133v1",
      "true_abstract": "We introduce a new PL invariant, called the balanced genus, for balanced\nnormal $d$-pseudomanifolds. As a key result, we establish that for any\n3-manifold $M$ that is not a sphere, the balanced genus satisfies the lower\nbound $\\mathcal{G}_M \\geq m+3$, where $m$ is the rank of its fundamental group.\nFurthermore, we prove that a 3-manifold $M$ is homeomorphic to the 3-sphere if\nand only if its balanced genus $\\mathcal{G}_M$ is at most 3.\n  For 4-manifolds, we establish a similar characterization: if $M$ is not\nhomeomorphic to a sphere, then its balanced genus is bounded below by\n$\\mathcal{G}_M \\geq 2\\chi(M) + 5m + 11$, where $m$ is the rank of $\\pi_1(M)$.\nAdditionally, we prove that a 4-manifold $M$ is PL-homeomorphic to the 4-sphere\nif and only if its balanced genus satisfies $\\mathcal{G}_M \\leq 2\\chi(M) + 10$.\n  We believe that the balanced genus provides a new perspective in\ncombinatorial topology and will inspire further developments in the field. To\nthis end, we outline several research directions for future exploration.",
      "generated_abstract": "In this paper, we introduce a new family of $\\mathbb{Q}$-curves with\nequatorial singularities. We define their degree, prove that they are\nnon-singular, and describe their singularities in terms of a special\nLagrange-Jacobi system. We also show that they are smooth in $\\mathbb{Q}$ and\nthat their general point is a smooth elliptic curve. Our method is based on\ndeformation theory and we prove that a general $\\mathbb{Q}$-curve is a smooth\nelliptic curve in $\\mathbb{Q}$ if and only if it is a singular $\\mathbb{Q}$-curve\nwith equatorial singularities.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15555555555555556,
          "p": 0.2857142857142857,
          "f": 0.20143884435588233
        },
        "rouge-2": {
          "r": 0.05970149253731343,
          "p": 0.1095890410958904,
          "f": 0.0772946814245376
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.2857142857142857,
          "f": 0.20143884435588233
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2501.01454v2",
      "true_abstract": "Infectious diseases remain a critical global health challenge, and the\nintegration of standardized ontologies plays a vital role in managing related\ndata. The Infectious Disease Ontology (IDO) and its extensions, such as the\nCoronavirus Infectious Disease Ontology (CIDO), are essential for organizing\nand disseminating information related to infectious diseases. The COVID-19\npandemic highlighted the need for updating IDO and its virus-specific\nextensions. There is an additional need to update IDO extensions specific to\nbacteria, fungus, and parasite infectious diseases. We adopt the \"hub and\nspoke\" methodology to generate pathogen-specific extensions of IDO: Virus\nInfectious Disease Ontology (VIDO), Bacteria Infectious Disease Ontology\n(BIDO), Mycosis Infectious Disease Ontology (MIDO), and Parasite Infectious\nDisease Ontology (PIDO). The creation of pathogen-specific reference ontologies\nadvances modularization and reusability of infectious disease data within the\nIDO ecosystem. Future work will focus on further refining these ontologies,\ncreating new extensions, and developing application ontologies based on them,\nin line with ongoing efforts to standardize biological and biomedical\nterminologies for improved data sharing and analysis.",
      "generated_abstract": "The study examined the effects of dietary fibre on the performance of\nteat bred and natural-bred cows, with the objective of evaluating the\nbenefits of increasing the fibre intake on the production of milk. A\nrandomised, double-blind, controlled, two-way, three-period, crossover\nexperiment was conducted to evaluate the effects of a diet supplemented with\n0.5, 1 or 2% of dietary fibre on the production of milk, with each period\ncomprising six weeks. The results showed that the inclusion of fibre in the\ndiet had a positive effect on milk production, with a significant increase in\nthe milk yield (P<0.05) and fat content (P<0.001). However, the fibre supplement\nhad a negative impact on the growth performance of cows (P<",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08490566037735849,
          "p": 0.1232876712328767,
          "f": 0.10055865438781586
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08490566037735849,
          "p": 0.1232876712328767,
          "f": 0.10055865438781586
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.03551v1",
      "true_abstract": "This is the second of three papers motivated by the author's desire to\nunderstand and explain \"algebraically\" one aspect of Dmitriy Zhuk's proof of\nthe CSP Dichotomy Theorem. In this paper we extend Zhuk's \"bridge\" construction\nto arbitrary meet-irreducible congruences of finite algebras in locally finite\nvarieties with a Taylor term. We then connect bridges to centrality and\nsimilarity. In particular, we prove that Zhuk's bridges and our \"proper\nbridges\" (defined in our first paper) convey the same information in locally\nfinite Taylor varieties.",
      "generated_abstract": "We prove that the family of all finite dimensional irreducible representations\nof a finite dimensional algebra is closed under tensor product. This\ncorresponds to the algebraic structure of a category in the sense of\nHirschman, and we show that the tensor product is a monoidal functor. We also\nshow that the tensor product is fully faithful if the algebra is semisimple,\nwhich generalizes a result of Keller, and that the tensor product is\nequivalent to the category of representations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21311475409836064,
          "p": 0.30952380952380953,
          "f": 0.25242717963615807
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.015873015873015872,
          "f": 0.013888883967015632
        },
        "rouge-l": {
          "r": 0.21311475409836064,
          "p": 0.30952380952380953,
          "f": 0.25242717963615807
        }
      }
    },
    {
      "paper_id": "cs.MM.cs/MA/2503.09448v1",
      "true_abstract": "Proactive virtual reality (VR) streaming requires users to upload\nviewpoint-related information, raising significant privacy concerns. Existing\nstrategies preserve privacy by introducing errors to viewpoints, which,\nhowever, compromises the quality of experience (QoE) of users. In this paper,\nwe first delve into the analysis of the viewpoint leakage probability achieved\nby existing privacy-preserving approaches. We determine the optimal\ndistribution of viewpoint errors that minimizes the viewpoint leakage\nprobability. Our analyses show that existing approaches cannot fully eliminate\nviewpoint leakage. Then, we propose a novel privacy-preserving approach that\nintroduces noise to uploaded viewpoint prediction errors, which can ensure zero\nviewpoint leakage probability. Given the proposed approach, the tradeoff\nbetween privacy preservation and QoE is optimized to minimize the QoE loss\nwhile satisfying the privacy requirement. Simulation results validate our\nanalysis results and demonstrate that the proposed approach offers a promising\nsolution for balancing privacy and QoE.",
      "generated_abstract": "This paper introduces a new method for learning to generate music from text,\nusing an autoregressive model with a transformer-based encoder. Our method\ncombines a transformer-based encoder with a music-specific decoder, which\nlearns how to generate music based on a textual description of the music. The\ntransformer-based encoder generates an embedding representation for the music,\nwhich is passed to the decoder. The decoder generates musical notes, which are\nthen added to the input embedding representation. This is repeated until the\ndesired number of notes are generated, and the output is the final musical\nsequence. We demonstrate the effectiveness of our method on the task of\ngenerating music from text, with an evaluation of its performance on the\nMusicGen-12k dataset, using both a large-scale unsupervised pretraining approach\nand a self-supervised approach. We also show that the method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2077922077922078,
          "f": 0.18497109332620548
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.024193548387096774,
          "f": 0.02316601817504319
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.19480519480519481,
          "f": 0.173410399684587
        }
      }
    },
    {
      "paper_id": "cs.GR.eess/IV/2503.02218v1",
      "true_abstract": "Purpose: This study proposes a novel anatomically-driven dynamic modeling\nframework for coronary arteries using skeletal skinning weights computation,\naiming to achieve precise control over vessel deformation while maintaining\nreal-time performance for surgical simulation applications. Methods: We\ndeveloped a computational framework based on biharmonic energy minimization for\nskinning weight calculation, incorporating volumetric discretization through\ntetrahedral mesh generation. The method implements temporal sampling and\ninterpolation for continuous vessel deformation throughout the cardiac cycle,\nwith mechanical constraints and volume conservation enforcement. The framework\nwas validated using clinical datasets from 5 patients, comparing interpolated\ndeformation results against ground truth data obtained from frame-by-frame\nsegmentation across cardiac phases. Results: The proposed framework effectively\nhandled interactive vessel manipulation. Geometric accuracy evaluation showed\nmean Hausdorff distance of 4.96 +- 1.78 mm and mean surface distance of 1.78 +-\n0.75 mm between interpolated meshes and ground truth models. The Branch\nCompleteness Ratio achieved 1.82 +- 0.46, while Branch Continuity Score\nmaintained 0.84 +- 0.06 (scale 0-1) across all datasets. The system\ndemonstrated capability in supporting real-time guidewire-vessel collision\ndetection and contrast medium flow simulation throughout the complete coronary\ntree structure. Conclusion: Our skinning weight-based methodology enhances\nmodel interactivity and applicability while maintaining geometric accuracy. The\nframework provides a more flexible technical foundation for virtual surgical\ntraining systems, demonstrating promising potential for both clinical practice\nand medical education applications. The code is available at\nhttps://github.com/ipoirot/DynamicArtery.",
      "generated_abstract": "A novel approach to multi-view object reconstruction is proposed using\ntechniques from computer vision, reinforcement learning, and reinforcement\nlearning. The approach, called the Reconstructive Reinforcement Learning\n(RRL) method, uses the Replay Buffer Reinforcement Learning (RRL) technique to\nlearn a set of views, called replays, of the object being reconstructed, and\nuses them to train a Generative Adversarial Network (GAN) to reconstruct the\nobject. The replays are used as rewards in the training of the GAN, which is\ntrained to generate images that closely resemble the object being reconstructed\nfrom the replays. This approach has the potential to improve the\nreconstruction quality, especially in cases where the reconstruction is\nunsupervised or where the reconstruction is complex. The approach is tested\nwith synthetic data and it is shown that it can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09883720930232558,
          "p": 0.2125,
          "f": 0.13492063058704978
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09883720930232558,
          "p": 0.2125,
          "f": 0.13492063058704978
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02850v1",
      "true_abstract": "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
      "generated_abstract": "This study examines the statistical performance of two Bayesian hierarchical\nmodeling approaches for identifying and interpreting the association between\nexposure to a risk factor and the risk of developing a disease. The first\napproach uses a Bayesian hierarchical model for the conditional distribution\nof the exposure given the risk factor, whereas the second uses a\nmultilevel-multinomial logistic regression model for the conditional\ndistribution of the exposure given the risk factor, where the logistic\nregression model uses a logit transformation of the exposure. The study\ncompares the performance of these two models for the same risk factor in a\nreal-world cohort study of lung cancer patients. The study finds that the\nBayesian hierarchical model performs better than the multilevel-multinomial\nlogistic regression model for identifying the association between exposure to\na risk factor and the risk of developing lung cancer.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.2807017543859649,
          "f": 0.1748633836889726
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.0425531914893617,
          "f": 0.02826854680018549
        },
        "rouge-l": {
          "r": 0.10317460317460317,
          "p": 0.22807017543859648,
          "f": 0.142076498443071
        }
      }
    },
    {
      "paper_id": "econ.GN.stat/AP/2502.18253v1",
      "true_abstract": "Participants in online experiments often enroll over time, which can\ncompromise sample representativeness due to temporal shifts in covariates. This\nissue is particularly critical in A/B tests, online controlled experiments\nextensively used to evaluate product updates, since these tests are\ncost-sensitive and typically short in duration. We propose a novel framework\nthat dynamically assesses sample representativeness by dividing the ongoing\nsampling process into three stages. We then develop stage-specific estimators\nfor Population Average Treatment Effects (PATE), ensuring that experimental\nresults remain generalizable across varying experiment durations. Leveraging\nsurvival analysis, we develop a heuristic function that identifies these stages\nwithout requiring prior knowledge of population or sample characteristics,\nthereby keeping implementation costs low. Our approach bridges the gap between\nexperimental findings and real-world applicability, enabling product decisions\nto be based on evidence that accurately represents the broader target\npopulation. We validate the effectiveness of our framework on three levels: (1)\nthrough a real-world online experiment conducted on WeChat; (2) via a synthetic\nexperiment; and (3) by applying it to 600 A/B tests on WeChat in a\nplatform-wide application. Additionally, we provide practical guidelines for\npractitioners to implement our method in real-world settings.",
      "generated_abstract": "This paper investigates the impact of the COVID-19 pandemic on the\ndistribution of the wages of female and male workers in China. By using\nhistorical data from 1999 to 2021, the paper employs a time series\ndynamic panel data model to analyze the impact of the COVID-19 outbreak on\nwage inequality in China. The findings show that the COVID-19 pandemic\nsignificantly affected the wage distribution of female and male workers. The\nwage gap between male and female workers decreased by 1.89 percentage points\nafter the outbreak. Furthermore, the wage inequality of female workers decreased\nby 1.84 percentage points and that of male workers increased by 1.28 percentage\npoints. These results indicate that the impact of the COVID-19 outbreak on\nwage inequality is more pronounced in female workers than in male",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10638297872340426,
          "p": 0.24193548387096775,
          "f": 0.14778324698876472
        },
        "rouge-2": {
          "r": 0.005291005291005291,
          "p": 0.01020408163265306,
          "f": 0.006968636617662459
        },
        "rouge-l": {
          "r": 0.10638297872340426,
          "p": 0.24193548387096775,
          "f": 0.14778324698876472
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.09692v1",
      "true_abstract": "We present a model for interacting electrons in a continuum band structure\nthat resembles a ``trashcan'', with a flat bottom of radius $k_b$ beyond which\nthe dispersion increases rapidly with velocity $v$. The form factors of the\nBloch wavefunctions can be well-approximated by the Girvin-MacDonald-Platzman\nalgebra, which encodes the uniform Berry curvature. We demonstrate how this\nmodel captures the salient features of the low-energy Hamiltonian for\nelectron-doped pristine $n$-layer rhombohedral graphene (R$n$G) for appropriate\nvalues of the displacement field, and provide corresponding expressions for\n$k_b$. In the regime where the Fermi wavevector is close to $k_b$, we\nanalytically solve the Hartree-Fock equations for a gapped Wigner crystal in\nseveral limits of the model. We introduce a new method, the sliver-patch\napproximation, which extends the previous few-patch approaches and is crucial\nin both differentiating even vs odd Chern numbers of the ground state and\ngapping the Hartree-Fock solution. A key parameter is the Berry flux\n$\\varphi_{\\text{BZ}}$ enclosed by the (flat) bottom of the band. We\nanalytically show that there is a ferromagnetic coupling between the signs of\n$\\varphi_{\\text{BZ}}$ and the Chern number $C$ of the putative Wigner crystal.\nWe also study the competition between the $C=0$ and $1$ solutions as a function\nof the interaction potential for parameters relevant to R$n$G. By exhaustive\ncomparison to numerical Hartree-Fock calculations, we demonstrate how the\nanalytic results capture qualitative trends of the phase diagram, as well as\nquantitative details such as the enhancement of the effective velocity. Our\nanalysis paves the way for an analytic and numerical examination of the\nstability and competition beyond mean-field theory of the Wigner crystals in\nthis model.",
      "generated_abstract": "We study the effect of disorder on the phase diagram of the Kitaev model of\ntwo-dimensional spin chains with nearest-neighbor interactions. Our analysis\nrelies on the localization theory of disordered systems, which gives a\ndescription of the phase transition in terms of the critical localization\nlength. In the presence of disorder, the critical localization length is\naffected by both the disorder strength and the magnetic length, which is\ndetermined by the distance between nearest-neighbor spins. When the\ndisorder-spin-distance distance is much larger than the magnetic length, the\ncritical localization length behaves as $\\xi \\sim 1/\\sqrt{D}$, where $D$ is the\ndisorder strength. This behavior is in contrast to the case of a single\nspin-1/2, which exhibits a linear dependence of the critical localization\nlength on the disorder strength",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13580246913580246,
          "p": 0.3142857142857143,
          "f": 0.18965516820005954
        },
        "rouge-2": {
          "r": 0.036,
          "p": 0.08571428571428572,
          "f": 0.0507042211862729
        },
        "rouge-l": {
          "r": 0.12345679012345678,
          "p": 0.2857142857142857,
          "f": 0.17241378888971473
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.08435v1",
      "true_abstract": "We show how small molecules in an intense and cold molecular beam can be\nhighly nuclear-spin polarized via microwave or infrared rotational excitation\nschemes, followed by hyperfine-induced quantum beats. Repumping schemes can be\nused to achieve polarization above $90\\%$ in cases where single-pumping schemes\nare insufficient. Projected production rates in excess of $10^{21}$\n${\\text{s}^{-1}}$ allow applications including nuclear-magnetic-resonance\nsignal enhancement, and spin-polarized nuclear fusion, where polarized nuclei\nare known to enhance D-T and D-$^3$He fusion cross sections by $50\\%$.",
      "generated_abstract": "We present a novel approach to designing molecular assemblies for energy\ntransformations, based on the concept of molecular \"lenses\" that can be used to\ndirect the energy of a photon through a molecule. Our approach combines\nexperimental techniques with ab initio molecular dynamics simulations to\ninvestigate the photon-molecule interaction, focusing on the role of\nenvironmental effects on the dynamics of molecular lenses. We observe a\nsignificant influence of the molecular environment on the photon-molecule\ninteraction, with the photon's absorption rate increasing with the number of\nmolecules in the system. This effect is quantified through the \"lensing\ncoefficient\", which quantifies the photon's absorption rate when it is\ndirected through a molecule. Our results reveal that the absorption rate is\nhigher for molecules",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13432835820895522,
          "p": 0.13043478260869565,
          "f": 0.1323529361775521
        },
        "rouge-2": {
          "r": 0.05194805194805195,
          "p": 0.03773584905660377,
          "f": 0.04371584212009968
        },
        "rouge-l": {
          "r": 0.13432835820895522,
          "p": 0.13043478260869565,
          "f": 0.1323529361775521
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.02285v1",
      "true_abstract": "Monitoring a process/phenomenon of specific interest is prevalent in\nCyber-Physical Systems (CPS), remote healthcare, smart buildings, intelligent\ntransport, industry 4.0, etc. A key building block of the monitoring system is\na sensor sampling the process and communicating the status updates to a monitor\nfor detecting events of interest. Measuring the freshness of the status updates\nis essential for the timely detection of events, and it has received\nsignificant research interest in recent times. In this paper, we propose a new\nfreshness metric, Age of Detection (AoD), for monitoring the state transitions\nof a Discrete Time Markov Chain (DTMC) source over a lossy wireless channel. We\nconsider the pull model where the sensor samples DTMC state whenever the\nmonitor requests a status update. We formulate a Constrained Markov Decision\nProblem (CMDP) for optimising the AoD subject to a constraint on the average\nsampling frequency and solve it using the Lagrangian MDP formulation and\nRelative Value Iteration (RVI) algorithm. Our numerical results show\ninteresting trade-offs between AoD, sampling frequency, and transmission\nsuccess probability. Further, the AoD minimizing policy provides a lower\nestimation error than the Age of Information (AoI) minimizing policy, thus\ndemonstrating the utility of AoD for monitoring DTMC sources.",
      "generated_abstract": "This paper presents a novel framework for the automatic generation of\nacoustic event detection (AED) models from speech signals, leveraging\npre-trained speech models for feature extraction and a novel time-frequency\ntransform (TFT) based joint feature learning mechanism. The proposed framework\nis designed to address the challenges of scalability and robustness. The\nframework leverages a pre-trained speech model to extract speech features,\nwhich are then used for feature learning. The feature learning module uses a\nTFT based joint feature learning mechanism to enhance the robustness and\nscalability of the model. The framework is evaluated on a publicly available\ndataset, AED-MUSDB18, using a standard evaluation metric and is found to\noutperform state-of-the-art methods. The framework also demonstrates a\nsignificant reduction in the training time compared to existing approaches.\nThis framework provides a scalable and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1037037037037037,
          "p": 0.1794871794871795,
          "f": 0.1314553944190969
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.025210084033613446,
          "f": 0.019292599777091964
        },
        "rouge-l": {
          "r": 0.08888888888888889,
          "p": 0.15384615384615385,
          "f": 0.11267605169609224
        }
      }
    },
    {
      "paper_id": "math.OC.stat/AP/2502.18332v1",
      "true_abstract": "National teams from different continents can play against each other only in\nafew sports competitions. Therefore, a reasonable aim is maximising the number\nof intercontinental games in world cups, as done in basketball and football, in\ncontrast to handball and volleyball. However, this objective requires\nadditional draw constraints that imply the violation of equal treatment. In\naddition, the standard draw mechanism is non-uniformly distributed on the set\nof valid assignments, which may lead to further distortions. Our paper analyses\nthis novel trade-off between attractiveness and fairness through the example of\nthe 2025 World Men's Handball Championship. We introduce a measure of\ninequality, which enables considering 32 sets of reasonable geographical\nrestrictions to determine the Pareto frontier. The proposed methodology can be\nused by policy-makers to select the optimal set of draw constraints.",
      "generated_abstract": "We consider the problem of optimizing the performance of a network of\nnetworked sensors that collect data from an unknown physical system. The\nsystem is modeled by a Markov chain, and the goal is to find a\ndistribution-free optimal control policy that maximizes the expected performance\nof the network of sensors. We present a novel approach that combines the\nrecent developments in control theory for Markov chains with the recently\nproposed algorithmic control theory for Markov chains. Our approach is based on\nan optimal control problem with an operator-valued state and a Markovian\nobservable. We characterize the optimal control policy and present a\ncomputationally efficient algorithm that achieves optimal performance. Numerical\nsimulations validate our approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16831683168316833,
          "p": 0.26153846153846155,
          "f": 0.20481927234359137
        },
        "rouge-2": {
          "r": 0.015503875968992248,
          "p": 0.019230769230769232,
          "f": 0.017167377031812658
        },
        "rouge-l": {
          "r": 0.15841584158415842,
          "p": 0.24615384615384617,
          "f": 0.19277107957250703
        }
      }
    },
    {
      "paper_id": "nlin.PS.nlin/PS/2503.06289v1",
      "true_abstract": "We study a nonlinear magnetic metamaterial modeled as a split-ring resonator\narray, where the standard discrete laplacian is replaced by its fractional\nform. We find a closed-form expression for the dispersion relation as a\nfunction of the fractional exponent s and the gain/loss parameter {\\gamma} and\nexamine the conditions under which stable magneto-inductive waves exist. The\ndensity of states is computed in closed form and suggests that the main effect\nof fractionality is the flattening of the bands, while gain/loss increase tends\nto reduce the bandgaps. The spatial extent of the modes for a finite array is\ncomputed by means of the participation ratio R, which is also obtained in\nclosed form. For a fixed fractionality exponent, an increase in gain/loss\n{\\gamma} decreases the overall R, from the number of sites N towards N/2 at\nlarge {\\gamma}. The nonlinear dynamics of the average magnetic energy on an\ninitial ring during a cycle shows a monotonic increase with {\\gamma}, and it is\nqualitatively similar for all fractional exponents. This is explained as mainly\ndue to the interplay of nonlinearity and PT symmetry.",
      "generated_abstract": "The study of the nonlinear dynamics of a population of particles, with a\nparticularly complex structure, is a key issue in many disciplines. In this\nwork, we consider a class of nonlinear particle systems, which are based on\nthe interaction between particles that are subject to an internal stochastic\nnoise. The aim is to explore the properties of the dynamics in a given\nenvironment, in particular the stability of the population against\nperturbations. To do this, we introduce the concept of \\textit{quasi-stationary\ndynamics}, which allows us to establish the existence of invariant probability\ndensity functions. We then perform a series of numerical simulations, which\nillustrate the existence of a phase transition in the dynamics, that can be\nunderstood from the results of a study of the local stability of the\ninvariant density. Finally, we discuss the relevance of this phenomenon in\nreal-world situations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.20689655172413793,
          "f": 0.17999999508450013
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.030303030303030304,
          "f": 0.026315784560250226
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.19540229885057472,
          "f": 0.16999999508450017
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.06776v1",
      "true_abstract": "We address safe multi-robot interaction under uncertainty. In particular, we\nformulate a chance-constrained linear quadratic Gaussian game with coupling\nconstraints and system uncertainties. We find a tractable reformulation of the\ngame and propose a dual ascent algorithm. We prove that the algorithm converges\nto a generalized Nash equilibrium of the reformulated game, ensuring the\nsatisfaction of the chance constraints. We test our method in driving\nsimulations and real-world robot experiments. Our method ensures safety under\nuncertainty and generates less conservative trajectories than single-agent\nmodel predictive control.",
      "generated_abstract": "This paper presents a new method for the control of a quadrotor in a\ncone-shaped environment. The quadrotor is equipped with a gyroscope and a\nmagnetometer. We assume that the quadrotor is equipped with an inertial\nmeasurement unit (IMU) that provides the current pose of the quadrotor,\nrelative to the center of the cone-shaped environment. The IMU's data is\nprocessed using a Kalman filter, and then the position of the IMU is used to\nestimate the position of the center of the cone-shaped environment. The\nquadrotor is controlled using a nonlinear controller that is a polynomial\napproximation of the position-time history of the gyroscope and the\nmagnetometer. The proposed method is tested on a quadrotor flying in a\ncone-shaped environment and it is found that the proposed method can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16923076923076924,
          "p": 0.18032786885245902,
          "f": 0.17460316960821382
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.02,
          "f": 0.02197801702693023
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.16393442622950818,
          "f": 0.15873015373519797
        }
      }
    },
    {
      "paper_id": "math.AP.math/CA/2503.05140v1",
      "true_abstract": "In this paper, we investigate the mixed norm estimates for the operator $ T\n$associated with a dilated plane curve $(ut, u\\gamma(t))$, defined by \\[ Tf(x,\nu) := \\int_{0}^{1} f(x_1 - ut, x_2 - u\\gamma(t)) \\, dt, \\] where $ x := (x_1,\nx_2) $ and $\\gamma $ is a general plane curve satisfying appropriate smoothness\nand curvature conditions. Our results partially address a problem posed by\nHickman [J. Funct. Anal. 2016] in the two-dimensional setting. More precisely,\nwe establish the $ L_x^p(\\mathbb{R}^2) \\rightarrow L_x^q L_u^r(\\mathbb{R}^2\n\\times [1, 2]) $ (space-time) estimates for $ T $, whenever\n$(\\frac{1}{p},\\frac{1}{q})$ satisfy \\[ \\max\\left\\{0, \\frac{1}{2p} -\n\\frac{1}{2r}, \\frac{3}{p} - \\frac{r+2}{r}\\right\\} < \\frac{1}{q} \\leq\n\\frac{1}{p} < \\frac{r+1}{2r} \\] and $$1 + (1 + \\omega)\\left(\\frac{1}{q} -\n\\frac{1}{p}\\right) > 0,$$ where $ r \\in [1, \\infty] $ and $ \\omega :=\n\\limsup_{t \\rightarrow 0^+} \\frac{\\ln|\\gamma(t)|}{\\ln t} $. These results are\nsharp, except for certain borderline cases. Additionally, we examine the $\nL_x^p(\\mathbb{R}^2) \\rightarrow L_u^r L_x^q(\\mathbb{R}^2 \\times [1, 2]) $\n(time-space) estimates for $T $, which are especially almost sharp when $p=2$.",
      "generated_abstract": "In this paper we propose a novel approach to the study of the nonlinear\nsystems of the form\n\\begin{equation}\n\\label{nonlinear} \\begin{cases} \\frac{\\partial}{\\partial t} u(t,x) = \\Delta u(t,x) +\nf(x) u(t,x) \\\\ v(t,x) = g(x) u(t,x) \\end{cases}\n\\end{equation}\nwhere $f, g \\in C^{\\infty}(\\mathbb{R}^n \\times \\mathbb{R}^n)$, $\\Delta$ is the\nLaplace-Beltrami operator and $u(0,x) = x$. We show that the solution to\n(\\ref{nonlinear}) is in the form of a linear combination of the solutions of\nthe system of nonlinear equations\n\\begin{equation}\n\\label{nonlinear2}",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11475409836065574,
          "p": 0.25,
          "f": 0.15730336647393017
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.02702702702702703,
          "f": 0.0169491482375765
        },
        "rouge-l": {
          "r": 0.09836065573770492,
          "p": 0.21428571428571427,
          "f": 0.13483145636157065
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.14161v1",
      "true_abstract": "We derive the short-maturity asymptotics for option prices in the local\nvolatility model in a new short-maturity limit $T\\to 0$ at fixed $\\rho = (r-q)\nT$, where $r$ is the interest rate and $q$ is the dividend yield. In cases of\npractical relevance $\\rho$ is small, however our result holds for any fixed\n$\\rho$. The result is a generalization of the Berestycki-Busca-Florent formula\nfor the short-maturity asymptotics of the implied volatility which includes\ninterest rates and dividend yield effects of $O(((r-q) T)^n)$ to all orders in\n$n$. We obtain analytical results for the ATM volatility and skew in this\nasymptotic limit. Explicit results are derived for the CEV model. The\nasymptotic result is tested numerically against exact evaluation in the\nsquare-root model model $\\sigma(S)=\\sigma/\\sqrt{S}$, which demonstrates that\nthe new asymptotic result is in very good agreement with exact evaluation in a\nwide range of model parameters relevant for practical applications.",
      "generated_abstract": "This study introduces a novel methodology to enhance the understanding of\nasset price movements in volatile markets. The proposed framework leverages\ndeep learning techniques to analyze daily transaction volumes of stocks and\ncommodities from the NYSE and CME, focusing on the emerging markets of India and\nChina. By examining the temporal, spatial, and financial characteristics of\ntransaction volumes, the study highlights the key drivers of asset price\nmovements. The findings underscore the importance of considering both the\nmacro and micro factors in analyzing volatile markets and providing actionable\ninsights for investors and policymakers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.125,
          "f": 0.1052631530193908
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07954545454545454,
          "p": 0.109375,
          "f": 0.09210525828254873
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06673v1",
      "true_abstract": "We initiate systematic study of EZ-structures (and associated boundaries) of\ngroups acting on spaces that admit consistent and conical (equivalently,\nconsistent and convex) geodesic bicombings. Such spaces recently drew a lot of\nattention due to the fact that many classical groups act `nicely' on them. We\nrigorously construct EZ-structures, discuss their uniqueness (up to\nhomeomorphism), provide examples, and prove some boundary-related features\nanalogous to the ones exhibited by CAT(0) spaces and groups, which form a\nsubclass of the discussed class of spaces and groups.",
      "generated_abstract": "We study the $L^p$-norm on the space of $p$-dimensional closed convex bodies\nin the Euclidean space $\\mathbb{R}^n$ for $1\\leq p<\\infty$. In particular, we\nexhibit a sharp bound on the $L^p$-norm of the convex hull of the unit ball in\n$\\mathbb{R}^n$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.2222222222222222,
          "f": 0.13483145644741837
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.027777777777777776,
          "f": 0.01724137502972758
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.2222222222222222,
          "f": 0.13483145644741837
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.06528v1",
      "true_abstract": "This paper integrates the damped harmonic oscillator into DSGE models to\nbetter capture delayed economic adjustments. By introducing a damping\ncoefficient, I model economic recoveries as under-damped, critically damped, or\nover-damped processes. Numerical simulations illustrate how different damping\nlevels affect recovery speed and stability. This approach enhances DSGE models'\nrealism, offering insights into historical economic crises and improving\nmacroeconomic forecasting.",
      "generated_abstract": "This paper explores the impact of the COVID-19 pandemic on the global\neconomy through a dynamic global modeling approach. The model is built using\nthe GEMPAK 6.1 code, with the aim of simulating the evolution of economic\nactivity and its interaction with different types of shocks, including\npandemics. The results indicate that the COVID-19 crisis had a negative\nimpact on economic activity, with significant declines in the output of both\ndeveloped and developing countries. However, the analysis highlights the\nsignificant role played by fiscal stimulus measures, such as increased\npublic-sector spending, in mitigating the impact of the pandemic on economic\nactivity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.14492753623188406,
          "f": 0.16393442131550673
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.010638297872340425,
          "f": 0.01307189068648982
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.13043478260869565,
          "f": 0.14754097869255592
        }
      }
    },
    {
      "paper_id": "math.DS.math/CA/2503.07508v1",
      "true_abstract": "This paper relates to the Fourier decay properties of images of self-similar\nmeasures $\\mu$ on $\\mathbb{R}^k$ under nonlinear smooth maps $f \\colon\n\\mathbb{R}^k \\to \\mathbb{R}$. For example, we prove that if the linear parts of\nthe similarities defining $\\mu$ commute and the graph of $f$ has nonvanishing\nGaussian curvature, then the Fourier dimension of the image measure is at least\n$\\max\\left\\{ \\frac{2(2\\kappa_2 - k)}{4 + 2\\kappa_* - k} , 0 \\right\\}$, where\n$\\kappa_2$ is the lower correlation dimension of $\\mu$ and $\\kappa_*$ is the\nAssouad dimension of the support of $\\mu$. Under some additional assumptions on\n$\\mu$, we use recent breakthroughs in the fractal uncertainty principle to\nobtain further improvements for the decay exponents.\n  We give several applications to nonlinear arithmetic of self-similar sets $F$\nin the line. For example, we prove that if $\\dim_{\\mathrm H} F > (\\sqrt{65} -\n5)/4 = 0.765\\dots$ then the arithmetic product set $F \\cdot F = \\{ xy : x,y \\in\nF \\}$ has positive Lebesgue measure, while if $\\dim_{\\mathrm H} F > (-3 +\n\\sqrt{41})/4 = 0.850\\dots$ then $F \\cdot F \\cdot F$ has non-empty interior. One\nfeature of the above results is that they do not require any separation\nconditions on the self-similar sets.",
      "generated_abstract": "In this paper, we study the existence of solutions to the semilinear\nsysteem $Lu=f, L\\in \\mathbb{R}^{n\\times n}, f\\in \\mathbb{R}^n$ subject to\nperiodic boundary conditions and nonhomogeneous Neumann boundary conditions. We\nprove that the system is non-trivial for $n\\geq 2$ and $f\\in\nC([0,T),L^2(\\mathbb{T}^n))$. Moreover, we study the local existence of the\nsolution for $n\\geq 2$. We obtain the local existence results for the\nperiodic case and the local existence results for the non-periodic case by\nusing the fixed point theorem. We also obtain the global existence results for\nthe periodic case and the non-periodic case.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.26,
          "f": 0.1444444404320989
        },
        "rouge-2": {
          "r": 0.027932960893854747,
          "p": 0.07142857142857142,
          "f": 0.040160638528411244
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.26,
          "f": 0.1444444404320989
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.15035v1",
      "true_abstract": "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
      "generated_abstract": "We demonstrate that the number of distinct states of the protein\nbiochemical system can be significantly reduced if it is equipped with a\nspatially organized system of molecular switches. These molecular switches\nenforce an alternating switching of the protein's state, a process that can be\ndescribed as a sequence of switch-on and switch-off events. We show that this\nsequence can be represented by a Markov chain, which we refer to as the\nswitching chain, and show how it can be used to derive a Markov process that\nallows one to obtain the entire set of possible protein states from the\nswitching chain alone. In this way, we demonstrate that a spatially organized\nsystem of molecular switches can be used to design the entire set of possible\nprotein states, a concept that is also known as the switch-state\nrepresentation. The existence of a switch-state representation of the protein",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.3013698630136986,
          "f": 0.19047618615318312
        },
        "rouge-2": {
          "r": 0.01568627450980392,
          "p": 0.03361344537815126,
          "f": 0.021390369992708706
        },
        "rouge-l": {
          "r": 0.13291139240506328,
          "p": 0.2876712328767123,
          "f": 0.18181817749517445
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10554v1",
      "true_abstract": "The evolution from motion capture and teleoperation to robot skill learning\nhas emerged as a hotspot and critical pathway for advancing embodied\nintelligence. However, existing systems still face a persistent gap in\nsimultaneously achieving four objectives: accurate tracking of full upper limb\nmovements over extended durations (Accuracy), ergonomic adaptation to human\nbiomechanics (Comfort), versatile data collection (e.g., force data) and\ncompatibility with humanoid robots (Versatility), and lightweight design for\noutdoor daily use (Convenience). We present a wearable exoskeleton system,\nincorporating user-friendly immersive teleoperation and multi-modal sensing\ncollection to bridge this gap. Due to the features of a novel shoulder\nmechanism with synchronized linkage and timing belt transmission, this system\ncan adapt well to compound shoulder movements and replicate 100% coverage of\nnatural upper limb motion ranges. Weighing 5.2 kg, NuExo supports backpack-type\nuse and can be conveniently applied in daily outdoor scenarios. Furthermore, we\ndevelop a unified intuitive teleoperation framework and a comprehensive data\ncollection system integrating multi-modal sensing for various humanoid robots.\nExperiments across distinct humanoid platforms and different users validate our\nexoskeleton's superiority in motion range and flexibility, while confirming its\nstability in data collection and teleoperation accuracy in dynamic scenarios.",
      "generated_abstract": "This paper presents a novel method for the estimation of the ground-truth\nparameters of a robot's virtual environment. Inspired by the work of Kittler et\nal. (2021), we present a method that combines a pre-trained model, which estimates\nthe parameters of the virtual environment, with a deep learning model, which\nestimates the parameters of the robot's virtual environment. This method\nsignificantly outperforms existing methods in terms of estimation accuracy,\nproviding accurate estimates for the parameters of the robot's virtual\nenvironment. The method is implemented as a python package that is publicly\navailable at https://github.com/KittlerLab/virtual_env.git.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.2545454545454545,
          "f": 0.14358973953977658
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.05128205128205128,
          "f": 0.029962542680638537
        },
        "rouge-l": {
          "r": 0.09285714285714286,
          "p": 0.23636363636363636,
          "f": 0.13333332928336636
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.13868v1",
      "true_abstract": "Policy makers need to decide whether to treat or not to treat heterogeneous\nindividuals. The optimal treatment choice depends on the welfare function that\nthe policy maker has in mind and it is referred to as the policy learning\nproblem. I study a general setting for policy learning with semiparametric\nSocial Welfare Functions (SWFs) that can be estimated by locally\nrobust/orthogonal moments based on U-statistics. This rich class of SWFs\nsubstantially expands the setting in Athey and Wager (2021) and accommodates a\nwider range of distributional preferences. Three main applications of the\ngeneral theory motivate the paper: (i) Inequality aware SWFs, (ii) Inequality\nof Opportunity aware SWFs and (iii) Intergenerational Mobility SWFs. I use the\nPanel Study of Income Dynamics (PSID) to assess the effect of attending\npreschool on adult earnings and estimate optimal policy rules based on parental\nyears of education and parental income.",
      "generated_abstract": "We introduce a new measure of the fit of a Bayesian hierarchical model to\ndata, the Bayesian Information Criterion. We show that it is an unbiased\nestimator of the Bayesian information criterion (BIC) of a hierarchical model.\nThe BIC is an information-theoretic measure of the fit of a model to data. The\nBIC is defined as the sum of the log-likelihoods of the model parameters with\nall parameters fixed to their maximum likelihood estimates. A model is said to\nbe Bayesian when it is a function of the prior distribution of all model\nparameters. We show that the BIC is a Bayesian information criterion when the\nprior distribution of the model parameters is a Gaussian distribution. We\ndemonstrate that the BIC is unbiased, in the sense that its estimator is equal\nto the Bayesian information criterion when the prior is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12745098039215685,
          "p": 0.23636363636363636,
          "f": 0.1656050909894926
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.04081632653061224,
          "f": 0.0336134405338613
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.21818181818181817,
          "f": 0.1528662374863079
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.10447v1",
      "true_abstract": "In the Feedback Arc Set in Tournaments (Subset-FAST) problem, we are given a\ntournament $D$ and a positive integer $k$, and the objective is to determine\nwhether there exists an arc set $S \\subseteq A(D)$ of size at most $k$ whose\nremoval makes the graph acyclic. This problem is well-known to be equivalent to\na natural tournament ranking problem, whose task is to rank players in a\ntournament such that the number of pairs in which the lower-ranked player\ndefeats the higher-ranked player is no more than $k$. Using the PTAS for\nSubset-FAST [STOC 2007], Bessy et al. [JCSS 2011] present a $(2 +\n\\varepsilon)k$-vertex kernel for this problem, given any fixed $\\varepsilon >\n0$. A generalization of Subset-FAST, called Subset-FAST, further includes an\nadditional terminal subset $T \\subseteq V(D)$ in the input. The goal of\nSubset-FAST is to determine whether there is an arc set $S \\subseteq A(D)$ of\nsize at most $k$ whose removal ensures that no directed cycle passes through\nany terminal in $T$. Prior to our work, no polynomial kernel for Subset-FAST\nwas known. In our work, we show that Subset-FAST admits an $\\mathcal{O}((\\alpha\nk)^{2})$-vertex kernel, provided that Subset-FAST has an approximation\nalgorithm with an approximation ratio $\\alpha$. Consequently, based on the\nknown $\\mathcal{O}(\\log k \\log \\log k)$-approximation algorithm, we obtain an\nalmost quadratic kernel for Subset-FAST.",
      "generated_abstract": "We study the NP-hardness of the maximum-weight bipartite matching problem on\na bipartite graph with $n$ vertices and $m$ edges. For the special case of\nbipartite weighted graphs, we show that the maximum-weight bipartite matching\nproblem is NP-hard even if the graph is not connected. This result is a\ngeneralization of a result by Parekh et al. for bipartite weighted graphs in\n\\cite{Parekh14}. In addition, we show that the maximum-weight bipartite matching\nproblem on a connected graph is NP-hard. Finally, we show that the maximum-weight\nbipartite matching problem on a bipartite graph with $n$ vertices and $m$ edges\nis NP-hard even if the graph is not connected.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.41304347826086957,
          "f": 0.20994474759012247
        },
        "rouge-2": {
          "r": 0.035897435897435895,
          "p": 0.1076923076923077,
          "f": 0.053846150096154115
        },
        "rouge-l": {
          "r": 0.14074074074074075,
          "p": 0.41304347826086957,
          "f": 0.20994474759012247
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.02477v1",
      "true_abstract": "Two high-level \"pictures\" of probability theory have emerged: one that takes\nas central the notion of random variable, and one that focuses on distributions\nand probability channels (Markov kernels). While the channel-based picture has\nbeen successfully axiomatized, and widely generalized, using the notion of\nMarkov category, the categorical semantics of the random variable picture\nremain less clear. Simpson's probability sheaves are a recent approach, in\nwhich probabilistic concepts like random variables are allowed vary over a site\nof sample spaces. Simpson has identified rich structure on these sites, most\nnotably an abstract notion of conditional independence, and given examples\nranging from probability over databases to nominal sets. We aim bring this\ndevelopment together with the generality and abstraction of Markov categories:\nWe show that for any suitable Markov category, a category of sample spaces can\nbe defined which satisfies Simpson's axioms, and that a theory of probability\nsheaves can be developed purely synthetically in this setting. We recover\nSimpson's examples in a uniform fashion from well-known Markov categories, and\nconsider further generalizations.",
      "generated_abstract": "Let $X$ be a compact, connected, oriented manifold of dimension $n \\ge 3$,\nand let $\\omega$ be a $k$-th power invariant of $X$.  In this paper we\nconsider the class of manifolds $X$ for which there exists a $k$-th power\ninvariant $\\omega$ such that $\\omega$ is amenable and satisfies a\nmodularity property, i.e., there exists a group $H$ and a positive integer\n$m$ such that $H$ acts amenable on $X$ and $\\omega$ is isotopic to the\n$m$-th power invariant of $X$.  In the case that $X$ is a torus, we show that\nthe modularity property is equivalent to the existence of a non-trivial\nquasi-isometry between the sphere $S^n$ and the torus $T^{n-1}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.21875,
          "f": 0.15909090446281005
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.01,
          "f": 0.007692302958582794
        },
        "rouge-l": {
          "r": 0.08928571428571429,
          "p": 0.15625,
          "f": 0.11363635900826466
        }
      }
    },
    {
      "paper_id": "stat.ML.cs/AI/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "In this paper, we propose a novel approach for learning the hyperparameters of\nthe Gaussian Process (GP) regression model. Specifically, we leverage the\nproximal gradient algorithm to update the GP hyperparameters in a way that\nmaintains the covariance matrix of the GP and maximizes the Kullback-Leibler\ndivergence between the updated GP and the true GP. We first provide theoretical\nguarantees for our method and then evaluate its performance on a range of\nbenchmark datasets. Our method is particularly useful in the context of\nlearning the hyperparameters of the GP regression model, where the covariance\nmatrix of the GP is typically unknown or hard to estimate. In the context of\nlearning the hyperparameters of the GP regression model, we show that the\nproximal gradient algorithm has a lower bound that is close to optimal and can\nbe used to learn the hyperparameters of the GP regression model with a\nprobability",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14102564102564102,
          "p": 0.28205128205128205,
          "f": 0.1880341835897437
        },
        "rouge-2": {
          "r": 0.01485148514851485,
          "p": 0.02586206896551724,
          "f": 0.018867919893992672
        },
        "rouge-l": {
          "r": 0.12179487179487179,
          "p": 0.24358974358974358,
          "f": 0.1623931579487181
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08244v1",
      "true_abstract": "We establish the existence of intermittent two-point dynamics and infinite\nstationary measures for a class of random circle endomorphisms with zero\nLyapunov exponent, as a dynamical characterisation of the transition from\nsynchronisation (negative Lyapunov exponent) to chaos (positive Lyapunov\nexponent).",
      "generated_abstract": "In this paper, we study the dynamics of a model of a large class of\nnon-autonomous dynamical systems, the so-called $2$-step $C$-dynamics, where\n$C$ is a positive constant. We prove that the dynamical system has a unique\nlocal-in-time weak solution in $H^{2s}$ for any $s\\in(0,1)$ if $C<\\frac{1}{4}$\nand no such solution exists if $C>\\frac{1}{4}$. In addition, we prove that the\ndynamical system has no local-in-time weak solution in $H^{s}$ for any $s\\in\n(1/4,1)$ if $C>\\frac{1}{4}$. Our results are extended to the $2$-step\n$C$-dynamics with a non-negative real-valued non-linearity, and we show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.1864406779661017,
          "f": 0.23913043018194713
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.0125,
          "f": 0.016949148175812674
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.11864406779661017,
          "f": 0.15217390844281678
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08546v1",
      "true_abstract": "Positron Emission Tomography (PET) is a functional imaging modality that\nenables the visualization of biochemical and physiological processes across\nvarious tissues. Recently, deep learning (DL)-based methods have demonstrated\nsignificant progress in directly mapping sinograms to PET images. However,\nregression-based DL models often yield overly smoothed reconstructions lacking\nof details (i.e., low distortion, low perceptual quality), whereas GAN-based\nand likelihood-based posterior sampling models tend to introduce undesirable\nartifacts in predictions (i.e., high distortion, high perceptual quality),\nlimiting their clinical applicability. To achieve a robust\nperception-distortion tradeoff, we propose Posterior-Mean Denoising Diffusion\nModel (PMDM-PET), a novel approach that builds upon a recently established\nmathematical theory to explore the closed-form expression of\nperception-distortion function in diffusion model space for PET image\nreconstruction from sinograms. Specifically, PMDM-PET first obtained\nposterior-mean PET predictions under minimum mean square error (MSE), then\noptimally transports the distribution of them to the ground-truth PET images\ndistribution. Experimental results demonstrate that PMDM-PET not only generates\nrealistic PET images with possible minimum distortion and optimal perceptual\nquality but also outperforms five recent state-of-the-art (SOTA) DL baselines\nin both qualitative visual inspection and quantitative pixel-wise metrics PSNR\n(dB)/SSIM/NRMSE.",
      "generated_abstract": "We present a novel framework for joint segmentation and alignment of\nsegmented anatomical images. Our method, referred to as JAIN, leverages\npre-trained transformer-based models to simultaneously align and segment anatomical\nimages. By combining segmentation and alignment, JAIN achieves significant\nspeed-up and accuracy improvements. To address the challenges of segmentation\nand alignment, we propose a novel self-attention mechanism called\nself-attention-aligned-representation (SAR). SAR leverages the self-attention\nlayer to align the semantic features of segmented regions, which are then used\nfor alignment. We demonstrate the effectiveness of our method on an\nanatomical image dataset, achieving higher accuracy and speed than state-of-the-art\nmethods. Our code is available at https://github.com/SophieLiu95/JAIN.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1292517006802721,
          "p": 0.25,
          "f": 0.17040358295079341
        },
        "rouge-2": {
          "r": 0.010869565217391304,
          "p": 0.020202020202020204,
          "f": 0.014134271069436717
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.23684210526315788,
          "f": 0.16143497308532256
        }
      }
    },
    {
      "paper_id": "math.CV.math/CV/2503.05976v1",
      "true_abstract": "We prove that the (hermitian) rank of $QP^d$ is bounded from below by the\nrank of $P^d$ whenever $Q$ is not identically zero and real-analytic in a\nneighborhood of some point on the zero set of $P$ in $\\mathbb{C}^n$ and $P$ is\na polynomial of bidegree at most $(1,1)$. This result generalizes the theorem\nof D'Angelo and the second author which assumed that $P$ was bihomogeneous.\nExamples show that no hypothesis can be dropped.",
      "generated_abstract": "We study the stability of a class of linear operators $T:\\mathbb{R}^{n}\\to\n\\mathbb{R}^{m}$ that preserve the $L^{p}$-norm for some $1\\leq p<\\infty$. We\nexamine the conditions under which the operator is stable, i.e. the existence\nof a positive constant $C$ such that the operator $T$ is stable if and only if\nthe corresponding operator $\\hat{T}$ is stable with the same $L^{p}$-norm. We\nalso study the conditions under which the operator is unstable. We present a\nsufficient condition for the operator to be unstable. We provide some\nillustrative examples for the conditions under which the operator is stable or\nunstable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.19230769230769232,
          "f": 0.19047618547664413
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.012987012987012988,
          "f": 0.013422813797578544
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.19230769230769232,
          "f": 0.19047618547664413
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2411.12010v2",
      "true_abstract": "The advancement of novel combinatorial CRISPR screening technologies enables\nthe identification of synergistic gene combinations on a large scale. This is\ncrucial for developing novel and effective combination therapies, but the\ncombinatorial space makes exhaustive experimentation infeasible. We introduce\nNAIAD, an active learning framework that efficiently discovers optimal gene\npairs capable of driving cells toward desired cellular phenotypes. NAIAD\nleverages single-gene perturbation effects and adaptive gene embeddings that\nscale with the training data size, mitigating overfitting in small-sample\nlearning while capturing complex gene interactions as more data is collected.\nEvaluated on four CRISPR combinatorial perturbation datasets totaling over\n350,000 genetic interactions, NAIAD, trained on small datasets, outperforms\nexisting models by up to 40\\% relative to the second-best. NAIAD's\nrecommendation system prioritizes gene pairs with the maximum predicted\neffects, resulting in the highest marginal gain in each AI-experiment round and\naccelerating discovery with fewer CRISPR experimental iterations. Our NAIAD\nframework (https://github.com/NeptuneBio/NAIAD) improves the identification of\nnovel, effective gene combinations, enabling more efficient CRISPR library\ndesign and offering promising applications in genomics research and therapeutic\ndevelopment.",
      "generated_abstract": "The molecular basis of cancer has been well studied, but the role of\nenvironmental exposures in cancer initiation and progression remains\nunder-explored. Recent advances in high-throughput cancer cell imaging techniques\nhave enabled the study of the interaction between tumor cells and their\nenvironment. Here, we provide a brief overview of the various types of\ncellular imaging technologies used in cancer research, highlighting their\nadvantages and limitations, and discussing their potential for enhancing our\nunderstanding of cancer biology. Additionally, we outline the various\napproaches used to assess the interaction between tumor cells and their\nenvironment, focusing on the role of the extracellular matrix in cancer cell\nadhesion and invasion. Finally, we discuss the potential applications of these\ntechniques in cancer research, emphasizing the potential for advancements in\ncancer treatment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1076923076923077,
          "p": 0.18421052631578946,
          "f": 0.13592232544066377
        },
        "rouge-2": {
          "r": 0.0058823529411764705,
          "p": 0.009345794392523364,
          "f": 0.007220211865138851
        },
        "rouge-l": {
          "r": 0.08461538461538462,
          "p": 0.14473684210526316,
          "f": 0.1067961118484308
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2501.02214v1",
      "true_abstract": "One approach to estimating the average treatment effect in binary treatment\nwith unmeasured confounding is the proximal causal inference, which assumes the\navailability of outcome and treatment confounding proxies. The key identifying\nresult relies on the existence of a so-called bridge function. A parametric\nspecification of the bridge function is usually postulated and estimated using\nstandard techniques. The estimated bridge function is then plugged in to\nestimate the average treatment effect. This approach may have two efficiency\nlosses. First, the bridge function may not be efficiently estimated since it\nsolves an integral equation. Second, the sequential procedure may fail to\naccount for the correlation between the two steps. This paper proposes to\napproximate the integral equation with increasing moment restrictions and\njointly estimate the bridge function and the average treatment effect. Under\nsufficient conditions, we show that the proposed estimator is efficient. To\nassist implementation, we propose a data-driven procedure for selecting the\ntuning parameter (i.e., number of moment restrictions). Simulation studies\nreveal that the proposed method performs well in finite samples, and\napplication to the right heart catheterization dataset from the SUPPORT study\ndemonstrates its practical value.",
      "generated_abstract": "We propose a novel semiparametric bootstrap method for estimating the\ndistribution of the mean of a random vector of continuous random variables\nassuming a finite number of moments, which can be computed from a finite sample\nof observations. The method is based on the construction of a predictive\ndistribution of the random vector and a semiparametric bootstrap of the\npredictive distribution. The predictive distribution is constructed by\nincorporating information from the moments of the random vector. We derive a\nclosed-form expression for the predictive distribution and propose two\nsemiparametric bootstrap methods. We illustrate the proposed method by using\nsimulation studies and an application to the estimation of the mean of a random\nvector of standard Gaussian random variables.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19834710743801653,
          "p": 0.42857142857142855,
          "f": 0.27118643635226153
        },
        "rouge-2": {
          "r": 0.06976744186046512,
          "p": 0.1276595744680851,
          "f": 0.09022555933970287
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.39285714285714285,
          "f": 0.24858756629576434
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.06882v1",
      "true_abstract": "Maximum Inner Product Search (MIPS) for high-dimensional vectors is pivotal\nacross databases, information retrieval, and artificial intelligence. Existing\nmethods either reduce MIPS to Nearest Neighbor Search (NNS) while suffering\nfrom harmful vector space transformations, or attempt to tackle MIPS directly\nbut struggle to mitigate redundant computations due to the absence of the\ntriangle inequality. This paper presents a novel theoretical framework that\nequates MIPS with NNS without requiring space transformation, thereby allowing\nus to leverage advanced graph-based indices for NNS and efficient edge pruning\nstrategies, significantly reducing unnecessary computations. Despite a strong\nbaseline set by our theoretical analysis, we identify and address two\npersistent challenges to further refine our method: the introduction of the\nProximity Graph with Spherical Pathway (PSP), designed to mitigate the issue of\nMIPS solutions clustering around large-norm vectors, and the implementation of\nAdaptive Early Termination (AET), which efficiently curtails the excessive\nexploration once an accuracy bottleneck is reached. Extensive experiments\nreveal the superiority of our method over existing state-of-the-art techniques\nin search efficiency, scalability, and practical applicability. Compared with\nstate-of-the-art graph based methods, it achieves an average 35% speed-up in\nquery processing and a 3x reduction in index size. Notably, our approach has\nbeen validated and deployed in the search engines of Shopee, a well-known\nonline shopping platform. Our code and an industrial-scale dataset for offline\nevaluation will also be released to address the absence of e-commerce data in\npublic benchmarks.",
      "generated_abstract": "This paper introduces a novel framework for querying data from multiple\ndata sources. The framework is based on the concept of a data grid, where each\ndata source is treated as a separate, distinct grid. The query execution\nprocess is decomposed into three phases: (1) data grid query, (2) data grid\npartitioning, and (3) data grid execution. The data grid query phase is\noptimized for performance by leveraging techniques such as graph partitioning,\ndeduplication, and compression. The data grid partitioning phase is\noptimized for scalability by using techniques such as range partitioning and\noptimized aggregations. The data grid execution phase is optimized for\nefficient execution by using techniques such as parallelism, distributed\nexecution, and parallelized aggregations. The framework is evaluated on a\nreal-world dataset and demonstrates significant performance gains over existing\nframeworks. The paper also provides a detailed technical description of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.29333333333333333,
          "f": 0.17529880059046693
        },
        "rouge-2": {
          "r": 0.017391304347826087,
          "p": 0.03389830508474576,
          "f": 0.022988501265029278
        },
        "rouge-l": {
          "r": 0.11931818181818182,
          "p": 0.28,
          "f": 0.1673306731004271
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.19333v1",
      "true_abstract": "Despite advances in methods to interrogate tumor biology, the observational\nand population-based approach of classical cancer research and clinical\noncology does not enable anticipation of tumor outcomes to hasten the discovery\nof cancer mechanisms and personalize disease management. To address these\nlimitations, individualized cancer forecasts have been shown to predict tumor\ngrowth and therapeutic response, inform treatment optimization, and guide\nexperimental efforts. These predictions are obtained via computer simulations\nof mathematical models that are constrained with data from a patient's cancer\nand experiments. This book chapter addresses the validation of these\nmathematical models to forecast tumor growth and treatment response. We start\nwith an overview of mathematical modeling frameworks, model selection\ntechniques, and fundamental metrics. We then describe the usual strategies\nemployed to validate cancer forecasts in preclinical and clinical scenarios.\nFinally, we discuss existing barriers in validating these predictions along\nwith potential strategies to address them.",
      "generated_abstract": "This paper presents the results of a study on the performance of artificial\nnuclear magnetic resonance (ANMR) in the detection of the nuclear magnetic\nresonance (NMR) signal of a single nucleotide polymorphism (SNP) in a\nmetagenomic dataset. We conducted a comparative analysis of the performance of\nANMR and NMR techniques in the detection of the NMR signal of a SNP, as well as\nthe accuracy and sensitivity of the two techniques. The results of the study\nshow that ANMR is superior to NMR in terms of accuracy and sensitivity,\nparticularly for low signal-to-noise ratios. The performance of ANMR in the\ndetection of the NMR signal of the SNP was found to be comparable to that of\nNMR, with an average of 86.14% accuracy and 95.45% sensitivity. In",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10891089108910891,
          "p": 0.16666666666666666,
          "f": 0.1317365221657286
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.009900990099009901,
          "f": 0.008298750317662675
        },
        "rouge-l": {
          "r": 0.10891089108910891,
          "p": 0.16666666666666666,
          "f": 0.1317365221657286
        }
      }
    },
    {
      "paper_id": "physics.pop-ph.physics/pop-ph/2501.08583v1",
      "true_abstract": "In this work, I propose a way to help high school students and the general\npopulation understand quantum concepts by adopting a new inherently dual\nrepresentation. Major difficulties in explaining to people the basic concepts\nof quantum mechanics reside in the apparent impossibility of representing\nquantum superposition with examples taken from everyday life. In this context,\nI propose a new pictorial paradigm that illustrates a number of quantum\nconcepts by means of optical illusions, potentially without raising\nmisconceptions. The method is based on \"bistable reversible figures\", which\ninduce in the viewer a multistable perception, conveying a direct understanding\nof superposition, random collapse, and observer effect via a sensorial\nexperience. I present the advantages and discuss the limitations of this\nanalogy, and show how it extends to the concepts of complementarity and quantum\nentanglement, also helping to avoiding misconceptions in quantum teleportation.\nFinally, I also address quantum spin and quantum measurement by using different\ntypes of optical illusions.",
      "generated_abstract": "The effect of a background field on the evolution of a plasma is a topic of\ninterest in astrophysics, where the field can be a magnetic field or a\nmolecular or atomic cloud. Here we consider the evolution of the plasma in a\nmagnetized vacuum in the presence of a uniform electric field. We derive the\ngeneral form of the evolution equation for the plasma density $n$ and the\nelectric field $E$, and we show that the field can cause a time-dependent\nshrinking of the plasma. We also derive the evolution equation for the\nelectric field. The resulting system of equations is nonlinear and is solved\nnumerically, and we show that the field can cause a time-dependent\nexpansion of the plasma. The results are illustrated with a simple example.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1346153846153846,
          "p": 0.23333333333333334,
          "f": 0.1707317026769781
        },
        "rouge-2": {
          "r": 0.020689655172413793,
          "p": 0.030612244897959183,
          "f": 0.02469135321174034
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.21666666666666667,
          "f": 0.15853658072575863
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04480v1",
      "true_abstract": "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
      "generated_abstract": "Recent advancements in large language models (LLMs) have demonstrated\naccurate prediction of complex time series, enabling the development of\ntime-series-based forecasting models. However, the inherent ambiguity of time\nseries, especially in complex and non-stationary settings, poses significant\nchallenges in generating accurate forecasts. In this paper, we introduce\nAmbiguity-Aware Time Series Forecasting (A2TSF), a novel method that leverages\nthe ambiguity of time series to improve forecast accuracy. A2TSF introduces\ntwo key components: (1) a novel ambiguity-aware loss function that quantifies\nthe degree of ambiguity in time series, and (2) a novel uncertainty\nquantification technique that generates uncertainty maps for each time series\nfeature. These features are then used to inform the generation of forecasts.\nExperimental results on a variety of datasets",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1016949152542373,
          "p": 0.14285714285714285,
          "f": 0.1188118763297718
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1016949152542373,
          "p": 0.14285714285714285,
          "f": 0.1188118763297718
        }
      }
    },
    {
      "paper_id": "math.ST.cs/CC/2503.02802v1",
      "true_abstract": "In this work, we show the first average-case reduction transforming the\nsparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a\nconsequence obtain the first computational equivalence result between two\nwell-studied high-dimensional statistics models. Our approach leverages a new\nperturbation equivariance property for Gram-Schmidt orthogonalization, enabling\nremoval of dependence in the noise while preserving the signal.",
      "generated_abstract": "We propose a novel way to define a set of linear equations in a matrix\n$A$ of size $m\\times n$ over $\\mathbb{F}_q$ as a set of linear equations in a\n$m\\times n$ matrix $B$ of size $n\\times p$ over $\\mathbb{F}_q$, where $B$ is\ndefined as a product of $m$ copies of $A$. We show that if $A$ is invertible,\nthen $B$ is invertible and the matrix $B$ is of size $m\\times n/p$. In other\nwords, $B$ is obtained from $A$ by a sequence of row operations of size $m$\nresulting in an invertible matrix. We prove that for a sufficiently large $p$\nand $A$ of size $m\\times n$, if $B$ is obtained from $A$ by a sequence of row\noperations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1836734693877551,
          "p": 0.16071428571428573,
          "f": 0.17142856645079382
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.011111111111111112,
          "f": 0.013793098739597326
        },
        "rouge-l": {
          "r": 0.1836734693877551,
          "p": 0.16071428571428573,
          "f": 0.17142856645079382
        }
      }
    },
    {
      "paper_id": "math.RT.math/GN/2502.08847v1",
      "true_abstract": "A representation $\\rho$ of a compact group $\\mathbb{G}$ selects eigenvalues\nif there is a continuous circle-valued map on $\\mathbb{G}$ assigning an\neigenvalue of $\\rho(g)$ to every $g\\in \\mathbb{G}$. For every compact connected\n$\\mathbb{G}$, we characterize the irreducible $\\mathbb{G}$-representations\nwhich select eigenvalues as precisely those annihilating the intersection\n$Z_0(\\mathbb{G})\\cap \\mathbb{G}'$ of the connected center of $\\mathbb{G}$ with\nits derived subgroup. The result applies more generally to finite-spectrum\nrepresentations isotypic on $Z_0(\\mathbb{G})$, and recovers as applications\n(noted in prior work) the existence of a continuous eigenvalue selector for the\nnatural representation of $\\mathrm{SU}(n)$ and the non-existence of such a\nselector for $\\mathrm{U}(n)$.",
      "generated_abstract": "We present a new method to prove the strong uniqueness of solutions for\nintegro-differential equations (IDEs). Our approach relies on the existence of\nsolutions of the IDEs under a certain condition on the coefficients, which can\nbe checked by solving a system of differential equations (DEs) that can be\nderived from the IDEs. This approach is applied to the Keller-Segel system\nintroduced by Keller and Segel, which describes the interaction between\nbacteria. By solving the DEs arising from the IDEs of the Keller-Segel system,\nwe prove that the strong uniqueness of solutions of the IDEs holds under the\ncondition that the coefficients of the IDEs are positive. Furthermore, we\nprove the strong weak uniqueness of solutions of the IDEs under the condition\nthat the coefficients of the IDEs are nonnegative.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16901408450704225,
          "p": 0.1935483870967742,
          "f": 0.1804511228424446
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.03225806451612903,
          "f": 0.0317460267472923
        },
        "rouge-l": {
          "r": 0.16901408450704225,
          "p": 0.1935483870967742,
          "f": 0.1804511228424446
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.05418v1",
      "true_abstract": "Integrated sensing and communication (ISAC) has been identified as a\npromising technology for the sixth generation (6G) of communication networks.\nTarget privacy in ISAC is essential to ensure that only legitimate sensors can\ndetect the target while keeping it hidden from malicious ones. In this paper,\nwe consider a downlink reconfigurable intelligent surface (RIS)-assisted ISAC\nsystem capable of protecting a sensing region against an adversarial detector.\nThe RIS consists of both reflecting and sensing elements, adaptively changing\nthe element assignment based on system needs. To achieve this, we minimize the\nmaximum sensing signal-to-interference-plus-noise-ratio (SINR) at the\nadversarial detector within sample points in the sensing region, by optimizing\nthe transmit beamformer at the base station, the RIS phase shift matrix, the\nreceived beamformer at the RIS, and the division between reflecting and\nabsorptive elements at the RIS, where the latter function as sensing elements.\nAt the same time, the system is designed to maintain a minimum sensing SINR at\neach monitored location, as well as minimum communication SINR for each user.\nTo solve this challenging optimization problem, we develop an alternating\noptimization approach combined with a successive convex approximation based\nmethod tailored for each subproblem. Our results show that the proposed\napproach achieves a 25 dB reduction in the maximum sensing SINR at the\nadversarial detector compared to scenarios without sensing area protection.\nAlso, the optimal RIS element assignment can further improve sensing protection\nby 3 dB over RISs with fixed element configuration.",
      "generated_abstract": "In this paper, we propose a novel spectral clustering method based on the\ndiscrete Fourier transform (DFT), which is called Spectral Discrete Fourier\nTransform (SDFT). SDFT is a discrete Fourier transform (DFT) based algorithm\nthat utilizes the Fourier transform of the spectral density function to\nsimplify the computation of the DFT, enabling it to be implemented in a\ncomputationally efficient manner. The proposed algorithm is used to cluster\nspectral data, which is a form of spectral information used in many\napplications, such as voice identification, voice activity detection, and\nspeech enhancement. By applying SDFT to the spectral data, the algorithm\nreduces the dimension of the spectral data, making it easier to cluster. In\nthis paper, we also propose a novel spectral clustering method based on the\ndiscrete Fourier transform (DFT), which is called Spectral Discrete Fourier\nTransform (SDFT). SD",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13245033112582782,
          "p": 0.2857142857142857,
          "f": 0.18099547078479158
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.038834951456310676,
          "f": 0.02446482748889526
        },
        "rouge-l": {
          "r": 0.12582781456953643,
          "p": 0.2714285714285714,
          "f": 0.17194569702913545
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.10055v1",
      "true_abstract": "While 3D point clouds are widely utilized across various vision applications,\ntheir irregular and sparse nature make them challenging to handle. In response,\nnumerous encoding approaches have been proposed to capture the rich semantic\ninformation of point clouds. Yet, a critical limitation persists: a lack of\nconsideration for colored point clouds which are more capable 3D\nrepresentations as they contain diverse attributes: color and geometry. While\nexisting methods handle these attributes separately on a per-point basis, this\nleads to a limited receptive field and restricted ability to capture\nrelationships across multiple points. To address this, we pioneer a point cloud\nencoding methodology that leverages 3D Fourier decomposition to disentangle\ncolor and geometric features while extending the receptive field through\nspectral-domain operations. Our analysis confirms that this encoding approach\neffectively separates feature components, where the amplitude uniquely captures\ncolor attributes and the phase encodes geometric structure, thereby enabling\nindependent learning and utilization of both attributes. Furthermore, the\nspectral-domain properties of these components naturally aggregate local\nfeatures while considering multiple points' information. We validate our point\ncloud encoding approach on point cloud classification and style transfer tasks,\nachieving state-of-the-art results on the DensePoint dataset with improvements\nvia a proposed amplitude-based data augmentation strategy.",
      "generated_abstract": "Motivated by the growing popularity of self-supervised pre-training (SSP) on\ngenerative models, we propose the SSP-Learned GAN (SSPGAN) for self-supervised\nimage generation. SSPGAN employs self-supervised learning to enhance the\ngenerative capabilities of the pre-trained model. Specifically, we introduce\nthe SSPGAN-Loss to integrate the self-supervised learning into the training of\nthe pre-trained model. The SSPGAN-Loss is designed to capture the essential\nfeatures of self-supervised learning, which enhances the model's ability to\ngenerate high-quality images. Additionally, we propose the SSPGAN-Mixer to\nintegrate the SSPGAN-Loss into the mixer module of the pre-trained model. The\nSSPGAN-Mixer ensures",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07857142857142857,
          "p": 0.20754716981132076,
          "f": 0.11398963332170006
        },
        "rouge-2": {
          "r": 0.015789473684210527,
          "p": 0.03896103896103896,
          "f": 0.022471906007940266
        },
        "rouge-l": {
          "r": 0.07857142857142857,
          "p": 0.20754716981132076,
          "f": 0.11398963332170006
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06370v1",
      "true_abstract": "This paper presents a method for load balancing and dynamic pricing in\nelectric vehicle (EV) charging networks, utilizing reinforcement learning (RL)\nto enhance network performance. The proposed framework integrates a pre-trained\ngraph neural network to predict demand elasticity and inform pricing decisions.\nThe spatio-temporal EV charging demand prediction (EVCDP) dataset from Shenzhen\nis utilized to capture the geographic and temporal characteristics of the\ncharging stations. The RL model dynamically adjusts prices at individual\nstations based on occupancy, maximum station capacity, and demand forecasts,\nensuring an equitable network load distribution while preventing station\noverloads. By leveraging spatially-aware demand predictions and a carefully\ndesigned reward function, the framework achieves efficient load balancing and\nadaptive pricing strategies that respond to localized demand and global network\ndynamics, ensuring improved network stability and user satisfaction. The\nefficacy of the approach is validated through simulations on the dataset,\nshowing significant improvements in load balancing and reduced overload as the\nRL agent iteratively interacts with the environment and learns to dynamically\nadjust pricing strategies based on real-time demand patterns and station\nconstraints. The findings highlight the potential of adaptive pricing and\nload-balancing strategies to address the complexities of EV infrastructure,\npaving the way for scalable and user-centric solutions.",
      "generated_abstract": "This paper presents a novel two-way communication system for an intelligent\ncommunication network (ICN) that connects a communication node to an\ninformation processing node via a wired link. The ICN uses a block-fading\nchannel model, which describes the interference-limited propagation of\ninformation and communication signals. The ICN also includes a\ntwo-way-communication node that can communicate with a remote information\nprocessing node through a wired link. The two-way-communication node transmits\ninformation to the information processing node, and the information processing\nnode transmits information to the two-way-communication node through the wired\nlink. To enhance the robustness of the two-way-communication node, we\nintroduce a new method for determining the block fading channel. This method\nis based on the analysis of the block fading channel in the presence of\ncorrelation between the block f",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.3230769230769231,
          "f": 0.2164948409049847
        },
        "rouge-2": {
          "r": 0.036458333333333336,
          "p": 0.06666666666666667,
          "f": 0.04713804256708544
        },
        "rouge-l": {
          "r": 0.14728682170542637,
          "p": 0.2923076923076923,
          "f": 0.19587628420395378
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.09287v1",
      "true_abstract": "We study the properties of macroeconomic survey forecast response averages as\nthe number of survey respondents grows. Such averages are \"portfolios\" of\nforecasts. We characterize the speed and pattern of the gains from\ndiversification and their eventual decrease with portfolio size (the number of\nsurvey respondents) in both (1) the key real-world data-based environment of\nthe U.S. Survey of Professional Forecasters (SPF), and (2) the theoretical\nmodel-based environment of equicorrelated forecast errors. We proceed by\nproposing and comparing various direct and model-based \"crowd size signature\nplots,\" which summarize the forecasting performance of k-average forecasts as a\nfunction of k, where k is the number of forecasts in the average. We then\nestimate the equicorrelation model for growth and inflation forecast errors by\nchoosing model parameters to minimize the divergence between direct and\nmodel-based signature plots. The results indicate near-perfect equicorrelation\nmodel fit for both growth and inflation, which we explicate by showing\nanalytically that, under conditions, the direct and fitted equicorrelation\nmodel-based signature plots are identical at a particular model parameter\nconfiguration, which we characterize. We find that the gains from\ndiversification are greater for inflation forecasts than for growth forecasts,\nbut that both gains nevertheless decrease quite quickly, so that fewer SPF\nrespondents than currently used may be adequate.",
      "generated_abstract": "We consider a model where agents have preferences over random variables\nand have a positive utility from each random variable. We model this as a\ntwo-armed bandit problem. We propose an algorithm that uses randomization\nto improve the regret in the expected utility setting. Our algorithm uses a\nnovel approach that uses a randomization mechanism to create a series of\nsub-bandit problems, where the agents' utility depends on the value of the\nsub-bandit problem. Our algorithm is guaranteed to be optimal up to a logarithmic\nfactor in the number of sub-bandit problems. We also show that the regret of the\nalgorithm is at least $n^{-1/2}$, where $n$ is the number of sub-bandit\nproblems. Our algorithm is simple and can be implemented in a real-time setting\nwithin a few milliseconds.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.22857142857142856,
          "f": 0.16666666203342026
        },
        "rouge-2": {
          "r": 0.03684210526315789,
          "p": 0.06140350877192982,
          "f": 0.04605262689144784
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.22857142857142856,
          "f": 0.16666666203342026
        }
      }
    },
    {
      "paper_id": "cs.HC.eess/SY/2503.03570v1",
      "true_abstract": "Traditional XR and Metaverse applications prioritize user experience (UX) for\nadoption and success but often overlook a crucial aspect of user interaction:\nemotions. This article addresses this gap by presenting an emotion-aware\nMetaverse application: a Virtual Reality (VR) fire drill simulator designed to\nprepare crews for shipboard emergencies. The simulator detects emotions in real\ntime, assessing trainees responses under stress to improve learning outcomes.\nIts architecture incorporates eye-tracking and facial expression analysis via\nMeta Quest Pro headsets. The system features four levels whose difficulty is\nincreased progressively to evaluate user decision-making and emotional\nresilience. The system was evaluated in two experimental phases. The first\nphase identified challenges, such as navigation issues and lack of visual\nguidance. These insights led to an improved second version with a better user\ninterface, visual cues and a real-time task tracker. Performance metrics like\ncompletion times, task efficiency and emotional responses were analyzed. The\nobtained results show that trainees with prior VR or gaming experience\nnavigated the scenarios more efficiently. Moreover, the addition of\ntask-tracking visuals and navigation guidance significantly improved user\nperformance, reducing task completion times between 14.18\\% and 32.72\\%.\nEmotional responses were captured, revealing that some participants were\nengaged, while others acted indifferently, indicating the need for more\nimmersive elements. Overall, this article provides useful guidelines for\ncreating the next generation of emotion-aware Metaverse applications.",
      "generated_abstract": "The growing volume of sensory data collected in various environments\ninvolves the complex task of integrating the information from multiple sensors\nfor better decision-making. This paper presents a framework for the real-time\nintegration of sensory data and decision-making for intelligent decision-making\nsystems in an industrial environment. The framework consists of a sensor\nprocessing unit, a decision-making unit, and a communications unit. The\nsensory-processing unit receives data from multiple sensors and transmits the\ndata to the decision-making unit. The decision-making unit uses the data\nreceived from the sensor processing unit to optimize the decision-making\nprocess. The communications unit ensures that the data is transmitted in a\ntimely manner to the decision-making unit. The framework is implemented using\nthe MATLAB/Simulink software. The performance of the framework is validated\nthrough simulations. The results show that the framework",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10909090909090909,
          "p": 0.27692307692307694,
          "f": 0.15652173507561448
        },
        "rouge-2": {
          "r": 0.013761467889908258,
          "p": 0.02631578947368421,
          "f": 0.018072284647264878
        },
        "rouge-l": {
          "r": 0.09696969696969697,
          "p": 0.24615384615384617,
          "f": 0.13913043072778838
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09565v1",
      "true_abstract": "Despite deep neural networks' powerful representation learning capabilities,\ntheoretical understanding of how networks can simultaneously achieve meaningful\nfeature learning and global convergence remains elusive. Existing approaches\nlike the neural tangent kernel (NTK) are limited because features stay close to\ntheir initialization in this parametrization, leaving open questions about\nfeature properties during substantial evolution. In this paper, we investigate\nthe training dynamics of infinitely wide, $L$-layer neural networks using the\ntensor program (TP) framework. Specifically, we show that, when trained with\nstochastic gradient descent (SGD) under the Maximal Update parametrization\n($\\mu$P) and mild conditions on the activation function, SGD enables these\nnetworks to learn linearly independent features that substantially deviate from\ntheir initial values. This rich feature space captures relevant data\ninformation and ensures that any convergent point of the training process is a\nglobal minimum. Our analysis leverages both the interactions among features\nacross layers and the properties of Gaussian random variables, providing new\ninsights into deep representation learning. We further validate our theoretical\nfindings through experiments on real-world datasets.",
      "generated_abstract": "We present a novel method for learning and using a single model to handle\nmultiple tasks within the framework of probabilistic graphical models. Our\napproach is based on a novel framework for multitask learning that provides a\nunified view of tasks within a probabilistic graphical model. Our framework\nallows for a unified treatment of different tasks by leveraging the underlying\nstructure of the probabilistic graphical model. We show that our framework\nenables a unified treatment of tasks within a probabilistic graphical model.\nSpecifically, we show that a multitask learning approach is equivalent to a\nmultitask probabilistic graphical model and we show how this equivalence\ntranslates to a single model. We show that this single model can be\nefficiently estimated using a variety of estimators. We also show how our\nframework enables a unified treatment of different tasks by leveraging the\nstructure of the probabilistic graphical model. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15555555555555556,
          "p": 0.375,
          "f": 0.21989528381349202
        },
        "rouge-2": {
          "r": 0.023952095808383235,
          "p": 0.04081632653061224,
          "f": 0.030188674584265586
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.35714285714285715,
          "f": 0.2094240796250103
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.18647v1",
      "true_abstract": "Urban overheating, exacerbated by climate change, threatens public health and\nurban sustainability. Traditional approaches, such as numerical simulations and\nfield measurements, face challenges due to uncertainties in input data. This\nstudy integrates field measurements with machine learning models to predict the\nduration and severity of future urban overheating events, focusing on the role\nof urban greening under different global warming (GW) scenarios. Field\nmeasurements were conducted in summer 2024 at an office campus in Ottawa, a\ncold-climate city. Microclimate data were collected from four locations with\nvarying levels of greenery: a large lawn without trees (Lawn), a parking lot\nwithout greenery (Parking), an area with sparsely distributed trees (Tree), and\na fully covered forested area (Forest). Machine learning models, including\nArtificial Neural Networks (ANN), Recurrent Neural Networks (RNN), and Long\nShort-Term Memory (LSTM) networks, were trained on local microclimate data,\nwith LSTM achieving the best predictions. Four GW scenarios were analyzed,\ncorresponding to different Shared Socioeconomic Pathways (SSP) for 2050 and\n2090. Results show that the Universal Thermal Climate Index (UTCI) at the\n\"Parking\" location rises from about 27,\\textdegree C under GW1.0 to\n31,\\textdegree C under GW3.5. Moreover, low health risk conditions (UTCI >\n26,\\textdegree C) increase across all locations due to climate change,\nregardless of greenery levels. However, tree-covered areas such as \"Tree\" and\n\"Forest\" effectively prevent extreme heat conditions (UTCI > 38.9,\\textdegree\nC). These findings highlight the crucial role of urban greening in mitigating\nsevere thermal stress and enhancing thermal comfort under future climate\nscenarios.",
      "generated_abstract": "This paper presents a novel methodology for designing and conducting\noptimization experiments that incorporate the application of artificial\nintelligence (AI) techniques. By leveraging the capabilities of AI, we\nintroduce a framework that enables the design of experiments that are\nflexible, scalable, and adaptable to changing conditions. The proposed method\nintegrates the use of machine learning models, such as random forests, for\nsimultaneously optimizing both objective and constraint values. This approach\nenables the generation of optimal solutions, even in highly complex\noptimization scenarios. Additionally, the methodology enables the automation\nof experimental design, significantly reducing the time and effort required for\nthe generation of optimization experiments. By incorporating AI techniques into\noptimization experiments, we are able to improve the overall efficiency and\neffectiveness of optimization processes. The proposed methodology provides a\nrobust and scalable solution for the design of optimization experiments,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.096045197740113,
          "p": 0.19540229885057472,
          "f": 0.12878787436897401
        },
        "rouge-2": {
          "r": 0.012552301255230125,
          "p": 0.024,
          "f": 0.016483511973947616
        },
        "rouge-l": {
          "r": 0.096045197740113,
          "p": 0.19540229885057472,
          "f": 0.12878787436897401
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.09507v1",
      "true_abstract": "For one dimensional stochastic Burgers equation driven by space-time white\nnoise we consider the problem of estimation of the diffusivity parameter in\nfront of the second-order spatial derivative. Based on local observations in\nspace, we study the estimator derived in [Altmeyer, Rei{\\ss}, Ann. Appl.\nProbab.(2021)] for linear stochastic heat equation that has also been used in\n[Altmeyer, Cialenco, Pasemann, Bernoulli (2023)] to cover large class of\nsemilinear SPDEs and has been examined for the stochastic Burgers equation\ndriven by trace class noise. We extend the achieved results by considering the\nspace-time white noise case which has also relevant physical motivations. After\nwe establish new regularity results for the solution, we are able to show that\nour proposed estimator is strongly consistent and asymptotically normal.",
      "generated_abstract": "In this paper, we investigate the $p$-adic exponential sum $S_p(X,Y)$\nfor $p>2$, and prove the $p$-adic exponential sum conjecture for\n$S_p(X,Y)$. More precisely, we prove that\n\\[ \\sum_{n\\leq x} \\frac{1}{n^p} = \\frac{1}{x^{1/p}} + \\mathcal{O}_p(x^{-1/2}) +\n\\mathcal{O}_p(x^{-1/p}) \\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05952380952380952,
          "p": 0.17857142857142858,
          "f": 0.08928571053571445
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.05952380952380952,
          "p": 0.17857142857142858,
          "f": 0.08928571053571445
        }
      }
    },
    {
      "paper_id": "q-bio.QM.stat/AP/2503.07664v1",
      "true_abstract": "The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified\nresource derived from electronic health records (EHR) that facilitates research\ninto antimicrobial resistance (AMR). ARMD encompasses data from adult patients,\nfocusing on microbiological cultures, antibiotic susceptibilities, and\nassociated clinical and demographic features. Key attributes include organism\nidentification, susceptibility patterns for 55 antibiotics, implied\nsusceptibility rules, and de-identified patient information. This dataset\nsupports studies on antimicrobial stewardship, causal inference, and clinical\ndecision-making. ARMD is designed to be reusable and interoperable, promoting\ncollaboration and innovation in combating AMR. This paper describes the\ndataset's acquisition, structure, and utility while detailing its\nde-identification process.",
      "generated_abstract": "This paper introduces the framework of multiple linear regression with\nnon-negative Gaussian random design to address the challenges of the\nincorporation of non-Gaussian covariates into linear regression models. The\nframework is general and applicable to both continuous and binary covariates.\nTheoretical results are derived under the linear regression model, revealing\nthat the proposed framework is consistent in the limit of large $n$ and $p$.\nSimulation studies and empirical applications in multiple myeloma and\nmultiple-sclerosis are conducted to validate the proposed framework. The\nproposed framework is expected to be beneficial in modeling the relationships\nbetween covariates and clinical outcomes, particularly when non-Gaussian\ncovariates are present. This work is of interest to clinicians, researchers,\nand statisticians.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2028985507246377,
          "f": 0.183006530995771
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.02857142857142857,
          "f": 0.029556645252251563
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2028985507246377,
          "f": 0.183006530995771
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/NE/2503.08301v2",
      "true_abstract": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
      "generated_abstract": "Recent advances in natural language processing have led to the emergence of\nhigh-capacity models, including the world's first language model with\ntrillion-parameters, surpassing the performance of human-level models. While\nthese models are essential for many applications, their extreme size and\ncomplexity pose significant challenges in training, inference, and deployment.\nThis paper introduces the Language Model Transformer (LMT), a transformer-based\nframework that scales up to 100 trillion parameters and achieves superior\nperformance across a variety of tasks. The LMT architecture is inspired by\ntransformer-based language models, which use attention mechanisms to capture\nlong-range dependencies. The LMT design consists of a language model (LM) head,\na transformer encoder, and a multi-head attention module. The LMT head is\ntrained end-to-end, while the transformer",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.171875,
          "p": 0.25,
          "f": 0.2037036988751716
        },
        "rouge-2": {
          "r": 0.016483516483516484,
          "p": 0.02586206896551724,
          "f": 0.020134223433179808
        },
        "rouge-l": {
          "r": 0.171875,
          "p": 0.25,
          "f": 0.2037036988751716
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15891v1",
      "true_abstract": "We consider the problem of estimating the number of communities in a weighted\nbalanced Stochastic Block Model. We construct hypothesis tests based on\nsemidefinite programming and with a statistic coming from a GOE matrix to\ndistinguish between any two candidate numbers of communities. This is possible\ndue to a universality result for a semidefinite programming-based function that\nwe also prove. The tests are then used to form a sequential test to estimate\nthe number of communities. Furthermore, we also construct estimators of the\ncommunities themselves.",
      "generated_abstract": "This paper studies the problem of estimating the mean of a Gaussian random\nvector with an unknown covariance matrix. We show that the Fisher Information\nmatrix (FIM) can be decomposed as a sum of two components, depending on the\ncovariance structure. We propose two estimators for the FIM, based on\nconditional expectation and empirical covariance matrix. The first estimator\nuses the conditional expectation of the mean and the first two moments of the\ncovariance matrix, while the second uses the empirical covariance matrix. We\nshow that both estimators are consistent and asymptotically normal.\nAdditionally, we propose a new estimator that combines the conditional expectation\nand empirical covariance matrix, and show that it is asymptotically normal. We\nalso discuss the properties of the estimators, and show that they are not\noptimal when the covariance matrix has a large eigenvalue.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.35,
          "p": 0.2916666666666667,
          "f": 0.3181818132231406
        },
        "rouge-2": {
          "r": 0.0759493670886076,
          "p": 0.05309734513274336,
          "f": 0.06249999515679293
        },
        "rouge-l": {
          "r": 0.31666666666666665,
          "p": 0.2638888888888889,
          "f": 0.28787878292011027
        }
      }
    },
    {
      "paper_id": "physics.med-ph.eess/SY/2503.06172v1",
      "true_abstract": "Noninvasive brain stimulation can activate neurons in the brain but requires\npower electronics with exceptionally high power in the mega-volt-ampere and\nhigh frequencies in the kilohertz range. Whereas oscillator circuits offered\nonly one or very few pulse shapes, modular power electronics solved a\nlong-standing problem for the first time and enabled arbitrary software-based\ndesign of the temporal shape of stimuli. However, synthesizing arbitrary\nstimuli with a high output quality requires a large number of modules. Systems\nwith few modules and pulse-width modulation may generate apparently smooth\ncurrent shapes in the highly inductive coil, but the stimulation effect of the\nneurons depends on the electric field and the electric field becomes a burst of\nultra-brief rectangular pulses. We propose an alternative solution that\nachieves high-resolution pulse shaping with fewer modules by implementing\nhigh-power wide-bandwidth voltage asymmetry. Rather than equal voltage steps,\nour system strategically assigns different voltages to each module to achieve a\nnear-exponential improvement in resolution. Compared to prior designs, our\nexperimental prototype achieved better output quality, although it uses only\nhalf the number of modules.",
      "generated_abstract": "This work presents a comprehensive review of the current literature on\ntreatment of neuro-oncology by ultrasound-guided biopsy. A total of 615\ncitations were collected for a systematic literature review. A total of 24 studies\nwere included in the review. The studies were divided into two broad categories:\n(i) studies on biopsy using the transient elastography (TE) technique and (ii)\nstudies on biopsy using the ultrasound (US) technique. The studies were\ncomparatively evaluated based on the type of biopsy, tumor type, and\ncomplementary diagnostic modalities such as MRI, CT, and PET/CT. The most\ncommonly used biopsy technique was the transient elastography technique, with\napproximately 12% of the studies using this technique. The most common tum",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08264462809917356,
          "p": 0.14285714285714285,
          "f": 0.1047120372413039
        },
        "rouge-2": {
          "r": 0.018072289156626505,
          "p": 0.030303030303030304,
          "f": 0.02264150475357875
        },
        "rouge-l": {
          "r": 0.06611570247933884,
          "p": 0.11428571428571428,
          "f": 0.08376962886434061
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2502.13785v2",
      "true_abstract": "mRNA-based vaccines have become a major focus in the pharmaceutical industry.\nThe coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can\nstrongly influence translation efficiency, stability, degradation, and other\nfactors that collectively determine a vaccine's effectiveness. However,\noptimizing mRNA sequences for those properties remains a complex challenge.\nExisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based and\nattention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model\nwith high-quality data. We employ single nucleotide tokenization of mRNA\nsequences with codon separation, ensuring prior biological and structural\ninformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,\noutperforms existing methods in analysing both UTRs and coding region\nproperties. It can process sequences 6x longer than current approaches while\nusing only 10% of the parameters of existing foundation models. Its predictive\ncapabilities extend to all mRNA regions. We open-source the model\n(https://github.com/helicalAI/helical) and model weights\n(https://huggingface.co/helical-ai/helix-mRNA).",
      "generated_abstract": "The human gut microbiota plays a key role in the health and development of\nthe host, and its composition is influenced by diet, lifestyle, and environmental\nfactors. In this study, we aimed to investigate the effect of dietary\nhydrogen-rich water (H2RW) on the gut microbiota of children with\nirritable bowel syndrome (IBS). We recruited 60 children with IBS aged 4-12\nyears. Children were randomly divided into two groups: H2RW and H2RW +\nplacebo. The H2RW group received 1200 mL of H2RW daily for 12 weeks. The\nH2RW + placebo group received 1200 mL of plain water for 12 weeks. We\nconducted a microbiome analysis using a 16S",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.20270270270270271,
          "f": 0.14705881890619008
        },
        "rouge-2": {
          "r": 0.011627906976744186,
          "p": 0.021052631578947368,
          "f": 0.014981268824083658
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.20270270270270271,
          "f": 0.14705881890619008
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.05908v1",
      "true_abstract": "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces.",
      "generated_abstract": "We propose a novel approach for the analysis of multi-omics data in which\nparticipants are clustered into groups based on their biological phenotypes.\nSpecifically, we assume that each participant is drawn from a cluster with a\ngiven probability, and we focus on the estimation of the mean of each cluster.\nWe develop a novel method for estimating the mean of each cluster, and\nprovide theoretical guarantees for the consistency and asymptotic normality of\nthe estimator. We then derive an efficient estimator for the mean of each\ncluster, and provide theoretical guarantees for the consistency and asymptotic\nnormality of the estimator. Finally, we provide a simulation study to\ncompare the performance of our estimator against existing methods. The\nresults indicate that our estimator performs well in terms of the asymptotic\nnormality, while the performance of the existing methods is comparable. Our\nmethod is particularly useful for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21818181818181817,
          "p": 0.15584415584415584,
          "f": 0.1818181769570708
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16363636363636364,
          "p": 0.11688311688311688,
          "f": 0.13636363150252542
        }
      }
    },
    {
      "paper_id": "math.GT.math/QA/2503.08861v1",
      "true_abstract": "We give invariants of flat bundles over 4-manifolds generalizing a result by\nChaidez, Cotler, and Cui (Alg. \\& Geo. Topology '22). We utilize a structure\ncalled a Hopf $G$-triplet for $G$ a group, which generalizes the notion of a\nHopf triplet by Chaidez, Cotler, and Cui. In our construction, we present flat\nbundles over 4-manifolds using colored trisection diagrams: a direct analogue\nof colored Heegaard diagrams as described by Virelizier. Our main result is\nthat involutory Hopf $G$-triplets of finite type yield well-defined invariants\nof $G$-colored trisection diagrams, and that if the monodromy of a flat bundle\nhas image in $G$ we obtain invariants of flat bundles. We also show that a\nspecial Hopf $G$-triplet yields the invariant from Hopf $G$-algebras described\nby Mochida, thus generalizing the construction.",
      "generated_abstract": "We show that the moduli space of rank one flat connections on an\n$n$-dimensional smooth manifold is a smooth projective variety in a neighborhood\nof the identity, for any positive integer $n$. We also show that the moduli\nspace of rank two flat connections on an $n$-dimensional smooth manifold is a\nsmooth projective variety in a neighborhood of the identity, for any positive\ninteger $n\\geq 4$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.34375,
          "f": 0.19819819409463527
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.07894736842105263,
          "f": 0.039999996216889246
        },
        "rouge-l": {
          "r": 0.11392405063291139,
          "p": 0.28125,
          "f": 0.16216215805859924
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.nlin/CD/2503.08983v1",
      "true_abstract": "We study decaying turbulence in the 1D Burgers equation (Burgulence) and 3D\nNavier-Stokes (NS) turbulence. We first investigate the decay in time $t$ of\nthe energy $E(t)$ in Burgulence, for a fractional Brownian initial potential,\nwith Hurst exponent $H$, and demonstrate rigorously a self-similar time-decay\nof $E(t)$, previously determined heuristically. This is a consequence of the\nnontrivial boundedness of the energy for any positive time. We define a\nspatially forgetful \\textit{oblivious fractional Brownian motion} (OFBM), with\nHurst exponent $H$, and prove that Burgulence, with an OFBM as initial\npotential $\\varphi_0(x)$, is not only intermittent, but it also displays, a\nhitherto unanticipated, large-scale bifractality or multifractality; the latter\noccurs if we combine OFBMs, with different values of $H$. This is the first\nrigorous proof of genuine multifractality for turbulence in a nonlinear\nhydrodynamical partial differential equation. We then present direct numerical\nsimulations (DNSs) of freely decaying turbulence, capturing some aspects of\nthis multifractality. For Burgulence, we investigate such decay for two cases:\n(A) $\\varphi_0(x)$ a multifractal random walk that crosses over to a fractional\nBrownian motion beyond a crossover scale $\\mathcal{L}$, tuned to go from small-\nto large-scale multifractality; (B) initial energy spectra $E_0(k)$, with\nwavenumber $k$, having one or more power-law regions, which lead, respectively,\nto self-similar and non-self-similar energy decay. Our analogous DNSs of the 3D\nNS equations also uncover self-similar and non-self-similar energy decay.\nChallenges confronting the detection of genuine large-scale multifractality, in\nnumerical and experimental studies of NS and MHD turbulence, are highlighted.",
      "generated_abstract": "This paper explores the existence and asymptotic behavior of solutions to\nthe one-dimensional coupled linear Stokes-Newton-Oseen system of equations with\nconstant coefficients. We use a method based on the variational framework of\nthe Stokes-Newton-Oseen system and show that the solution is characterized by\ntwo parameters, namely the shape parameter $\\gamma$ and the frequency $\\omega$,\nand depends on the parameters through the Laplace transform. We further show\nthat the solution is a solution of the Stokes system, and its asymptotic\nbehavior depends on the parameters through the Laplace transform. We also show\nthat the solution is a solution of the Newton-Oseen system and its asymptotic\nbehavior depends on the parameters through the Laplace transform.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08496732026143791,
          "p": 0.24528301886792453,
          "f": 0.12621358841125474
        },
        "rouge-2": {
          "r": 0.00881057268722467,
          "p": 0.02631578947368421,
          "f": 0.013201316373777063
        },
        "rouge-l": {
          "r": 0.0718954248366013,
          "p": 0.20754716981132076,
          "f": 0.10679611268309938
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.09383v2",
      "true_abstract": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
      "generated_abstract": "We introduce the concept of social welfare, which we define as the average\nvalue of utility in the population minus the average marginal utility. We\nderive a simple closed-form expression for social welfare for any monotone\nutility function, including the traditional Gini index, and show that it is\nalways lower than the average utility. We also show that social welfare is\nconcave and concave-convex, and that it is a monotone function of the\nsubadditive utility functions. Finally, we show that social welfare is a\nsupermodular function. We apply the concept of social welfare to economic\nproblems and show that it is a useful alternative to the traditional Gini index\nin various contexts.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.2631578947368421,
          "f": 0.13513513131848076
        },
        "rouge-2": {
          "r": 0.013953488372093023,
          "p": 0.03409090909090909,
          "f": 0.01980197607642038
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.2631578947368421,
          "f": 0.13513513131848076
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/GN/2502.12966v1",
      "true_abstract": "Ethereum has adopted a rollup-centric roadmap to scale by making rollups\n(layer 2 scaling solutions) the primary method for handling transactions. The\nfirst significant step towards this goal was EIP-4844, which introduced blob\ntransactions that are designed to meet the data availability needs of layer 2\nprotocols. This work constitutes the first rigorous and comprehensive empirical\nanalysis of transaction- and mempool-level data since the institution of blobs\non Ethereum on March 13, 2024. We perform a longitudinal study of the early\ndays of the blob fee market analyzing the landscape and the behaviors of its\nparticipants. We identify and measure the inefficiencies arising out of\nsuboptimal block packing, showing that at times it has resulted in up to 70%\nrelative fee loss. We hone in and give further insight into two (congested)\npeak demand periods for blobs. Finally, we document a market design issue\nrelating to subset bidding due to the inflexibility of the transaction\nstructure on packing data as blobs and suggest possible ways to fix it. The\nlatter market structure issue also applies more generally for any discrete\nobjects included within transactions.",
      "generated_abstract": "In this paper, we propose a new framework for managing large-scale\ninfrastructure networks with a view towards achieving more sustainable and\nresilient solutions. We focus on three major challenges in managing such\nnetworks: (i) optimizing the network topology and its utilization, (ii)\noptimizing the resource allocation, and (iii) optimizing the network\nmanagement and monitoring. To tackle these challenges, we propose a novel\nmulti-agent system that includes both centralized and decentralized agents. The\ncentralized agent is responsible for managing the topology of the network and\nthe utilization of the resources. The decentralized agents are responsible for\nmanaging the allocation of resources and the management of the network. The\ncentralized agent is also responsible for coordinating the decentralized agents\nand ensuring that they are in sync with the centralized agent's decisions.\nAdditionally,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13178294573643412,
          "p": 0.22972972972972974,
          "f": 0.1674876800960957
        },
        "rouge-2": {
          "r": 0.011049723756906077,
          "p": 0.01834862385321101,
          "f": 0.013793098756481976
        },
        "rouge-l": {
          "r": 0.10077519379844961,
          "p": 0.17567567567567569,
          "f": 0.12807881310102182
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.16142v1",
      "true_abstract": "In this study, we investigate the integration of a large language model (LLM)\nwith an automatic speech recognition (ASR) system, specifically focusing on\nenhancing rare word recognition performance. Using a 190,000-hour dataset\nprimarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling,\nwe demonstrate that the LLM-ASR architecture outperforms traditional\nZipformer-Transducer models in the zero-shot rare word recognition task, after\ntraining on a large dataset. Our analysis reveals that the LLM contributes\nsignificantly to improvements in rare word error rate (R-WER), while the speech\nencoder primarily determines overall transcription performance (Orthographic\nWord Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through\nextensive ablation studies, we highlight the importance of adapter integration\nin aligning speech encoder outputs with the LLM's linguistic capabilities.\nFurthermore, we emphasize the critical role of high-quality labeled data in\nachieving optimal performance. These findings provide valuable insights into\nthe synergy between LLM-based ASR architectures, paving the way for future\nadvancements in large-scale LLM-based speech recognition systems.",
      "generated_abstract": "This paper explores the potential of a multi-modal transformer architecture to\ntransform speech and language understanding, specifically for multimodal\nspeech-language pathology (SLP) applications. The proposed architecture\nintegrates a multimodal transformer encoder with an attention-based decoder.\nThis architecture is trained in an end-to-end manner using a speech-language\npathology (SLP) speech database and a multimodal SLP database. The proposed\nmodel demonstrates superior performance across various SLP tasks, including\nautomatic speech recognition (ASR), speech-to-text (STT), and speech\nlanguage pathology (SLP) translation. Furthermore, the proposed model is\ntrained and tested on a speech-language pathology SLP database, demonstrating\nits ability to generalize to unseen speakers and new tasks. These findings\nhighlight the potential of the proposed architecture to address the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20175438596491227,
          "p": 0.3333333333333333,
          "f": 0.25136611552091737
        },
        "rouge-2": {
          "r": 0.04697986577181208,
          "p": 0.07,
          "f": 0.0562248947920199
        },
        "rouge-l": {
          "r": 0.18421052631578946,
          "p": 0.30434782608695654,
          "f": 0.22950819202364964
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.02381v1",
      "true_abstract": "We propose a new approach to estimating the random coefficient logit demand\nmodel for differentiated products when the vector of market-product level\nshocks is sparse. Assuming sparsity, we establish nonparametric identification\nof the distribution of random coefficients and demand shocks under mild\nconditions. Then we develop a Bayesian procedure, which exploits the sparsity\nstructure using shrinkage priors, to conduct inference about the model\nparameters and counterfactual quantities. Comparing to the standard BLP (Berry,\nLevinsohn, & Pakes, 1995) method, our approach does not require demand\ninversion or instrumental variables (IVs), thus provides a compelling\nalternative when IVs are not available or their validity is questionable. Monte\nCarlo simulations validate our theoretical findings and demonstrate the\neffectiveness of our approach, while empirical applications reveal evidence of\nsparse demand shocks in well-known datasets.",
      "generated_abstract": "This paper introduces a novel multivariate time series regression model\nthat extends the traditional multivariate autoregressive moving average (AMM)\nmodel by incorporating individual-level time-varying effects. The proposed\nmodel extends the AMM model by including individual-level time-varying\neffects. To address the heterogeneity in the individual-level time-varying\neffects, the model uses a multivariate autoregressive integrated moving\naverage (MARIMA) structure. The MARIMA structure, which incorporates both\nstationary and nonstationary components, is designed to capture the\ntime-varying heterogeneity in the individual-level time-varying effects. The\nproposed model is estimated using the maximum likelihood (ML) and the\nsequential maximum likelihood (SML) methods. The proposed model is applied to\nestimate the impact of the COVID-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11224489795918367,
          "p": 0.1774193548387097,
          "f": 0.13749999525312515
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.022727272727272728,
          "f": 0.01851851368998754
        },
        "rouge-l": {
          "r": 0.10204081632653061,
          "p": 0.16129032258064516,
          "f": 0.1249999952531252
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.17271v1",
      "true_abstract": "In the context of scientific policy and science management, this study\nexamines the system of nonuniform wage distribution for researchers. A\nnonlinear mathematical model of optimal remuneration for scientific workers has\nbeen developed, considering key and additive aspects of scientific activity:\nbasic qualifications, research productivity, collaborative projects, skill\nenhancement, distinctions, and international collaborations. Unlike traditional\nlinear schemes, the proposed approach is based on exponential and logarithmic\ndependencies, allowing for the consideration of saturation effects and\npreventing artificial wage growth due to mechanical increases in scientific\nproductivity indicators.\n  The study includes detailed calculations of optimal, minimum, and maximum\nwages, demonstrating a fair distribution of remuneration on the basis of\nresearcher productivity. A linear increase in publication activity or grant\nfunding should not lead to uncontrolled salary growth, thus avoiding\ndistortions in the motivational system. The results of this study can be used\nto reform and modernize the wage system for researchers in Kazakhstan and other\ncountries, as well as to optimize grant-based science funding mechanisms. The\nproposed methodology fosters scientific motivation, long-term productivity, and\nthe internationalization of research while also promoting self-actualization\nand ultimately forming an adequate and authentic reward system for the research\ncommunity.\n  Specifically, in resource-limited scientific systems, science policy should\nfocus on the qualitative development of individual researchers rather than\nquantitative expansion (e.g., increasing the number of scientists). This can be\nachieved through the productive progress of their motivation and\nself-actualization.",
      "generated_abstract": "We develop a novel methodology to evaluate the impact of financial\ncrises on macroeconomic outcomes. Our framework combines a dynamic\nfinancial shock model with a stochastic volatility framework to model\nfinancial shocks and their impact on the economy. We then estimate the\nimpact of the financial crisis on macroeconomic indicators using a\nstochastic volatility model. Our methodology provides a comprehensive\nanalysis of the impact of financial crises on macroeconomic outcomes and\npaves the way for future research in financial crises and macroeconomic\nperformance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08496732026143791,
          "p": 0.3023255813953488,
          "f": 0.13265305779935452
        },
        "rouge-2": {
          "r": 0.004405286343612335,
          "p": 0.014705882352941176,
          "f": 0.006779657469464654
        },
        "rouge-l": {
          "r": 0.0784313725490196,
          "p": 0.27906976744186046,
          "f": 0.12244897616670147
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.14984v1",
      "true_abstract": "We examine the growing gender gap in venture capital funding, focusing on\naccelerator programs in the U.S. We collect a unique dataset with detailed\ninformation on accelerators and startups. Using a two-stage methodology, we\nfirst estimate a matching model between startups and accelerators, and then use\nits output to analyze the gender gap in post-graduation outcomes through a\ncontrol function approach. Our results show that female-founded startups face a\nsignificant funding disadvantage, primarily due to relocation challenges tied\nto family obligations. However, larger cohorts and higher-quality accelerators\nhelp reduce this gap by offering female founders better networking\nopportunities and mentorship.",
      "generated_abstract": "The global economy has been facing multiple crises: climate change,\nenergy crisis, food crisis, pandemic, and social unrest. The pandemic is\nexacerbating the energy crisis and food crisis. The global energy crisis has\nbeen exacerbated by the war in Ukraine and the increase in the cost of energy.\nThe global food crisis is caused by the lack of investment in agriculture,\nclimate change, and the conflict in Ukraine. The global social unrest is caused\nby the lack of investment in infrastructure and the economic slowdown in China.\nThe COVID-19 pandemic, the energy crisis, and the food crisis have created\nmultiple crises. These crises have led to political instability, social\nunrest, and the rise of extremist groups. The solution to these crises is to\ninvest in infrastructure and the economy. The solution to the global energy\ncrisis is to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06329113924050633,
          "p": 0.09090909090909091,
          "f": 0.07462686083203418
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.009615384615384616,
          "f": 0.009900985103423753
        },
        "rouge-l": {
          "r": 0.06329113924050633,
          "p": 0.09090909090909091,
          "f": 0.07462686083203418
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19006v2",
      "true_abstract": "In this note, I introduce a novel performance rating system called\nPerformance Rating Equilibrium (PRE). A PRE is a vector of hypothetical ratings\nfor each player, such that if these ratings were each player's initial rating\nat the start of a tournament, scoring the same points against the same\nopponents would leave each player's initial rating unchanged. In other words,\nall players' initial ratings perfectly predict their actual scores in the\ntournament. This property, however, does not hold for the well-known Tournament\nPerformance Rating. PRE is defined as a fixed point of a multidimensional\nrating function. I show that such a fixed point, and hence a PRE, exists under\nmild conditions. I provide an implementation of PRE along with several\nempirical applications. PREs have broad applicability, from sports competitions\nto the evaluation of large language models.",
      "generated_abstract": "This paper develops a theory of economic growth that accounts for the\ncomplicated interplay between technological progress and welfare. We first\nintroduce a novel definition of economic growth, which accounts for both\ntechnological progress and welfare, and then we derive a generalization of the\nSraffian utility function that captures the interplay between technological\nprogress and welfare. We then apply our theory to the standard growth model\nand the standard welfare model, and we show that our theory provides a\ncomplete account of both models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1326530612244898,
          "p": 0.2765957446808511,
          "f": 0.1793103404461357
        },
        "rouge-2": {
          "r": 0.03937007874015748,
          "p": 0.07042253521126761,
          "f": 0.050505045905010114
        },
        "rouge-l": {
          "r": 0.1326530612244898,
          "p": 0.2765957446808511,
          "f": 0.1793103404461357
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/GT/2503.07558v1",
      "true_abstract": "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
      "generated_abstract": "The rapid development of deep learning (DL) has led to an explosion of\nrecent advances in Large Language Models (LLMs), which have been applied to a\nvariety of tasks, including natural language processing (NLP), computer vision,\nand speech recognition. However, despite their remarkable performance, LLMs\nface significant challenges in real-world applications, such as the ability to\ngeneralize across different tasks and scenarios, the need for robust training\ndata, and the inability to effectively interact with the outside world. To\naddress these challenges, this survey explores the latest advancements in LLMs,\nincluding their architecture, training, and application. By introducing the\nfoundations of LLMs, this survey discusses the key challenges that need to be\naddressed for LLMs to be effectively used in real-world applications. It also\nprovides a comprehensive review of the latest adv",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10404624277456648,
          "p": 0.1875,
          "f": 0.13382899169221005
        },
        "rouge-2": {
          "r": 0.0036900369003690036,
          "p": 0.007874015748031496,
          "f": 0.005025121282673387
        },
        "rouge-l": {
          "r": 0.08670520231213873,
          "p": 0.15625,
          "f": 0.11152415897845543
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2411.04694v1",
      "true_abstract": "Introduction: Circadian rhythm disruption has garnered significant attention\nfor its adverse effects on human health, particularly in reproductive medicine\nand fetal well-being. Assessing pregnancy health often relies on diagnostic\nmarkers such as the labyrinth zone (LZ) proportion within the placenta. This\nstudy aimed to investigate the impact of disrupted circadian rhythms on\nplacental health and fetal development using animal models. Methods and\nResults: Employing unstained photo-acoustic microscopy (PAM) and hematoxylin\nand eosin (HE)-stained images, we found them mutually reinforcing. Our images\nrevealed the role of MCRD on the LZ and fetus weight: a decrease in LZ area\nfrom 5.01-HE(4.25-PAM) mm2 to 3.58-HE (2.62-PAM) mm2 on day 16 and\n6.48-HE(5.16-PAM) mm2 to 4.61-HE (3.03-PAM) mm2 on day 18, resulting in 0.71\ntimes lower fetus weights. We have discriminated a decrease in the mean LZ to\nplacenta area ratio from 64% to 47% on day 18 in mice with disrupted circadian\nrhythms with PAM. Discussion: The study highlights the negative influence of\ncircadian rhythm disruption on placental development and fetal well-being.\nReduced LZ area and fetal weights in the MCRD group suggest compromised\nplacental function under disrupted circadian rhythms. PAM imaging proved to be\nan efficient technique for assessing placental development, offering advantages\nover traditional staining methods. These findings contribute to understanding\nthe mechanisms underlying circadian disruption's effects on reproductive health\nand fetal development, emphasizing the importance of maintaining normal\ncircadian rhythms for optimal pregnancy outcomes. Further research is needed to\nexplore interventions to mitigate these effects and improve pregnancy outcomes.",
      "generated_abstract": "This paper presents a novel method for predicting protein-protein interactions\n(PPIs) using deep learning techniques. The approach incorporates a\nmultilayer perceptron (MLP) network with a fully connected layer, followed by\na softmax layer. The network is trained on the PPI dataset from the\nProtein-Protein Interaction (PPI) Dataset 2015 (PPI2015) and fine-tuned on a\ndataset of PPIs from the PPI Dataset 2023 (PPI2023). The results demonstrate\nthat the proposed approach outperforms traditional machine learning methods,\nparticularly the support vector machine and random forest, in terms of\naccuracy and recall. The proposed method also demonstrates strong performance\nacross different PPI datasets, with the best results achieved on the\nPPI2023 dataset. These",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09815950920245399,
          "p": 0.2077922077922078,
          "f": 0.1333333289753474
        },
        "rouge-2": {
          "r": 0.004273504273504274,
          "p": 0.009523809523809525,
          "f": 0.005899700738771471
        },
        "rouge-l": {
          "r": 0.09815950920245399,
          "p": 0.2077922077922078,
          "f": 0.1333333289753474
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2406.10612v1",
      "true_abstract": "A key output of network meta-analysis (NMA) is the relative ranking of the\ntreatments; nevertheless, it has attracted a lot of criticism. This is mainly\ndue to the fact that ranking is an influential output and prone to\nover-interpretations even when relative effects imply small differences between\ntreatments. To date, common ranking methods rely on metrics that lack a\nstraightforward interpretation, while it is still unclear how to measure their\nuncertainty. We introduce a novel framework for estimating treatment\nhierarchies in NMA. At first, we formulate a mathematical expression that\ndefines a treatment choice criterion (TCC) based on clinically important\nvalues. This TCC is applied to the study treatment effects to generate paired\ndata indicating treatment preferences or ties. Then, we synthesize the paired\ndata across studies using an extension of the so-called \"Bradley-Terry\" model.\nWe assign to each treatment a latent variable interpreted as the treatment\n\"ability\" and we estimate the ability parameters within a regression model.\nHigher ability estimates correspond to higher positions in the final ranking.\nWe further extend our model to adjust for covariates that may affect treatment\nselection. We illustrate the proposed approach and compare it with alternatives\nin two datasets: a network comparing 18 antidepressants for major depression\nand a network comparing 6 antihypertensives for the incidence of diabetes. Our\napproach provides a robust and interpretable treatment hierarchy which accounts\nfor clinically important values and is presented alongside with uncertainty\nmeasures. Overall, the proposed framework offers a novel approach for ranking\nin NMA based on concrete criteria and preserves from over-interpretation of\nunimportant differences between treatments.",
      "generated_abstract": "In many applications, data are collected over time, or over multiple\ntime points, but are not collected at the same time. Time-series data are\noften described by a time series of values, but they can also be represented by\na time series of time points. In this paper, we introduce a new framework for\ntime-series data, which combines both time-series representations. The framework\nis based on a generalization of the Multivariate Time Series (MTS) model,\nwhich allows for the presence of both time-invariant and time-varying factors.\nThe model can be interpreted as a linear time series model for a general\ntime-series representation, and the time-series model for a general time\nseries representation. We propose a novel modeling framework for time-series\ndata, which we call the Multivariate Time Series with Time-Varying Factors\n(MTSVF). We use the M",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12578616352201258,
          "p": 0.2564102564102564,
          "f": 0.16877636689205802
        },
        "rouge-2": {
          "r": 0.028225806451612902,
          "p": 0.059322033898305086,
          "f": 0.03825136175102323
        },
        "rouge-l": {
          "r": 0.12578616352201258,
          "p": 0.2564102564102564,
          "f": 0.16877636689205802
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.04929v1",
      "true_abstract": "Planning and control for high-dimensional robot manipulators in cluttered,\ndynamic environments require both computational efficiency and robust safety\nguarantees. Inspired by recent advances in learning configuration-space\ndistance functions (CDFs) as robot body representations, we propose a unified\nframework for motion planning and control that formulates safety constraints as\nCDF barriers. A CDF barrier approximates the local free configuration space,\nsubstantially reducing the number of collision-checking operations during\nmotion planning. However, learning a CDF barrier with a neural network and\nrelying on online sensor observations introduce uncertainties that must be\nconsidered during control synthesis. To address this, we develop a\ndistributionally robust CDF barrier formulation for control that explicitly\naccounts for modeling errors and sensor noise without assuming a known\nunderlying distribution. Simulations and hardware experiments on a 6-DoF xArm\nmanipulator show that our neural CDF barrier formulation enables efficient\nplanning and robust real-time safe control in cluttered and dynamic\nenvironments, relying only on onboard point-cloud observations.",
      "generated_abstract": "In this paper, we propose a novel 3D human pose estimation method that\nposes human body models with human-in-the-loop (HITL) control. We first\nrepresent human body models using human-in-the-loop (HITL) control to reduce\nthe number of parameters, thereby reducing the inference latency of the\nrepresented models. Then, we leverage the HITL control to generate the\nrepresented 3D human pose in real-time using a HITL controller. To train the\nrepresented model, we utilize a novel method that simultaneously trains the\nrepresented model and the controller. Finally, we propose a novel method that\noptimizes the parameters of the represented model and controller simultaneously\nby using the loss function of the representation and the controller. We\nevaluate our method on the human pose estimation dataset of the FP5 dataset.\nExperimental results show that our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18095238095238095,
          "p": 0.2923076923076923,
          "f": 0.2235294070415226
        },
        "rouge-2": {
          "r": 0.04794520547945205,
          "p": 0.06796116504854369,
          "f": 0.05622489474750449
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.23076923076923078,
          "f": 0.17647058351211087
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.10036v1",
      "true_abstract": "Concurrency control (CC) algorithms are important in modern transactional\ndatabases, as they enable high performance by executing transactions\nconcurrently while ensuring correctness. However, state-of-the-art CC\nalgorithms struggle to perform well across diverse workloads, and most do not\nconsider workload drifts.\n  In this paper, we propose CCaaLF (Concurrency Control as a Learnable\nFunction), a novel learned concurrency control algorithm designed to achieve\nhigh performance across varying workloads. The algorithm is quick to optimize,\nmaking it robust against dynamic workloads. CCaaLF learns an agent function\nthat captures a large number of design choices from existing CC algorithms. The\nfunction is implemented as an efficient in-database lookup table that maps\ndatabase states to concurrency control actions. The learning process is based\non a combination of Bayesian optimization and a novel graph reduction\nalgorithm, which converges quickly to a function that achieves high transaction\nthroughput. We compare CCaaLF against five state-of-the-art CC algorithms and\nshow that our algorithm consistently outperforms them in terms of transaction\nthroughput and optimization time.",
      "generated_abstract": "In the context of the current healthcare system, the growing prevalence of\nmechanistic data and the lack of comprehensive data management systems have\nresulted in the emergence of a new challenge: how to manage and integrate\nmulti-domain healthcare data and the resulting metadata? To address this\nchallenge, we present DataShield, an innovative framework for managing and\nintegrating metadata, data, and knowledge within healthcare contexts. DataShield\nis a data-centric approach that leverages the principles of data protection,\ndata privacy, and data security to provide a robust and scalable solution for\nmanaging, integrating, and protecting healthcare-related data. The framework\nenables the secure and efficient exchange of data across multiple stakeholders\nand systems, while ensuring privacy, security, and integrity. DataShield\nintegrates multiple data types, including clinical data, administrative data,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14782608695652175,
          "p": 0.20238095238095238,
          "f": 0.1708542664781194
        },
        "rouge-2": {
          "r": 0.00641025641025641,
          "p": 0.00819672131147541,
          "f": 0.0071942396791090085
        },
        "rouge-l": {
          "r": 0.12173913043478261,
          "p": 0.16666666666666666,
          "f": 0.1407035127092752
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/plasm-ph/2503.09557v1",
      "true_abstract": "We present a novel approach for generating collider-quality electron bunches\nusing a plasma photoinjector. The approach leverages recently developed\ntechniques for the spatiotemporal control of laser pulses to produce a moving\nionization front in a nonlinear plasma wave. The moving ionization front\ngenerates an electron bunch with a current profile that balances the\nlongitudinal electric field of an electron beam-driven plasma wave, creating a\nuniform accelerating field across the bunch. Particle-in-cell (PIC) simulations\nof the ionization stage show the formation of an electron bunch with 220 pC\ncharge and low emittance ($\\epsilon_x = 171$ nm-rad, $\\epsilon_y = 76$ nm-rad).\nQuasistatic PIC simulations of the acceleration stage show that the bunch is\nefficiently accelerated to 20 GeV over 2 meters with a final energy spread of\nless than 1\\% and emittances of $\\epsilon_x = 177$ nm-rad and $\\epsilon_y = 82$\nnm-rad. This high-quality electron bunch meets the requirements outlined by the\nSnowmass process for intermediate-energy colliders and compares favorably to\nthe beam quality of proposed and existing accelerator facilities. The results\nestablish the feasibility of plasma photoinjectors for future collider\napplications making a significant step towards the realization of\nhigh-luminosity, compact accelerators for particle physics research.",
      "generated_abstract": "We present a novel framework for the description of plasma flow in a\nfinite volume\n  by employing a set of adaptive mesh refinement (AMR) criteria. In particular,\nwe develop a\n  method to construct a coarse mesh that is adapted to a set of critical\npoints\n  of the solution. The new adaptive mesh refinement (AMR) criteria are based\non the\n  critical points of the solution, which are determined by a\n  non-linear\n  system of partial differential equations (PDEs). Our approach is\n  computationally\n  efficient and provides a general solution to the problem of constructing\n  adaptive\n  meshes. The proposed method is used to solve the 3D compressible MHD\n  equations\n  on a 2D torus, including the 2D non-linear Euler equations. The effect of\n  the\n  new AMR criteria on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1349206349206349,
          "p": 0.2361111111111111,
          "f": 0.17171716708907264
        },
        "rouge-2": {
          "r": 0.044444444444444446,
          "p": 0.07142857142857142,
          "f": 0.05479451581910343
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.20833333333333334,
          "f": 0.15151514688705248
        }
      }
    },
    {
      "paper_id": "astro-ph.HE.astro-ph/HE/2503.09562v1",
      "true_abstract": "The prompt emission of Gamma-Ray Bursts (GRBs) could be composed of different\nspectral components, such as a dominant non-thermal Band component in the\nkeV-MeV range, a subdominant quasi-thermal component, and an additional hard\nnon-thermal component extending into the GeV range. The existence and\nevolutionary behaviors of these components could place strong implication on\nphysical models, such as ejecta composition and dissipation processes. Although\nnumerous GRBs have been found to exhibit one or two spectral components,\nreports of GRBs containing all three components remain rare. In this letter,\nbased on the joint observations of GRB 240825A from multiple gamma-ray\ntelescopes, we conduct a comprehensive temporal and spectral analysis to\nidentify the presence and evolution of all three components. The bulk Lorentz\nfactor of this bright and relatively short-duration burst is independently\ncalculated using the thermal and hard non-thermal components, supporting a jet\npenetration scenario. The multi-segment broken powerlaw feature observed in the\nflux light curves suggests the presence of an early afterglow in the keV-MeV\nband and hints at a possible two-jet structure. Furthermore, the observed\ntransition from positive to negative on the spectral lag can be interpreted as\nan independent evolution of the soft and hard components, leading to\nmisalignment in the cross-correlation function (CCF) analysis of pulses.",
      "generated_abstract": "The LATS-1 spacecraft, launched in 2015, has observed many transient\ngalactic sources, including the first known transient gamma-ray event, the\nLATS-1 GRB 150915A. The LATS-1 data are now available in the Fermi-LAT data\nprocessing pipeline. This paper summarizes the current state of the LATS-1\ndata processing pipeline and presents the results of the analysis of the\nLATS-1 data, including a search for transient gamma-ray sources in the Galactic\nplane, a search for transient gamma-ray sources in the Galactic bulge, and an\ninvestigation of the temporal evolution of the diffuse gamma-ray emission.\nThese results are discussed in the context of the LATS-1 GRB 150915A event,\nwhich",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1037037037037037,
          "p": 0.2545454545454545,
          "f": 0.1473684169390583
        },
        "rouge-2": {
          "r": 0.02577319587628866,
          "p": 0.06329113924050633,
          "f": 0.036630032517275625
        },
        "rouge-l": {
          "r": 0.08148148148148149,
          "p": 0.2,
          "f": 0.11578946957063728
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.17072v2",
      "true_abstract": "General Equilibrium Theory is the benchmark of economics, especially its\nresults concerning the efficient allocation of resources, known as the First\nand Second Welfare Theorems. Yet, General Equilibrium Theory is beyond the\nscope of most economists. This paper is pitched as the first entry point into\nthe theory. General Equilibrium Theory proves that at least one state of\nequilibrium always exists. In its most general approach, it uses fixed-point\ntheorems to this end. This paper discusses the assumptions on individuals'\nbehaviour and the structure of the system of exchange that guarantee that the\nconditions of the fixed-point theorems are satisfied. The purpose is to lay\nbare the role each plays in proving the existence of equilibrium and provide a\nclear picture of the relationship between the assumptions and the result. The\ndiscussion is presented in the simplest possible setting that captures the\nfundamental features of commodity exchange.",
      "generated_abstract": "This paper investigates the role of social norms in the formation of\nsocietal preferences. We introduce the notion of social norm-driven preference\n(SNP), which captures how the preferences of individuals are shaped by the\npreferences of others. SNP provides a theoretical foundation for the\nexistence of preferences that do not fit within the conventional concept of\n\"natural preferences,\" and we develop a framework for analyzing the social\npressures that can shape preferences. We then consider the case of the\ndistribution of social norms across society, where individuals are exposed to\nmany different norms and must balance the preferences they develop against\ntheir preferences derived from social norms. We show that SNP provides a\nreliable framework for explaining the formation of preferences in such\nsocieties. Additionally, we provide evidence that social norms can shape the\npreferences of individuals when the distribution of norms is une",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16304347826086957,
          "p": 0.189873417721519,
          "f": 0.175438591520126
        },
        "rouge-2": {
          "r": 0.045112781954887216,
          "p": 0.049586776859504134,
          "f": 0.04724408949934952
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.17721518987341772,
          "f": 0.16374268508737747
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.02075v1",
      "true_abstract": "Aligning a lens system relative to an imager is a critical challenge in\ncamera manufacturing. While optimal alignment can be mathematically computed\nunder ideal conditions, real-world deviations caused by manufacturing\ntolerances often render this approach impractical. Measuring these tolerances\ncan be costly or even infeasible, and neglecting them may result in suboptimal\nalignments. We propose a reinforcement learning (RL) approach that learns\nexclusively in the pixel space of the sensor output, eliminating the need to\ndevelop expert-designed alignment concepts. We conduct an extensive benchmark\nstudy and show that our approach surpasses other methods in speed, precision,\nand robustness. We further introduce relign, a realistic, freely explorable,\nopen-source simulation utilizing physically based rendering that models optical\nsystems with non-deterministic manufacturing tolerances and noise in robotic\nalignment movement. It provides an interface to popular machine learning\nframeworks, enabling seamless experimentation and development. Our work\nhighlights the potential of RL in a manufacturing environment to enhance\nefficiency of optical alignments while minimizing the need for manual\nintervention.",
      "generated_abstract": "This study presents an adaptive control approach for the VTOL drone using\na reinforcement learning framework. The VTOL drone is a model-free system,\nwhere the flight dynamics are based on the linearized model of the drone\npropeller. The proposed control framework is implemented in an open-source\nsimulation environment and integrated with a learning-based approach. The\nframework incorporates a control law to minimize the maximum takeoff weight,\noptimizing the propeller and rotor angles. A reinforcement learning agent is\ntrained to optimize the control law using a simulated VTOL drone with\nadditional constraints. The results demonstrate that the proposed control\nframework enables the drone to take off and hover while minimizing the maximum\ntakeoff weight.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17355371900826447,
          "p": 0.31343283582089554,
          "f": 0.22340425073166603
        },
        "rouge-2": {
          "r": 0.04375,
          "p": 0.06862745098039216,
          "f": 0.05343510974884956
        },
        "rouge-l": {
          "r": 0.1652892561983471,
          "p": 0.29850746268656714,
          "f": 0.2127659528593256
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.00450v1",
      "true_abstract": "We propose and study three confidence intervals (CIs) centered at an\nestimator that is intentionally biased to reduce mean squared error. The first\nCI simply uses an unbiased estimator's standard error; compared to centering at\nthe unbiased estimator, this CI has higher coverage probability for confidence\nlevels above 91.7%, even if the biased and unbiased estimators have equal mean\nsquared error. The second CI trades some of this \"excess\" coverage for shorter\nlength. The third CI is centered at a convex combination of the two estimators\nto further reduce length. Practically, these CIs apply broadly and are simple\nto compute.",
      "generated_abstract": "This study proposes a novel methodology for the identification of the\nparameters of the two-dimensional (2D) nonparametric regression model, which\nis known to be nonidentifiable. The method is based on the identification of\nthe regression coefficients of the 2D nonparametric regression model by the\napproach of the simultaneous identification of the regression coefficients of\nthe two 1D nonparametric regression models. The theoretical properties of the\nproposed method are established, and the results of the simulation studies are\ncompared with those of the existing methods. The proposed method is applied to\nthe estimation of the 2D nonparametric regression model for the covariate\nstructure of the data of the Cobb-Douglas production function. The\nsimulation results show that the proposed methodology is more efficient than\nthe existing methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18571428571428572,
          "p": 0.20967741935483872,
          "f": 0.19696969198806258
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.02127659574468085,
          "f": 0.021052626579502572
        },
        "rouge-l": {
          "r": 0.18571428571428572,
          "p": 0.20967741935483872,
          "f": 0.19696969198806258
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.03976v1",
      "true_abstract": "Asynchronous Boolean networks are a type of discrete dynamical system in\nwhich each variable can take one of two states, and a single variable state is\nupdated in each time step according to pre-selected rules. Boolean networks are\npopular in systems biology due to their ability to model long-term biological\nphenotypes within a qualitative, predictive framework. Boolean networks model\nphenotypes as attractors, which are closely linked to minimal trap spaces\n(inescapable hypercubes in the system's state space). In biological\napplications, attractors and minimal trap spaces are typically in one-to-one\ncorrespondence. However, this correspondence is not guaranteed: motif-avoidant\nattractors (MAAs) that lie outside minimal trap spaces are possible.\n  MAAs are rare and (despite recent efforts) poorly understood. Here we\nsummarize the current state of knowledge regarding MAAs and present several\nnovel observations regarding their response to node deletion reductions and\nlinear extensions of edges. We conduct large-scale computational studies on an\nensemble of 14,000 models derived from published Boolean models of biological\nsystems, and more than 100 million Random Boolean Networks. Our findings\nquantify the rarity of MAAs (in particular, we found no MAAs in the biological\nmodels), but highlight the role of network reduction in introducing MAAs into\nthe dynamics. We also show that MAAs are fragile to linear extensions: in\nsparse networks, even a single linear node can disrupt virtually all MAAs.\nMotivated by this observation, we improve the upper bound on the number of\ndelays needed to disrupt a motif-avoidant attractor.",
      "generated_abstract": "The search for new therapeutic targets in cancer is a key challenge in\naccelerating drug development. To address this challenge, we have developed a\nnovel approach, the Cancer Tissue-Specific Drug-Drug Interaction (CDD-DI)\napproach, aiming to predict the Tissue-Specific Drug-Drug Interaction\n(TSDDI) between the target protein and the drugs used to treat it. This\napproach is based on a deep learning model that predicts the TSDDI between the\ntarget protein and drugs, and then compares it with the TSDDI of a target\nprotein in normal tissue. We have used this model to evaluate the TSDDI between\nthe target protein and the drugs used to treat colorectal cancer (CRC) in\nnormal and tumor tissues, and we have compared this approach with other\napproaches",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0915032679738562,
          "p": 0.20588235294117646,
          "f": 0.1266968283188306
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0915032679738562,
          "p": 0.20588235294117646,
          "f": 0.1266968283188306
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2412.17354v3",
      "true_abstract": "In this study, we introduce a novel methodological framework called Bayesian\nPenalized Empirical Likelihood (BPEL), designed to address the computational\nchallenges inherent in empirical likelihood (EL) approaches. Our approach has\ntwo primary objectives: (i) to enhance the inherent flexibility of EL in\naccommodating diverse model conditions, and (ii) to facilitate the use of\nwell-established Markov Chain Monte Carlo (MCMC) sampling schemes as a\nconvenient alternative to the complex optimization typically required for\nstatistical inference using EL. To achieve the first objective, we propose a\npenalized approach that regularizes the Lagrange multipliers, significantly\nreducing the dimensionality of the problem while accommodating a comprehensive\nset of model conditions. For the second objective, our study designs and\nthoroughly investigates two popular sampling schemes within the BPEL context.\nWe demonstrate that the BPEL framework is highly flexible and efficient,\nenhancing the adaptability and practicality of EL methods. Our study highlights\nthe practical advantages of using sampling techniques over traditional\noptimization methods for EL problems, showing rapid convergence to the global\noptima of posterior distributions and ensuring the effective resolution of\ncomplex statistical inference challenges.",
      "generated_abstract": "This paper presents a novel method to assess the quality of an estimation\nmethod for a parametric model. We propose a novel method to assess the\nquality of an estimation method for a parametric model, and demonstrate the\napplicability of this method for various estimation methods. We provide a\ntheory for the proposed method, and demonstrate its applicability by applying\nthe method to a simulation study. In addition, we provide a real data analysis\nto show the validity of the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1322314049586777,
          "p": 0.38095238095238093,
          "f": 0.1963190145793971
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.047619047619047616,
          "f": 0.025316451792982496
        },
        "rouge-l": {
          "r": 0.10743801652892562,
          "p": 0.30952380952380953,
          "f": 0.1595091986284769
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12024v1",
      "true_abstract": "Mean field equilibrium (MFE) has emerged as a computationally tractable\nsolution concept for large dynamic games. However, computing MFE remains\nchallenging due to nonlinearities and the absence of contraction properties,\nlimiting its reliability for counterfactual analysis and comparative statics.\nThis paper focuses on MFE in dynamic models where agents interact through a\nscalar function of the population distribution, referred to as the\n\\textit{scalar interaction function}. Such models naturally arise in a wide\nrange of applications in operations and economics, including quality ladder\nmodels, inventory competition, online marketplaces, and heterogeneous-agent\nmacroeconomic models. The main contribution of this paper is to introduce\niterative algorithms that leverage the scalar interaction structure and are\nguaranteed to converge to the MFE under mild assumptions. Unlike existing\napproaches, our algorithms do not rely on monotonicity or contraction\nproperties, significantly broadening their applicability. Furthermore, we\nprovide a model-free algorithm that learns the MFE by employing simulation and\nreinforcement learning techniques such as Q-learning and policy gradient\nmethods without requiring prior knowledge of payoff or transition functions. We\nestablish finite-time performance bounds for this algorithm under technical\nLipschitz continuity assumptions. We apply our algorithms to classic models of\ndynamic competition, such as capacity competition, and to competitive models\nmotivated by online marketplaces, including ridesharing, dynamic reputation,\nand inventory competition, as well as to social learning models. Using our\nalgorithms, we derive reliable comparative statics results that illustrate how\nkey market parameters influence equilibrium outcomes in these stylized models,\nproviding insights that could inform the design of competitive systems in these\ncontexts.",
      "generated_abstract": "This paper addresses a fundamental question in economics: What are the\nresources needed to produce an optimal amount of output? This question is\nimportant in many fields, including ecology and economics, where the\nproduction of resources can be used to produce goods. We study the production\nof goods by a firm that produces a resource and sells it to a buyer. In this\nsetting, the firm seeks to maximize its profits. We show that in this setting,\nthe optimal resource production strategy is to produce the resource at the\nhighest level of output possible, without exceeding the buyer's demand. This\nmaximizes the profits of the firm, and also ensures that the demand for the\nresource is met. Furthermore, we show that the optimal production strategy is\nalso the optimal production strategy under the condition that the demand for\nthe resource is the same for every buyer, and that the firm produces the\nresource at the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1393939393939394,
          "p": 0.30666666666666664,
          "f": 0.19166666236979177
        },
        "rouge-2": {
          "r": 0.020491803278688523,
          "p": 0.0390625,
          "f": 0.026881715916291658
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.26666666666666666,
          "f": 0.16666666236979177
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15655v1",
      "true_abstract": "We study the local geometry of empirical risks in high dimensions via the\nspectral theory of their Hessian and information matrices. We focus on settings\nwhere the data, $(Y_\\ell)_{\\ell =1}^n\\in \\mathbb R^d$, are i.i.d. draws of a\n$k$-component Gaussian mixture model, and the loss depends on the projection of\nthe data into a fixed number of vectors, namely $\\mathbf{x}^\\top Y$, where\n$\\mathbf{x}\\in \\mathbb{R}^{d\\times C}$ are the parameters, and $C$ need not\nequal $k$. This setting captures a broad class of problems such as\nclassification by one and two-layer networks and regression on multi-index\nmodels. We prove exact formulas for the limits of the empirical spectral\ndistribution and outlier eigenvalues and eigenvectors of such matrices in the\nproportional asymptotics limit, where the number of samples and dimension\n$n,d\\to\\infty$ and $n/d=\\phi \\in (0,\\infty)$. These limits depend on the\nparameters $\\mathbf{x}$ only through the summary statistic of the $(C+k)\\times\n(C+k)$ Gram matrix of the parameters and class means, $\\mathbf{G} =\n(\\mathbf{x},\\mathbf{\\mu})^\\top(\\mathbf{x},\\mathbf{\\mu})$. It is known that\nunder general conditions, when $\\mathbf{x}$ is trained by stochastic gradient\ndescent, the evolution of these same summary statistics along training\nconverges to the solution of an autonomous system of ODEs, called the effective\ndynamics. This enables us to connect the spectral theory to the training\ndynamics. We demonstrate our general results by analyzing the effective\nspectrum along the effective dynamics in the case of multi-class logistic\nregression. In this setting, the empirical Hessian and information matrices\nhave substantially different spectra, each with their own static and even\ndynamical spectral transitions.",
      "generated_abstract": "We consider the problem of estimating the mean of a function of two\nparameters, $\\mathbf{X} = (X_1, X_2)^\\top$ and $\\mathbf{Z} = (Z_1, Z_2)^\\top$\nwhere $\\mathbf{X}$ and $\\mathbf{Z}$ are drawn independently from a joint\ndistribution $\\mathcal{D}$. We assume that $\\mathbf{X}$ and $\\mathbf{Z}$ are\njointly Gaussian, and we focus on the case when the mean of $\\mathbf{X}$ is\nunknown. We develop a novel approach for estimating $\\mathbb{E}(\\mathbf{X})$\nthat relies on the joint distribution of $\\mathbf{X}$ and $\\mathbf{Z}$ and on\nthe covariance matrix of $\\mathbf{Z}$. Our estimator is based on the\nempirical covariance matrix of $\\mathbf{Z}$, which is estimated using a\nproposed method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11875,
          "p": 0.3275862068965517,
          "f": 0.1743119227001095
        },
        "rouge-2": {
          "r": 0.02553191489361702,
          "p": 0.07058823529411765,
          "f": 0.03749999609863322
        },
        "rouge-l": {
          "r": 0.10625,
          "p": 0.29310344827586204,
          "f": 0.1559632988468985
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2503.07811v2",
      "true_abstract": "The theory of optimal transportation has developed into a powerful and\nelegant framework for comparing probability distributions, with wide-ranging\napplications in all areas of science. The fundamental idea of analyzing\nprobabilities by comparing their underlying state space naturally aligns with\nthe core idea of causal inference, where understanding and quantifying\ncounterfactual states is paramount. Despite this intuitive connection, explicit\nresearch at the intersection of optimal transport and causal inference is only\nbeginning to develop. Yet, many foundational models in causal inference have\nimplicitly relied on optimal transport principles for decades, without\nrecognizing the underlying connection. Therefore, the goal of this review is to\noffer an introduction to the surprisingly deep existing connections between\noptimal transport and the identification of causal effects with observational\ndata -- where optimal transport is not just a set of potential tools, but\nactually builds the foundation of model assumptions. As a result, this review\nis intended to unify the language and notation between different areas of\nstatistics, mathematics, and econometrics, by pointing out these existing\nconnections, and to explore novel problems and directions for future work in\nboth areas derived from this realization.",
      "generated_abstract": "This study examines the impact of a single, moderately effective COVID-19\nresponse on changes in a macroeconomic indicator. The study employs a\ndynamic stochastic general equilibrium (DSGE) model to analyze the impact of a\npolicy response that includes (i) a temporary, 2-week lockdown, (ii) a\nvoluntary work-from-home (WFH) mandate, and (iii) a gradual reopening of\neconomic activities. The model is calibrated using data from the United States\nduring the first 15 months of the pandemic. The results reveal that the\nreopening of economic activities significantly increased employment growth.\nHowever, the lockdown and WFH mandates had little impact on economic growth.\nThese results suggest that the COVID-19 response had a limited impact on\neconomic outcomes, highlighting the critical role",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.15789473684210525,
          "f": 0.11999999528800019
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07258064516129033,
          "p": 0.11842105263157894,
          "f": 0.08999999528800025
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.08976v1",
      "true_abstract": "Federated Ranking Learning (FRL) is a state-of-the-art FL framework that\nstands out for its communication efficiency and resilience to poisoning\nattacks. It diverges from the traditional FL framework in two ways: 1) it\nleverages discrete rankings instead of gradient updates, significantly reducing\ncommunication costs and limiting the potential space for malicious updates, and\n2) it uses majority voting on the server side to establish the global ranking,\nensuring that individual updates have minimal influence since each client\ncontributes only a single vote. These features enhance the system's scalability\nand position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain\nedges are particularly vulnerable to poisoning attacks. Through a theoretical\ninvestigation, we prove the existence of these vulnerable edges and establish a\nlower bound and an upper bound for identifying them in each layer. Based on\nthis finding, we introduce a novel local model poisoning attack against FRL,\nnamely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on\nidentifying and perturbing the most vulnerable edges in each layer and\nleveraging an optimization-based approach to maximize the attack's impact.\nThrough extensive experiments on benchmark datasets, we demonstrate that our\nattack achieves an overall 53.23% attack impact and is 3.7x more impactful than\nexisting methods. Our findings highlight significant vulnerabilities in\nranking-based FL systems and underline the urgency for the development of new\nrobust FL frameworks.",
      "generated_abstract": "We study the problem of learning a single linear transformation in a\nrepresentation with a large number of dimensions. This is a challenging\nproblem due to the intrinsic complexity of the optimization problem. We\nintroduce a novel framework for learning this transformation in the\n$L_2$-norm, which we call the $L_2$-NMF (Nuclear Min-Max Factorization)\nframework. In this framework, we reformulate the problem of learning a single\nlinear transformation as a nonconvex problem, and then we propose a novel\napproach based on the $L_2$-NMF that achieves a near-optimal learning\ncomplexity. We provide a theoretical analysis of our approach and we conduct\nexperiments to validate its effectiveness.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.39344262295081966,
          "f": 0.21719456613910446
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.07526881720430108,
          "f": 0.04320987245027474
        },
        "rouge-l": {
          "r": 0.1375,
          "p": 0.36065573770491804,
          "f": 0.19909501862779233
        }
      }
    },
    {
      "paper_id": "math.NT.cs/DS/2503.10158v1",
      "true_abstract": "Integral linear systems $Ax=b$ with matrices $A$, $b$ and solutions $x$ are\nalso required to be in integers, can be solved using invariant factors of $A$\n(by computing the Smith Canonical Form of $A$). This paper explores a new\nproblem which arises in applications, that of obtaining conditions for solving\nthe Modular Linear System $Ax=b\\rem n$ given $A,b$ in $\\zz_n$ for $x$ in\n$\\zz_n$ along with the constraint that the value of the linear function\n$\\phi(x)=<w,x>$ is coprime to $n$ for some solution $x$. In this paper we\ndevelop decomposition of the system to coprime moduli $p^{r(p)}$ which are\ndivisors of $n$ and show how such a decomposition simplifies the computation of\nSmith form. This extends the well known index calculus method of computing the\ndiscrete logarithm where the moduli over which the linear system is reduced\nwere assumed to be prime (to solve the reduced systems over prime fields) to\nthe case when the factors of the modulus are prime powers $p^{r(p)}$. It is\nshown how this problem can be addressed effciently using the invariant factors\nand Smith form of the augmented matrix $[A,-p^{r(p)}I]$ and conditions modulo\n$p$ satisfied by $w$, where $p^{r(p)}$ vary over all divisors of $n$ with $p$\nprime.",
      "generated_abstract": "We provide a new proof of the classical Gale duality theorem for graphs,\nshowing that a graph with a certain type of symmetry is equivalent to a\ngraph with the same symmetry, and hence to an equivalent graph with the same\ndegree. Our proof is based on a variant of the Gale duality theorem for\nmatroids and relies on an interplay between a notion of degree, called\n`degree-type degree' in the paper, and the degree of an element, called\n`degree of a vertex' in the paper. This new notion of degree is defined for\ngraphs and matroids, and we show that it is equivalent to the usual notion of\ndegree for graphs and matroids.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13157894736842105,
          "p": 0.26785714285714285,
          "f": 0.17647058381730113
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.041666666666666664,
          "f": 0.02807017097100718
        },
        "rouge-l": {
          "r": 0.11403508771929824,
          "p": 0.23214285714285715,
          "f": 0.15294117205259528
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.15346v1",
      "true_abstract": "Drug discovery remains a slow and expensive process that involves many steps,\nfrom detecting the target structure to obtaining approval from the Food and\nDrug Administration (FDA), and is often riddled with safety concerns. Accurate\nprediction of how drugs interact with their targets and the development of new\ndrugs by using better methods and technologies have immense potential to speed\nup this process, ultimately leading to faster delivery of life-saving\nmedications. Traditional methods used for drug-target interaction prediction\nshow limitations, particularly in capturing complex relationships between drugs\nand their targets. As an outcome, deep learning models have been presented to\novercome the challenges of interaction prediction through their precise and\nefficient end results. By outlining promising research avenues and models, each\nwith a different solution but similar to the problem, this paper aims to give\nresearchers a better idea of methods for even more accurate and efficient\nprediction of drug-target interaction, ultimately accelerating the development\nof more effective drugs. A total of 180 prediction methods for drug-target\ninteractions were analyzed throughout the period spanning 2016 to 2025 using\ndifferent frameworks based on machine learning, mainly deep learning and graph\nneural networks. Additionally, this paper discusses the novelty, architecture,\nand input representation of these models.",
      "generated_abstract": "The integration of omics data into the context of systems biology is\ndeveloping rapidly, with the integration of omics data into the context of\nsystems biology being a key focus. The emergence of systems biology in the\nlast decade has allowed for the integration of omics data into systems biology,\nwhich has resulted in significant advancements in the field. The integration of\nomics data into systems biology has allowed for the development of advanced\ntools and techniques, such as the use of machine learning models to analyze\nomics data. The integration of omics data into systems biology has allowed for\nthe development of advanced tools and techniques, such as the use of machine\nlearning models to analyze omics data. The integration of omics data into\nsystems biology has allowed for the development of advanced tools and techniques,\nsuch as the use of machine learning models to analyze omics data. The integration\nof omics data",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0962962962962963,
          "p": 0.29545454545454547,
          "f": 0.14525139294029535
        },
        "rouge-2": {
          "r": 0.015544041450777202,
          "p": 0.04918032786885246,
          "f": 0.023622043594457756
        },
        "rouge-l": {
          "r": 0.08888888888888889,
          "p": 0.2727272727272727,
          "f": 0.13407820858275346
        }
      }
    },
    {
      "paper_id": "cs.AI.nlin/CD/2503.09858v1",
      "true_abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "generated_abstract": "We present a model of the perception of time-varying, non-stationary\ntime-dependent events, which we refer to as time-dependent events (TDEs). We\nexamine the role of TDEs in perception, including how TDEs are perceived in\nattention, and how this affects attention allocation. We demonstrate that\nattention allocation is strongly affected by TDEs, and that this affects how\npeople think about time. We also explore the role of TDEs in the perception of\ntime-varying, non-stationary events, and how TDEs can be exploited to guide\nperception.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.28,
          "f": 0.17073170307852478
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.02857142857142857,
          "f": 0.018099543182982545
        },
        "rouge-l": {
          "r": 0.07894736842105263,
          "p": 0.18,
          "f": 0.10975609332242729
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06070v1",
      "true_abstract": "This paper bridges optimization and control, and presents a novel closed-loop\ncontrol framework based on natural gradient descent, offering a\ntrajectory-oriented alternative to traditional cost-function tuning. By\nleveraging the Fisher Information Matrix, we formulate a preconditioned\ngradient descent update that explicitly shapes system trajectories. We show\nthat, in sharp contrast to traditional controllers, our approach provides\nflexibility to shape the system's low-level behavior. To this end, the proposed\nmethod parameterizes closed-loop dynamics in terms of stationary covariance and\nan unknown cost function, providing a geometric interpretation of control\nadjustments. We establish theoretical stability conditions. The simulation\nresults on a rotary inverted pendulum benchmark highlight the advantages of\nnatural gradient descent in trajectory shaping.",
      "generated_abstract": "This paper addresses the problem of optimizing the transmission of power\nbetween a power source and a receiver in a multi-antenna system. The power\ntransmission problem is formulated as a constrained convex optimization problem\nin which the objective function is constrained to be the transmit power at the\nreceiver. To address the constraints, we develop an algorithm that iteratively\nsolves a sequence of convex optimization problems to find an optimal power\ntransmission scheme. The proposed algorithm is shown to converge to an\noptimal solution under a suitable regularity condition. Numerical results\ndemonstrate the effectiveness of the proposed algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19101123595505617,
          "p": 0.29310344827586204,
          "f": 0.2312925122291639
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.021505376344086023,
          "f": 0.019801975229390523
        },
        "rouge-l": {
          "r": 0.1797752808988764,
          "p": 0.27586206896551724,
          "f": 0.21768707005229312
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/CG/2503.08863v1",
      "true_abstract": "We study three fundamental three-dimensional (3D) geometric packing problems:\n3D (Geometric) Bin Packing (3D-BP), 3D Strip Packing (3D-SP), and Minimum\nVolume Bounding Box (3D-MVBB), where given a set of 3D (rectangular) cuboids,\nthe goal is to find an axis-aligned nonoverlapping packing of all cuboids. In\n3D-BP, we need to pack the given cuboids into the minimum number of unit cube\nbins. In 3D-SP, we need to pack them into a 3D cuboid with a unit square base\nand minimum height. Finally, in 3D-MVBB, the goal is to pack into a cuboid box\nof minimum volume.\n  It is NP-hard to even decide whether a set of rectangles can be packed into a\nunit square bin -- giving an (absolute) approximation hardness of 2 for 3D-BP\nand 3D-SP. The previous best (absolute) approximation for all three problems is\nby Li and Cheng (SICOMP, 1990), who gave algorithms with approximation ratios\nof 13, $46/7$, and $46/7+\\varepsilon$, respectively, for 3D-BP, 3D-SP, and\n3D-MVBB. We provide improved approximation ratios of 6, 6, and $3+\\varepsilon$,\nrespectively, for the three problems, for any constant $\\varepsilon > 0$.\n  For 3D-BP, in the asymptotic regime, Bansal, Correa, Kenyon, and Sviridenko\n(Math.~Oper.~Res., 2006) showed that there is no asymptotic polynomial-time\napproximation scheme (APTAS) even when all items have the same height. Caprara\n(Math.~Oper.~Res., 2008) gave an asymptotic approximation ratio of\n$T_{\\infty}^2 + \\varepsilon\\approx 2.86$, where $T_{\\infty}$ is the well-known\nHarmonic constant in Bin Packing. We provide an algorithm with an improved\nasymptotic approximation ratio of $3 T_{\\infty}/2 +\\varepsilon \\approx 2.54$.\nFurther, we show that unlike 3D-BP (and 3D-SP), 3D-MVBB admits an APTAS.",
      "generated_abstract": "We present a novel method for the generation of synthetic high-quality\ntextured mesh models based on the combination of 3D point cloud data and\nphysically-based surface models. This approach is aimed at enhancing the\nrepresentativeness of the generated mesh models, particularly in the\npresence of complex geometries. Our method utilizes a novel technique for\nreconstructing the 3D point cloud from a 3D mesh, referred to as the\n\"point-cloud-from-mesh\" (PCFM) approach, which is able to capture the\nstructural and topological details of the mesh. Additionally, the PCFM is\nemployed to compute a surface mesh of the mesh, referred to as the \"mesh-from-\npoint-cloud\" (MCPC) model, which is used to generate the surface model of the\ngenerated mesh. The synthetic mesh model is generated using a unified",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07096774193548387,
          "p": 0.15714285714285714,
          "f": 0.09777777349135822
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.036036036036036036,
          "f": 0.02253520696941165
        },
        "rouge-l": {
          "r": 0.07096774193548387,
          "p": 0.15714285714285714,
          "f": 0.09777777349135822
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.06490v1",
      "true_abstract": "Seismic data acquisition is often affected by various types of noise, which\ndegrade data quality and hinder subsequent interpretation. Recovery of seismic\ndata becomes particularly challenging in the presence of strong noise, which\nsignificantly impacts both data accuracy and geological analysis. This study\nproposes a novel single-encoder, multiple-decoder network based on Nash\nequalization (SEMD-Nash) for effective strong noise attenuation in seismic\ndata. The main contributions of this method are as follows: First, we design a\nshared encoder-multi-decoder architecture, where an improved encoder extracts\nkey features from the noisy data, and three parallel decoders reconstruct the\ndenoised seismic signal from different perspectives. Second, we develop a\nmulti-objective optimization system that integrates three loss functions-Mean\nSquared Error (MSE), Perceived Loss, and Structural Similarity Index (SSIM)-to\nensure effective signal reconstruction, high-order feature preservation, and\nstructural integrity. Third, we introduce the Nash Equalization Weight\nOptimizer, which dynamically adjusts the weights of the loss functions,\nbalancing the optimization objectives to improve the models robustness and\ngeneralization. Experimental results demonstrate that the proposed method\neffectively suppresses strong noise while preserving the geological\ncharacteristics of the seismic data.",
      "generated_abstract": "The aim of this work is to develop a method for the calculation of the\nenergy density of an idealized model of the Earth's interior. The method is\nbased on the assumption that the Earth's interior is divided into an inner\ncore and an outer crust. The core is assumed to be filled with a density of\n$1.4$ g/cm$^3$ and the crust with a density of $1.7$ g/cm$^3$. We assume that\nthe pressure in the crust is equal to 1100 MPa, and the pressure in the core\nis equal to 100 MPa. The method is based on the Maxwell stress and energy\ndensity formulae. The energy density is calculated from the density and the\npressure of the Earth's interior. The method is applied to the model of the\nEarth's interior. We assume that the temperature in the core",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12878787878787878,
          "p": 0.3090909090909091,
          "f": 0.18181817766593278
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.07142857142857142,
          "f": 0.051282046679816326
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.2909090909090909,
          "f": 0.17112299050015736
        }
      }
    },
    {
      "paper_id": "cs.NI.math/NA/2503.09869v1",
      "true_abstract": "A well-known expression for the saturation throughput of heterogeneous\ntransmitting nodes in a wireless network using p-CSMA, derived from Renewal\nTheory, implicitly assumes that all transmitting nodes are in range of, and\ntherefore conflicting with, each other. This expression, as well as simple\nmodifications of it, does not correctly capture the saturation throughput\nvalues when an arbitrary topology is specified for the conflict graph between\ntransmitting links. For example, we show numerically that calculations based on\nrenewal theory can underestimate throughput by 48-62% for large packet sizes\nwhen the conflict graph is represented by a star topology. This is problematic\nbecause real-world wireless networks, such as wireless IoT mesh networks, are\noften deployed over a large area, resulting in non-complete conflict graphs.\n  To address this gap, we present a computational approach based on a novel\nMarkov chain formulation that yields the exact saturation throughput for each\nnode in the general network case for any given set of access probabilities, as\nwell as a more compact expression for the special case where the packet length\nis twice the slot length. Using our approach, we show how the transmit\nprobabilities could be optimized to maximize weighted utility functions of the\nsaturation throughput values. This would allow a wireless system designer to\nset transmit probabilities to achieve desired throughput trade-offs in any\ngiven deployment.",
      "generated_abstract": "This paper introduces the concept of an ``extended-capability'' network,\nwhich represents a network of distributed computing systems that can be\nreconfigured to meet a new capability by a process of ``extending'' or\n``augmenting'' existing resources. The extended-capability network is defined\nas a graph with a directed edge between nodes if and only if the system can\nmeet the capability represented by the edge. This definition allows for the\ndevelopment of an algorithm that can find an extended-capability network for a\ngiven set of systems. We show that, for any given graph of systems, there is a\ndeterministic polynomial-time algorithm that, given the set of systems, finds\nthe extended-capability network that can meet the given capability. We also\nprovide a polynomial-time algorithm that, given the set of systems, finds an\nextended-capability network that can be extended to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17266187050359713,
          "p": 0.3582089552238806,
          "f": 0.2330097043486663
        },
        "rouge-2": {
          "r": 0.03980099502487562,
          "p": 0.07079646017699115,
          "f": 0.05095540940545298
        },
        "rouge-l": {
          "r": 0.17266187050359713,
          "p": 0.3582089552238806,
          "f": 0.2330097043486663
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.12141v2",
      "true_abstract": "The FAO-GAEZ crop productivity data are widely used in Economics. However,\nthe existence of measurement error is rarely recognized in the empirical\nliterature. We propose a novel method to partially identify the effect of\nagricultural productivity, deriving bounds that allow for nonclassical\nmeasurement error by leveraging two proxies. These bounds exhaust all the\ninformation contained in the first two moments of the data. We reevaluate three\ninfluential studies, documenting that measurement error matters and that the\nimpact of agricultural productivity on economic outcomes may be smaller than\npreviously reported. Our methodology has broad applications in empirical\nresearch involving mismeasured variables.",
      "generated_abstract": "In this paper, we introduce a new framework for analyzing the impact of\nthe COVID-19 pandemic on the U.S. economy. Our approach incorporates three\ncomponents: (1) a dynamic model that simulates the evolution of the U.S. economy\nover time; (2) an optimization problem that quantifies the impact of\nincreasing the federal minimum wage on the unemployment rate; and (3) a\ndynamic model that quantifies the impact of increasing the minimum wage on\nemployment. Our analysis highlights the importance of the government's\nresponse to the COVID-19 pandemic on the U.S. economy and its implications for\nfuture policy decisions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.19298245614035087,
          "f": 0.166666661759642
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.0410958904109589,
          "f": 0.03571428080002902
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.17543859649122806,
          "f": 0.1515151466081269
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CL/2503.10515v1",
      "true_abstract": "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.",
      "generated_abstract": "This paper introduces a novel method for generating multilingual dialogue\nprocesses from textual prompts, leveraging pre-trained transformer language\nmodels. Our approach uses an open-source dataset, OpenAI's GPT-3.5, to\ngenerate prompts and then trains a large language model on these prompts.\nAdditionally, we incorporate a translation component, using a\nmultilingual-capable transformer, to further enhance the multilingual\ncapabilities of our system. We demonstrate that our method can generate\nmultilingual dialogue processes that closely mimic human-generated dialogue\nprocesses, achieving a BLEU score of 23.5 on a test-set, comparable to state-of-the-art\nmethods. Additionally, our method achieves a 17.9 BLEU score on a test-set of\nsynthetic dialogue processes,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.26666666666666666,
          "f": 0.22099447028478997
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.04040404040404041,
          "f": 0.03433475906132068
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.26666666666666666,
          "f": 0.22099447028478997
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.03131v1",
      "true_abstract": "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
      "generated_abstract": "In this work, we propose a framework for the computation of the quantum\ndynamics of a one-dimensional model of a spiking neuron, in the presence of a\nnoise source. The model is based on the stochastic Schr\\\"odinger equation, which\nis used to describe the dynamics of the neuron's state. This equation is\ncoupled with the classical Langevin equation, which describes the noise source.\nThe coupling between the two equations is described through a dissipative\nterm. We analyze the properties of the resulting dynamics, and we show how to\nfind the appropriate dissipative coupling term to obtain the correct dynamics.\nWe also show that the system can be described in the context of quantum\nstatistical mechanics, and we provide a discussion of the properties of the\ndynamics in the quantum domain. The resulting dynamics of the neuron is then\nused to study its response to external stimuli",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1780821917808219,
          "p": 0.3291139240506329,
          "f": 0.23111110655446915
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.048,
          "f": 0.033613440827311936
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.27848101265822783,
          "f": 0.19555555099891367
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06411v1",
      "true_abstract": "This paper examines the intricate interplay among AI safety, security, and\ngovernance by integrating technical systems engineering with principles of\nmoral imagination and ethical philosophy. Drawing on foundational insights from\nWeapons of Math Destruction and Thinking in Systems alongside contemporary\ndebates in AI ethics, we develop a comprehensive multi-dimensional framework\ndesigned to regulate AI technologies deployed in high-stakes domains such as\ndefense, finance, healthcare, and education. Our approach combines rigorous\ntechnical analysis, quantitative risk assessment, and normative evaluation to\nexpose systemic vulnerabilities inherent in opaque, black-box models. Detailed\ncase studies, including analyses of Microsoft Tay (2016) and the UK A-Level\nGrading Algorithm (2020), demonstrate how security lapses, bias amplification,\nand lack of accountability can precipitate cascading failures that undermine\npublic trust. We conclude by outlining targeted strategies for enhancing AI\nresilience through adaptive regulatory mechanisms, robust security protocols,\nand interdisciplinary oversight, thereby advancing the state of the art in\nethical and technical AI governance.",
      "generated_abstract": "In this paper, we propose a novel reinforcement learning-based controller for\nnonlinear and stochastic system identification and parameter estimation in\ntime-varying chaotic systems. The proposed controller is based on a\nconventional deep RL approach that learns to control the system by minimizing\nthe objective function defined over a set of trajectories. The trajectories\nare generated by simulating the nonlinear and stochastic system under\nvarious input perturbations. The objective function includes the\nreconstructed-error metric, which is computed using the trajectories. The\nreconstructed-error metric is used as a reward function for the reinforcement\nlearning task. In this paper, we present a two-stage reinforcement learning\n(RL) procedure to train the controller, where the first stage is based on\noffline learning and the second stage is based on online learning. In the\noffline learning stage, the controller is trained",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11290322580645161,
          "p": 0.1794871794871795,
          "f": 0.13861385664542708
        },
        "rouge-2": {
          "r": 0.006535947712418301,
          "p": 0.00847457627118644,
          "f": 0.007380068884141564
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.15384615384615385,
          "f": 0.11881187644740729
        }
      }
    },
    {
      "paper_id": "cs.IT.math/IT/2503.08451v1",
      "true_abstract": "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
      "generated_abstract": "The increasing importance of data in various industries has prompted the\ndevelopment of advanced data analysis techniques. These techniques enable the\nacquisition and processing of large datasets to extract valuable information.\nHowever, the complexity of these datasets and the lack of appropriate\ntechniques can lead to inaccurate results. To overcome these challenges, this\npaper proposes an automated machine learning-based approach for the detection\nand classification of suspicious objects in aerial images. This approach\nrelies on the use of a novel feature extraction method based on convolutional\nneural networks (CNN) and a hybrid approach for the detection and classification\nof suspicious objects. The proposed approach is tested on a dataset of\npilotless aerial vehicle (PAV) images acquired by a small UAV, and its\nperformance is compared with that of traditional machine learning-based\napproaches. The results show that the proposed approach outperforms the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.29545454545454547,
          "f": 0.2131147494866972
        },
        "rouge-2": {
          "r": 0.008928571428571428,
          "p": 0.015384615384615385,
          "f": 0.011299430380799254
        },
        "rouge-l": {
          "r": 0.14743589743589744,
          "p": 0.26136363636363635,
          "f": 0.18852458555227097
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2501.16522v1",
      "true_abstract": "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis.",
      "generated_abstract": "This paper explores the use of a genetic algorithm (GA) in an optimal\ndesign problem for a two-species population of mutualistic and antagonistic\nspecies. The population is composed of mutualistic and antagonistic species,\neach of which can be either a mutualistic or an antagonistic species. The\ngoal of the optimal design problem is to identify a set of parameter values\nfor the two species such that the mutualistic and antagonistic species\npopulations are balanced. The optimal parameter values are selected by the\nGA. The problem is formulated as a constrained optimization problem where the\nconstraints are defined in terms of the population parameters. The GA is used\nto search for parameter values that minimize the objective function. The\nobjective function is the number of mutualistic species in the population\ncompared to the number of antagonistic species in the population. The GA\noptimizes the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08771929824561403,
          "p": 0.234375,
          "f": 0.12765957050466287
        },
        "rouge-2": {
          "r": 0.004273504273504274,
          "p": 0.00847457627118644,
          "f": 0.005681813724822711
        },
        "rouge-l": {
          "r": 0.08771929824561403,
          "p": 0.234375,
          "f": 0.12765957050466287
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08991v1",
      "true_abstract": "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
      "generated_abstract": "We consider the problem of solving a first-order differential equation by\nusing the implicit function theorem. The equation is described by a system of\ndifferential equations of the second order, whose coefficients depend on a\nfunction $u(x,y)$. We solve this problem by means of the implicit function\ntheorem, using the method of separation of variables. In this way, we obtain\nan explicit formula for the solution of the problem. In addition, we obtain\nexplicit formulas for the first and second derivatives of the solution. As an\napplication, we solve the problem of determining the solution of the\ndifferential equation $u''+u=0$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17142857142857143,
          "p": 0.23076923076923078,
          "f": 0.1967213065842517
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.04819277108433735,
          "f": 0.04571428072751075
        },
        "rouge-l": {
          "r": 0.17142857142857143,
          "p": 0.23076923076923078,
          "f": 0.1967213065842517
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2407.21190v2",
      "true_abstract": "Predictive values are measures of the clinical accuracy of a binary\ndiagnostic test, and depend on the sensitivity and the specificity of the test\nand on the disease prevalence among the population being studied. This article\nstudies hypothesis tests to simultaneously compare the predictive values of two\nbinary diagnostic tests in the presence of missing data. The hypothesis tests\nwere solved applying two computational methods: the EM and SEM algorithms and\nmultiple imputation. Simulation experiments were carried out to study the sizes\nand the power of the hypothesis tests, giving some general rules of\napplication. Two R programmes were written to apply each method, and they are\navailable as supplementary material for the manuscript. The results were\napplied to the diagnosis of Alzheimer's disease.",
      "generated_abstract": "This paper introduces a new, efficient, and scalable method for\nassessing the likelihood of a null hypothesis in a simulation study. By\nintroducing a latent variable to describe the null hypothesis, the method\neliminates the need for a null hypothesis and its associated parameters. By\nusing the latent variable as a parameter, the method allows for the\nsimulation of multiple alternative hypotheses and multiple outcomes. Additionally,\nby utilizing a latent variable to represent the null hypothesis, the method\nmakes it possible to perform an exact simulation study without any\nassumptions about the null hypothesis. This allows the method to be used to\nassess the likelihood of any specific null hypothesis. This paper introduces a\nsimple simulation study and provides an implementation of the method in\nR.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1625,
          "p": 0.21311475409836064,
          "f": 0.18439715821135771
        },
        "rouge-2": {
          "r": 0.03418803418803419,
          "p": 0.039603960396039604,
          "f": 0.03669724273335646
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.19672131147540983,
          "f": 0.17021276104823713
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.00564v1",
      "true_abstract": "For a many-to-one market with substitutable preferences on the firm's side,\nbased on the Aizerman-Malishevski decomposition, we define an associated\none-to-one market. Given that the usual notion of stability for a one-to-one\nmarket does not fit well for this associated one-to-one market, we introduce a\nnew notion of stability. This notion allows us to establish an isomorphism\nbetween the set of stable matchings in the many-to-one market and the matchings\nin the associated one-to-one market that meet this new stability criterion.\nFurthermore, we present an adaptation of the well-known deferred acceptance\nalgorithm to compute a matching that satisfies this new notion of stability for\nthe associated one-to-one market.",
      "generated_abstract": "We investigate how well the classical mean-variance portfolio selection\nestimand can perform when the mean and variance are estimated from\nhigh-dimensional data. We consider the mean-variance portfolio selection\nproblem with an estimated mean and variance, and derive the corresponding\nestimator of the optimal portfolio. We show that the estimator is\nasymptotically unbiased and asymptotically efficient. However, we show that the\noptimal portfolio selection problem is no longer convex, and the optimal\nportfolio varies over time. We present an empirical example to show that the\noptimal portfolio selection problem is not convex, and the optimal portfolio\nvaries over time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1896551724137931,
          "p": 0.22,
          "f": 0.20370369873113864
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.05555555555555555,
          "f": 0.049689436049535615
        },
        "rouge-l": {
          "r": 0.1896551724137931,
          "p": 0.22,
          "f": 0.20370369873113864
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00254v1",
      "true_abstract": "Growth curve analysis (GCA) has a wide range of applications in various\nfields where growth trajectories need to be modeled. Heteroscedasticity is\noften present in the error term, which can not be handled with sufficient\nflexibility by standard linear fixed or mixed-effects models. One situation\nthat has been addressed is where the error variance is characterized by a\nlinear predictor with certain covariates. A frequently encountered scenario in\nGCA, however, is one in which the variance is a smooth function of the mean\nwith known shape restrictions. A naive application of standard linear\nmixed-effects models would underestimate the variance of the fixed effects\nestimators and, consequently, the uncertainty of the estimated growth curve. We\npropose to model the variance of the response variable as a shape-restricted\n(increasing/decreasing; convex/concave) function of the marginal or conditional\nmean using shape-restricted splines. A simple iteratively reweighted fitting\nalgorithm that takes advantage of existing software for linear mixed-effects\nmodels is developed. For inference, a parametric bootstrap procedure is\nrecommended. Our simulation study shows that the proposed method gives\nsatisfactory inference with moderate sample sizes. The utility of the method is\ndemonstrated using two real-world applications.",
      "generated_abstract": "We propose a novel approach for estimating the posterior distribution of\nthe regression parameter of a linear regression model. Our approach is based\non the construction of the so-called empirical likelihood. Theoretical\nproperties of the proposed estimator are studied using the theory of\nmultivariate normal distributions. The performance of the proposed estimator is\nevaluated through simulation studies and its practical implementation is\ndemonstrated through an application to the analysis of the data on the\nprevalence of autism in Sweden.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.35185185185185186,
          "f": 0.21714285287575516
        },
        "rouge-2": {
          "r": 0.028735632183908046,
          "p": 0.07042253521126761,
          "f": 0.04081632241432778
        },
        "rouge-l": {
          "r": 0.1322314049586777,
          "p": 0.2962962962962963,
          "f": 0.18285713859004094
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.05151v1",
      "true_abstract": "In a previous note, it is claimed that every surface-link consisting of\ntrivial components and having at most one non-sphere component is a ribbon\nsurface-link, but it was false. In this revised note, this claim is replaced by\nthe claim that a surface-link $L$ with trivial components is a ribbon\nsurface-link if and only if the surface-link obtained from $L$ by every fusion\nis a ribbon surface-link if and only if the surface-link obtained from $L$ by\nany one fusion is a ribbon surface-link. For any closed oriented disconnected\nsurface ${\\mathbf F}$ containing at least two non-sphere components, there is a\npair of a ribbon ${\\mathbf F}$-link $L$ consisting of trivial components and a\nnon-ribbon ${\\mathbf F}$-link $L'$ consisting of trivial components such that\nthe fundamental groups of $L$ and $L'$ are the same group up to\nmeridian-preserving isomorphisms and the pair of the ${\\mathbf F}'$-links $K$\nand $K'$ obtained from $L$ and $L'$ by every corresponding fusion is a pair of\na ribbon surface-link and a non-ribbon surface-link such that the fundamental\ngroups of $K$ and $K'$ are the same group up to meridian-preserving\nisomorphisms.",
      "generated_abstract": "We define the category of $G$-algebras and prove that it is equivalent to\nthe category of $G$-graded algebras over an algebraically closed field. We\nprove that, if $G$ is a finite group, the category of $G$-algebras is equivalent\nto the category of $G$-graded $k[G",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.37037037037037035,
          "f": 0.20618556299287918
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.0625,
          "f": 0.02531645246595138
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.2962962962962963,
          "f": 0.1649484495908174
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-ex/2503.07055v1",
      "true_abstract": "The valence quark parton distribution functions (PDFs) of all ground state\nheavy mesons that composed of $b$ or $c$ quarks, are discussed; namely, the\npseudoscalar $\\eta_c(1S)$, $\\eta_b(1S)$ and $B_c$, together with the\ncorresponding vector ones, $J/\\psi$, $\\Upsilon(1S)$ and $B_c^\\ast$. We use a\nQCD-inspired constituent quark model, which has been applied with success to\nconventional heavy mesons, so that one advantage here is that all parameters\nhave already been fixed by previous studies. The wave functions of the heavy\nmesons in the rest frame are obtained by solving the Schr\\\"odinger equation,\nthen boosted to its light-front based on Susskind's Lorentz transformation. The\nPDFs at the hadron scale, are then obtained by integrating out the transverse\nmomenta of the modulus square of the light-front wave function. Our study shows\nhow the valence quark distributions differ between pseudoscalar and vector\nmesons, as well as among charmonia, bottomonia and bottom-charmed mesons.\nComparisons with other theoretical calculations demonstrate that the PDFs\nobtained herein are in general narrower but align well with the expected\npatterns. Moreover, each PDF's point-wise behavior is squeezed with respect to\nthe scale-free parton-like PDF.",
      "generated_abstract": "The $\\Delta(1232)$ resonance, with a mass of $1232$ MeV, was discovered in\nfine-structure measurements in 1959 and confirmed in subsequent experiments\nusing photoproduction, $e^+e^-$, and $\\mu^+\\mu$ reactions. It is an interesting\nobject of study in the field of hadron-nucleon interactions, since it is the\nheavy-light analogue of the $N(1520)$. In the present work, we consider the\ncoupling of the $\\Delta(1232)$ to the $\\Delta(1520)$ in the framework of\nresonance-exchange model. We have analyzed the dependence of the resonant\ncontributions on the couplings and the momentum transfer to the resonance. The\nresonant contribution of the $\\Delta(1232)$ to the $\\Delta",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.104,
          "p": 0.20634920634920634,
          "f": 0.13829786788422377
        },
        "rouge-2": {
          "r": 0.017045454545454544,
          "p": 0.03409090909090909,
          "f": 0.02272726828282915
        },
        "rouge-l": {
          "r": 0.088,
          "p": 0.1746031746031746,
          "f": 0.11702127213954293
        }
      }
    },
    {
      "paper_id": "physics.med-ph.q-bio/TO/2502.20406v1",
      "true_abstract": "In computational modelling of coronary haemodynamics, imposing\npatient-specific flow conditions is paramount, yet often impractical due to\nresource and time constraints, limiting the ability to perform a large number\nof simulations particularly for diseased cases. We aimed to compare coronary\nhaemodynamics quantified using a simplified flow-split strategy with varying\nexponents against the clinically verified but computationally intensive\nmultiscale simulations under both resting and hyperaemic conditions in arteries\nwith varying degrees of stenosis.\n  Six patient-specific left coronary artery trees were segmented and\nreconstructed, including three with severe (>70%) and three with mild (<50%)\nfocal stenoses. Simulations were performed for the entire coronary tree to\naccount for the flow-limiting effects from epicardial artery stenoses. Both a\n0D-3D coupled multiscale model and a flow-split approach with four different\nexponents (2.0, 2.27, 2.33, and 3.0) were used. The resulting prominent\nhaemodynamic metrics were statistically compared between the two methods.\n  Flow-split and multiscale simulations did not significantly differ under\nresting conditions regardless of the stenosis severity. However, under\nhyperaemic conditions, the flow-split method significantly overestimated the\ntime-averaged wall shear stress by up to 16.8 Pa (p=0.031) and underestimate\nthe fractional flow reserve by 0.327 (p=0.043), with larger discrepancies\nobserved in severe stenoses than in mild ones. Varying the exponent from 2.0 to\n3.0 within the flow-split methods did not significantly affect the haemodynamic\nresults (p>0.141).\n  Flow-split strategies with exponents between 2.0 and 3.0 are appropriate for\nmodelling stenosed coronaries under resting conditions. Multiscale simulations\nare recommended for accurate modelling of hyperaemic conditions, especially in\nseverely stenosed arteries.",
      "generated_abstract": "A key challenge in the clinical application of artificial intelligence (AI)\nin cancer research is the development of robust and scalable computational\nmodels that can effectively integrate heterogeneous patient-related data from\nmultiple sources, enabling the detection of cancer-related patterns and\nclinical prognostic indicators. In this study, we present a deep learning\napproach that leverages large language models (LLMs) to extract patient\ninformation from textual clinical records. Our method employs a text-to-graph\ntransformer architecture to generate a graph representation of the patient\nrecords, which is then used to train a graph neural network (GNN) to predict\nthe presence of cancer. This approach demonstrates significant improvements in\npredictive performance compared to existing state-of-the-art methods,\nespecially when incorporating patient-related clinical data. Additionally,\nextensive experiments were conducted to evaluate the model's robust",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1069182389937107,
          "p": 0.18085106382978725,
          "f": 0.134387347108688
        },
        "rouge-2": {
          "r": 0.004,
          "p": 0.007936507936507936,
          "f": 0.005319144479972044
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.1595744680851064,
          "f": 0.11857707042884612
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.21295v1",
      "true_abstract": "Gene regulatory networks (GRNs) play a crucial role in the control of\ncellular functions. Numerous methods have been developed to infer GRNs from\ngene expression data, including mechanism-based approaches, information-based\napproaches, and more recent deep learning techniques, the last of which often\noverlooks the underlying gene expression mechanisms. In this work, we introduce\nTRENDY, a novel GRN inference method that integrates transformer models to\nenhance the mechanism-based WENDY approach. Through testing on both simulated\nand experimental datasets, TRENDY demonstrates superior performance compared to\nexisting methods. Furthermore, we apply this transformer-based approach to\nthree additional inference methods, showcasing its broad potential to enhance\nGRN inference.",
      "generated_abstract": "We present a novel framework for integrating molecular biology and\nintegrative biology, which we call Molecular Biology Integrative Biology\n(MBIB). MBIB is based on the idea that molecular biology is a central\ncomponent of integrative biology. In this framework, biological knowledge is\norganized as a hierarchical structure, enabling the integration of molecular\nand functional information at different levels. The framework allows for a\nflexible and modular approach to integrative biology, allowing the integration\nof diverse biological knowledge sources, including data from experiments,\nmodeling, and theory, and enabling the integration of different biological\nknowledge domains. In this paper, we introduce a framework for integrating\nmolecular biology and integrative biology, and we apply it to a case study on\nthe development of the hormone progesterone. The framework enables the\ncombination",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2073170731707317,
          "p": 0.2361111111111111,
          "f": 0.22077921580030369
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.046296296296296294,
          "f": 0.04807691808432005
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.20833333333333334,
          "f": 0.19480518982627776
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.15173v1",
      "true_abstract": "Stable and efficient food markets are crucial for global food security, yet\ninternational staple food markets are increasingly exposed to complex risks,\nincluding intensified risk contagion and escalating external uncertainties.\nThis paper systematically investigates risk spillovers in global staple food\nmarkets and explores the key determinants of these spillover effects, combining\ninnovative decomposition-reconstruction techniques, risk connectedness\nanalysis, and random forest models. The findings reveal that short-term\ncomponents exhibit the highest volatility, with futures components generally\nmore volatile than spot components. Further analysis identifies two main risk\ntransmission patterns, namely cross-grain and cross-timescale transmission, and\nclarifies the distinct roles of each component in various net risk spillover\nnetworks. Additionally, price drivers, external uncertainties, and core\nsupply-demand indicators significantly influence these spillover effects, with\nheterogeneous importance of varying factors in explaining different risk\nspillovers. This study provides valuable insights into the risk dynamics of\nstaple food markets, offers evidence-based guidance for policymakers and market\nparticipants to enhance risk warning and mitigation efforts, and supports the\nstabilization of international food markets and the safeguarding of global food\nsecurity.",
      "generated_abstract": "We study the problem of estimating a parametric functional form from data\nunder some restrictions on the model's parameter and functional forms. We\nintroduce a novel method, based on functional regression, for estimating the\nfunctional form of the parameter. Our method can be used for both parametric\nand non-parametric models. We show that our method is asymptotically\nconsistent under the null hypothesis of parametricity, and is asymptotically\nnormal under the alternative. We illustrate our method by applying it to the\nestimation of the parameters of a multinomial logistic regression model with\ncontinuous covariates.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07563025210084033,
          "p": 0.15254237288135594,
          "f": 0.10112359107372827
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07563025210084033,
          "p": 0.15254237288135594,
          "f": 0.10112359107372827
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/other/2503.06672v1",
      "true_abstract": "Ferroelastic materials (materials with switchable spontaneous strain) often\nare centrosymmetric, but their domain walls are always polar, as their internal\nstrain gradients cause polarization via flexoelectricity. This polarization is\ngenerally not switchable by an external electric field, because reversing the\ndomain wall polarity would require reversing the strain gradient, which in turn\nwould require switching the spontaneous strain of the adjacent domains,\ndestroying the domain wall in the process. However, domain wall polarization\ncan also arise from biquadratic coupling between polar and non-polar order\nparameters (e.g. octahedral tilts in perovskites). Such coupling is independent\nof the sign of the polarization and thus allows switching between +P and -P. In\nthis work, we seek to answer the question of whether the polarization of domain\nwalls in ferroelastic perovskites is switchable, as per the symmetric\nbiquadratic term, or non-switchable due to the unipolar flexoelectric bias.\nUsing perovskite calcium titanate (CaTiO3) as a paradigm, molecular dynamics\ncalculations indicate that high electric fields broaden the ferroelastic domain\nwalls, thereby reducing flexoelectricity (as the domain wall strain gradient is\ninversely proportional to the wall width), eventually enabling switching. The\npolarization switching, however, is not ferroelectric-like with a simple\nhysteresis loop, but antiferroelectric-like with a double hysteresis loop.\nFerroelastic domain walls thus behave as functional antiferroelectric elements,\nand also as nucleation points for a bulk phase transition to a polar state.",
      "generated_abstract": "The electronic and magnetic properties of graphene-based transition metal dichalcogenide\nlayers (TMDCs) have been studied extensively in the past two decades, but the\ninfluence of interfacial layers on the electronic and magnetic properties of\nthe bulk TMDCs is still understudied. Here, we perform a comprehensive study of\nthe electronic and magnetic properties of bulk (111) and interface (111)\nTMDCs, where the 111 interface is formed by a single-layer graphene. By\nperforming first-principles calculations, we demonstrate that the presence of\nan interfacial layer (e.g., a 1T-TiTe2 layer) can significantly change the\nelectronic and magnetic properties of the bulk TMDC. The interfacial layer\ninfluences the band gap, band alignment, and magnetism of the bulk T",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11971830985915492,
          "p": 0.2463768115942029,
          "f": 0.16113743635677558
        },
        "rouge-2": {
          "r": 0.014218009478672985,
          "p": 0.031914893617021274,
          "f": 0.01967212688331186
        },
        "rouge-l": {
          "r": 0.1056338028169014,
          "p": 0.21739130434782608,
          "f": 0.14218009038521162
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.11449v3",
      "true_abstract": "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
      "generated_abstract": "This paper presents a novel framework for identifying and characterizing\nthe heterogeneous effects of policy interventions in a dynamic social network.\nWe introduce a novel graph-based representation for quantifying the\ninterdependence of individuals' responses to policy interventions. Our\napproach enables us to analyze the heterogeneity of the effects of policy\ninterventions across individuals and social networks. Additionally, we\nintroduce a novel approach for characterizing the effect of policy interventions\non the network. We demonstrate the effectiveness of the proposed framework\nthrough case studies involving taxes and vaccination policy interventions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11188811188811189,
          "p": 0.3137254901960784,
          "f": 0.16494844973270284
        },
        "rouge-2": {
          "r": 0.00904977375565611,
          "p": 0.02666666666666667,
          "f": 0.013513509729959058
        },
        "rouge-l": {
          "r": 0.11188811188811189,
          "p": 0.3137254901960784,
          "f": 0.16494844973270284
        }
      }
    },
    {
      "paper_id": "stat.ML.q-fin/ST/2411.16666v2",
      "true_abstract": "We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM with the Gaussian Mirror\n(GM) method. To evaluate the feature importance of LSTM in time series, we\nintroduce a vector of the derivative of the SHapley Additive exPlanations\n(SHAP) to measure feature importance. We also propose a new kernel-based\ndependence measure to avoid multicollinearity in the GM algorithm, to make a\nrobust feature selection with controlled FDR. We use simulated data to evaluate\nCatNet's performance in both linear models and LSTM models with different link\nfunctions. The algorithm effectively controls the FDR while maintaining a high\nstatistical power in all cases. We also evaluate the algorithm's performance in\ndifferent low-dimensional and high-dimensional cases, demonstrating its\nrobustness in various input dimensions. To evaluate CatNet's performance in\nreal world applications, we construct a multi-factor investment portfolio to\nforecast the prices of S\\&P 500 index components. The results demonstrate that\nour model achieves superior predictive accuracy compared to traditional LSTM\nmodels without feature selection and FDR control. Additionally, CatNet\neffectively captures common market-driving features, which helps informed\ndecision-making in financial markets by enhancing the interpretability of\npredictions. Our study integrates of the Gaussian Mirror algorithm with LSTM\nmodels for the first time, and introduces SHAP values as a new feature\nimportance metric for FDR control methods, marking a significant advancement in\nfeature selection and error control for neural networks.",
      "generated_abstract": "This paper studies the problem of optimal risk allocation in a\nrisk-sensitive market that can observe past data. We consider the case where\nthe past data are noisy and are observed over multiple time steps. We develop a\nrecently proposed algorithm, called the Bayesian Optimal Directional\nAllocation (BADA) algorithm, and show that it is optimal in the sense that\nits expected utility is the optimal expected utility. We show that this\noptimality follows from the fact that the Bayesian Optimal Directional\nAllocation algorithm is a policy iteration algorithm and can be seen as a\npolicy iteration algorithm with a time-varying discount factor. We further show\nthat the BADA algorithm is the solution to a constrained optimization problem\nthat is an extension of the well-known problem of risk allocation in\noptimal stopping. We show that this problem admits a polynomial-time\nsolution for any number of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11971830985915492,
          "p": 0.22077922077922077,
          "f": 0.1552511369929736
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.032520325203252036,
          "f": 0.02388059236783338
        },
        "rouge-l": {
          "r": 0.09859154929577464,
          "p": 0.18181818181818182,
          "f": 0.12785387671900103
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2411.19902v1",
      "true_abstract": "We propose a pair of completely data-driven algorithms for unsupervised\nclassification and dimension reduction, and we empirically study their\nperformance on a number of data sets, both simulated data in three-dimensions\nand images from the COIL-20 data set. The algorithms take as input a set of\npoints sampled from a uniform distribution supported on a metric space, the\nlatter embedded in an ambient metric space, and they output a clustering or\nreduction of dimension of the data. They work by constructing a natural family\nof graphs from the data and selecting the graph which maximizes the relative\nvon Neumann entropy of certain normalized heat operators constructed from the\ngraphs. Once the appropriate graph is selected, the eigenvectors of the graph\nLaplacian may be used to reduce the dimension of the data, and clusters in the\ndata may be identified with the kernel of the associated graph Laplacian.\nNotably, these algorithms do not require information about the size of a\nneighborhood or the desired number of clusters as input, in contrast to popular\nalgorithms such as $k$-means, and even more modern spectral methods such as\nLaplacian eigenmaps, among others.\n  In our computational experiments, our clustering algorithm outperforms\n$k$-means clustering on data sets with non-trivial geometry and topology, in\nparticular data whose clusters are not concentrated around a specific point,\nand our dimension reduction algorithm is shown to work well in several simple\nexamples.",
      "generated_abstract": "This paper introduces the concept of a \"co-clustering\" of data points in\nmeasures of association, which extends the notion of co-clustering in\nstatistical learning theory to the setting of clustering in high-dimensional\ndata. We derive the statistical properties of the co-clustering measure\nassociated with a given statistic, and discuss its use in hypothesis testing.\nFurther, we introduce the notion of a \"co-clustering graph\" that captures the\nconnectivity of the co-clustering graph associated with a given statistic. We\nuse this graph to analyze the connectivity of the co-clustering graph for the\nstatistics of two different types: those that are based on empirical risk\nminimization, and those that are based on the Wasserstein metric. We use the\nconnectivity of the co-clustering graph to analyze the connectivity of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.296875,
          "f": 0.19095476950582066
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.03125,
          "f": 0.019230764970415145
        },
        "rouge-l": {
          "r": 0.11851851851851852,
          "p": 0.25,
          "f": 0.16080401573697647
        }
      }
    },
    {
      "paper_id": "math.CA.math/SP/2503.06957v1",
      "true_abstract": "Volterra integral and integro-differential equations have been extensively\nstudied in both pure mathematics and applied science. In one direction,\ndevelopments in analysis have yielded far-ranging existence, uniqueness, and\nregularity results. In the other, applications in science have inspired a\nsubstantial library of practical techniques to deal with such equations.\n  The present work connects these research areas by examining five large\nclasses of linear Volterra equations: integral and integro-differential\nequations with completely monotone (CM) kernels, corresponding to linear\nviscoelastic models; those with positive definite (PD) kernels, corresponding\nto partially-observed quantum systems; difference equations with PD kernels; a\nclass of generalized delay differential equations; and a class of generalized\nfractional differential equations. We develop a system of correspondences\nbetween these problems, showing that all five can be understood within the\nsame, spectral theory. We leverage this theory to recover practical,\nclosed-form solutions of all five classes, and we show that interconversion\nyields a natural, continuous involution within each class. Our work unifies\nseveral results from science: the interconversion formula of Gross, recent\nresults in viscoelasticity and operator theory for integral equations of the\nsecond type, classical formulas for Prony series and fractional differential\nequations, and the convergence of Prony series to CM kernels. Finally, our\ntheory yields a novel, geometric construction of the regularized Hilbert\ntransform, extends it to a wide class of infinite measures, and reveals a\nnatural connection to delay and fractional differential equations.\n  We leverage our theory to develop a powerful, spectral method to handle\nscalar Volterra equations numerically, and illustrate it with a number of\npractical examples.",
      "generated_abstract": "This article presents a non-perturbative approach to the construction of\nvarious types of non-abelian groups of finite index from abelian groups using\ntheir automorphic representations. We prove that for any finite group $G$, the\nconjugacy class of any automorphic representation $\\pi$ of $G$ is of finite\nindex if and only if $\\pi$ is unramified in the automorphic representations of\nall other finite groups that are isomorphic to $G$. In particular, this\nresult implies that for any finite group $G$, there exists a non-abelian\n$G$-module of finite index if and only if $G$ is a direct product of finite\ngroups that are isomorphic to $G$. We also show that for any non-abelian\n$G$-module $\\mathcal{M}$, the conjugacy class of any automorphic representation\nof $G$ is of finite index if and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10457516339869281,
          "p": 0.2807017543859649,
          "f": 0.15238094842585043
        },
        "rouge-2": {
          "r": 0.017094017094017096,
          "p": 0.0449438202247191,
          "f": 0.024767797865215494
        },
        "rouge-l": {
          "r": 0.0915032679738562,
          "p": 0.24561403508771928,
          "f": 0.1333333293782314
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.06280v1",
      "true_abstract": "Hopf braces are the quantum analogues of skew braces and, as such, their\ncocommutative counterparts provide solutions to the quantum Yang-Baxter\nequation. We investigate various properties of categories related to Hopf\nbraces. In particular, we prove that the category of Hopf braces is accessible\nwhile the category of cocommutative Hopf braces is even locally presentable. We\nalso show that functors forgetting multiple antipodes and/or multiplications\ndown to coalgebras are monadic. Colimits in the category of cocommutative Hopf\nbraces are described explicitly and a free cocommutative Hopf brace on an\narbitrary cocommutative Hopf algebra is constructed.",
      "generated_abstract": "In this paper we provide a simple and effective method for finding all\nsubspaces of a given Hilbert space which are invariant under the action of a\nHadamard matrix. This method is based on a natural concept of a \"block-Hadamard\nsubspace\". The new method is illustrated by several examples and applied to\nfind the subspaces of the Hilbert space of two-dimensional functions. The\nsubspaces are computed in several different ways and the corresponding\nHadamard matrices are computed using a new algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19672131147540983,
          "p": 0.22641509433962265,
          "f": 0.21052631081409676
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.16981132075471697,
          "f": 0.15789473186672837
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.02242v1",
      "true_abstract": "Approaches for improving generative adversarial networks (GANs) training\nunder a few samples have been explored for natural images. However, these\nmethods have limited effectiveness for synthetic aperture radar (SAR) images,\nas they do not account for the unique electromagnetic scattering properties of\nSAR. To remedy this, we propose a physics-inspired regularization method dubbed\n$\\Phi$-GAN, which incorporates the ideal point scattering center (PSC) model of\nSAR with two physical consistency losses. The PSC model approximates SAR\ntargets using physical parameters, ensuring that $\\Phi$-GAN generates SAR\nimages consistent with real physical properties while preventing discriminator\noverfitting by focusing on PSC-based decision cues. To embed the PSC model into\nGANs for end-to-end training, we introduce a physics-inspired neural module\ncapable of estimating the physical parameters of SAR targets efficiently. This\nmodule retains the interpretability of the physical model and can be trained\nwith limited data. We propose two physical loss functions: one for the\ngenerator, guiding it to produce SAR images with physical parameters consistent\nwith real ones, and one for the discriminator, enhancing its robustness by\nbasing decisions on PSC attributes. We evaluate $\\Phi$-GAN across several\nconditional GAN (cGAN) models, demonstrating state-of-the-art performance in\ndata-scarce scenarios on three SAR image datasets.",
      "generated_abstract": "Large Language Models (LLMs) have been widely adopted for generating\ntransparent, high-quality, and high-resolution (HR) images for clinical\ndiagnosis. However, existing LLM-based image generation methods are still\nrestricted by the low image quality and HR images produced by LLMs, which may\nnot meet the demand of clinical diagnosis. To address this issue, we propose\nan image generation method for clinical diagnosis based on the LLM\nTransformer-based Transparent Image Generation (TIG) model. Specifically,\ninstead of directly generating HR images from the LLM, our model first generates\nthe LLM-generated image pairs and then generates the HR image pairs through\ntransformer-based image generation model. To further enhance the generated HR\nimages, we propose a fine-tuning strategy based on the Image Transformer\nGenerator (ITG) model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16176470588235295,
          "p": 0.275,
          "f": 0.20370369903978067
        },
        "rouge-2": {
          "r": 0.016304347826086956,
          "p": 0.027522935779816515,
          "f": 0.020477811027269872
        },
        "rouge-l": {
          "r": 0.16176470588235295,
          "p": 0.275,
          "f": 0.20370369903978067
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20877v1",
      "true_abstract": "Quantitative magnetic resonance imaging (qMRI) requires multi-phase\nacqui-sition, often relying on reduced data sampling and reconstruction\nalgorithms to accelerate scans, which inherently poses an ill-posed inverse\nproblem. While many studies focus on measuring uncertainty during this process,\nfew explore how to leverage it to enhance reconstruction performance. In this\npaper, we in-troduce PUQ, a novel approach that pioneers the use of uncertainty\ninfor-mation for qMRI reconstruction. PUQ employs a two-stage reconstruction\nand parameter fitting framework, where phase-wise uncertainty is estimated\nduring reconstruction and utilized in the fitting stage. This design allows\nuncertainty to reflect the reliability of different phases and guide\ninformation integration during parameter fitting. We evaluated PUQ on in vivo\nT1 and T2 mapping datasets from healthy subjects. Compared to existing qMRI\nreconstruction methods, PUQ achieved the state-of-the-art performance in\nparameter map-pings, demonstrating the effectiveness of uncertainty guidance.\nOur code is available at https://anonymous.4open.science/r/PUQ-75B2/.",
      "generated_abstract": "We present a novel method for the simultaneous segmentation and alignment\nof microscopy images. Our method is based on a 3D-LSTM architecture, which\nenables the model to capture the global structure of the images and to align\nthem with the structure of the brain tissue. In our experiments, we compare\nour approach with existing methods, such as the Deep Learning-based\nSegmentation-and-Alignment method (DLSA), the Bi-directional Convolutional\nNetwork (BCN), and the Bi-directional Convolutional Network (BCN-2). Our\nmethod, however, outperforms all the competing methods, demonstrating that the\n3D-LSTM architecture is essential for the successful alignment of the images.\nOur results show that the 3D-LSTM architecture achieves the best performance in\nterms of segmentation quality and the number of detected structures. Moreover",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.2876712328767123,
          "f": 0.23204419408198781
        },
        "rouge-2": {
          "r": 0.013986013986013986,
          "p": 0.019230769230769232,
          "f": 0.016194327108460877
        },
        "rouge-l": {
          "r": 0.18518518518518517,
          "p": 0.273972602739726,
          "f": 0.22099447032508177
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2408.01185v1",
      "true_abstract": "We introduce a new class of anticipative backward stochastic differential\nequations with a dependence of McKean type on the law of the solution, that we\nname MKABSDE. We provide existence and uniqueness results in a general\nframework with relatively general regularity assumptions on the coefficients.\nWe show how such stochastic equations arise within the modern paradigm of\nderivative pricing where a central counterparty (CCP) requires the members to\ndeposit variation and initial margins to cover their exposure. In the case when\nthe initial margin is proportional to the Conditional Value-at-Risk (CVaR) of\nthe contract price, we apply our general result to define the price as a\nsolution of a MKABSDE. We provide several linear and non-linear simpler\napproximations, which we solve using different numerical (deterministic and\nMonte-Carlo) methods.",
      "generated_abstract": "We study the problem of pricing American options in an infinite-horizon\nfinancial market with infinite information, where the risk-neutral probability\nmeasure is the Lebesgue measure. We consider a general class of risk-neutral\nprobability measures on the market, and derive a complete and explicit\npricing formula for the American option in the infinite-horizon model.\nMoreover, we show that, under suitable regularity conditions, the price of the\nAmerican option can be analytically obtained from the value function of the\ninfinite-horizon model. We also provide an efficient numerical method for\ncomputing the value function and the American option price. In addition, we\npropose a novel risk-neutral measure to model the risks of the European option\nand demonstrate the effectiveness of the proposed measure by comparing it to\nthe traditional risk-neutral measure. This paper provides a rigorous and\nquantitative theoretical framework for pr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24719101123595505,
          "p": 0.2716049382716049,
          "f": 0.2588235244228374
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.048,
          "f": 0.048387091774519254
        },
        "rouge-l": {
          "r": 0.23595505617977527,
          "p": 0.25925925925925924,
          "f": 0.24705881854048453
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2502.14893v1",
      "true_abstract": "Symbolic music is represented in two distinct forms: two-dimensional,\nvisually intuitive score images, and one-dimensional, standardized text\nannotation sequences. While large language models have shown extraordinary\npotential in music, current research has primarily focused on unimodal symbol\nsequence text. Existing general-domain visual language models still lack the\nability of music notation understanding. Recognizing this gap, we propose NOTA,\nthe first large-scale comprehensive multimodal music notation dataset. It\nconsists of 1,019,237 records, from 3 regions of the world, and contains 3\ntasks. Based on the dataset, we trained NotaGPT, a music notation visual large\nlanguage model. Specifically, we involve a pre-alignment training phase for\ncross-modal alignment between the musical notes depicted in music score images\nand their textual representation in ABC notation. Subsequent training phases\nfocus on foundational music information extraction, followed by training on\nmusic notation analysis. Experimental results demonstrate that our NotaGPT-7B\nachieves significant improvement on music understanding, showcasing the\neffectiveness of NOTA and the training pipeline. Our datasets are open-sourced\nat https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.",
      "generated_abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized the\ndomain of computer vision (CV). Despite their impressive performance in\nimage-based tasks, LLMs struggle to generalize to unseen data, particularly\nfor complex tasks such as object recognition and scene understanding. In this\nstudy, we examine how the difficulty of a task affects the LLM's ability to\ngeneralize. Specifically, we investigate the impact of task difficulty on\nLLMs' performance in scene understanding tasks, focusing on the CLEVR and\nNorViz benchmarks. We employ a multi-task learning approach, combining\nreinforcement learning with attention-based neural networks, to enhance the\nLLM's ability to generalize to unseen tasks. Our findings reveal that\ndifficulty plays a crucial role in the LLM's ability to generalize,\nsignific",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14049586776859505,
          "p": 0.2125,
          "f": 0.16915422406376093
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.01904761904761905,
          "f": 0.015209120678340075
        },
        "rouge-l": {
          "r": 0.12396694214876033,
          "p": 0.1875,
          "f": 0.14925372655132313
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2502.03290v1",
      "true_abstract": "Flagellar motors enable bacteria to navigate their environments by switching\nrotation direction in response to external cues with high sensitivity. Previous\nwork suggested that ultrasensitivity of the flagellar motor originates from\nconformational spread, in which subunits of the switching complex are strongly\ncoupled to their neighbors as in an equilibrium Ising model. However, dynamic\nsingle-motor measurements indicated that rotation switching is driven out of\nequilibrium, and the mechanism for this dissipative driving remains unknown.\nHere, based on recent cryo-EM structures, we propose that local mechanical\ntorques on motor subunits can affect their conformation dynamics. This gives\nrise to a tug of war between stator-associated subunits, which produces\ncooperative, non-equilibrium switching responses without requiring\nnearest-neighbor interactions. Since subunits are effectively coupled at a\ndistance, we call this mechanism ``Global Mechanical Coupling.\" Our model makes\na qualitatively new prediction that the motor response cooperativity grows with\nthe number of stators driving rotation. Re-analyzing published motor\ndose-response curves in varying load conditions, we find tentative experimental\nevidence for this prediction. Finally, we show that operating out of\nequilibrium enables motors to achieve high cooperativity with faster responses\ncompared to equilibrium motors. Our results suggest a general role for\nmechanics in sensitive chemical regulation.",
      "generated_abstract": "The ability to reproduce is a critical feature of biological systems.\nA critical step in the process of reproduction is the production of a new\norganism. A new organism can be generated from the genetic material of a\nparent, which can be inherited from a parent or can be made through\nindividual mutations. Individual mutations are those that alter a gene's\nsequence and can have either a neutral or deleterious effect on the organism.\nIn this study, we consider a population of individuals that can be either\nreproducing or non-reproducing. The reproducing individuals produce a new\norganism with the same genetic material as the parent. The non-reproducing\nindividuals do not produce a new organism. The reproduction rate of a\npopulation is the number of reproducing individuals per unit time. The\npopulation size is the total number of individuals in the population. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1386861313868613,
          "p": 0.2676056338028169,
          "f": 0.18269230319572866
        },
        "rouge-2": {
          "r": 0.015228426395939087,
          "p": 0.02459016393442623,
          "f": 0.018808772705851164
        },
        "rouge-l": {
          "r": 0.13138686131386862,
          "p": 0.2535211267605634,
          "f": 0.17307691858034405
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.05664v1",
      "true_abstract": "In financial applications, we often observe both global and local factors\nthat are modeled by a multi-level factor model. When detecting unknown local\ngroup memberships under such a model, employing a covariance matrix as an\nadjacency matrix for local group memberships is inadequate due to the\npredominant effect of global factors. Thus, to detect a local group structure\nmore effectively, this study introduces an inverse covariance matrix-based\nfinancial adjacency matrix (IFAM) that utilizes negative values of the inverse\ncovariance matrix. We show that IFAM ensures that the edge density between\ndifferent groups vanishes, while that within the same group remains\nnon-vanishing. This reduces falsely detected connections and helps identify\nlocal group membership accurately. To estimate IFAM under the multi-level\nfactor model, we introduce a factor-adjusted GLASSO estimator to address the\nprevalent global factor effect in the inverse covariance matrix. An empirical\nstudy using returns from international stocks across 20 financial markets\ndemonstrates that incorporating IFAM effectively detects latent local groups,\nwhich helps improve the minimum variance portfolio allocation performance.",
      "generated_abstract": "This study investigates the effects of financial intermediation on credit\nrisk. We model credit risk as a function of the riskiness of loan origination,\nand loan-related riskiness, and the creditworthiness of borrowers. The risk\nintermediation model is estimated using a novel data set on the provision of\nloans by non-bank financial intermediaries in 2010. We apply an\ninverse-probability weighted (IPW) estimation method to the data. Our results\nshow that the provision of loans by financial intermediaries reduces the\nprobability of default of the borrowers. This reduction in default probability\nis a result of the reduction in riskiness of loan origination by financial\nintermediaries. The reduction in default probability also reduces the risk of\ndefault of the borrowers by reducing the riskiness of loan origination.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.3275862068965517,
          "f": 0.22352940726920423
        },
        "rouge-2": {
          "r": 0.025477707006369428,
          "p": 0.04081632653061224,
          "f": 0.03137254428727484
        },
        "rouge-l": {
          "r": 0.16964285714285715,
          "p": 0.3275862068965517,
          "f": 0.22352940726920423
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/ET/2503.08062v1",
      "true_abstract": "Orthogonal frequency division multiplexing (OFDM), which has been the\ndominating waveform for contemporary wireless communications, is also regarded\nas a competitive candidate for future integrated sensing and communication\n(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum\nsensing range should be limited by the cyclic prefix (CP) length since\ninter-symbol interference (ISI) and inter-carrier interference (ICI) should be\navoided. However, in this paper, we provide rigorous analysis to reveal that\nthe random data embedded in OFDM-ISAC signal can actually act as a free ``mask\"\nfor ISI, which makes ISI/ICI random and hence greatly attenuated after radar\nsignal processing. The derived signal-to-interference-plus-noise ratio (SINR)\nin the range profile demonstrates that the maximum sensing range of OFDM-ISAC\ncan greatly exceed the ISI-free distance that is limited by the CP length,\nwhich is validated by simulation results. To further mitigate power degradation\nfor long-range targets, a novel sliding window sensing method is proposed,\nwhich iteratively detects and cancels short-range targets before shifting the\ndetection window. The shifted detection window can effectively compensate the\npower degradation due to insufficient CP length for long-range targets. Such\nresults provide valuable guidance for the CP length design in OFDM-ISAC\nsystems.",
      "generated_abstract": "The increasing deployment of 6G base stations (BSs) in urban areas is\nchallenged by the limited spatial coverage of radio waves and the difficulty in\novercoming the high cost of radio frequency (RF) components. This paper\nproposes a low-cost, low-power, and energy-efficient 6G BS based on\nsingle-chip, low-power RF front-end. It introduces a novel 6G BS with a\nsingle-chip RF front-end, featuring low power consumption and high efficiency,\nwhich enhances the BS's coverage. A 6G BS with a single-chip RF front-end is\ndesigned to reduce the number of RF components and the cost of the BS, while\nstill achieving the required performance. The design methodology includes\nanalytical calculations, finite element simulations, and the optimization of\nthe design parameters",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.2077922077922078,
          "f": 0.15763546327161557
        },
        "rouge-2": {
          "r": 0.0111731843575419,
          "p": 0.01904761904761905,
          "f": 0.014084502381721438
        },
        "rouge-l": {
          "r": 0.10317460317460317,
          "p": 0.16883116883116883,
          "f": 0.12807881302531016
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.06862v1",
      "true_abstract": "Weight-only quantization has emerged as a promising solution to the\ndeployment challenges of large language models (LLMs). However, it necessitates\nFP-INT operations, which make implementation on general-purpose hardware like\nGPUs difficult. In this paper, we propose FIGLUT, an efficient look-up table\n(LUT)-based GEMM accelerator architecture. Instead of performing traditional\narithmetic operations, FIGLUT retrieves precomputed values from an LUT based on\nweight patterns, significantly reducing the computational complexity. We also\nintroduce a novel LUT design that addresses the limitations of conventional\nmemory architectures. To further improve LUT-based operations, we propose a\nhalf-size LUT combined with a dedicated decoding and multiplexing unit. FIGLUT\nefficiently supports different bit precisions and quantization methods using a\nsingle fixed hardware configuration. For the same 3-bit weight precision,\nFIGLUT demonstrates 59% higher TOPS/W and 20% lower perplexity than\nstate-of-the-art accelerator designs. When targeting the same perplexity,\nFIGLUT achieves 98% higher TOPS/W by performing 2.4-bit operations.",
      "generated_abstract": "This paper addresses the challenges of dynamic routing and network\nmanagement for wireless ad-hoc networks. We propose a novel routing algorithm\nthat dynamically assigns the network to each user based on the wireless\nconditions. Additionally, we introduce a novel mechanism that allows the\nnetwork to manage the dynamic network by adjusting the routing rules for the\nusers based on the current network conditions. Additionally, we propose a\nmechanism that allows the network to manage the dynamic network by adjusting the\nrouting rules for the users based on the current network conditions. This\nmechanism allows the network to balance the load between the users, while\nmanaging the network efficiently.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1440677966101695,
          "p": 0.3617021276595745,
          "f": 0.2060606019864096
        },
        "rouge-2": {
          "r": 0.04827586206896552,
          "p": 0.09722222222222222,
          "f": 0.06451612459810177
        },
        "rouge-l": {
          "r": 0.1440677966101695,
          "p": 0.3617021276595745,
          "f": 0.2060606019864096
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2503.05185v1",
      "true_abstract": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
      "generated_abstract": "This paper investigates the optimal trading strategies for stocks and bonds\nin the presence of information asymmetry. The asymmetry is represented by\nthe skewness parameter $\\beta$ and the kurtosis parameter $\\gamma$. The\ninformation asymmetry is modeled by the Sharpe ratio, which is the ratio of\nexpected return and variance. We derive the optimal trading strategies under\ndifferent asymmetry regimes. Furthermore, we analyze the impact of the\ninformation asymmetry on the optimal strategies. The results demonstrate that\nthe asymmetry in the market impacts the optimal trading strategies. For\nexample, when the asymmetry is small, the optimal strategies are more likely\nto be cautious. In contrast, when the asymmetry is large, the optimal\nstrategies are more likely to be aggressive. The findings have important\nimplications for investors and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10596026490066225,
          "p": 0.23880597014925373,
          "f": 0.14678898656804995
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10596026490066225,
          "p": 0.23880597014925373,
          "f": 0.14678898656804995
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.05300v1",
      "true_abstract": "This article introduces a subbagging (subsample aggregating) approach for\nvariable selection in regression within the context of big data. The proposed\nsubbagging approach not only ensures that variable selection is scalable given\nthe constraints of available computational resources, but also preserves the\nstatistical efficiency of the resulting estimator. In particular, we propose a\nsubbagging loss function that aggregates the least-squares approximations of\nthe loss function for each subsample. Subsequently, we penalize the subbagging\nloss function via an adaptive LASSO-type regularizer, and obtain a regularized\nestimator to achieve variable selection. We then demonstrate that the\nregularized estimator exhibits $\\sqrt{N}$-consistency and possesses the oracle\nproperties, where $N$ represents the size of the full sample in the big data.\nIn addition, we propose a subbagging Bayesian information criterion to select\nthe regularization parameter, ensuring that the regularized estimator achieves\nselection consistency. Simulation experiments are conducted to demonstrate the\nnumerical performance. A U.S. census dataset is analyzed to illustrate the\nusefulness and computational scalability of the subbagging variable selection\nmethod.",
      "generated_abstract": "This paper addresses the problem of assessing the predictive accuracy of\nthe Bayesian Gaussian process (BGP) model in a finite sample context. We\nintroduce the Bayesian Information Criterion (BIC) and compare it to the\nBayesian Information Criterion (BIC) for the Gaussian process in the\nclassification problem. The BIC is a generalization of the well-known\nBayesian Information Criterion (BIC) for regression, which has been applied to\nBayesian models in statistical learning. The BIC is a more robust criterion\nthan the BIC for regression since it does not depend on the number of\nindependent samples. This criterion is also more efficient in finite samples\nthan the BIC for regression. In the classification problem, the BIC is a more\nrobust criterion than the BIC for regression since it does not depend on the\nnumber of independent samples. This criter",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.32786885245901637,
          "f": 0.24539876832248111
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.021505376344086023,
          "f": 0.016666661919793015
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2786885245901639,
          "f": 0.20858895237156097
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.09541v1",
      "true_abstract": "The paper studies the problem of detecting and locating change points in\nmultivariate time-evolving data. The problem has a long history in statistics\nand signal processing and various algorithms have been developed primarily for\nsimple parametric models. In this work, we focus on modeling the data through\nfeed-forward neural networks and develop a detection strategy based on the\nfollowing two-step procedure. In the first step, the neural network is trained\nover a prespecified window of the data, and its test error function is\ncalibrated over another prespecified window. Then, the test error function is\nused over a moving window to identify the change point. Once a change point is\ndetected, the procedure involving these two steps is repeated until all change\npoints are identified. The proposed strategy yields consistent estimates for\nboth the number and the locations of the change points under temporal\ndependence of the data-generating process. The effectiveness of the proposed\nstrategy is illustrated on synthetic data sets that provide insights on how to\nselect in practice tuning parameters of the algorithm and in real data sets.\nFinally, we note that although the detection strategy is general and can work\nwith different neural network architectures, the theoretical guarantees\nprovided are specific to feed-forward neural architectures.",
      "generated_abstract": "This paper presents a method for generating synthetic data that can be used\nfor exploratory data analysis (EDA) and out-of-distribution (OOD) evaluation.\nOur method, called EDAPT, combines synthetic data generation and\nout-of-distribution evaluation in a single framework. The synthetic data are\ngenerated by the target model through a pre-trained encoder, and are then\nevaluated using a different model on a different dataset. The two models are\ntrained on the synthetic data and the synthetic data is used for evaluating the\nother model. The synthetic data generation is performed by sampling from a\npre-trained model's output distribution. We show that EDAPT is able to generate\nsynthetic data that are useful for OOD evaluation. We evaluate EDAPT on\nsynthetic data generated from several models including a multilayer perceptron\nand a G",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1721311475409836,
          "p": 0.29577464788732394,
          "f": 0.2176165756600178
        },
        "rouge-2": {
          "r": 0.026737967914438502,
          "p": 0.04424778761061947,
          "f": 0.03333332863755622
        },
        "rouge-l": {
          "r": 0.1557377049180328,
          "p": 0.2676056338028169,
          "f": 0.19689118705898156
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.02030v1",
      "true_abstract": "We study policy evaluation problems in multi-task reinforcement learning (RL)\nunder a low-rank representation setting. In this setting, we are given $N$\nlearning tasks where the corresponding value function of these tasks lie in an\n$r$-dimensional subspace, with $r<N$. One can apply the classic\ntemporal-difference (TD) learning method for solving these problems where this\nmethod learns the value function of each task independently. In this paper, we\nare interested in understanding whether one can exploit the low-rank structure\nof the multi-task setting to accelerate the performance of TD learning. To\nanswer this question, we propose a new variant of TD learning method, where we\nintegrate the so-called truncated singular value decomposition step into the\nupdate of TD learning. This additional step will enable TD learning to exploit\nthe dominant directions due to the low rank structure to update the iterates,\ntherefore, improving its performance. Our empirical results show that the\nproposed method significantly outperforms the classic TD learning, where the\nperformance gap increases as the rank $r$ decreases.\n  From the theoretical point of view, introducing the truncated singular value\ndecomposition step into TD learning might cause an instability on the updates.\nWe provide a theoretical result showing that the instability does not happen.\nSpecifically, we prove that the proposed method converges at a rate\n$\\mathcal{O}(\\frac{\\ln(t)}{t})$, where $t$ is the number of iterations. This\nrate matches that of the standard TD learning.",
      "generated_abstract": "The growing demand for more reliable and energy-efficient wireless networks\nmotivates the development of energy-aware communication systems. In this\ncontext, we introduce the concept of energy-aware communication (EAC) in\nrecent years. EAC is a key concept in wireless communications, and it aims to\nbalance the energy consumption and transmission rate in wireless networks. This\npaper introduces an energy-aware communication system with multiple agents\nworking in parallel. Each agent has a local objective function to optimize its\nenergy consumption. The objective function is a weighted sum of the\nenergy-aware transmission rate and a cost function to minimize the overall\nenergy consumption. The energy-aware transmission rate is formulated as a\nweighted sum of two parts: the first part is the energy-aware transmission\nrate that is optimized based on the objective function of each agent, while the\nsecond part is the conventional transmission rate that is optimized based on\nthe objective",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2564102564102564,
          "f": 0.19230768762019243
        },
        "rouge-2": {
          "r": 0.02926829268292683,
          "p": 0.05172413793103448,
          "f": 0.0373831729544556
        },
        "rouge-l": {
          "r": 0.14615384615384616,
          "p": 0.24358974358974358,
          "f": 0.18269230300480782
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2503.02923v1",
      "true_abstract": "Diverse organisms exploit the geomagnetic field (GMF) for migration.\nMigrating birds employ an intrinsically quantum mechanical mechanism for\ndetecting the geomagnetic field: absorption of a blue photon generates a\nradical pair whose two electrons precess at different rates in the magnetic\nfield, thereby sensitizing cells to the direction of the GMF. In this work,\nusing an in vitro injury model, we discovered a quantum-based mechanism of\ncellular migration. Specifically, we show that migrating cells detect the GMF\nvia an optically activated, electron spin-based mechanism. Cell injury provokes\nacute emission of blue photons, and these photons sensitize muscle progenitor\ncells to the magnetic field. We show that the magnetosensitivity of muscle\nprogenitor cells is (a) activated by blue light, but not by green or red light,\nand (b) disrupted by the application of an oscillatory field at the frequency\ncorresponding to the energy of the electron-spin/magnetic field interaction. A\ncomprehensive analysis of protein expression reveals that the ability of blue\nphotons to promote cell motility is mediated by activation of calmodulin\ncalcium sensors. Collectively, these data suggest that cells possess a\nlight-dependent magnetic compass driven by electron spin dynamics.",
      "generated_abstract": "The regulation of gene expression by posttranslational modifications (PTMs)\nis essential for the development and function of many biological systems.\nHowever, the dynamic nature of PTMs, which are often coupled with other\nsignals, poses challenges in understanding their regulatory functions. Here, we\npresent a comprehensive review of the current status of PTM-driven gene\nregulation. We first introduce the basics of PTMs, emphasizing their key roles\nin biological processes such as transcription, translation, and posttranslational\nmodifications (PTMs). Then, we delve into the complex interplay between PTMs\nand other regulatory signals, such as transcription factors and epigenetic\nmarks, highlighting the interplay between these signals in regulating gene\nexpression. We also discuss the emerging technologies, such as gene editing,\nCRISPR-associated proteins",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10833333333333334,
          "p": 0.1625,
          "f": 0.12999999520000016
        },
        "rouge-2": {
          "r": 0.005714285714285714,
          "p": 0.00909090909090909,
          "f": 0.007017539119732349
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.15,
          "f": 0.1199999952000002
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.00358v1",
      "true_abstract": "This paper considers nonparametric estimation and inference in first-order\nautoregressive (AR(1)) models with deterministically time-varying parameters. A\nkey feature of the proposed approach is to allow for time-varying stationarity\nin some time periods, time-varying nonstationarity (i.e., unit root or\nlocal-to-unit root behavior) in other periods, and smooth transitions between\nthe two. The estimation of the AR parameter at any time point is based on a\nlocal least squares regression method, where the relevant initial condition is\nendogenous. We obtain limit distributions for the AR parameter estimator and\nt-statistic at a given point $\\tau$ in time when the parameter exhibits unit\nroot, local-to-unity, or stationary/stationary-like behavior at time $\\tau$.\nThese results are used to construct confidence intervals and median-unbiased\ninterval estimators for the AR parameter at any specified point in time. The\nconfidence intervals have correct asymptotic coverage probabilities with the\ncoverage holding uniformly over stationary and nonstationary behavior of the\nobservations.",
      "generated_abstract": "This paper develops a novel method for estimation of a generalized\nequilibrium model in a multinational setting. The proposed method is based on\na multi-level hierarchical modeling approach that allows for the estimation of\nthe model parameters under various conditions. The model is designed to capture\nthe multi-level interactions among the firms, countries, and firms in each\ncountry. A hierarchical Bayesian model is constructed to estimate the\nparameters. The estimation is conducted in three steps. First, a nonparametric\nmethod is developed to estimate the parameters of the global equilibrium model\nunder a normality assumption. Second, the model parameters are estimated under\nan asymptotically normality assumption. Third, the model parameters are\nestimated under the normality assumption. The proposed method is validated on\ntwo cases, including a single firm with one country and a firm with multiple\nfirms in each country. The results demonstrate that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23,
          "p": 0.3194444444444444,
          "f": 0.2674418555976204
        },
        "rouge-2": {
          "r": 0.05673758865248227,
          "p": 0.06722689075630252,
          "f": 0.061538456574260755
        },
        "rouge-l": {
          "r": 0.22,
          "p": 0.3055555555555556,
          "f": 0.25581394862087625
        }
      }
    },
    {
      "paper_id": "math.CO.math/AT/2503.05385v1",
      "true_abstract": "A Bier sphere is a simplicial sphere obtained as the deleted join of a\nsimplicial complex and its combinatorial Alexander dual. We focus on particular\nclasses of full subcomplexes of Bier spheres, and determine their topological\ntypes. As applications, we explicitly describe the cohomology of real toric\nspaces associated with Bier spheres.",
      "generated_abstract": "We present a novel approach to the study of the geometric properties of\ngeodesic rays in a convex body of arbitrary dimension. We prove that for a\nconvex body $K$ with a given diameter, there exists a unique geodesic ray in\n$K$ with the smallest possible length, which we call the shortest geodesic ray\nin $K$. We show that the shortest geodesic ray is unique up to translation in\n$K$. Moreover, we prove that the shortest geodesic ray in $K$ is the unique\ngeodesic ray in $K$ whose endpoints are the vertices of the convex body. We\nalso present a sufficient condition for the existence of the shortest geodesic\nray in $K$, which is a refinement of a condition introduced by Sierpi\\'nski.\nWe obtain a sufficient condition for the existence of the shortest geodesic\nray in a convex body",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.12727272727272726,
          "f": 0.14432989199702428
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.021052631578947368,
          "f": 0.027586202378122024
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.10909090909090909,
          "f": 0.12371133529599339
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.10564v1",
      "true_abstract": "We prove that holomorphic maps from an open subset of a complex smooth\nprojective curve to a complex smooth projective rationally simply connected\nvariety can be approximated by algebraic maps for the compact-open topology.\nThis theorem can be applied in particular when the target is a smooth\nhypersurface of degree d in P^n with n greater than or equal to d^2-1. We\ndeduce it from a more general result: the tight approximation property holds\nfor rationally simply connected varieties over function fields of complex\ncurves.",
      "generated_abstract": "We study the cohomology of the stack $\\mathbf{LG}$, the category of left\ngpms and their representations, with $\\mathbf{LG}$ being the stack of\nleft-angled Artin groups. We define a filtration on the stack $\\mathbf{LG}$ by\nsubstacks $\\mathbf{LG}_n$ which are the stacks of left-angled Artin groups\nwith rank at most $n$. We show that the cohomology of $\\mathbf{LG}$ is\ntorsion-free, and that the filtration is compatible with the stacks of left-angled\nArtin groups. We also show that the cohomology of $\\mathbf{LG}_n$ is\n$n$-torsion-free, and that the filtration is compatible with the stacks of\nleft-angled Artin groups for $n \\geq 3$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14516129032258066,
          "p": 0.20454545454545456,
          "f": 0.16981131589889656
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11290322580645161,
          "p": 0.1590909090909091,
          "f": 0.1320754668422928
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.09155v1",
      "true_abstract": "We consider time-invariant nonlinear $n$-dimensional strongly $2$-cooperative\nsystems, that is, systems that map the set of vectors with up to weak sign\nvariation to its interior. Strongly $2$-cooperative systems enjoy a strong\nPoincare-Bendixson property: bounded solutions that maintain a positive\ndistance from the set of equilibria converge to a periodic solution. For\nstrongly $2$-cooperative systems whose trajectories evolve in a bounded and\ninvariant set that contains a single unstable equilibrium, we provide a simple\ncriterion for the existence of periodic trajectories. Moreover, we explicitly\ncharacterize a positive-measure set of initial conditions which yield solutions\nthat asymptotically converge to a periodic trajectory. We demonstrate our\ntheoretical results using two models from systems biology, the $n$-dimensional\nGoodwin oscillator and a $4$-dimensional biomolecular oscillator with\nRNA-mediated regulation, and provide numerical simulations that verify the\ntheoretical results.",
      "generated_abstract": "In this paper, we study the asymptotic behavior of the entropy of a\nentire function in the case that the exponent of the function is an integer.\nWe provide a simple proof of the main result and prove a general result in the\ncase that the exponent is non-integer.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11904761904761904,
          "p": 0.3333333333333333,
          "f": 0.17543859261311182
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.04878048780487805,
          "f": 0.024539873535323702
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.3,
          "f": 0.157894732963989
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2502.17518v1",
      "true_abstract": "This paper presents a comprehensive study on the use of ensemble\nReinforcement Learning (RL) models in financial trading strategies, leveraging\nclassifier models to enhance performance. By combining RL algorithms such as\nA2C, PPO, and SAC with traditional classifiers like Support Vector Machines\n(SVM), Decision Trees, and Logistic Regression, we investigate how different\nclassifier groups can be integrated to improve risk-return trade-offs. The\nstudy evaluates the effectiveness of various ensemble methods, comparing them\nwith individual RL models across key financial metrics, including Cumulative\nReturns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our\nresults demonstrate that ensemble methods consistently outperform base models\nin terms of risk-adjusted returns, providing better management of drawdowns and\noverall stability. However, we identify the sensitivity of ensemble performance\nto the choice of variance threshold {\\tau}, highlighting the importance of\ndynamic {\\tau} adjustment to achieve optimal performance. This study emphasizes\nthe value of combining RL with classifiers for adaptive decision-making, with\nimplications for financial trading, robotics, and other dynamic environments.",
      "generated_abstract": "We introduce a novel multi-step prediction model for the pricing of European\n$n$-dimensional options under continuous-time stochastic volatility models.\nSpecifically, we develop a framework for pricing the European call option in\nwhich the underlying asset price process is modeled as an infinite-dimensional\nBrownian motion with a piecewise constant drift. Our framework relies on the\n$n$-dimensional Ornstein-Uhlenbeck (OU) process, which is a continuous-time\nstochastic process with a Gaussian white noise process. We show that the\ndiscrete-time OU model can be extended to continuous-time and can be used to\nmodel the continuous-time stochastic volatility process. This is the first\ncontemporary model for pricing European options with continuous-time stochastic\nvolatility. We further develop a two-step pricing algorithm to compute the\noption price. In the first step",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14166666666666666,
          "p": 0.24285714285714285,
          "f": 0.17894736376731316
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.018018018018018018,
          "f": 0.014760142764942148
        },
        "rouge-l": {
          "r": 0.14166666666666666,
          "p": 0.24285714285714285,
          "f": 0.17894736376731316
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.06441v1",
      "true_abstract": "Company financial risks pose a significant threat to personal wealth and\nnational economic stability, stimulating increasing attention towards the\ndevelopment of efficient andtimely methods for monitoring them. Current\napproaches tend to use graph neural networks (GNNs) to model the momentum\nspillover effect of risks. However, due to the black-box nature of GNNs, these\nmethods leave much to be improved for precise and reliable explanations towards\ncompany risks. In this paper, we propose CF3, a novel Counterfactual and\nFactual learning method for company Financial risk detection, which generates\nevidence subgraphs on company knowledge graphs to reliably detect and explain\ncompany financial risks. Specifically, we first propose a meta-path attribution\nprocess based on Granger causality, selecting the meta-paths most relevant to\nthe target node labels to construct an attribution subgraph. Subsequently, we\npropose anedge-type-aware graph generator to identify important edges, and we\nalso devise a layer-based feature masker to recognize crucial node features.\nFinally, we utilize counterfactual-factual reasoning and a loss function based\non attribution subgraphs to jointly guide the learning of the graph generator\nand feature masker. Extensive experiments on three real-world datasets\ndemonstrate the superior performance of our method compared to state-of-the-art\napproaches in the field of financial risk detection.",
      "generated_abstract": "Recent advances in large language models (LLMs) have demonstrated strong\naccuracy in natural language processing, yet their performance in\ninterpretability remains limited. This is due to the inherent limitations of\nLLMs, which rely on a predefined set of parameters and often struggle to\nunderstand complex inputs. To address these challenges, this paper introduces\nMirror-LSTM, a novel framework for interpretable LLMs that leverages mirror\nself-attention to capture semantic relationships in inputs. We demonstrate that\nMirror-LSTM outperforms existing methods in terms of accuracy and interpretability\nwhile maintaining comparable performance on downstream tasks. Furthermore, we\nprovide insights into the role of mirror self-attention in interpretable LLMs,\nhighlighting the potential of this mechanism for enhancing human-like\nunderstanding in AI systems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1297709923664122,
          "p": 0.19540229885057472,
          "f": 0.1559632979559803
        },
        "rouge-2": {
          "r": 0.015544041450777202,
          "p": 0.02631578947368421,
          "f": 0.019543969272460236
        },
        "rouge-l": {
          "r": 0.1297709923664122,
          "p": 0.19540229885057472,
          "f": 0.1559632979559803
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2411.07674v2",
      "true_abstract": "We prove that a two-cycle equilibrium in a general equilibrium model with\ninfinitely-lived agents also constitutes an equilibrium in an overlapping\ngenerations (OLG) model. Conversely, an equilibrium in an OLG model that\nsatisfies additional conditions is part of an equilibrium in a general\nequilibrium model with infinitely-lived agents. Applying this result, we\ndemonstrate that equilibrium indeterminacy and rational asset price bubbles may\narise in both types of models.",
      "generated_abstract": "This study examines the potential of using the Markowitz portfolio\noptimization method for portfolio selection in the context of the cryptocurrency\nmarkets. By applying the portfolio optimization method, we aim to obtain the\noptimal portfolio that maximizes the expected utility of the portfolio in the\ncryptocurrency market. The results of the analysis show that the Markowitz\nportfolio optimization method is effective in portfolio selection in the\ncryptocurrency market. The analysis shows that the optimal portfolio is a\ncombination of cryptocurrency and traditional asset portfolios, which\nprovides a high level of diversification and risk management. The results of the\nanalysis also show that the optimal portfolio performs better than the benchmark\nportfolio in terms of returns and risk. This study provides insights into the\nuse of portfolio optimization methods in the cryptocurrency market and\npromotes the use of these methods in other",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.140625,
          "f": 0.16981131597009624
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.140625,
          "f": 0.16981131597009624
        }
      }
    },
    {
      "paper_id": "nlin.CD.nlin/CD/2503.05462v1",
      "true_abstract": "The wave kinetic equation has become an important tool in different fields of\nphysics. In particular, for surface gravity waves, it is the backbone of wave\nforecasting models. Its derivation is based on the Hamiltonian dynamics of\nsurface gravity waves. Only at the end of the derivation are the\nnon-conservative effects, such as forcing and dissipation, included as\nadditional terms to the collision integral. In this paper, we present a first\nattempt to derive the wave kinetic equation when the dissipation/forcing is\nincluded in the deterministic dynamics. If, in the dynamical equations, the\ndissipation/forcing is one order of magnitude smaller than the nonlinear\neffect, then the classical wave action balance equation is obtained and the\nkinetic time scale corresponds to the dissipation/forcing time scale. However,\nif we assume that the nonlinearity and the dissipation/forcing act on the same\ndynamical time scale, we find that the dissipation/forcing dominates the\ndynamics and the resulting collision integral appears in a modified form, at a\nhigher order.",
      "generated_abstract": "We study the nonlinear dynamics of a class of Hamiltonian systems defined by\nthe following 4D Hamiltonian:\n  \\begin{equation}\n  H(q,p,t) = \\frac{1}{2} \\sum_{i=1}^n \\left( \\frac{\\partial q_i}{\\partial t}\n  - \\frac{\\partial p_i}{\\partial q_i} \\right)^2 + V(q) + g \\sum_{i=1}^n\n  \\left( p_i - \\frac{\\partial V}{\\partial q_i} \\right)^2\n  \\end{equation}\n  where the $V$ is a smooth function. We consider the case $g=0$. We show that\nfor the Hamiltonian $H$ given above, there is a one-dimensional subspace of\ninitial conditions that asymptotically approaches the equilibrium point with\nstable dynamics, as the system approaches the equilibrium",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10638297872340426,
          "p": 0.16129032258064516,
          "f": 0.1282051234155163
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.037037037037037035,
          "f": 0.026431713471638127
        },
        "rouge-l": {
          "r": 0.09574468085106383,
          "p": 0.14516129032258066,
          "f": 0.11538461059500349
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.16108v1",
      "true_abstract": "The binary expansions of irrational algebraic numbers can serve as\nhigh-quality pseudorandom binary sequences. This study presents an efficient\nmethod for computing the exact binary expansions of real quadratic algebraic\nintegers using Newton's method. To this end, we clarify conditions under which\nthe first $N$ bits of the binary expansion of an irrational number match those\nof its upper rational approximation. Furthermore, we establish that the\nworst-case time complexity of generating a sequence of length $N$ with the\nproposed method is equivalent to the complexity of multiplying two $N$-bit\nintegers, showing its efficiency compared to a previously proposed true orbit\ngenerator. We report the results of numerical experiments on computation time\nand memory usage, highlighting in particular that the proposed method\nsuccessfully accelerates true orbit pseudorandom number generation. We also\nconfirm that a generated pseudorandom sequence successfully passes all the\nstatistical tests included in RabbitFile of TestU01.",
      "generated_abstract": "The C-statistic is a widely used statistic in the analysis of classification\nand regression problems. In this paper, we consider the case where the\nclassification and regression problems are multiclass and we develop a\ncomprehensive methodology to estimate the C-statistic for the CART decision\ntree. We derive the asymptotic normality of the estimated C-statistic and its\nquantile function, and establish the consistency and asymptotic normality of the\nestimator. We also discuss the asymptotic normality of the residuals of the\nestimated C-statistic. We then apply the proposed methodology to the classification\nproblem of detecting the presence of a cancerous tumor in a biopsy sample and to\nthe regression problem of predicting the average annual rate of return of an\ninvestment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.265625,
          "f": 0.20858895228574664
        },
        "rouge-2": {
          "r": 0.03597122302158273,
          "p": 0.04950495049504951,
          "f": 0.04166666179201446
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.234375,
          "f": 0.1840490749851332
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/ET/2503.07599v1",
      "true_abstract": "Generative AI is transforming education by enabling personalized, on-demand\nlearning experiences. However, AI tutors lack the ability to assess a learner's\ncognitive state in real time, limiting their adaptability. Meanwhile,\nelectroencephalography (EEG)-based neuroadaptive systems have successfully\nenhanced engagement by dynamically adjusting learning content. This paper\npresents NeuroChat, a proof-of-concept neuroadaptive AI tutor that integrates\nreal-time EEG-based engagement tracking with generative AI. NeuroChat\ncontinuously monitors a learner's cognitive engagement and dynamically adjusts\ncontent complexity, response style, and pacing using a closed-loop system. We\nevaluate this approach in a pilot study (n=24), comparing NeuroChat to a\nstandard LLM-based chatbot. Results indicate that NeuroChat enhances cognitive\nand subjective engagement but does not show an immediate effect on learning\noutcomes. These findings demonstrate the feasibility of real-time cognitive\nfeedback in LLMs, highlighting new directions for adaptive learning, AI\ntutoring, and human-AI interaction.",
      "generated_abstract": "The COVID-19 pandemic has brought an unprecedented level of disruption to\nthe healthcare industry, with many healthcare workers struggling to meet\npatient demands. This paper investigates the impact of a novel COVID-19\ndiagnostic tool, called Nuclear Medicine Scan, on the workload of a hospital's\nradiology department. We evaluate the performance of the diagnostic tool,\nparticularly the nuclear medicine scanner, across 100 different patient\ncases. The results show that the Nuclear Medicine Scan performs better than\nstandard diagnostic modalities, particularly in detecting cancerous tumors.\nHowever, the tool also introduces a significant workload increase to the\nradiology department, with an average of 33.6% more radiology reports per day\ncompared to the baseline. This increase in workload can be attributed to the\nincreased complexity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1523809523809524,
          "p": 0.1927710843373494,
          "f": 0.17021276102591687
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.017391304347826087,
          "f": 0.01599999503200154
        },
        "rouge-l": {
          "r": 0.1523809523809524,
          "p": 0.1927710843373494,
          "f": 0.17021276102591687
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2412.17005v1",
      "true_abstract": "This study examines the phytochemical characteristics of Ayurvedic products.\nAn analysis was performed on Kottakkal Ayurveda Triphala (T), Kottakkal\nAyurveda Hinguvachadi Churnam (H), and Kottakkal Ayurveda Jirakadyarishtam (J)\nusing GC-MS and LC-MS techniques to determine their bioactive constituents,\nwhile also assessing their antimicrobial, docking, anticancer, and\nanti-diabetic activities. The GC-MS analysis identified 30, 45, and 8 chemical\ncomponents in Kottakkal Ayurveda Triphala (T), Kottakkal Ayurveda Hinguvachadi\nChurnam (H), and Kottakkal Ayurveda Jirakadyarishtam (J), respectively. The\nLC-MS analysis produced 15, 20, and 16 peaks for Kottakkal Ayurveda Triphala\n(T), Kottakkal Ayurveda Hinguvachadi Churnam (H), and Kottakkal Ayurveda\nJirakadyarishtam (J), with m/z values of 982, 981, 972, and 933; 987, 985, 974,\nand 945; and 969, 965, 951, and 941, respectively, confirming their precision.\nMoreover, characterization of the Ayurvedic products was carried out using\nFT-IR, UV-vis, and 1H-NMR spectroscopy to identify significant functional\ngroups and chemical substances. Kottakkal Ayurveda Triphala (T) was evaluated\nfor antibacterial activity against Gram-positive bacteria (Streptococcus\npneumoniae and Staphylococcus aureus) along with Gram-negative bacteria\n(Escherichia coli and Klebsiella pneumoniae), yielding a P value of 0.0650 (P <\n0.0001). Both Kottakkal Ayurveda Hinguvachadi Churnam (H) and Kottakkal\nAyurveda Jirakadyarishtam (J) were subjected to analysis for their\neffectiveness against Aspergillus niger and Aspergillus fumigatus, also\nrevealing a P value within the acceptable range of 0.0650 (P < 0.0001). The\nanti-diabetic properties of Kottakkal Ayurveda Triphala (T) were assessed using\nthe {\\alpha}-glucosidase inhibitory method, which exhibited a significant\ninhibitory effect on {\\alpha}-glucosidase, resulting in an average P value of\n0.001 (P < 0.0001).",
      "generated_abstract": "We introduce a new framework for integrating microbiome-host interactions\ninto mathematical models of complex biological systems. By defining the\nmicrobiome as an evolving, multivariate distribution, we extend the classical\nMarkovian framework to incorporate nonlinear dynamics and non-Gaussian\nbehaviors. This framework provides a unified framework for describing the\ninteractions of microbes with their host organisms, from single-cell to\npopulation-level analysis. We illustrate the utility of this framework through\nthe analysis of a model of the human microbiome in health and disease.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08633093525179857,
          "p": 0.20689655172413793,
          "f": 0.12182740701280646
        },
        "rouge-2": {
          "r": 0.0049261083743842365,
          "p": 0.01282051282051282,
          "f": 0.007117433711834682
        },
        "rouge-l": {
          "r": 0.07913669064748201,
          "p": 0.1896551724137931,
          "f": 0.11167512274884706
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/stat-mech/2503.10146v1",
      "true_abstract": "We find nonequilibrium phase transitions accompanied by multiple (nested)\nhysteresis behaviors in superconductors coupled to baths under a time-periodic\nlight driving.The transitions are demonstrated with a full phase diagram in the\ndomain of the driving amplitude and frequency by means of the Floquet many-body\ntheory. In the weak driving regime with a frequency smaller than half of the\nsuperconducting gap, excited quasiparticles are accumulated at the far edges of\nthe bands, realizing a distribution reminiscent of the Eliashberg effect, which\nsuddenly becomes unstable in the strong driving regime due to\nmulti-photon-assisted tunneling across the gap mediated by the in-gap Floquet\nsidebands. We also show that superconductivity is enhanced in the weak driving\nregime without effective cooling, which is attributed to the modulation of the\nspectrum due to Floquet sidebands.",
      "generated_abstract": "We study the nonequilibrium dynamics of a strongly correlated spinless\nfermion system in a two-dimensional square lattice with a transverse field\n$\\vec{B}$. The fermions interact through a Coulomb-like potential $V(x,t)$\nwith a short-range cutoff $\\Lambda$. The Hamiltonian is given by\n$H=\\sum_i \\varepsilon_i \\psi^\\dagger_i \\psi_i + \\sum_i V(x_i,t) \\psi^\\dagger_i\n\\psi_i$, where $\\varepsilon_i$ is the band dispersion and $x_i$ is the position\noperator. We assume that the transverse field $\\vec{B}$ is spatially homogeneous\nand can be applied across the entire lattice. The system is driven in the\n$x$-direction and is initially in an incompressible state. The dynamics is\ndescribed by the master",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.19402985074626866,
          "f": 0.17449663934597553
        },
        "rouge-2": {
          "r": 0.034782608695652174,
          "p": 0.042105263157894736,
          "f": 0.03809523314059022
        },
        "rouge-l": {
          "r": 0.15853658536585366,
          "p": 0.19402985074626866,
          "f": 0.17449663934597553
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00326v1",
      "true_abstract": "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
      "generated_abstract": "The influence of measurement error on the estimation of a functional\nmeasure is a well-known issue in functional analysis. Recently, the\npotential effect of measurement error on the estimation of the functional mean\nof a functional measure has been explored in the context of functional\nautoregressive models. In this paper, we extend the analysis to the\nestimation of the functional mean of a functional functional measure, which is\na generalization of the functional mean of a functional functional measure. In\nparticular, we consider the case where the functional functional measure is a\nfunction of a functional functional measure. We derive the asymptotic\ndistribution of the functional mean estimator in this general setting.\nAdditionally, we propose an efficient estimator based on the asymptotic\ndistribution of the functional mean estimator. Finally, we illustrate the\nperformance of the proposed estimator through simulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1839080459770115,
          "p": 0.27586206896551724,
          "f": 0.2206896503724139
        },
        "rouge-2": {
          "r": 0.03508771929824561,
          "p": 0.04395604395604396,
          "f": 0.03902438530684179
        },
        "rouge-l": {
          "r": 0.16091954022988506,
          "p": 0.2413793103448276,
          "f": 0.19310344347586217
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.18662v1",
      "true_abstract": "The peer-review process is broken and the problem is getting worse,\nespecially in AI: large conferences like NeurIPS increasingly struggle to\nadequately review huge numbers of paper submissions. I propose a scalable\nsolution that, foremost, recognizes reviewing as important, necessary,\n\\emph{work} and rewards it with crypto-coins owned and managed by the\nconferences themselves. The idea is at its core quite simple: paper submissions\nrequire work (reviews, meta-reviews, etc.) to be done, and therefore the\nsubmitter must pay for that work. Each reviewer submits their review to be\napproved by some designated conference officer (e.g. PC chair, Area Chair,\netc.), and upon approval is paid a single coin for a single review. If three\nreviews are required, the cost of submission should be three coins + a tax that\ncovers payments to all the volunteers who organize the conference. After some\none-time startup costs to fairly distribute coins, the process should be\nrelatively stable with new coins minted only when a conference grows.",
      "generated_abstract": "This paper examines the impact of the COVID-19 pandemic on the international\ndynamics of global production networks. Using a large-scale production network\nmodel, we analyze how the pandemic affected the structure of production\nnetworks in 2019 and 2020. Our findings reveal that the global production\nnetworks in 2019 were less connected and more segmented, while those in 2020\nwere more connected and more integrated. The pandemic also increased the\nfragmentation of production networks. Specifically, the network structure\nchanged from a \"closed\" to a \"semi-open\" structure. This shift was mainly\ndriven by the weakening of interdependence among countries. The global supply\nchain network was further segmented by the pandemic, with China, the United\nStates, and Europe emerging as the main production centers. This study provides\nan",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.15789473684210525,
          "f": 0.12244897484381526
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.008771929824561403,
          "f": 0.0072992652139197995
        },
        "rouge-l": {
          "r": 0.09166666666666666,
          "p": 0.14473684210526316,
          "f": 0.11224489321116221
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02118v2",
      "true_abstract": "An increase in availability of Software Defined Radios (SDRs) has caused a\ndramatic shift in the threat landscape of legacy satellite systems, opening\nthem up to easy spoofing attacks by low-budget adversaries. Physical-layer\nauthentication methods can help improve the security of these systems by\nproviding additional validation without modifying the space segment. This paper\nextends previous research on Radio Frequency Fingerprinting (RFF) of satellite\ncommunication to the Orbcomm satellite formation. The GPS and Iridium\nconstellations are already well covered in prior research, but the feasibility\nof transferring techniques to other formations has not yet been examined, and\nraises previously undiscussed challenges.\n  In this paper, we collect a novel dataset containing 8992474 packets from the\nOrbcom satellite constellation using different SDRs and locations. We use this\ndataset to train RFF systems based on convolutional neural networks. We achieve\nan ROC AUC score of 0.53 when distinguishing different satellites within the\nconstellation, and 0.98 when distinguishing legitimate satellites from SDRs in\na spoofing scenario. We also demonstrate the possibility of mixing datasets\nusing different SDRs in different physical locations.",
      "generated_abstract": "This paper presents a novel hybrid approach for the design of a distributed\ncommunication network, aimed at improving energy efficiency and reducing the\nnumber of inter-node communication channels. The proposed approach combines\ndistributed energy storage (DES) and energy-efficient communication (EEC) in\nthe form of a distributed energy storage network (DESN). The DESN is a\nmulti-agent system with a set of agents, each of which is responsible for\nmanaging a subset of communication channels. The agents coordinate their\ncommunication to reduce the energy required for communication while ensuring\nthe required service quality. A novel distributed algorithm for the design of\nthe DESN is proposed, which is based on the dynamic programming approach.\nAdditionally, an energy-efficient communication algorithm is proposed for the\nDESN, which takes into account the communication channels and the energy\nconsumption of each agent. Simulation results show that the proposed approach\ncan improve the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.20512820512820512,
          "f": 0.153846149158654
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.046875,
          "f": 0.03973509445375263
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.20512820512820512,
          "f": 0.153846149158654
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.05915v1",
      "true_abstract": "Multilevel regression and poststratification (MRP) is a computationally\nefficient indirect estimation method that can quickly produce improved\npopulation-adjusted estimates with limited data. Recent computational\nadvancements allow efficient, relatively simple, and quick approximate Bayesian\nestimation for MRP. As population health outcomes of interest including\nvaccination uptake are known to have spatial structure, precision may be gained\nby including space in the model. We test a recently proposed spatial MRP method\nthat includes a BYM2 spatial term that smooths across demographics and\ngeographic areas using a large, unrepresentative survey. We produce California\ncounty-level estimates of first-dose COVID-19 vaccination up to June 2021 using\nclassic and spatial MRP models, and poststratify using data from the American\nCommunity Survey (US Census Bureau). We assess validity using reported\nfirst-dose vaccination counts from the Centers for Disease Control (CDC).\nNeither classic nor spatial MRP models performed well, highlighting: 1. spatial\nMRP may be most appropriate for richer data contexts, 2. some demographics in\nthe survey data are over-sampled and -aggregated, producing model\nover-smoothing, and 3. a need for survey producers to share user-representative\nmetrics to better benchmark estimates.",
      "generated_abstract": "In the 2000s, a large body of research in machine learning demonstrated\nthat deep learning methods can be used to automatically generate hypotheses\nthat can be tested against large data sets, thereby achieving a level of\ninductive reasoning previously considered unattainable. This methodology\ngenerates hypotheses, often referred to as models, that can be tested and\nrefined using a process called hypothesis testing. This process is particularly\nuseful for complex and data-scarce problems, such as the development of\ntherapeutics for rare diseases, where the number of hypotheses to test is\ninfinite. In this paper, we introduce a novel methodology for testing hypotheses\nderived from deep learning-based models, called model testing. This methodology\nis based on the idea that a hypothesis testing problem can be viewed as a\nvariation of the hypothesis testing problem of a finite population, where the\nnumber of hypotheses",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13709677419354838,
          "p": 0.2073170731707317,
          "f": 0.16504853889716292
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.015873015873015872,
          "f": 0.013333328461335113
        },
        "rouge-l": {
          "r": 0.12096774193548387,
          "p": 0.18292682926829268,
          "f": 0.14563106316900762
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05758v1",
      "true_abstract": "Lipreading is an important technique for facilitating human-computer\ninteraction in noisy environments. Our previously developed self-supervised\nlearning method, AV2vec, which leverages multimodal self-distillation, has\ndemonstrated promising performance in speaker-independent lipreading on the\nEnglish LRS3 dataset. However, AV2vec faces challenges such as high training\ncosts and a potential scarcity of audio-visual data for lipreading in languages\nother than English, such as Chinese. Additionally, most studies concentrate on\nspeakerindependent lipreading models, which struggle to account for the\nsubstantial variation in speaking styles across di?erent speakers. To address\nthese issues, we propose a comprehensive approach. First, we investigate\ncross-lingual transfer learning, adapting a pre-trained AV2vec model from a\nsource language and optimizing it for the lipreading task in a target language.\nSecond, we enhance the accuracy of lipreading for specific target speakers\nthrough a speaker adaptation strategy, which is not extensively explored in\nprevious research. Third, after analyzing the complementary performance of\nlipreading with lip region-of-interest (ROI) and face inputs, we introduce a\nmodel ensembling strategy that integrates both, signi?cantly boosting model\nperformance. Our method achieved a character error rate (CER) of 77.3% on the\nevaluation set of the ChatCLR dataset, which is lower than the top result from\nthe 2024 Chat-scenario Chinese Lipreading Challenge.",
      "generated_abstract": "In the field of speech enhancement, it is important to analyze the effect of\nchanging the speech signal's energy distribution. However, previous studies only\nexamined the effect of energy distribution on speech enhancement by\nsampling-based methods. In this paper, we propose a new sampling-free method\nthat employs the kernel density estimation (KDE) technique to analyze the\neffect of energy distribution on speech enhancement. The KDE method has been\nused in numerous fields, such as image analysis and pattern recognition, and\nhas been applied to speech enhancement in the past. We conducted experiments\non synthetic data and real speech signals to analyze the effect of energy\ndistribution on speech enhancement. The results showed that KDE could effectively\ndetect the distribution of energy in the speech signal, and the effect of the\nenergy distribution on speech enhancement was significant. This study is the\nfirst to investigate the effect of energy distribution on speech",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16083916083916083,
          "p": 0.2987012987012987,
          "f": 0.20909090454090917
        },
        "rouge-2": {
          "r": 0.02030456852791878,
          "p": 0.036036036036036036,
          "f": 0.025974021363848007
        },
        "rouge-l": {
          "r": 0.14685314685314685,
          "p": 0.2727272727272727,
          "f": 0.19090908635909098
        }
      }
    },
    {
      "paper_id": "cs.AI.econ/GN/2502.16879v1",
      "true_abstract": "This paper pioneers a novel approach to economic and public policy analysis\nby leveraging multiple Large Language Models (LLMs) as heterogeneous artificial\neconomic agents. We first evaluate five LLMs' economic decision-making\ncapabilities in solving two-period consumption allocation problems under two\ndistinct scenarios: with explicit utility functions and based on intuitive\nreasoning. While previous research has often simulated heterogeneity by solely\nvarying prompts, our approach harnesses the inherent variations in analytical\ncapabilities across different LLMs to model agents with diverse cognitive\ntraits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB)\nframework by mapping these LLMs to specific educational groups and\ncorresponding income brackets. Using interest-income taxation as a case study,\nwe demonstrate how the MLAB framework can simulate policy impacts across\nheterogeneous agents, offering a promising new direction for economic and\npublic policy analysis by leveraging LLMs' human-like reasoning capabilities\nand computational power.",
      "generated_abstract": "This paper addresses the challenges of designing optimal, fair, and scalable\nalgorithmic mechanisms for the allocation of scarce resources, including\nvaccines, food, and other goods. We focus on mechanisms that are designed\nthrough an iterative process where each iteration is characterized by a\nmulti-agent optimization problem. We first analyze the properties of the\noptimal mechanism, including its utility and fairness. Next, we discuss the\nimpact of the iterative nature of the mechanism on its performance. Finally, we\ninvestigate the effect of the number of iterations on the performance of the\nmechanism. We use the example of vaccine allocation as a case study and\ndemonstrate the effectiveness of the proposed mechanism by comparing its\nperformance to the conventional random allocation mechanism.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16037735849056603,
          "p": 0.22666666666666666,
          "f": 0.1878452990140717
        },
        "rouge-2": {
          "r": 0.02962962962962963,
          "p": 0.03571428571428571,
          "f": 0.03238865901096636
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.21333333333333335,
          "f": 0.17679557525716566
        }
      }
    },
    {
      "paper_id": "cs.IT.math/AC/2503.03421v1",
      "true_abstract": "In this paper, we study the unit graph $ G(\\mathbb{Z}_n) $, where $ n $ is of\nthe form $n = p_1^{n_1} p_2^{n_2} \\dots p_r^{n_r}$, with $ p_1, p_2, \\dots, p_r\n$ being distinct prime numbers and $ n_1, n_2, \\dots, n_r $ being positive\nintegers. We establish the connectivity of $ G(\\mathbb{Z}_n) $, show that its\ndiameter is at most three, and analyze its edge connectivity. Furthermore, we\nconstruct $ q $-ary linear codes from the incidence matrix of $ G(\\mathbb{Z}_n)\n$, explicitly determining their parameters and duals. A primary contribution of\nthis work is the resolution of two conjectures from \\cite{Jain2023} concerning\nthe structural and coding-theoretic properties of $ G(\\mathbb{Z}_n) $. These\nresults extend the study of algebraic graph structures and highlight the\ninterplay between number theory, graph theory, and coding theory.",
      "generated_abstract": "We study the convergence properties of the iterates of a family of iterative\nalgorithm in the context of the non-convex functional setting. The iterates of\nthe algorithm are defined as the solution of a linear system of partial\ndifferential equations (PDEs) with a non-convex functional as the right hand\nside. The algorithm is based on a spectral method for solving the PDEs, and\nthe convergence of the iterates is analyzed in the sense of the Banach fixed\npoint theorem. We first study the convergence of the iterates in the case when\nthe non-convex functional is the functional of the second kind, and we show\nthat the convergence is in the sense of the Banach fixed point theorem. We then\ngeneralize the result to the case when the non-convex functional is the\nfunctional of the first kind, and we show that the convergence is in the sense\nof the local Lipsch",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13793103448275862,
          "p": 0.2,
          "f": 0.16326530129112884
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.05154639175257732,
          "f": 0.04504504012458459
        },
        "rouge-l": {
          "r": 0.10344827586206896,
          "p": 0.15,
          "f": 0.12244897476051664
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.09799v1",
      "true_abstract": "As we scale to more massive machine learning models, the frequent\nsynchronization demands inherent in data-parallel approaches create significant\nslowdowns, posing a critical challenge to further scaling. Recent work develops\nan approach (DiLoCo) that relaxes synchronization demands without compromising\nmodel quality. However, these works do not carefully analyze how DiLoCo's\nbehavior changes with model size. In this work, we study the scaling law\nbehavior of DiLoCo when training LLMs under a fixed compute budget. We focus on\nhow algorithmic factors, including number of model replicas, hyperparameters,\nand token budget affect training in ways that can be accurately predicted via\nscaling laws. We find that DiLoCo scales both predictably and robustly with\nmodel size. When well-tuned, DiLoCo scales better than data-parallel training\nwith model size, and can outperform data-parallel training even at small model\nsizes. Our results showcase a more general set of benefits of DiLoCo than\npreviously documented, including increased optimal batch sizes, improved\ndownstream generalization with scale, and improved evaluation loss for a fixed\ntoken budget.",
      "generated_abstract": "We present a new method for the generation of non-trivial, highly\nnonlinear, and highly non-trivially nonlinear functions, called nonlinear\nnonlinear functions (NNFs). Our method is based on the transformation of a\nconventional neural network into a multi-layer perceptron with additional\nnonlinear layers. The main advantage of our approach is that the network is\ntrained only once, and then the NNFs can be generated from the trained model\nwithin a few minutes. The efficiency of our method is illustrated by the\napplication to the problem of generating nonlinear nonlinear functions. Our\nmethod is also general, and we demonstrate that it can be used to generate\nnonlinear nonlinear functions for any problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13559322033898305,
          "p": 0.23880597014925373,
          "f": 0.17297296835295847
        },
        "rouge-2": {
          "r": 0.006369426751592357,
          "p": 0.01,
          "f": 0.007782096413271841
        },
        "rouge-l": {
          "r": 0.13559322033898305,
          "p": 0.23880597014925373,
          "f": 0.17297296835295847
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.11462v1",
      "true_abstract": "Deep learning based end-to-end multi-channel speech enhancement methods have\nachieved impressive performance by leveraging sub-band, cross-band, and spatial\ninformation. However, these methods often demand substantial computational\nresources, limiting their practicality on terminal devices. This paper presents\na lightweight multi-channel speech enhancement network with decoupled fully\nconnected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis\ndecoupled fully-connected attention (T-FCA) and frequency-axis decoupled\nfully-connected attention (F-FCA) mechanisms to effectively capture long-range\nnarrow-band and cross-band information without recurrent units. Experimental\nresults show that LMFCA-Net performs comparably to state-of-the-art methods\nwhile significantly reducing computational complexity and latency, making it a\npromising solution for practical applications.",
      "generated_abstract": "This paper presents a novel wireless power transfer (WPT) system for\nad-hoc networks, where a WPT node is located in the boundary of the network\nand serves as a bridge between the network and the users. The WPT node is\nequipped with a transmitter and a receiver to facilitate the transmission of\npower between the network and the users. In this work, we focus on the\noptimization of the transmit power of the WPT node to maximize the energy\ntransferred to the users. The power transfer efficiency is considered in the\noptimization problem. The problem is formulated as an optimal power allocation\n(OPA) problem, where the power allocation is optimized for each user. To\nsolve the OPA problem, we introduce a novel gradient descent method, and\npropose two different algorithms to solve the OPA problem. The first algorithm\nis based on a simple gradient descent method and the second one is a modified",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14634146341463414,
          "p": 0.1518987341772152,
          "f": 0.14906831798310266
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.0234375,
          "f": 0.0269058247059069
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.13924050632911392,
          "f": 0.1366459577346555
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.hep-ex/2503.09420v1",
      "true_abstract": "Diamond's exceptional properties make it highly suited for applications in\nchallenging radiation environments. Understanding radiation-induced damage in\ndiamond is crucial for enabling its practical applications and advancing\nmaterials science. However, direct imaging of radiation-induced crystal defects\nat the atomic scale remains rare due to diamond's compact lattice structure.\nHere, we report the atomic-level characterization of crystal defects induced by\nhigh-flux fast neutron radiation (up to $3 \\times10^{17}$ n/$cm^2$) in\nsingle-crystal chemical vapor deposition diamonds. Through Raman spectroscopy,\nthe phase transition from carbon $sp^3$ to $sp^2$ hybridization was identified,\nprimarily associated with the formation of dumbbell-shaped interstitial\ndefects. Using electron energy loss spectroscopy and aberration-corrected\ntransmission electron microscopy, we observed a clustering trend in defect\ndistribution, where $sp^2$ rich clusters manifested as dislocation structures\nwith a density up to $10^{14}$ $cm^{-2}$. Lomer-Cottrell junctions were\nidentified, offering a possible explanation for defect cluster formation.\nRadiation-induced point defects were found to be dispersed throughout the\ndiamond lattice, highlighting the widespread nature of primary defect\nformation. Vacancy defects, along with $\\langle 111 \\rangle$ and $\\langle 100\n\\rangle$ oriented dumbbell-shaped interstitial defects induced by high-dose\nneutron irradiation, were directly imaged, providing microscopic structural\nevidence that complements spectroscopic studies of point defects. Dynamical\nsimulations combined with an adiabatic recombination-based damage model\nprovided insights into the correlation between irradiation dose and resulting\ncrystal damage. These findings advance our understanding of neutron-induced\ndamage mechanisms in diamond and contribute to the development of\nradiation-resistant diamond materials.",
      "generated_abstract": "We study the spin dynamics in a system of $N$ coupled spin-1/2 particles in an\nhigh-temperature superconductor. Our model includes spin-orbit coupling and\ninteraction between the particles. We present a numerical analysis of the\nspin-spin and spin-magnetic-field correlation functions, as well as of the\ntime-averaged spin-spin and spin-magnetic-field correlation functions. We find\nthat the correlation functions show a power-law decay with an exponent\n$\\gamma=0.69\\pm0.01$ as the system is cooled down to a temperature $T\\sim\n100\\,\\mathrm{mK}$ below the superconducting transition. The temperature-dependence\nof the decay exponent is consistent with the temperature-dependence of the\nsuperconducting transition temperature $T_c$, indicating that the spin dynamics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08875739644970414,
          "p": 0.25,
          "f": 0.13100436294502404
        },
        "rouge-2": {
          "r": 0.013100436681222707,
          "p": 0.033707865168539325,
          "f": 0.01886792049740998
        },
        "rouge-l": {
          "r": 0.07100591715976332,
          "p": 0.2,
          "f": 0.10480348958257868
        }
      }
    },
    {
      "paper_id": "cs.DL.stat/OT/2405.20156v1",
      "true_abstract": "This paper seeks to bridge the gap between archival text analysis and network\nanalysis by applying network clustering methods to analyze the coverage of\nBulgaria in 123 issues of the newspaper Osservatore Romano published between\nJanuary and May 1877. Utilizing optical character recognition and generalized\nhomogeneity blockmodeling, the study constructs networks of relevant keywords.\nThose including the sets Bulgaria and Russia are rather isomorphic and they\nlargely overlap with those for Germany, Britain, and War. In structural terms,\nthe blockmodel of the two networks exhibits a clear\ncore-semiperiphery-periphery structure that reflects relations between concepts\nin the newpaper's coverage. The newspaper's lexical choices effectively\ndelegitimised the Bulgarian national revival, highlighting the influence of the\nHoly See on the newspaper's editorial line.",
      "generated_abstract": "We present the first comprehensive survey on data-driven methods for\nrepresenting and analyzing complex systems. The survey focuses on\nsystems-level data, with a particular emphasis on systems with dynamic\ninteractions and multiple agents. The survey includes state-of-the-art methods\nfor learning complex systems from data, as well as methods for analyzing\ncomplex systems from data. It also includes methods for analyzing complex\nsystems from data. The survey is structured around the following themes:\n(1) agent-based models (ABMs), (2) dynamical systems, (3) systems of systems,\n(4) networks, and (5) other methods. It presents a systematic analysis of\nstate-of-the-art methods, discussing the strengths and weaknesses of each, and\nidentifying future directions of research. The survey also includes\nrecommendations for future research directions",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.14925373134328357,
          "f": 0.1273885301391539
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.14925373134328357,
          "f": 0.1273885301391539
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/SC/2312.05876v3",
      "true_abstract": "Some proteins can find their targets on DNA faster than by pure diffusion in\nthe three-dimensional cytoplasm, through the process of facilitated diffusion:\nThey can loosely bind to DNA and temporarily slide along it, thus being guided\nby the DNA molecule itself to the target. This chapter examines this process in\nmathematical detail with a focus on including the effect of DNA coiling on the\nsearch process.",
      "generated_abstract": "We develop a novel framework for studying the interplay between genetic\ntrait evolution and population demography by integrating the effects of\nevolutionary forces on fitness landscapes into a framework for analyzing\npopulation dynamics. We focus on scenarios where demographic events, such as\nmigration, affect fitness landscapes through drift or selection. Under this\nframework, we derive a general expression for the expected number of\nrecombination events at a given mutation rate in a population, which is\nindependent of the specific form of the fitness landscape. We demonstrate that\nthis number can be used to estimate the evolutionary rate of a gene, the rate\nat which new variants arise in the population. We also derive a closed-form\nexpression for the expected number of mutations at a given recombination rate\nin a population, which is also independent of the fitness landscape. We\ndemonstrate that these numbers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24,
          "p": 0.15,
          "f": 0.1846153798816569
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.01680672268907563,
          "f": 0.021621617031995128
        },
        "rouge-l": {
          "r": 0.22,
          "p": 0.1375,
          "f": 0.16923076449704155
        }
      }
    },
    {
      "paper_id": "cs.AR.eess/SY/2503.04581v1",
      "true_abstract": "Most Wearable Ultrasound (WUS) devices lack the computational power to\nprocess signals at the edge, instead relying on remote offload, which\nintroduces latency, high power consumption, and privacy concerns. We present\nMaestro, a RISC-V SoC with unified Vector-Tensor Unit (VTU) and memory-coupled\nFast Fourier Transform (FFT) accelerators targeting edge processing for\nwearable ultrasound devices, fabricated using low-cost TSMC 65nm CMOS\ntechnology. The VTU achieves peak 302GFLOPS/W and 19.8GFLOPS at FP16, while the\nmulti-precision 16/32-bit floating-point FFT accelerator delivers peak\n60.6GFLOPS/W and 3.6GFLOPS at FP16, We evaluate Maestro on a US-based gesture\nrecognition task, achieving 1.62GFLOPS in signal processing at 26.68GFLOPS/W,\nand 19.52GFLOPS in Convolutional Neural Network (CNN) workloads at\n298.03GFLOPS/W. Compared to a state-of-the-art SoC with a similar mission\nprofile, Maestro achieves a 5x speedup while consuming only 12mW, with an\nenergy consumption of 2.5mJ in a wearable US channel preprocessing and ML-based\npostprocessing pipeline.",
      "generated_abstract": "This paper presents a novel deep learning-based methodology for\ninducing and optimizing control laws to stabilize a high-dimensional continuous\ntime nonlinear system under unknown disturbances. The proposed approach\nconsiders a high-dimensional system described by a nonlinear continuous time\nMarkov decision process (CTMDP), where the agent interacts with the environment\nthrough a finite set of actions. The CTMDP is modeled as an MDP, and the\ncontinuous-time optimal control problem is formulated as a linear quadratic\ncontrol problem. The optimal control law is estimated using a deep neural\nnetwork that is trained using a real-time dataset of the system dynamics and\nobservations. The proposed approach is tested on a simulated continuous-time\nnonlinear system with an unknown disturbance. The performance of the proposed\nmethodology is evaluated through the analysis of the system's dynamics, the\nestimation of the optimal control law, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.1518987341772152,
          "f": 0.1206030102876192
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.016,
          "f": 0.014598535184348213
        },
        "rouge-l": {
          "r": 0.09166666666666666,
          "p": 0.13924050632911392,
          "f": 0.11055275903133781
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.09494v2",
      "true_abstract": "In the era of big data, large-scale, multi-modal datasets are increasingly\nubiquitous, offering unprecedented opportunities for predictive modeling and\nscientific discovery. However, these datasets often exhibit complex\nheterogeneity, such as covariate shift, posterior drift, and missing\nmodalities, that can hinder the accuracy of existing prediction algorithms. To\naddress these challenges, we propose a novel Representation Retrieval ($R^2$)\nframework, which integrates a representation learning module (the representer)\nwith a sparsity-induced machine learning model (the learner). Moreover, we\nintroduce the notion of \"integrativeness\" for representers, characterized by\nthe effective data sources used in learning representers, and propose a\nSelective Integration Penalty (SIP) to explicitly improve the property.\nTheoretically, we demonstrate that the $R^2$ framework relaxes the conventional\nfull-sharing assumption in multi-task learning, allowing for partially shared\nstructures, and that SIP can improve the convergence rate of the excess risk\nbound. Extensive simulation studies validate the empirical performance of our\nframework, and applications to two real-world datasets further confirm its\nsuperiority over existing approaches.",
      "generated_abstract": "In the context of deep learning, the challenge of training large language\nmodel (LLM) models often arises from the large number of parameters and\ncomplexity of the model architecture. In this paper, we study a special case\nof the problem of parameter compression in LLMs, namely the problem of\nparameter reduction in LLMs that are built using a pre-trained model as a\nbackbone. In particular, we focus on the problem of reducing the number of\nparameters in the context of transformer-based LLMs. We propose a\nparameter-reduction method for transformer-based LLMs that leverages\npre-trained models to learn the most relevant parameters. This method\nincorporates the transformer architecture with a learning-based regularizer\nthat selects the most informative parameters and reduces the number of\nparameters by a factor of $1/\\sqrt{k}$, where $k$ is the number of parameters",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1487603305785124,
          "p": 0.2465753424657534,
          "f": 0.1855670056153684
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.035398230088495575,
          "f": 0.0295202903408186
        },
        "rouge-l": {
          "r": 0.1322314049586777,
          "p": 0.2191780821917808,
          "f": 0.16494844891433746
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.07934v1",
      "true_abstract": "Counterfactual explanations indicate the smallest change in input that can\ntranslate to a different outcome for a machine learning model. Counterfactuals\nhave generated immense interest in high-stakes applications such as finance,\neducation, hiring, etc. In several use-cases, the decision-making process often\nrelies on an ensemble of models rather than just one. Despite significant\nresearch on counterfactuals for one model, the problem of generating a single\ncounterfactual explanation for an ensemble of models has received limited\ninterest. Each individual model might lead to a different counterfactual,\nwhereas trying to find a counterfactual accepted by all models might\nsignificantly increase cost (effort). We propose a novel strategy to find the\ncounterfactual for an ensemble of models using the perspective of entropic risk\nmeasure. Entropic risk is a convex risk measure that satisfies several\ndesirable properties. We incorporate our proposed risk measure into a novel\nconstrained optimization to generate counterfactuals for ensembles that stay\nvalid for several models. The main significance of our measure is that it\nprovides a knob that allows for the generation of counterfactuals that stay\nvalid under an adjustable fraction of the models. We also show that a limiting\ncase of our entropic-risk-based strategy yields a counterfactual valid for all\nmodels in the ensemble (worst-case min-max approach). We study the trade-off\nbetween the cost (effort) for the counterfactual and its validity for an\nensemble by varying degrees of risk aversion, as determined by our risk\nparameter knob. We validate our performance on real-world datasets.",
      "generated_abstract": "This paper presents a novel framework for multi-source multi-channel (MSMC)\ncommunication systems, where multiple transmitters (Tx) use a single channel\nand transmit over multiple receivers (Rx). The framework addresses the\ncomplexities of simultaneous channel estimation and multiplexing, while\nmaintaining system-wide capacity. A system model is developed, with the\ntransmitters' channel gains and receiver positions fixed, and the receivers'\nchannel gains and positions evolving. The problem is formulated as a Markov\nchain, and the channel matrix is estimated using a variational Bayes\napproach. Simulation results demonstrate the effectiveness of the proposed\nframework in mitigating the effects of channel estimation errors and multiplexing\nconstraints.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11029411764705882,
          "p": 0.2112676056338028,
          "f": 0.14492753172489453
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.030927835051546393,
          "f": 0.018987337517426043
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.2112676056338028,
          "f": 0.14492753172489453
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.04941v2",
      "true_abstract": "Assessing the economic impacts of artificial intelligence requires\nintegrating insights from both computer science and economics. We present the\nGrowth and AI Transition Endogenous model (GATE), a dynamic integrated\nassessment model that simulates the economic effects of AI automation. GATE\ncombines three key ingredients that have not been brought together in previous\nwork: (1) a compute-based model of AI development, (2) an AI automation\nframework, and (3) a semi-endogenous growth model featuring endogenous\ninvestment and adjustment costs. The model allows users to simulate the\neconomic effects of the transition to advanced AI across a range of potential\nscenarios. GATE captures the interactions between economic variables, including\ninvestment, automation, innovation, and growth, as well as AI-related inputs\nsuch as compute and algorithms. This paper explains the model's structure and\nfunctionality, emphasizing AI development for economists and economic modeling\nfor the AI community. The model is implemented in an interactive sandbox,\nenabling users to explore the impact of AI under different parameter choices\nand policy interventions. The modeling sandbox is available at:\nwww.epoch.ai/GATE.",
      "generated_abstract": "This paper examines the effects of COVID-19 on the US workforce and\npart-time employment. We analyze the impact of the pandemic on the number of\npart-time jobs, the number of part-time workers, and the part-time employment\nshare using data from the Current Population Survey (CPS). We find that the\npandemic led to a 2.1% decrease in the number of part-time jobs in the US, a\n7.3% decrease in the part-time employment share, and a 0.9% increase in the\npart-time workers' part-time employment share. We also find that the pandemic\nled to a 2.3% decrease in the number of part-time workers, a 2.6% decrease in\nthe part-time employment share, and a 0.3% increase in the part-time workers'\npart-time employ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.2653061224489796,
          "f": 0.15662650186311522
        },
        "rouge-2": {
          "r": 0.03067484662576687,
          "p": 0.06493506493506493,
          "f": 0.041666662308681005
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.24489795918367346,
          "f": 0.14457830909203087
        }
      }
    },
    {
      "paper_id": "math.FA.math/FA/2503.08590v1",
      "true_abstract": "In this paper we study the essential spectra of the Toeplitz operator on the\nHardy space $H^1$. We give a counterexample to show that the Toeplitz operator\nwith symbol is not Fredholm, which gives a counterexample to the conjecture by\nJ.A. Virtanen J A in 2006.",
      "generated_abstract": "We give a characterization of all finite groups whose order divides the\ncharacteristic of the field. In particular, we show that there are no finite\ngroups of order $4$ or $7$ that are not cyclic. We also show that the\nautomorphism group of a finite group is finite if and only if it contains\nnon-abelian finite groups. Finally, we show that there are no finite groups\nof order $10$ that are not cyclic.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3055555555555556,
          "p": 0.28205128205128205,
          "f": 0.2933333283413334
        },
        "rouge-2": {
          "r": 0.12195121951219512,
          "p": 0.08771929824561403,
          "f": 0.10204081145980864
        },
        "rouge-l": {
          "r": 0.2777777777777778,
          "p": 0.2564102564102564,
          "f": 0.26666666167466674
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.07940v1",
      "true_abstract": "Recent advances in deep learning-based point cloud registration have improved\ngeneralization, yet most methods still require retraining or manual parameter\ntuning for each new environment. In this paper, we identify three key factors\nlimiting generalization: (a) reliance on environment-specific voxel size and\nsearch radius, (b) poor out-of-domain robustness of learning-based keypoint\ndetectors, and (c) raw coordinate usage, which exacerbates scale discrepancies.\nTo address these issues, we present a zero-shot registration pipeline called\nBUFFER-X by (a) adaptively determining voxel size/search radii, (b) using\nfarthest point sampling to bypass learned detectors, and (c) leveraging\npatch-wise scale normalization for consistent coordinate bounds. In particular,\nwe present a multi-scale patch-based descriptor generation and a hierarchical\ninlier search across scales to improve robustness in diverse scenes. We also\npropose a novel generalizability benchmark using 11 datasets that cover various\nindoor/outdoor scenarios and sensor modalities, demonstrating that BUFFER-X\nachieves substantial generalization without prior information or manual\nparameter tuning for the test datasets. Our code is available at\nhttps://github.com/MIT-SPARK/BUFFER-X.",
      "generated_abstract": "Accurate and efficient 3D object detection and instance segmentation are\nrequired for real-time human pose estimation. Current approaches rely on\nlarge-scale pre-trained models to extract global features, which are then used\nto localize and segment humans in 3D. However, this approach often struggles to\naccurately localize and segment humans in complex environments such as crowded\nindoor scenes. In this paper, we propose a new framework for 3D human pose\nestimation and instance segmentation in challenging indoor scenarios. Our\nframework first extracts 3D human bounding boxes using a pre-trained\n3D-ResNeXt-18-FPN model. Then, we leverage the features extracted from the\n3D human bounding boxes to localize and segment humans in 3D. Our method\nconsists of two key components: 1) feature refinement module for feature",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.25316455696202533,
          "f": 0.19323671025601544
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.038834951456310676,
          "f": 0.03112839986616072
        },
        "rouge-l": {
          "r": 0.140625,
          "p": 0.22784810126582278,
          "f": 0.1739130387584309
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PR/2409.06289v1",
      "true_abstract": "Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.",
      "generated_abstract": "The current economic crisis and its impact on the financial sector have\ndemonstrated the need to enhance the resilience of financial institutions.\nThis paper introduces a model to predict the impact of macroeconomic shocks on\nfinancial institutions. The model is based on the use of a set of factors\nextracted from the financial sector, as well as on the analysis of historical\ndata, and includes several methods of forecasting and evaluating the\nperformance of the model. The results show that the model has a high accuracy\nand is capable of predicting the impact of macroeconomic shocks on financial\ninstitutions, offering a useful tool for the financial sector to better\nmanage risks and respond to the current economic crisis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12121212121212122,
          "p": 0.31746031746031744,
          "f": 0.1754385924919207
        },
        "rouge-2": {
          "r": 0.0211864406779661,
          "p": 0.049019607843137254,
          "f": 0.029585794602430493
        },
        "rouge-l": {
          "r": 0.09696969696969697,
          "p": 0.25396825396825395,
          "f": 0.1403508731936751
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2308.13877v2",
      "true_abstract": "In the modern world, technology is at its peak. Different avenues in\nprogramming and technology have been explored for data analysis, automation,\nand robotics. Machine learning is key to optimize data analysis, make accurate\npredictions, and hasten/improve existing functions. Thus, presently, the field\nof machine learning in artificial intelligence is being developed and its uses\nin varying fields are being explored. One field in which its uses stand out is\nthat of microbial biosynthesis. In this paper, a comprehensive overview of the\ndiffering machine learning programs used in biosynthesis is provided, alongside\nbrief descriptions of the fields of machine learning and microbial biosynthesis\nseparately. This information includes past trends, modern developments, future\nimprovements, explanations of processes, and current problems they face. Thus,\nthis paper's main contribution is to distill developments in, and provide a\nholistic explanation of, 2 key fields and their applicability to improve\nindustry/research. It also highlights challenges and research directions,\nacting to instigate more research and development in the growing fields.\nFinally, the paper aims to act as a reference for academics performing\nresearch, industry professionals improving their processes, and students\nlooking to understand the concept of machine learning in biosynthesis.",
      "generated_abstract": "The development of cellular regeneration mechanisms is a critical aspect of\ncellular renewal. This paper presents a framework for studying cellular\nregeneration in a single-cell model where cell division is driven by\nreprogramming. By using the framework of cellular regeneration, we address the\nquestion of how to control cell division to achieve desired outcomes. Our\nframework introduces a number of new notions, including the notion of\n``reprogramming efficiency'', which allows us to quantify the effectiveness of\nreprogramming. Furthermore, we introduce a novel approach for analyzing the\neffect of reprogramming efficiency on the outcomes of cell division. The\nframework is implemented in the cellular regeneration software package\nRegenCELL, which allows the user to explore different regeneration strategies\nin real-time. Our results demonstrate that, under certain conditions, a\ncellular regeneration strategy that achieves high re",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09836065573770492,
          "p": 0.14814814814814814,
          "f": 0.11822659618918216
        },
        "rouge-2": {
          "r": 0.0055248618784530384,
          "p": 0.008130081300813009,
          "f": 0.006578942550427697
        },
        "rouge-l": {
          "r": 0.09836065573770492,
          "p": 0.14814814814814814,
          "f": 0.11822659618918216
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.19213v1",
      "true_abstract": "We consider an optimal investment-consumption problem for a\nutility-maximizing investor who has access to assets with different liquidity\nand whose consumption rate as well as terminal wealth are subject to\nlower-bound constraints. Assuming utility functions that satisfy standard\nconditions, we develop a methodology for deriving the optimal strategies in\nsemi-closed form. Our methodology is based on the generalized martingale\napproach and the decomposition of the problem into subproblems. We illustrate\nour approach by deriving explicit formulas for agents with power-utility\nfunctions and discuss potential extensions of the proposed framework.",
      "generated_abstract": "This paper considers the optimal investment problem under the uncorrelated\ntradable risk premium model. In this model, the investor can invest in tradable\nassets with the same risk premium, or choose to invest in a fixed portfolio of\nassets with a risk-free interest rate. We derive the optimal investment policy\nwithin a two-asset framework. We show that the optimal investment strategy\ndepends on the market volatility and the risk-free interest rate, and we\nanalyze the impact of the market volatility and the risk-free interest rate on\nthe optimal investment strategy. Furthermore, we consider the case where the\ninvestor has to trade the tradable risk premium with the fixed portfolio of\nassets. We derive the optimal trade strategy within a two-asset framework. We\nalso show that the optimal trade strategy depends on the market volatility and\nthe risk-free interest rate,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2753623188405797,
          "p": 0.3220338983050847,
          "f": 0.2968749950305177
        },
        "rouge-2": {
          "r": 0.05747126436781609,
          "p": 0.054945054945054944,
          "f": 0.05617977028342425
        },
        "rouge-l": {
          "r": 0.2753623188405797,
          "p": 0.3220338983050847,
          "f": 0.2968749950305177
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.08342v1",
      "true_abstract": "We consider an economic environment where a seller wants to sell an\nindivisible unit of good to a buyer. We show that revenue from any\nstrategy-proof and individually rational mechanism defined on closed intervals\nof rich single crossing domains considered in \\citep{Goswami1}, can be\napproximated by the revenue from a sequence of strategy-proof and individually\nrational mechanisms with finite range. Thus while studying optimal mechanisms\nwithout loss of generality we can study mechanisms with finite range.",
      "generated_abstract": "This paper addresses the problem of allocating resources to multiple tasks in\nthe presence of inter-task dependencies. In particular, we consider a\nmulti-armed bandit setting where each task has a unique arm, and the\navailable resources can be allocated to multiple tasks simultaneously. We\nintroduce a new notion of \"multi-armed bandit with inter-task dependencies\"\n(MABID), which generalizes the multi-armed bandit with inter-arm dependencies\n(MABID) problem. In this new setting, we allow the tasks to have different\narms, and we allow the resources to be allocated to multiple tasks at the same\ntime. We propose an algorithm for the MABID problem, which we call\n\"Multi-armed Bandit with Inter-task Dependencies\". We also introduce the\nmulti-armed bandit with inter-task dependencies with priority allocation\n(MABIDP), which extends the M",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.1917808219178082,
          "f": 0.2170542586527253
        },
        "rouge-2": {
          "r": 0.014705882352941176,
          "p": 0.009259259259259259,
          "f": 0.011363631621902803
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.1917808219178082,
          "f": 0.2170542586527253
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.10330v1",
      "true_abstract": "Motivated by the appearance of Majorana fermions in a broad range of\ncorrelated and topological electronic systems, we develop a general method to\ncompute the dynamical response of interacting Majorana fermions in the\nrandom-phase approximation (RPA). This can be applied self-consistently on top\nof Majorana mean-field theory (MFT) backgrounds, thereby in particular\nproviding a powerful tool to analyse $\\textit{generic}$ behaviour in the\nvicinity of (various heavily studied) exactly soluble models. Prime examples\nare quantum spin liquids (QSL) with emergent Majorana excitations, with the\ncelebrated exact solution of Kitaev. We employ the RPA to study in considerable\ndetail phase structure and dynamics of the extended Kitaev honeycomb\n$KJ\\Gamma$-model, with and without an applied field. First, we benchmark our\nmethod with Kitaev's exactly soluble model, finding a remarkable agreement. The\ninteractions between Majorana fermions even turn out to mimic the effect of\nlocal $\\mathbb{Z}_2$ flux excitations, which we explain analytically. Second,\nwe show how small non-Kitaev couplings $J$ and $\\Gamma$ induce Majorana bound\nstates, resulting in sharp features in the dynamical structure factor in the\npresence of fractionalisation: such 'spinon excitons' naturally appear, and can\ncoexist and interact with the broad Majorana continuum. Third, for increasing\ncouplings or field, our theory predicts instabilities of the KQSL triggered by\nthe condensation of the sharp modes. From the high symmetry momenta of the\ncondensation we can deduce which magnetically ordered phases surround the KQSL,\nin good agreement with previous finite-size numerics. We discuss implications\nfor experiments and the broad range of applicability of our method to other QSL\nand Majorana systems.",
      "generated_abstract": "The quantum spin Hall effect (QSHE) is a novel physics phenomenon that\nintroduces a spin-orbit coupling to the electronic band structure of a\ntwo-dimensional (2D) system, resulting in a non-vanishing Hall conductance.\nRecently, it was shown that the QSHE can be controlled by a magnetic field\nalong the 2D plane, which is a very general feature of the Kitaev model. Here,\nwe demonstrate that the QSHE can also be switched on and off by controlling the\nangle between the magnetic field and the 2D plane. This feature is similar to\nthe Kitaev model, and it can be used for controlling the QSHE through\nmagnetic-field-induced spin-orbit coupling. We also propose a method to\ngenerate spin-orbit-induced magnetic fields through a magnetic-field-induced\nrotation of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1728395061728395,
          "p": 0.3783783783783784,
          "f": 0.2372881312884229
        },
        "rouge-2": {
          "r": 0.029661016949152543,
          "p": 0.06481481481481481,
          "f": 0.0406976701108712
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.32432432432432434,
          "f": 0.20338982620367718
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/CP/2501.05975v1",
      "true_abstract": "In energy markets, joint historical and implied calibration is of paramount\nimportance for practitioners yet notoriously challenging due to the need to\nalign historical correlations of futures contracts with implied volatility\nsmiles from the option market. We address this crucial problem with a\nparsimonious multiplicative multi-factor Heath-Jarrow-Morton (HJM) model for\nforward curves, combined with a stochastic volatility factor coming from the\nLifted Heston model. We develop a sequential fast calibration procedure\nleveraging the Kemna-Vorst approximation of futures contracts: (i) historical\ncorrelations and the Variance Swap (VS) volatility term structure are captured\nthrough Level, Slope, and Curvature factors, (ii) the VS volatility term\nstructure can then be corrected for a perfect match via a fixed-point\nalgorithm, (iii) implied volatility smiles are calibrated using Fourier-based\ntechniques. Our model displays remarkable joint historical and implied\ncalibration fits - to both German power and TTF gas markets - and enables\nrealistic interpolation within the implied volatility hypercube.",
      "generated_abstract": "We propose a new class of pricing and hedging models for American options\nwith correlated risks, which we call correlated multiple-asset options. This\nclass extends the class of correlated multiple-asset options by incorporating\nrisk-correlated correlations in the underlying asset price. We develop a\nstochastic linear-quadratic optimal control (SLQOC) approach to the pricing\nproblem. We show that the optimal control problem can be solved efficiently\nusing a two-level stochastic programming approach. We then derive the\ncorresponding hedging strategies using the SLQOC. We test our approach on a\nvariety of simulated and real-world data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.26229508196721313,
          "f": 0.19393938927897167
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.04597701149425287,
          "f": 0.03539822535319979
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.26229508196721313,
          "f": 0.19393938927897167
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10472v1",
      "true_abstract": "In this letter, we propose to deploy rotatable antennas (RAs) at the base\nstation (BS) to enhance both communication and sensing (C&S) performances, by\nexploiting a new spatial degree-of-freedom (DoF) offered by array rotation.\nSpecifically, we formulate a multi-objective optimization problem to\nsimultaneously maximize the sum-rate of multiple communication users and\nminimize the Cram\\'er-Rao bound (CRB) for target angle estimation, by jointly\noptimizing the transmit beamforming vectors and the array rotation angle at the\nBS. To solve this problem, we first equivalently decompose it into two\nsubproblems, corresponding to an inner problem for beamforming optimization and\nan outer problem for array rotation optimization. Although these two\nsubproblems are non-convex, we obtain their high-quality solutions by applying\nthe block coordinate descent (BCD) technique and one-dimensional exhaustive\nsearch, respectively. Moreover, we show that for the communication-only case,\nRAs provide an additional rotation gain to improve communication performance;\nwhile for the sensing-only case, the equivalent spatial aperture can be\nenlarged by RAs for achieving higher sensing accuracy. Finally, numerical\nresults are presented to showcase the performance gains of RAs over\nfixed-rotation antennas in integrated sensing and communications (ISAC).",
      "generated_abstract": "The development of intelligent systems relies on the combination of human\nexpertise and machine learning, which is a powerful combination for a wide\nvariety of tasks. However, the emergence of such systems often requires\nsystematic and reliable assessments of their capabilities. This paper\nintroduces a novel approach for such assessments, which combines a\nhuman-in-the-loop process with the use of AI-based assessment models. This\napproach enables the system designer to gain a more thorough understanding of\nthe system's capabilities, which is important for ensuring the system's\neffectiveness and achieving its intended purpose. This study investigates the\neffectiveness of this approach in the context of speech recognition, and it\nfocuses on the combination of human and AI-based assessment models. To\ndemonstrate the effectiveness of the proposed approach, a series of experiments\nwas conducted using a speech recognition system",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10483870967741936,
          "p": 0.15853658536585366,
          "f": 0.1262135874408523
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08870967741935484,
          "p": 0.13414634146341464,
          "f": 0.106796111712697
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.cond-mat/soft/2503.10261v1",
      "true_abstract": "Flow birefringence measurement is an emerging technique for visualizing\nstress fields in fluid flows. This study investigates flow birefringence in the\nsteady radial Hele-Shaw flow of a shear-thinning fluid. In this flow\nconfiguration, stress is dominant along the optical axis, challenging the\napplicability of the conventional stress-optic law (SOL). We conduct flow\nbirefringence measurements at various flow rates and compare the results with\ntheoretical predictions. The observed phase retardation cannot be\nquantitatively explained using the conventional SOL, but is successfully\ndescribed using the second-order SOL, which accounts for stress along the\noptical direction, using the results of rheo-optical measurements. Furthermore,\nwe investigate the shear-thinning effects on phase retardation from two\nperspectives: (i) stress changes resulting from viscosity variations and (ii)\nthe variation of the shear-dependent stress-optic coefficient in the\nsecond-order SOL. Our findings indicate that the latter is more significant and\nshear-thinning behavior suppresses radial variations in phase retardation. This\nstudy demonstrates that the combination of the second-order SOL and\nrheo-optical measurements is essential for an accurate interpretation of flow\nbirefringence in Hele-Shaw flow, providing a noninvasive approach for stress\nfield analysis in high-aspect-ratio geometries.",
      "generated_abstract": "We present a novel model for the description of the mechanical response of\ncomplex fluids, built upon the concept of a generalized elasticity theory. The\nmodel consists of a family of elasticity theories, each of which can be\ncharacterized by a single parameter. The family of models, which we refer to as\nthe generalized elasticity theory, includes all of the classical elasticity\ntheories, and is the generalization of the elasticity theory for\nsolid-liquid interfaces. We demonstrate that the generalized elasticity theory\ncan account for a wide range of mechanical behaviors observed in complex\nfluids, including the following: (i) the presence of a viscoelastic response in\nthe presence of a solid-liquid interface, (ii) the existence of a nonlinear\nelasticity regime below a critical shear stress, (iii) the existence of a\nnon-trivial pressure-dependent shear mod",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.22535211267605634,
          "f": 0.17877094493430304
        },
        "rouge-2": {
          "r": 0.03067484662576687,
          "p": 0.04424778761061947,
          "f": 0.03623187922206533
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.19718309859154928,
          "f": 0.15642457621921926
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.06743v2",
      "true_abstract": "Structural changes in main retinal blood vessels serve as critical biomarkers\nfor the onset and progression of glaucoma. Identifying these vessels is vital\nfor vascular modeling yet highly challenging. This paper proposes X-GAN, a\ngenerative AI-powered unsupervised segmentation model designed for extracting\nmain blood vessels from Optical Coherence Tomography Angiography (OCTA) images.\nThe process begins with the Space Colonization Algorithm (SCA) to rapidly\ngenerate a skeleton of vessels, featuring their radii. By synergistically\nintegrating generative adversarial networks (GANs) with biostatistical modeling\nof vessel radii, X-GAN enables a fast reconstruction of both 2D and 3D\nrepresentations of the vessels. Based on this reconstruction, X-GAN achieves\nnearly 100\\% segmentation accuracy without relying on labeled data or\nhigh-performance computing resources. Also, to address the Issue, data scarity,\nwe introduce GSS-RetVein, a high-definition mixed 2D and 3D glaucoma retinal\ndataset. GSS-RetVein provides a rigorous benchmark due to its exceptionally\nclear capillary structures, introducing controlled noise for testing model\nrobustness. Its 2D images feature sharp capillary boundaries, while its 3D\ncomponent enhances vascular reconstruction and blood flow prediction,\nsupporting glaucoma progression simulations. Experimental results confirm\nGSS-RetVein's superiority in evaluating main vessel segmentation compared to\nexisting datasets. Code and dataset are here:\nhttps://github.com/VikiXie/SatMar8.",
      "generated_abstract": "The application of deep learning (DL) in medical image analysis (MIA)\nhas led to significant improvements in many areas of MIA, including segmentation\nand detection. However, the use of DL in segmentation poses significant\nchallenges, particularly in the context of brain imaging, where the small\nnumber of voxels and the complexity of the anatomical structures make it\ndifficult to achieve accurate segmentation. To address these challenges, we\npropose a novel approach to segment the brain tissue in CT scans using DL. The\napproach involves two stages: (1) an initial stage to enhance the signal-to-noise\nratio (SNR) of the image, followed by (2) a second stage to enhance the\nsegmentation accuracy by improving the feature extraction and learning\nalgorithms. In the first stage, we apply a low-pass filter to remove noise and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09655172413793103,
          "p": 0.16091954022988506,
          "f": 0.12068965048491398
        },
        "rouge-2": {
          "r": 0.010309278350515464,
          "p": 0.015873015873015872,
          "f": 0.012499995225783073
        },
        "rouge-l": {
          "r": 0.09655172413793103,
          "p": 0.16091954022988506,
          "f": 0.12068965048491398
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.08443v1",
      "true_abstract": "The use of valid surrogate endpoints is an important stake in clinical\nresearch to help reduce both the duration and cost of a clinical trial and\nspeed up the evaluation of interesting treatments. Several methods have been\nproposed in the statistical literature to validate putative surrogate\nendpoints. Two main approaches have been proposed: the meta-analytic approach\nand the mediation analysis approach. The former uses data from meta-analyses to\nderive associations measures between the surrogate and the final endpoint at\nthe individual and trial levels. The latter rather uses the proportion of the\ntreatment effect on the final endpoint through the surrogate as a measure of\nsurrogacy in a causal inference framework. Both approaches have remained\nseparated as the meta-analytic approach does not estimate the treatment effect\non the final endpoint through the surrogate while the mediation analysis\napproach have been limited to single-trial setting. However, these two\napproaches are complementary. In this work we propose an approach that combines\nthe meta-analytic and mediation analysis approaches using joint modeling for\nsurrogate validation. We focus on the cases where the final endpoint is a\ntime-to-event endpoint (such as time-to-death) and the surrogate is either a\ntime-to-event or a longitudinal biomarker. Two new joint models were proposed\ndepending on the nature of the surrogate. These model are implemented in the R\npackage frailtypack. We illustrate the developed approaches in three\napplications on real datasets in oncology.",
      "generated_abstract": "We propose a novel approach to analyze the spatial structure of\nlarge-scale time series data. Our method combines the Bayesian hierarchical\nmodel with the generalized graphical lasso, a popular nonparametric approach to\nmodeling high-dimensional graphical structures. This approach allows us to\ncapture spatial autocorrelation in a unified framework that incorporates both\nlinear and nonlinear trends. We develop an efficient Bayesian inference\nframework that utilizes a novel mixture-of-Gaussian prior to account for\nintermediate levels of the graph structure. We apply our method to analyze the\nspatial structure of the daily temperatures of 107 cities in 20 countries,\ncovering the period from 1979 to 2023. Our results suggest that cities with\nhigher temperatures have stronger interactions among themselves, and these\ninteractions are more pronounced in summer than in winter. These find",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1953125,
          "p": 0.28735632183908044,
          "f": 0.23255813471671183
        },
        "rouge-2": {
          "r": 0.020202020202020204,
          "p": 0.03361344537815126,
          "f": 0.02523658837046928
        },
        "rouge-l": {
          "r": 0.171875,
          "p": 0.25287356321839083,
          "f": 0.2046511579725258
        }
      }
    },
    {
      "paper_id": "cs.HC.stat/OT/2407.14072v2",
      "true_abstract": "Psychological research often involves understanding psychological constructs\nthrough conducting factor analysis on data collected by a questionnaire, which\ncan comprise hundreds of questions. Without interactive systems for\ninterpreting factor models, researchers are frequently exposed to subjectivity,\npotentially leading to misinterpretations or overlooked crucial information.\nThis paper introduces FAVis, a novel interactive visualization tool designed to\naid researchers in interpreting and evaluating factor analysis results. FAVis\nenhances the understanding of relationships between variables and factors by\nsupporting multiple views for visualizing factor loadings and correlations,\nallowing users to analyze information from various perspectives. The primary\nfeature of FAVis is to enable users to set optimal thresholds for factor\nloadings to balance clarity and information retention. FAVis also allows users\nto assign tags to variables, enhancing the understanding of factors by linking\nthem to their associated psychological constructs. Our user study demonstrates\nthe utility of FAVis in various tasks.",
      "generated_abstract": "The growing availability of high-throughput data, coupled with advances in\ncomputational methods, has enabled the discovery of novel genetic variants\nthat contribute to complex diseases. However, despite these advances,\nappropriate interpretation of these data remains challenging. To address this,\nwe present a novel framework for evaluating and interpreting complex genetic\nassociation studies, focusing on the analysis of complex diseases. Our\nframework, named GENE-CANNABIS, consists of two core components: a\ncomputational-based method for interpreting complex genetic association\nstudies, and a graph-based method for visualizing and analyzing the results.\nGENE-CANNABIS provides a framework for interpreting the results of genetic\nassociation studies, offering insights into the underlying mechanisms of\ndisease-related genetic variants and identifying promising avenues for future",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16831683168316833,
          "p": 0.22666666666666666,
          "f": 0.19318181329093506
        },
        "rouge-2": {
          "r": 0.021897810218978103,
          "p": 0.029411764705882353,
          "f": 0.025104597617689718
        },
        "rouge-l": {
          "r": 0.1485148514851485,
          "p": 0.2,
          "f": 0.17045454056366233
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/CR/2503.10171v1",
      "true_abstract": "Graph databases have garnered extensive attention and research due to their\nability to manage relationships between entities efficiently. Today, many graph\nsearch services have been outsourced to a third-party server to facilitate\nstorage and computational support. Nevertheless, the outsourcing paradigm may\ninvade the privacy of graphs. PeGraph is the latest scheme achieving encrypted\nsearch over social graphs to address the privacy leakage, which maintains two\ndata structures XSet and TSet motivated by the OXT technology to support\nencrypted conjunctive search. However, PeGraph still exhibits limitations\ninherent to the underlying OXT. It does not provide transparent search\ncapabilities, suffers from expensive computation and result pattern leakages,\nand it fails to support search over dynamic encrypted graph database and\nresults verification. In this paper, we propose SecGraph to address the first\ntwo limitations, which adopts a novel system architecture that leverages an\nSGX-enabled cloud server to provide users with secure and transparent search\nservices since the secret key protection and computational overhead have been\noffloaded to the cloud server. Besides, we design an LDCF-encoded XSet based on\nthe Logarithmic Dynamic Cuckoo Filter to facilitate efficient plaintext\ncomputation in trusted memory, effectively mitigating the risks of result\npattern leakage and performance degradation due to exceeding the limited\ntrusted memory capacity. Finally, we design a new dynamic version of TSet named\nTwin-TSet to enable conjunctive search over dynamic encrypted graph database.\nIn order to support verifiable search, we further propose VSecGraph, which\nutilizes a procedure-oriented verification method to verify all data structures\nloaded into the trusted memory, thus bypassing the computational overhead\nassociated with the client's local verification.",
      "generated_abstract": "Recent advancements in large language models (LLMs) have enabled\nadvancements in natural language processing (NLP) and led to the emergence of\nrobust, unsupervised, and large-scale models. However, the lack of access to\nexpert-annotated data, coupled with the need for high-quality data to train\nthese models, limits their practical utility. In this work, we propose a\nreinforcement learning (RL)-based approach for model selection in large\nlanguage models, leveraging large-scale, expert-annotated data to address\nchallenges in data selection and model selection. Specifically, we introduce\nthe concept of a model selection policy, which is designed to maximize model\naccuracy under a given budget while considering the data quality of the model\nand its potential to achieve further improvements. The proposed method\nenables the training of large language models that are tailored to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13125,
          "p": 0.25301204819277107,
          "f": 0.1728395016748803
        },
        "rouge-2": {
          "r": 0.02109704641350211,
          "p": 0.041666666666666664,
          "f": 0.028011200018831776
        },
        "rouge-l": {
          "r": 0.10625,
          "p": 0.20481927710843373,
          "f": 0.13991769097529183
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.08458v1",
      "true_abstract": "The information criterion AIC has been used successfully in many areas of\nstatistical modeling, and since it is derived based on the Taylor expansion of\nthe log-likelihood function and the asymptotic distribution of the maximum\nlikelihood estimator, it is not directly justified for likelihood functions\nthat include non-differentiable points such as the Laplace distribution. In\nfact, it is known to work effectively in many such cases. In this paper, we\nattempt to evaluate the bias correction directly for the case where the true\nmodel or the model to be estimated is a simple Laplace distribution model. As a\nresult, an approximate expression for the bias correction term was obtained.\nNumerical results show that the AIC approximations are relatively good except\nwhen the Gauss distribution model is fitted to data following the Laplace\ndistribution.",
      "generated_abstract": "The development of modern statistical methods and machine learning models\nleads to the need for a more precise and transparent understanding of the\nstatistical performance of the models. In this paper, we propose a novel\nstatistical framework, based on the Wasserstein distance, to analyze the\nstatistical performance of the models, and to derive more informative and\nsensitive error bounds for the model parameters and their distributions. Our\nmethodology enables to better understand the model performance by comparing\nthe Wasserstein distance with the empirical distribution. We apply our method to\ntwo specific problems: the Bayesian inference and the prediction of a\nmulti-class classification problem. The results show that our framework\nprovides more precise and reliable error bounds for the model parameters and\ndistributions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21839080459770116,
          "p": 0.2602739726027397,
          "f": 0.23749999503828137
        },
        "rouge-2": {
          "r": 0.09090909090909091,
          "p": 0.10377358490566038,
          "f": 0.096916294581304
        },
        "rouge-l": {
          "r": 0.21839080459770116,
          "p": 0.2602739726027397,
          "f": 0.23749999503828137
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.20067v1",
      "true_abstract": "The emergence of audio language models is empowered by neural audio codecs,\nwhich establish critical mappings between continuous waveforms and discrete\ntokens compatible with language model paradigms. The evolutionary trends from\nmulti-layer residual vector quantizer to single-layer quantizer are beneficial\nfor language-autoregressive decoding. However, the capability to handle\nmulti-domain audio signals through a single codebook remains constrained by\ninter-domain distribution discrepancies. In this work, we introduce UniCodec, a\nunified audio codec with a single codebook to support multi-domain audio data,\nincluding speech, music, and sound. To achieve this, we propose a partitioned\ndomain-adaptive codebook method and domain Mixture-of-Experts strategy to\ncapture the distinct characteristics of each audio domain. Furthermore, to\nenrich the semantic density of the codec without auxiliary modules, we propose\na self-supervised mask prediction modeling approach. Comprehensive objective\nand subjective evaluations demonstrate that UniCodec achieves excellent audio\nreconstruction performance across the three audio domains, outperforming\nexisting unified neural codecs with a single codebook, and even surpasses\nstate-of-the-art domain-specific codecs on both acoustic and semantic\nrepresentation capabilities.",
      "generated_abstract": "This paper addresses the challenge of estimating the time-varying interference\n(TI) at the receiver due to multiple antenna systems, including the case where\nthe TI is correlated between antennas. We consider a two-user, multiple-input\nsingle-output (MISO) system with a single-antenna transmitter and two\nantennas at the receiver, each serving one user. The interference from the\nother user's transmissions is assumed to be uncorrelated, and the TI is modeled\nas a random variable that depends on the transmitter's signal and the\nreceiver's antenna configuration. We derive a closed-form solution for the\nestimated TI, and propose a novel approach to estimate the TI through a\ncombination of a linear regression of the TI at the receiver and the\ncorresponding signal at the transmitter. Numerical simulations demonstrate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.2375,
          "f": 0.18905472157619874
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.03508771929824561,
          "f": 0.029197075432895476
        },
        "rouge-l": {
          "r": 0.1487603305785124,
          "p": 0.225,
          "f": 0.17910447281997982
        }
      }
    },
    {
      "paper_id": "math.AC.math/RA/2503.07271v1",
      "true_abstract": "A module $M$ is said to be stable if it has no nonzero projective direct\nsummand. For a ring $ R $, we study the conditions under which every $R$-module\n$M$ within a specific class can be decomposed into a direct sum of a projective\nmodule and a stable module, focusing on identifying the types of rings and the\nclass of $R$-modules where this property holds. Some well-known classes of\nrings over which every finitely presented module can be decomposed into a\ndirect sum of a projective submodule and a stable submodule are semiperfect\nrings or semilocal rings or rings satisfying some finiteness conditions like\nhaving finite uniform dimension or hollow dimension or being Noetherian or\nArtinian. By using the Auslander-Bridger transpose of modules, we prove that\nevery finitely presented right $R$-module over a left semihereditary ring $R$\nhas such a decomposition; note that the semihereditary condition is on the\nopposite side. Our main focus in this article is to give examples where such a\ndecomposition fails. We give some ring examples over which there exists an\ninfinitely generated or finitely generated or cyclic module or finitely\npresented module or cyclically presented module where such a decomposition\nfails. Our main example is a cyclically presented module $M$ over a commutative\nring such that~$M$ has no such decomposition and $M$ is not projectively\nequivalent to a stable module.",
      "generated_abstract": "In this paper, we propose a new method for constructing finite fields from\ngiven sets of integers. This method uses a new class of finite fields, called\nGalois fields, and a new type of functions, called Galois-mapped functions,\nwhich are functions defined by a mapping between two sets of integers. The\nconstruction of Galois fields from given sets of integers is done by\nconstructing a Galois-mapped function between the given sets of integers and\nthen constructing the Galois field using the mapping function. We give some\nexamples of Galois fields and Galois-mapped functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1415929203539823,
          "p": 0.3333333333333333,
          "f": 0.19875775979013166
        },
        "rouge-2": {
          "r": 0.026455026455026454,
          "p": 0.0641025641025641,
          "f": 0.03745317938475828
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.3125,
          "f": 0.18633539954168446
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.05880v1",
      "true_abstract": "Likelihood inference for max-stable random fields is in general impossible\nbecause their finite-dimensional probability density functions are unknown or\ncannot be computed efficiently. The weighted composite likelihood approach that\nutilizes lower dimensional marginal likelihoods (typically pairs or triples of\nsites that are not too distant) is rather favored. In this paper, we consider\nthe family of spatial max-stable Brown-Resnick random fields associated with\nisotropic fractional Brownian fields. We assume that the sites are given by\nonly one realization of a homogeneous Poisson point process restricted to\n$\\mathbf{C}=(-1/2,1/2]^{2}$ and that the random field is observed at these\nsites. As the intensity increases, we study the asymptotic properties of the\ncomposite likelihood estimators of the scale and Hurst parameters of the\nfractional Brownian fields using different weighting strategies: we exclude\neither pairs that are not edges of the Delaunay triangulation or triples that\nare not vertices of triangles.",
      "generated_abstract": "We propose a novel algorithm for the estimation of the mean of a Poisson\nprocess conditioned on a certain event. The proposed algorithm combines a\nfinite-dimensional version of the Stein-Chen estimator with a modified\ninformation-theoretic bound on the conditional distribution of the mean, which\nis constructed using a novel entropy-based bound on the conditional variance.\nThe estimator is shown to be consistent, asymptotically normal and\nstrongly-consistent. The method is applied to the estimation of the mean of\nBrownian motions conditioned on hitting a certain set of points in a\ndeterministic time-interval. We also show that the proposed method can be\nused to estimate the mean of a Poisson process conditioned on hitting a set of\npoints in a random time-interval.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1919191919191919,
          "p": 0.3114754098360656,
          "f": 0.2374999952820313
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.03225806451612903,
          "f": 0.02666666181688977
        },
        "rouge-l": {
          "r": 0.1919191919191919,
          "p": 0.3114754098360656,
          "f": 0.2374999952820313
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2502.18947v1",
      "true_abstract": "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
      "generated_abstract": "The molecular basis of adaptation has long been debated. Here, we present\nan experimental framework to test the hypothesis that molecular adaptation is\nfundamentally linked to changes in protein function, as opposed to changes in\nthe physical or chemical properties of the proteins. We demonstrate that a\nsingle mutation can drive the evolution of a protein that is functionally\ndifferent from the wild type. This result suggests that molecular adaptation\nmay be driven by functional changes rather than by changes in physical or\nchemical properties, which would be expected to be relatively rare. To test\nthis hypothesis, we use the recently introduced molecular adaptation\nmethodology (MAM), a powerful approach that integrates genomic, functional,\nand structural data. We applied this methodology to 317 proteins in two\ndifferent organisms and found that 44% of the proteins that we studied\nexhibited molecular adaptation. We also show",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.20224719101123595,
          "f": 0.15859030360301982
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.022900763358778626,
          "f": 0.01874999516425906
        },
        "rouge-l": {
          "r": 0.11594202898550725,
          "p": 0.1797752808988764,
          "f": 0.14096915822857048
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.12752v1",
      "true_abstract": "Using novel establishment-level observational data from Switzerland, we\nempirically examine whether the usage of key technologies of Industry 4.0\ndistinguishes across firms with different types of organizational culture.\nBased on the Technology-Organization-Environment and the Competing Values\nframework, we hypothesize that the developmental culture has the greatest\npotential to promote the usage of Industry 4.0 technologies. We also\nhypothesize that companies with a hierarchical or rational culture are\nespecially likely to make use of automation technologies, such as AI and\nrobotics. By means of descriptive statistics and multiple regression analysis,\nwe find empirical support for our first hypothesis, while we cannot con-firm\nour second hypothesis. Our empirical results provide important implications for\nmanagerial decision-makers. Specifically, the link between organizational\nculture and the implementation of Industry 4.0 technologies is relevant for\nmanagers, as this knowledge helps them to cope with digital transformation in\nturbulent times and keep their businesses competitive.",
      "generated_abstract": "The rise of the Internet of Things (IoT) and their impact on energy\ntransmission and distribution systems has garnered significant attention from\nresearchers, policymakers, and stakeholders. This study examines the\nimplications of IoT technologies on power system dynamics, focusing on the\nimpact of smart meters and Internet of Things (IoT) on voltage regulation and\nsmart grid operations. The study investigates the impact of IoT on voltage\nregulation and smart grid operations by analyzing power system data from the\nUS. The findings reveal that IoT-enabled smart meters and smart grid systems\nhave the potential to enhance voltage regulation and smart grid operations.\nHowever, there are potential risks associated with the adoption of IoT technologies,\nincluding data privacy, security, and ethical issues. The study offers\nrecommendations for policymakers and stake",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1588785046728972,
          "p": 0.24285714285714285,
          "f": 0.19209039069871378
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.02912621359223301,
          "f": 0.02489626066562311
        },
        "rouge-l": {
          "r": 0.1588785046728972,
          "p": 0.24285714285714285,
          "f": 0.19209039069871378
        }
      }
    },
    {
      "paper_id": "math-ph.math/DS/2503.08412v1",
      "true_abstract": "The article presents the concept of a cumulant representation for\ndistribution functions describing the states of many-particle systems with\ntopological nearest-neighbor interaction. A solution to the Cauchy problem for\nthe hierarchy of nonlinear evolution equations for the cumulants of\ndistribution functions of such systems is constructed. The connection between\nthe constructed solution and the series expansion structure for a solution to\nthe Cauchy problem of the BBGKY hierarchy has been established. Furthermore,\nthe expansion structure for a solution to the Cauchy problem of the hierarchy\nof evolution equations for reduced observables of topologically interacting\nparticles is established.",
      "generated_abstract": "We prove that the set of all $k$-th symmetric powers of a finite field is\nfinite. In particular, this proves the existence of infinitely many primes in\n$\\mathbb{F}_{p^k}$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08163265306122448,
          "p": 0.16666666666666666,
          "f": 0.10958903668230453
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.037037037037037035,
          "f": 0.019417471859742498
        },
        "rouge-l": {
          "r": 0.061224489795918366,
          "p": 0.125,
          "f": 0.082191776408332
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.10538v1",
      "true_abstract": "Given the power of large language and large vision models, it is of profound\nand fundamental interest to ask if a foundational model based on data and\nparameter scaling laws and pre-training strategies is possible for learned\nsimulations of chemistry and materials. The scaling of large and diverse\ndatasets and highly expressive architectures for chemical and materials\nsciences should result in a foundation model that is more efficient and broadly\ntransferable, robust to out-of-distribution challenges, and easily fine-tuned\nto a variety of downstream observables, when compared to specific training from\nscratch on targeted applications in atomistic simulation. In this Perspective\nwe aim to cover the rapidly advancing field of machine learned interatomic\npotentials (MLIP), and to illustrate a path to create chemistry and materials\nMLIP foundation models at larger scale.",
      "generated_abstract": "The quantum mechanical description of chemical reactions is a major challenge\nin chemical physics. We present a novel approach to the calculation of chemical\nreactions based on the adoption of the Born-Oppenheimer approximation. The\nBorn-Oppenheimer (BO) approximation is a quantum mechanical method to solve\nequations of motion for systems with multiple degrees of freedom. The BO\napproximation is widely used in the study of chemical reactions and has been\napplied to calculate reaction rates for a large number of chemical processes.\nHowever, it has never been applied to the study of chemical reactions in\nquantum mechanical context. We propose a novel Born-Oppenheimer (BO)\nreaction-rate formula that is based on the quantum mechanical\nHamiltonian-density-functional theory. This BO-type reaction rate formula is\ncalculated for a number of chemical processes, including the reaction of\ncarbon monoxide with oxygen",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.21739130434782608,
          "f": 0.18867924037023862
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.01818181818181818,
          "f": 0.017021271616117346
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.2028985507246377,
          "f": 0.1761006240180374
        }
      }
    },
    {
      "paper_id": "physics.app-ph.eess/SP/2503.07239v1",
      "true_abstract": "We present the \"Virtual VNA 3.0\" technique for estimating the scattering\nmatrix of a \\textit{non-reciprocal}, linear, passive, time-invariant device\nunder test (DUT) with $N$ monomodal ports using a single measurement setup\ninvolving a vector network analyzer (VNA) with only $N_\\mathrm{A}<N$ ports --\nthus eliminating the need for any reconnections. We partition the DUT ports\ninto $N_\\mathrm{A}$ \"accessible\" and $N_\\mathrm{S}$ \"not-directly-accessible\"\n(NDA) ports. We connect the accessible ports to the VNA and the NDA ports to\nthe \"virtual VNA ports\" of a VNA Extension Kit. This kit enables each NDA port\nto be terminated with three distinct individual loads or connected to\nneighboring DUT ports via coupled loads. We derive both a closed-form and a\ngradient-descent method to estimate the complete scattering matrix of the\nnon-reciprocal DUT from measurements conducted with the $N_\\mathrm{A}$-port VNA\nunder various NDA-port terminations. We validate both methods experimentally\nfor $N_\\mathrm{A}=N_\\mathrm{S}=4$, where our DUT is a complex eight-port\ntransmission-line network comprising circulators. Altogether, the presented\n\"Virtual VNA 3.0\" technique constitutes a scalable approach to unambiguously\ncharacterize a many-port \\textit{non-reciprocal} DUT with a few-port VNA (only\n$N_\\mathrm{A}>1$ is required) -- without any tedious and error-prone manual\nreconnections susceptible to inaccuracies. The VNA Extension Kit requirements\nmatch those for the \"Virtual VNA 2.0\" technique that was limited to reciprocal\nDUTs.",
      "generated_abstract": "This paper presents a novel framework for multi-target tracking of\ntargets moving in space-time. The proposed method exploits the spatio-temporal\ncorrelation between target trajectories to construct a target graph and\nformulate the tracking problem as a graph-based multi-target tracking problem.\nThis problem is solved by applying the Graph Neural Network (GNN) model, which\ncaptures the spatio-temporal correlation between target trajectories and\nefficiently models the correlations in the graph structure. The performance of\nthe proposed approach is evaluated using synthetic datasets and a real-world\ndataset. The results show that the proposed method outperforms the state-of-the-art\nalgorithms in terms of tracking accuracy, computational efficiency, and\noverall performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09848484848484848,
          "p": 0.19696969696969696,
          "f": 0.13131312686868704
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.021052631578947368,
          "f": 0.01365187275099443
        },
        "rouge-l": {
          "r": 0.09848484848484848,
          "p": 0.19696969696969696,
          "f": 0.13131312686868704
        }
      }
    },
    {
      "paper_id": "cs.IT.math/IT/2503.09991v1",
      "true_abstract": "A finite-field multiple-access (FFMA) system separates users within a finite\nfield by utilizing different element-pairs (EPs) as virtual resources. The\nCartesian product of distinct EPs forms an EP code, which serves as the input\nto a finite-field multiplexing module (FF-MUX), allowing the FFMA technique to\ninterchange the order of channel coding and multiplexing. This flexibility\nenables the FFMA system to support a large number of users with short packet\ntraffic, addressing the finite block length (FBL) problem in multiuser reliable\ntransmission. Designing EP codes is a central challenge in FFMA systems. In\nthis paper, we construct EP codes based on a bit(s)-to-codeword transformation\napproach and define the corresponding EP code as a codeword-wise EP (CWEP)\ncode. We then investigate the encoding process of EP codes, and propose unique\nsum-pattern mapping (USPM) structural property constraints to design uniquely\ndecodable CWEP codes. Next, we present the \\(\\kappa\\)-fold ternary orthogonal\nmatrix \\({\\bf T}_{\\rm o}(2^{\\kappa}, 2^{\\kappa})\\) over GF\\((3^m)\\), where \\(m\n= 2^{\\kappa}\\), and the ternary non-orthogonal matrix \\({\\bf T}_{\\rm no}(3,2)\\)\nover GF\\((3^2)\\), for constructing specific CWEP codes. Based on the proposed\nCWEP codes, we introduce three FFMA modes: channel codeword multiple access\n(FF-CCMA), code division multiple access (FF-CDMA), and non-orthogonal multiple\naccess (FF-NOMA). Simulation results demonstrate that all three modes\neffectively support massive user transmissions with strong error performance.",
      "generated_abstract": "In this paper, we present a novel method to determine the optimal number of\ngroups in a group synchronization network. Our method utilizes the Lagrange\nmultiplier technique to determine the optimal number of groups for a synchronized\nsystem in a given synchronization topology. We demonstrate the effectiveness of\nthe proposed method in simulated and real-world synchronized systems.\nAdditionally, we compare the proposed method with existing group synchronization\nmethods and demonstrate the superior performance of the proposed method in\ngroup synchronization networks. The source code is available at\nhttps://github.com/Rafiq-Ahmad/Group-Synchronization-Network-Optimization.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.4528301886792453,
          "f": 0.23645319811206295
        },
        "rouge-2": {
          "r": 0.033816425120772944,
          "p": 0.09722222222222222,
          "f": 0.05017920764018991
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.4528301886792453,
          "f": 0.23645319811206295
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2407.00022v1",
      "true_abstract": "Entropy is a very useful concept from physics that tries to explain how a\nsystem behaves from a point of view of the thermodynamics. However, there are\ntwo ways to explain entropy, and it depends on if we are studying a microsystem\nor a microsystem. From a macroscopically point of view, it is important to\ndescribe if the system is a reversible system or not. However, form the\nmicroscopically point of view, the concept of chaos is related to entropy. In\nsuch case, entropy measures the amount of disorder into the system. Otherwise,\nthe idea of connecting at the same time the analysis of the macro and micro\nsystem with the use of entropy it is not very common.",
      "generated_abstract": "This study explores the interrelationships between the asset allocation,\nfundamental factors, and risk factors in the context of the cryptocurrency\nmarket. The research focuses on the factors that determine the success of\ncryptocurrency investors and the factors that affect the volatility of cryptocurrency\nprices. The results of the study indicate that market participants can use a\ncombination of fundamental factors, risk factors, and asset allocation to\nincrease the chances of success in the cryptocurrency market. The analysis also\nshows that the factors that affect the volatility of cryptocurrency prices are\ndifficult to predict and require a more advanced approach. The study provides\ninsights into the factors that affect the success of cryptocurrency investors,\nhighlights the most significant risks, and proposes a framework for the\ndevelopment of strategies for cryptocurrency investment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.171875,
          "f": 0.16923076423195282
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.019230769230769232,
          "f": 0.018779337725761208
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.140625,
          "f": 0.13846153346272205
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.10345v1",
      "true_abstract": "Online conformal prediction enables the runtime calibration of a pre-trained\nartificial intelligence model using feedback on its performance. Calibration is\nachieved through set predictions that are updated via online rules so as to\nensure long-term coverage guarantees. While recent research has demonstrated\nthe benefits of incorporating prior knowledge into the calibration process,\nthis has come at the cost of replacing coverage guarantees with less tangible\nregret guarantees based on the quantile loss. This work introduces intermittent\nmirror online conformal prediction (IM-OCP), a novel runtime calibration\nframework that integrates prior knowledge, while maintaining long-term coverage\nand achieving sub-linear regret. IM-OCP features closed-form updates with\nminimal memory complexity, and is designed to operate under potentially\nintermittent feedback.",
      "generated_abstract": "This paper proposes a novel framework for learning robust deep representations\nfor audio-text understanding. Our approach consists of three key components:\n(1) a pre-trained audio encoder to extract global audio features; (2) a\nconvolutional transformer model to capture temporal dynamics; and (3) a\nsemantic-aware audio-text fusion mechanism to integrate both modalities. The\nframework is trained end-to-end on a dataset comprising 11 million audio and\n3 million text samples. Experimental results on the Audio-Text Benchmark\nshow that our approach achieves state-of-the-art performance across all\nevaluation metrics, particularly in challenging tasks such as text-to-audio\ntranslation and audio-text retrieval. The code is available at\nhttps://github.com/WenJingYang/Audio-Text-Fusion-with-Transformer.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.19047619047619047,
          "f": 0.18823528911833923
        },
        "rouge-2": {
          "r": 0.02727272727272727,
          "p": 0.029411764705882353,
          "f": 0.02830188179957368
        },
        "rouge-l": {
          "r": 0.18604651162790697,
          "p": 0.19047619047619047,
          "f": 0.18823528911833923
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08795v1",
      "true_abstract": "We propose a stochastic Model Predictive Control (MPC) framework that ensures\nclosed-loop chance constraint satisfaction for linear systems with general\nsub-Gaussian process and measurement noise. By considering sub-Gaussian noise,\nwe can provide guarantees for a large class of distributions, including\ntime-varying distributions. Specifically, we first provide a new\ncharacterization of sub-Gaussian random vectors using matrix variance proxies,\nwhich can more accurately represent the predicted state distribution. We then\nderive tail bounds under linear propagation for the new characterization,\nenabling tractable computation of probabilistic reachable sets of linear\nsystems. Lastly, we utilize these probabilistic reachable sets to formulate a\nstochastic MPC scheme that provides closed-loop guarantees for general\nsub-Gaussian noise. We further demonstrate our approach in simulations,\nincluding a challenging task of surgical planning from image observations.",
      "generated_abstract": "The development of autonomous vehicles is advancing rapidly, and the\nexisting control algorithms are prone to failure under unknown driving\nconditions. To address this challenge, we propose a hybrid deep neural network\n(DNN)-based control architecture that integrates the DNN model with a control\nloop. The DNN model learns driving behaviors based on real-world driving\ndata, enhancing the robustness of the control loop and reducing the probability\nof error. The control loop then optimizes the driving behavior, ensuring\nsafety and effective performance. The proposed architecture is verified through\nsimulation and evaluated in real-world driving experiments. Our results show\nthat the proposed approach can effectively mitigate the impact of driving\nuncertainty and improve the overall driving performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.18421052631578946,
          "f": 0.16766466569902125
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.00909090909090909,
          "f": 0.008733619461874289
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.17105263157894737,
          "f": 0.15568861779482965
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.10524v1",
      "true_abstract": "We discuss invariants which are helpful for the computation of the vanishing\nlocus of a finitely presented functor $\\mathcal{G}$, i.e., the set of points in\nthe Ziegler spectrum on which $\\mathcal{G}$ vanishes. These invariants are: the\nrank of $\\mathcal{G}$, the supports of its co- and contravariant defect, and\nthe class of $\\mathcal{G}$ in the Grothendieck group of the category of\nfinitely presented functors. We show that these invariants determine the\nvanishing locus in the case of a finitely presented functor over a Dedekind\ndomain.",
      "generated_abstract": "We show that the group of automorphisms of the projective line $\\mathbb P^1$\nis isomorphic to the group of automorphisms of the plane cubic $\\mathbb C^3$.\nIn particular, the group of automorphisms of the projective plane $\\mathbb P^2$\nis isomorphic to the group of automorphisms of $\\mathbb C^3$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.3,
          "f": 0.17142856734693887
        },
        "rouge-2": {
          "r": 0.05333333333333334,
          "p": 0.14285714285714285,
          "f": 0.07766989895371874
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.3,
          "f": 0.17142856734693887
        }
      }
    },
    {
      "paper_id": "cs.MM.eess/AS/2502.03897v2",
      "true_abstract": "As a natural multimodal content, audible video delivers an immersive sensory\nexperience. Consequently, audio-video generation systems have substantial\npotential. However, existing diffusion-based studies mainly employ relatively\nindependent modules for generating each modality, which lack exploration of\nshared-weight generative modules. This approach may under-use the intrinsic\ncorrelations between audio and visual modalities, potentially resulting in\nsub-optimal generation quality. To address this, we propose UniForm, a unified\ndiffusion transformer designed to enhance cross-modal consistency. By\nconcatenating auditory and visual information, UniForm learns to generate audio\nand video simultaneously within a unified latent space, facilitating the\ncreation of high-quality and well-aligned audio-visual pairs. Extensive\nexperiments demonstrate the superior performance of our method in joint\naudio-video generation, audio-guided video generation, and video-guided audio\ngeneration tasks. Our demos are available at https://uniform-t2av.github.io/.",
      "generated_abstract": "We present a new framework for audio-visual language modeling (AVLM)\nusing a transformer-based multimodal architecture. We propose an\nadversarially-trained encoder-decoder architecture that incorporates\ninter-modality-specific attention mechanisms. The encoder-decoder model\ncombines a pre-trained speech encoder with a text-specific transformer-based\ndecoder to enhance audio-visual language modeling. Our approach employs a\nself-attention-based transformer-based encoder that is trained to\ninter-modally attend to visual features while encoding audio data. A\ntext-specific transformer-based decoder is trained to integrate audio and\nvisual information to produce a more comprehensive representation of the input.\nWe validate our model on the AVLM task and show that our approach outperforms\nprevious state-of-the-art methods in terms of F",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16037735849056603,
          "p": 0.2361111111111111,
          "f": 0.19101123113748272
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.030303030303030304,
          "f": 0.02678570935307808
        },
        "rouge-l": {
          "r": 0.16037735849056603,
          "p": 0.2361111111111111,
          "f": 0.19101123113748272
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/RM/2503.08693v1",
      "true_abstract": "We construct liquidity-adjusted return and volatility using purposely\ndesigned liquidity metrics (liquidity jump and liquidity diffusion) that\nincorporate additional liquidity information. Based on these measures, we\nintroduce a liquidity-adjusted ARMA-GARCH framework to address the limitations\nof traditional ARMA-GARCH models, which are not effectively in modeling\nilliquid assets with high liquidity variability, such as cryptocurrencies. We\ndemonstrate that the liquidity-adjusted model improves model fit for\ncryptocurrencies, with greater volatility sensitivity to past shocks and\nreduced volatility persistence of erratic past volatility. Our model is\nvalidated by the empirical evidence that the liquidity-adjusted mean-variance\n(LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.",
      "generated_abstract": "We introduce a novel approach for constructing a stochastic volatility model\nfor the pricing of European options, where the volatility is determined by\na random variable. Our model is based on the recently proposed\nstochastic volatility model for options with correlated volatility, and is\nderived from a general class of stochastic volatility models. The model is\ndefined by a set of parameters, including the mean and the variance of the\ncorrelated volatility, which are unknown. We propose to estimate these unknown\nparameters from observed data, including the option prices. Our estimator is\nbased on a maximum likelihood approach. We provide a detailed theoretical\nanalysis of the estimator and the associated confidence intervals. We also\nprovide an empirical analysis of the model, showing that it can effectively\nhandle both small and large correlated volatility. Finally, we discuss the\nimplications of our model for the pricing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3055555555555556,
          "p": 0.26506024096385544,
          "f": 0.2838709627671177
        },
        "rouge-2": {
          "r": 0.041237113402061855,
          "p": 0.031746031746031744,
          "f": 0.035874434546442395
        },
        "rouge-l": {
          "r": 0.2777777777777778,
          "p": 0.24096385542168675,
          "f": 0.2580645111542144
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/TH/2503.03206v1",
      "true_abstract": "We developed an analytical framework for understanding how the learned\ndistribution evolves during diffusion model training. Leveraging the Gaussian\nequivalence principle, we derived exact solutions for the gradient-flow\ndynamics of weights in one- or two-layer linear denoiser settings with\narbitrary data. Remarkably, these solutions allowed us to derive the generated\ndistribution in closed form and its KL divergence through training. These\nanalytical results expose a pronounced power-law spectral bias, i.e., for\nweights and distributions, the convergence time of a mode follows an inverse\npower law of its variance. Empirical experiments on both Gaussian and image\ndatasets demonstrate that the power-law spectral bias remains robust even when\nusing deeper or convolutional architectures. Our results underscore the\nimportance of the data covariance in dictating the order and rate at which\ndiffusion models learn different modes of the data, providing potential\nexplanations for why earlier stopping could lead to incorrect details in image\ngenerative models.",
      "generated_abstract": "The study of stochastic optimization problems where the objective function\nis coupled with a noisy observation of a random variable is of great interest\nin many areas of science and engineering, including machine learning,\nstatistical physics, and biology. We focus on a class of stochastic\noptimization problems with a convex objective function and a noise-free\nobservation of a random variable. This setting is of particular interest in\nmachine learning, where it is often assumed that the noise level of the\nobservation is unknown. In this paper, we study the problem of finding a\nmaximizer of the objective function in the presence of a noise-free observation\nof a random variable, where the noise level is unknown. We propose a novel\nalgorithm for the problem and show that it achieves a $O(1/\\sqrt{n})$\napproximation guarantee for the problem. This algorithm is based on a novel\napproach for solving the problem, which comb",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.16666666666666666,
          "f": 0.12903225331945908
        },
        "rouge-2": {
          "r": 0.026490066225165563,
          "p": 0.03389830508474576,
          "f": 0.029739772026921393
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.16666666666666666,
          "f": 0.12903225331945908
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SP/2503.03338v1",
      "true_abstract": "We offer a new in-depth investigation of global path planning (GPP) for\nunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP\nis essential for ROMIE's optimal performance, which is translated into solving\nthe traveling salesman problem, a complex graph theory challenge that is\ncrucial for determining the most effective route to cover all sampling\nlocations in a mining field. This problem is central to enhancing ROMIE's\noperational efficiency and competitiveness against human labor by optimizing\ncost and time. The primary aim of this research is to advance GPP by\ndeveloping, evaluating, and improving a cost-efficient software and web\napplication. We delve into an extensive comparison and analysis of Google\noperations research (OR)-Tools optimization algorithms. Our study is driven by\nthe goal of applying and testing the limits of OR-Tools capabilities by\nintegrating Reinforcement Learning techniques for the first time. This enables\nus to compare these methods with OR-Tools, assessing their computational\neffectiveness and real-world application efficiency. Our analysis seeks to\nprovide insights into the effectiveness and practical application of each\ntechnique. Our findings indicate that Q-Learning stands out as the optimal\nstrategy, demonstrating superior efficiency by deviating only 1.2% on average\nfrom the optimal solutions across our datasets.",
      "generated_abstract": "This paper introduces a novel approach to the task of 3D reconstruction of\nrear-view objects in outdoor scenarios. The proposed method employs a\ndistinctly different strategy for estimating the 3D positions of rear-view\nobjects compared to existing approaches. Instead of using a single front-view\nimage as input, we utilize a set of front-view images to estimate the 3D\npositions of rear-view objects. This approach enables us to capture the\nobject-object interactions in the scene and estimate the 3D positions of\nrear-view objects, which are crucial for accurate reconstruction. The proposed\napproach is evaluated using both synthetic and real-world data. The results\ndemonstrate that the proposed approach outperforms existing methods in terms of\nreconstruction quality and object recognition accuracy. Moreover, the\nexperimental results demonstrate that the proposed approach is robust to\nvari",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.24324324324324326,
          "f": 0.16744185595067615
        },
        "rouge-2": {
          "r": 0.020202020202020204,
          "p": 0.03636363636363636,
          "f": 0.02597402138219005
        },
        "rouge-l": {
          "r": 0.12056737588652482,
          "p": 0.22972972972972974,
          "f": 0.15813953036928083
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2503.10117v1",
      "true_abstract": "Using introduced concept of the exchange and inflation rates adequacy, the\nrelevance of them to the determining factors is found. We established close\npositive relation between hryvnia / dollar exchange and inflation rates, fiscal\ndeficit, price level of energy sources, and money supply. On this basis, we\ngive proposals for state macroeconomic policy to stabilize Ukrainian economy.",
      "generated_abstract": "We consider a general class of linearly correlated and stochastically\nreversible Markov chains that exhibit a high degree of flexibility. We show\nthat such models admit a unique equilibrium distribution, and we characterize\ntheir joint distributions in terms of their marginals. We then propose a\nclass of models whose joint distributions are characterized by a deterministic\nfunction of their marginals, which is a generalization of the laws of large\nnumbers. We demonstrate that this class of models, which we call generalized\nMarkov chains, exhibits many features of interest, including the\ndeterministic behavior of the joint distribution, the existence of a unique\nstationary distribution, and the limiting distribution of the joint distribution\nunder the stationary distribution. We also discuss the relationship between\nour model and a recently proposed class of models with a deterministic\nfunction of their marginals. We present numerical simulations to illustrate\nthe properties of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.1111111111111111,
          "f": 0.13953487904813427
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.0078125,
          "f": 0.010989006815603565
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.1111111111111111,
          "f": 0.13953487904813427
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09172v1",
      "true_abstract": "Low Ambiguity Zone (LAZ) sequences play a pivotal role in modern integrated\nsensing and communication (ISAC) systems. Recently, Wang et al.[1] proposed a\ndefinition of locally perfect nonlinear functions (LPNFs) and constructed three\nclasses of both periodic and aperiodic LAZ sequence sets with flexible\nparameters by applying such functions and interleaving method. Some of these\nLAZ sequence sets are asymptotically optimal with respect to the\nYe-Zhou-Liu-Fan-Lei-Tang bounds undercertain conditions. In this paper, we\nproceed with the construction of LPNFs with new parameters. By using these\nLPNFs, we also present a series of LAZ sequence sets with more flexible\nparameters, addressing the limitations of existing parameter choices.\nFurthermore, our results show that one of these classes is asymptotically\noptimal in both the periodic and aperiodic cases, respectively.",
      "generated_abstract": "This paper presents a novel methodology for the assessment of software\ntrade-offs. We introduce a formalism that combines metrics, metrics\nindicator, and metrics methodology to provide a framework for the quantification\nand comparison of software trade-offs. This approach is designed to improve\nthe transparency and reproducibility of trade-off assessment by providing\nrobust and reproducible metrics for quantifying and comparing trade-offs. Our\nmethodology has been applied to evaluate the trade-offs of different\ncombinations of features in a hypothetical game-playing system, providing a\nfoundation for the assessment of trade-offs in complex systems. The methodology\nis implemented in the software tool COCOtrade. COCOtrade provides a\nuser-friendly interface for the analysis and visualization of trade-offs. The\ntool can be used to assess and compare trade-offs in complex systems. It",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11235955056179775,
          "p": 0.14492753623188406,
          "f": 0.12658227356112822
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11235955056179775,
          "p": 0.14492753623188406,
          "f": 0.12658227356112822
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.21141v1",
      "true_abstract": "How do transport infrastructures shape economic transformation and social\nchange? We examine the impact of railway expansion in nineteenth-century\nDenmark on local population growth, occupational shifts, and the diffusion of\nideas. Using a historical panel dataset and a difference-in-differences\napproach, we document that railway access significantly increased population\ngrowth and accelerated structural change. Moreover, railway-connected areas\nwere more likely to establish key institutions linked to civic engagement and\nthe cooperative movement. These findings suggest that improved market access\nwas not only a driver of economic modernization but also a catalyst for\ninstitutional and cultural transformation.",
      "generated_abstract": "The COVID-19 pandemic has caused significant economic disruption, particularly\nin developing countries. This paper examines the effects of the pandemic on\ninternational trade in a time-varying trade liberalization framework. The\nanalysis is conducted using an econometric approach that integrates trade and\ndemand shocks, as well as country-specific shocks. The findings reveal that\nthe pandemic has significantly reduced international trade, with significant\ndifferences across regions. Additionally, the study highlights the importance\nof trade policy reforms in addressing trade-related issues. The findings\ncontribute to the understanding of the economic effects of the COVID-19\npandemic on international trade and suggest policy implications for future\neconomic development.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17105263157894737,
          "p": 0.19402985074626866,
          "f": 0.18181817683798734
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.14925373134328357,
          "f": 0.1398601348799454
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.05695v1",
      "true_abstract": "We develop a Functional Augmented Vector Autoregression (FunVAR) model to\nexplicitly incorporate firm-level heterogeneity observed in more than one\ndimension and study its interaction with aggregate macroeconomic fluctuations.\nOur methodology employs dimensionality reduction techniques for tensor data\nobjects to approximate the joint distribution of firm-level characteristics.\nMore broadly, our framework can be used for assessing predictions from\nstructural models that account for micro-level heterogeneity observed on\nmultiple dimensions. Leveraging firm-level data from the Compustat database, we\nuse the FunVAR model to analyze the propagation of total factor productivity\n(TFP) shocks, examining their impact on both macroeconomic aggregates and the\ncross-sectional distribution of capital and labor across firms.",
      "generated_abstract": "We study a model of non-linear dynamic heterogeneous-agent production\nfits. We develop a simple identification strategy for the parameters of this\nmodel, based on the identification of a certain functional form of the\ninteraction term. We provide a theoretical justification for this identification\nstrategy and provide empirical evidence from a detailed empirical analysis of\na dataset of firms in the United States.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16470588235294117,
          "p": 0.35,
          "f": 0.22399999564800005
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.03389830508474576,
          "f": 0.024691353393538437
        },
        "rouge-l": {
          "r": 0.1411764705882353,
          "p": 0.3,
          "f": 0.1919999956480001
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04453v1",
      "true_abstract": "Given the need to elucidate the mechanisms underlying illnesses and their\ntreatment, as well as the lack of harmonization of acquisition and\npost-processing protocols among different magnetic resonance system vendors,\nthis work is to determine if metabolite concentrations obtained from different\nsessions, machine models and even different vendors of 3 T scanners can be\nhighly reproducible and be pooled for diagnostic analysis, which is very\nvaluable for the research of rare diseases. Participants underwent magnetic\nresonance imaging (MRI) scanning once on two separate days within one week (one\nsession per day, each session including two proton magnetic resonance\nspectroscopy (1H-MRS) scans with no more than a 5-minute interval between scans\n(no off-bed activity)) on each machine. were analyzed for reliability of\nwithin- and between- sessions using the coefficient of variation (CV) and\nintraclass correlation coefficient (ICC), and for reproducibility of across the\nmachines using correlation coefficient. As for within- and between- session,\nall CV values for a group of all the first or second scans of a session, or for\na session were almost below 20%, and most of the ICCs for metabolites range\nfrom moderate (0.4-0.59) to excellent (0.75-1), indicating high data\nreliability. When it comes to the reproducibility across the three scanners,\nall Pearson correlation coefficients across the three machines approached 1\nwith most around 0.9, and majority demonstrated statistical significance\n(P<0.01). Additionally, the intra-vendor reproducibility was greater than the\ninter-vendor ones.",
      "generated_abstract": "We present a novel method for training and applying Bayesian neural networks\n(BNNs) for continuous-valued data. The method consists of two steps: (i)\ntraining a Gaussian process (GP) to represent the data, and (ii) applying the\nBNN to predict the GP. The proposed method combines the advantages of both\ntraditional methods for training BNNs, namely the GP-based regularization and\nthe data-driven approach, while avoiding the disadvantages, such as the\ncomputational complexity and the need for a large amount of data. In our\nexperiments, we demonstrate that the proposed method outperforms the\ntraditional approaches for both synthetic and real-world data, while requiring\nmuch lower computational costs. The code and materials used for the experiments\nare publicly available at https://github.com/Hossein-Ghafouri/BNN_GP.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06451612903225806,
          "p": 0.1282051282051282,
          "f": 0.0858369054173039
        },
        "rouge-2": {
          "r": 0.017467248908296942,
          "p": 0.034782608695652174,
          "f": 0.023255809502603607
        },
        "rouge-l": {
          "r": 0.05806451612903226,
          "p": 0.11538461538461539,
          "f": 0.07725321443017949
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/SC/2309.06907v3",
      "true_abstract": "Plasma membrane calcium influx through ion channels is crucial for many\nevents in cellular physiology. Cell surface stimuli lead to the production of\ninositol 1,4,5-trisphosphate (IP3), which binds to IP3 receptors in the\nendoplasmic reticulum (ER) to release calcium pools from the ER lumen. This\nleads to depletion of ER calcium pools which has been termed store-depletion.\nStore-depletion leads the dissociation of calcium ions from the EF-hand motif\nof the ER calcium sensor Stromal Interaction Molecule 1 (STIM1). This leads to\na conformational change in STIM1 which helps it to interact with a plasma\nmembrane (PM) at ER:PM junctions. At these ER:PM junctions, STIM1 binds to and\nactivates a calcium channel known as Orai1 to form calcium-release activated\ncalcium (CRAC) channels. Activation of Orai1 leads to calcium influx, known as\nstore-operated calcium entry (SOCE). In addition to Orai1 and STIM1, the\nhomologs of Orai1 and STIM1, such as Orai2/3 and STIM2 also play a crucial role\nin calcium homeostasis. The influx of calcium through the Orai channel\nactivates a calcium current that has been termed CRAC currents. CRAC channels\nform multimers and cluster together in large macromolecular assemblies termed\npuncta. How these CRAC channels form puncta has been contentious since their\ndiscovery. In this review, we will outline the history of SOCE, the molecular\nplayers involved in this process (Orai and STIM proteins, TRP channels,\nSOCE-associated regulatory factor etc.), as well as the models that have been\nproposed to explain this important mechanism in cellular physiology.",
      "generated_abstract": "Understanding the fundamental mechanisms of biological evolution is a\ntopical issue in molecular biology. A key question is how natural selection\noperates on DNA sequences to generate evolutionary change. This is a\nfundamental challenge in biology, but it has attracted limited attention in\nthe computational literature, which tends to focus on evolutionary problems\nsuch as the optimization of gene expression, the prediction of phenotypes from\ngenetic sequences, or the design of gene regulatory networks. In this review, we\nintroduce a new framework for understanding evolutionary dynamics in DNA,\nintroducing the notion of a sequence \"cell\". We show that the evolutionary\nprocess in a sequence cell is a dynamical system, and that the dynamics of a\nsequence cell can be described by a general form of stochastic differential\nequation. This framework allows us to address several open questions in the\nfield of evolutionary dynamics of DNA sequences. First, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1736111111111111,
          "p": 0.25773195876288657,
          "f": 0.20746887485821536
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.044444444444444446,
          "f": 0.033426179151310784
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.24742268041237114,
          "f": 0.1991701196714934
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2410.04748v2",
      "true_abstract": "We introduce a fairly general, recombining trinomial tree model in the\nnatural world. Market-completeness is ensured by considering a market\nconsisting of two risky assets, a riskless asset, and a European option. The\ntwo risky assets consist of a stock and a perpetual derivative of that stock.\nThe option has the stock and its derivative as its underlying. Using a\nreplicating portfolio, we develop prices for European options and generate the\nunique relationships between the risk-neutral and real-world parameters of the\nmodel. We discuss calibration of the model to empirical data in the cases in\nwhich the risky asset returns are treated as either arithmetic or logarithmic.\nFrom historical price and call option data for select large cap stocks, we\ndevelop implied parameter surfaces for the real-world parameters in the model.",
      "generated_abstract": "We consider a model where agents invest in a financial market and the\nmarket price of risk is the sum of the market price of the risk of the\nagents and the risk of the market. This model is known as the risk-neutral\nvolatility model. In this paper, we consider a model in which agents have\npreferences over the market price of risk. We analyze the existence, uniqueness\nand asymptotic stability of the Nash equilibrium. We also study the\nasymptotic stability of the market price of risk under the influence of\nagents' preferences.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1728395061728395,
          "p": 0.3333333333333333,
          "f": 0.22764227192544123
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.058823529411764705,
          "f": 0.042553186871888235
        },
        "rouge-l": {
          "r": 0.1728395061728395,
          "p": 0.3333333333333333,
          "f": 0.22764227192544123
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04129v1",
      "true_abstract": "This work aims to synthesize a controller that ensures that an unknown\ndiscrete-time system is incrementally input-to-state stable ($\\delta$-ISS). In\nthis work, we introduce the notion of $\\delta$-ISS control Lyapunov function\n($\\delta$-ISS-CLF), which, in conjunction with the controller, ensures that the\nclosed-loop system is incrementally ISS. To address the unknown dynamics of the\nsystem, we parameterize the controller as well as the $\\delta$-ISS-CLF as\nneural networks and learn them by utilizing the sampled data from the state\nspace of the unknown system. To formally verify the obtained $\\delta$-ISS-CLF,\nwe develop a validity condition and incorporate the condition into the training\nframework to ensure a provable correctness guarantee at the end of the training\nprocess. Finally, the usefulness of the proposed approach is proved using\nmultiple case studies - the first one is a scalar system with a non-affine\nnon-polynomial structure, the second example is a one-link manipulator system,\nthe third system is a nonlinear Moore-Grietzer model of the jet engine and the\nfinal one is a rotating rigid spacecraft model.",
      "generated_abstract": "The increasing prevalence of multi-agent systems (MAS) in modern society has\nsignificantly contributed to the development of control algorithms for MAS.\nHowever, the lack of a systematic framework for designing control laws for MAS\nhas hindered the progress of this field. This study proposes a framework for\ndesigning control laws for MAS based on the concepts of state-space\nrepresentations, observability, and controllability. The framework is based on\nthe use of the Lyapunov function to ensure stability and robustness, which is\nthen used to design control laws. The framework is applied to a multi-agent\nsystem composed of a robot and an obstacle, where the robot is controlled by a\nsingle agent and the obstacle is controlled by a MAS of two agents. The\nresults show that the proposed framework provides a more efficient and\neffective solution compared to existing methods, as it can achieve",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.24096385542168675,
          "f": 0.21164020671425784
        },
        "rouge-2": {
          "r": 0.03870967741935484,
          "p": 0.047619047619047616,
          "f": 0.04270462138777429
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.18072289156626506,
          "f": 0.15873015380420497
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09998v1",
      "true_abstract": "We numerically investigate the sensitivity of the scattered wave field to\nperturbations in the shape of a scattering body illuminated by an incident\nplane wave. This study is motivated by recent work on the inverse problem of\nreconstructing a scatterer shape from measurements of the scattered wave at\nlarge distances from the scatterer. For this purpose we consider star-shaped\nscatterers represented using cubic splines, and our approach is based on a\nNystr\\\"om method-based discretisation of the shape derivative. Using the\nsingular value decomposition, we identify fundamental geometric modes that most\nstrongly influence the scattered wave, providing insight into the most visible\nboundary features in scattering data.",
      "generated_abstract": "We present a new approach to solving the Korteweg-de Vries equation by\nusing the generalized Schr\\\"odinger equation. The new approach is based on\nthe use of the generalized Schr\\\"odinger equation to study the Korteweg-de\nVries equation. This approach is based on the use of the generalized\nSchr\\\"odinger equation to study the Korteweg-de Vries equation, and provides\na powerful tool for solving the Korteweg-de Vries equation. The new approach is\nbased on the use of the generalized Schr\\\"odinger equation to study the\nKorteweg-de Vries equation. This approach is based on the use of the generalized\nSchr\\\"odinger equation to study the Korteweg-de Vries equation, and provides a\npowerful tool for solving the Korteweg-de Vries equation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18421052631578946,
          "p": 0.4827586206896552,
          "f": 0.26666666266848077
        },
        "rouge-2": {
          "r": 0.050505050505050504,
          "p": 0.13157894736842105,
          "f": 0.07299269672118942
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.3793103448275862,
          "f": 0.20952380552562366
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.03498v1",
      "true_abstract": "In this paper, we provide a comprehensive analysis of involutive quantales,\nwith a particular focus on quantic frames. We extend the axiomatic foundations\nof quantale-enriched topological spaces to include closure under the\nanti-homomorphic involution, facilitating a balanced topologization of the\nspectrum of unital $C^*$-algebras that encompasses both closed right and left\nideals through the concept of quantic frames. Specifically, certain subspaces\nof pure states are identified as strongly Hausdorff separated quantale-enriched\ninvolutive topological spaces.",
      "generated_abstract": "We prove the existence of a unique solution to a system of nonlinear\ndiscretized differential equations arising from the study of the motion of\nparticles in a gravitational field. The model has been constructed in the\nliterature as a nonlinear version of the Euler equations of fluid dynamics. The\nmodel is constructed by discretizing the equations of motion of the particles\nin a gravitational field by means of a set of nonlinear differential equations\nof second order.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10344827586206896,
          "p": 0.15,
          "f": 0.12244897476051664
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.015873015873015872,
          "f": 0.01492536815215136
        },
        "rouge-l": {
          "r": 0.10344827586206896,
          "p": 0.15,
          "f": 0.12244897476051664
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2410.02463v1",
      "true_abstract": "In this study, the predominant lactic acid bacteria (LAB) isolates were\nobtained from Gouda, Jack, Cheddar, and Parmesan cheeses produced in Uganda.\nThe isolates were identified through Gram staining, catalase and oxidase tests,\nand 16S rDNA sequencing. Approximately 90% of the isolates were cocci (n=192),\nincluding Streptococcus, Enterococcus, and Lactococcus. The remaining 10% were\nidentified as rod-shaped bacteria, primarily belonging to the Lactobacillus\nspecies (n=23). BLAST analysis revealed that Pediococcus pentosaceus dominated\nin all cheese samples (23.7%, of the total 114 isolates). This was followed by\nuncultured bacterium (15.8%), uncultured Pediococcus species (13.2%),\nLacticaseibacillus rhamnosus (8.8%) among others",
      "generated_abstract": "In this paper, we propose a novel method for the computation of the\neigenvalues and eigenvectors of the stochastic Schr\\\"odinger operator (SSO)\nin the presence of a spatially varying chemical potential. We derive the\nexplicit expression of the SSO eigenvalues and eigenvectors in terms of\nprobability density functions (pdfs) and pdf-based moment generating functions\n(mgfs) of the chemical potential. We also provide a detailed analysis of the\neigenvalue stability, which reveals that the SSO eigenvalues exhibit a\nsignificant sensitivity to the chemical potential. Furthermore, we investigate\nthe relationship between the SSO eigenvalues and the corresponding eigenvectors\nand propose an optimal algorithm for the SSO eigenvalue problem. Finally, we\nnumerically illustrate the proposed method on several examples, including the\nGaussian SSO, the Ornstein-Uhlenbeck SSO, the logistic SSO",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11904761904761904,
          "p": 0.136986301369863,
          "f": 0.12738853005639192
        },
        "rouge-2": {
          "r": 0.030927835051546393,
          "p": 0.028037383177570093,
          "f": 0.029411759717897806
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.136986301369863,
          "f": 0.12738853005639192
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.05192v1",
      "true_abstract": "Symplectic integrator plays a pivotal role in the long-term tracking of\ncharged particles within accelerators. To get symplectic maps in accurate\nsimulation of single-particle trajectories, two key components are addressed:\nprecise analytical expressions for arbitrary electromagnetic fields and a\nrobust treatment of the equations of motion. In a source-free region, the\nelectromagnetic fields can be decomposed into harmonic functions, applicable to\nboth scalar and vector potentials, encompassing both straight and curved\nreference trajectories. These harmonics are constructed according to the\nboundary surface's field data due to uniqueness theorem. Finding generating\nfunctions to meet the Hamilton-Jacobi equation via a perturbative ansatz, we\nderive symplectic maps that are independent of the expansion order. This method\nyields a direct mapping from initial to final coordinates in a large step,\nbypassing the transition of intermediate coordinates. Our developed\nparticle-tracking algorithm translates the field harmonics into beam\ntrajectories, offering a high-resolution integration method in accelerator\nsimulations.",
      "generated_abstract": "Accurate, comprehensive, and high-resolution 3D particle images are critical\nfor analyzing particle beam interactions in particle accelerators. Traditional\napproaches to 3D particle imaging often rely on particle tracking velocimetry\n(PTV) or kinetic energy spectroscopy (KE) techniques to acquire kinetic\ninformation and then use the results to perform particle image velocimetry\n(PIV) on the resulting kinetic images. This approach, however, often fails to\ncapture the entirety of the beam profile and is limited by the limitations of\nthe available sensors in current particle beam systems. In this work, we\npropose a novel method for 3D particle image acquisition based on the use of\nelectrostatic fields to capture the beam profile. The method relies on the\nexistence of a static electrostatic field along the beam axis to guide the\nbeam particles along a prescribed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17117117117117117,
          "p": 0.24050632911392406,
          "f": 0.1999999951418284
        },
        "rouge-2": {
          "r": 0.006802721088435374,
          "p": 0.00847457627118644,
          "f": 0.0075471648712029495
        },
        "rouge-l": {
          "r": 0.17117117117117117,
          "p": 0.24050632911392406,
          "f": 0.1999999951418284
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.10230v1",
      "true_abstract": "Lucatelli Nunes obtained a 2-categorical version of the adjoint triangle\ntheorem of Dubuc using the descent object of a specific diagram. In some cases,\nsuch a diagram can be filled with an extra cell. We show then how to obtain a\nbiadjoint as an inverter of this additional datum (under suitable hypotheses).\nThe problem addressed here is slightly different however: we still have a\ntriangle of pseudofunctors but the lifted biadjoint is not the same. The\nconstruction is simplified when the pseudofunctor whose left biadjoint is\nsought is fully faithful. As an example, we get the biadjoint of the inclusion\npseudofunctor of a bicategory associated to a KZ-monad preserving\npseudomonicity.",
      "generated_abstract": "We prove the main theorem of Alonso-Blanco-Sun, a generalized version of\nthe Kesten-Stigum theorem that applies to the sum of independent random\nmatrices. Our proof relies on a recent theorem of Gao-Li-Xie that relates the\nmaximal entropy of a sum of independent random matrices to the maximal entropy\nof a random matrix. We also provide an alternative proof of the theorem of\nGao-Li-Xie, using the method of partial transpose. We show that the Gao-Li-Xie\ntheorem can be obtained from our theorem by an appropriate linear transformation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16883116883116883,
          "p": 0.2765957446808511,
          "f": 0.20967741464750272
        },
        "rouge-2": {
          "r": 0.0660377358490566,
          "p": 0.0958904109589041,
          "f": 0.07821228567273211
        },
        "rouge-l": {
          "r": 0.15584415584415584,
          "p": 0.2553191489361702,
          "f": 0.1935483823894382
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02156v1",
      "true_abstract": "WiFi-based mobility monitoring in urban environments can provide valuable\ninsights into pedestrian and vehicle movements. However, MAC address\nrandomization introduces a significant obstacle in accurately estimating\ncongestion levels and path trajectories. To this end, we consider radio\nfrequency fingerprinting and re-identification for attributing WiFi traffic to\nemitting devices without the use of MAC addresses.\n  We present MobRFFI, an AI-based device fingerprinting and re-identification\nframework for WiFi networks that leverages an encoder deep learning model to\nextract unique features based on WiFi chipset hardware impairments. It is\nentirely independent of frame type. When evaluated on the WiFi fingerprinting\ndataset WiSig, our approach achieves 94% and 100% device accuracy in multi-day\nand single-day re-identification scenarios, respectively.\n  We also collect a novel dataset, MobRFFI, for granular multi-receiver WiFi\ndevice fingerprinting evaluation. Using the dataset, we demonstrate that the\ncombination of fingerprints from multiple receivers boosts re-identification\nperformance from 81% to 100% on a single-day scenario and from 41% to 100% on a\nmulti-day scenario.",
      "generated_abstract": "This paper presents a novel, low-cost, and energy-efficient distributed\nsignal processing (DSP) architecture that integrates a wireless network and\na local oscillator (LO) within a single integrated circuit (IC). The proposed\narchitecture combines a frequency-hopping spread-spectrum (FHSS) technique with\na multi-antenna system to improve the energy efficiency and reduce the cost of\nthe system. The FHSS-based frequency hopping scheme is employed to dynamically\nadapt the spreading code to optimize the system performance. The proposed\narchitecture is implemented in a 10-QPU FPGA, and the energy efficiency is\nevaluated by comparing the FPGA-based implementation with a conventional\nhardware-based implementation. The results show that the FPGA-based implementation\nreduces the energy consumption by up to 69.91\\% compared to the hardware-based\nimplementation, while maintaining",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0990990990990991,
          "p": 0.14285714285714285,
          "f": 0.11702127175928043
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.00909090909090909,
          "f": 0.007575752714649584
        },
        "rouge-l": {
          "r": 0.09009009009009009,
          "p": 0.12987012987012986,
          "f": 0.10638297388694
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.02815v1",
      "true_abstract": "Full waveform inversion (FWI) plays an important role in velocity modeling\ndue to its high-resolution advantages. However, its highly non-linear\ncharacteristic leads to numerous local minimums, which is known as the\ncycle-skipping problem. Therefore, effectively addressing the cycle-skipping\nissue is crucial to the success of FWI. Well-log data contain rich information\nabout subsurface medium parameters, providing inherent advantages for velocity\nmodeling. Traditional well-log data interpolation methods to build velocity\nmodels have limited accuracy and poor adaptability to complex geological\nstructures. This study introduces a well interpolation algorithm based on a\ngenerative diffusion model (GDM) to generate initial models for FWI, addressing\nthe cycle-skipping problem. Existing convolutional neural network (CNN)-based\nmethods face difficulties in handling complex feature distributions and lack\neffective uncertainty quantification, limiting the reliability of their\noutputs. The proposed GDM-based approach overcomes these challenges by\nproviding geologically consistent well interpolation while incorporating\nuncertainty assessment. Numerical experiments demonstrate that the method\nproduces accurate and reliable initial models, enhancing FWI performance and\nmitigating cycle-skipping issues.",
      "generated_abstract": "In this paper, we present a detailed study of the numerical solution of the\nFourier-space finite-difference discretization of the wave equation for\nnon-local media. We provide a comprehensive comparison of various finite-difference\nmethods, including the Finite-Difference Time-Domain (FDTD) method, the\nDiscontinuous Galerkin Method (DG), and the Finite-Element Method (FEM) for\nnon-local media. We demonstrate the accuracy and efficiency of the DG method\nwith a 1D problem, and compare its performance to the FDTD method for\nnon-local media. We also examine the convergence of the DG method for\nnon-local media and provide a detailed analysis of the effects of boundary\nconditions and discretization parameters. This study provides a comprehensive\noverview of the numerical solution of the wave equation for non-local media\nwith different discret",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.104,
          "p": 0.203125,
          "f": 0.1375661330869798
        },
        "rouge-2": {
          "r": 0.01910828025477707,
          "p": 0.03225806451612903,
          "f": 0.02399999532768091
        },
        "rouge-l": {
          "r": 0.104,
          "p": 0.203125,
          "f": 0.1375661330869798
        }
      }
    },
    {
      "paper_id": "cs.CR.q-bio/GN/2411.16744v1",
      "true_abstract": "Counting distinct permutations with replacement, especially when involving\nmultiple subwords, is a longstanding challenge in combinatorial analysis, with\ncritical applications in cryptography, bioinformatics, and statistical\nmodeling. This paper introduces a novel framework that presents closed-form\nformulas for calculating distinct permutations with replacement, fundamentally\nreducing the time complexity from exponential to linear relative to the\nsequence length for single-subword calculations. We then extend our\nfoundational formula to handle multiple subwords through the development of an\nadditional formula. Unlike traditional methods relying on brute-force\nenumeration or recursive algorithms, our approach leverages novel combinatorial\nconstructs and advanced mathematical techniques to achieve unprecedented\nefficiency. This comprehensive advancement in reducing computational complexity\nnot only simplifies permutation counting but also establishes a new benchmark\nfor scalability and versatility. We also demonstrate the practical utility of\nour formulas through diverse applications, including the simultaneous\nidentification of multiple genetic motifs in DNA sequences and complex pattern\nanalysis in cryptographic systems, using a computer program that runs the\nproposed formulae.",
      "generated_abstract": "The field of computational biology has made significant advancements in\nbiomolecular sequence analysis, but the integration of these tools with\ncomputational models has remained limited. While protein-protein interaction\n(PPI) networks have been studied using graph-based models, it remains challenging\nto capture complex interactions within protein-protein interactions (PPIs)\nusing standard graph models. In this work, we introduce a novel modeling\nframework that leverages graph neural networks (GNNs) to analyze PPIs. We\npropose a GNN model that uses PPIs as node features and relies on a graph\nembedding layer for node representation. The model is trained using\nstructural constraints and a dynamic graph regularizer that ensures the\npreservation of key structural properties of the PPI. We demonstrate the\neffectiveness of the proposed framework through a case study of the\nBeta-Birchwood-K",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21008403361344538,
          "p": 0.2717391304347826,
          "f": 0.2369668197264213
        },
        "rouge-2": {
          "r": 0.025477707006369428,
          "p": 0.032,
          "f": 0.028368789390625072
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.22826086956521738,
          "f": 0.1990521277832934
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.09325v1",
      "true_abstract": "We initiate the study of $\\lambda$-fold near-factorizations of groups with\n$\\lambda > 1$. While $\\lambda$-fold near-factorizations of groups with $\\lambda\n= 1$ have been studied in numerous papers, this is the first detailed treatment\nfor $\\lambda > 1$. We establish fundamental properties of $\\lambda$-fold\nnear-factorizations and introduce the notion of equivalence. We prove various\nnecessary conditions of $\\lambda$-fold near-factorizations, including upper\nbounds on $\\lambda$. We present three constructions of infinite families of\n$\\lambda$-fold near-factorizations, highlighting the characterization of two\nsubfamilies of $\\lambda$-fold near-factorizations. We discuss a computational\napproach to $\\lambda$-fold near-factorizations and tabulate computational\nresults for abelian groups of small order.",
      "generated_abstract": "In this paper, we consider the local existence of the initial value problem for\nthe nonlinear wave equation with Dirichlet and Neumann boundary conditions\nand a free-boundary condition at the origin. Our main result is the local\nexistence of the unique global strong solution for a time interval $(0,T^*)$\nunder the Neumann condition on the boundary and the Dirichlet condition in the\ncenter, where $T^*$ is the maximal existence time of the solution. The\nboundary term is treated by the Lagrangian method. The proof is based on the\nfixed point theorem for a certain differential operator and the\n$L^p$-$L^q$ boundedness of some creations of this operator, where $p$ and $q$\nare some positive numbers. The proof is constructive and is valid for general\nnonlinearities and boundary conditions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1774193548387097,
          "p": 0.1506849315068493,
          "f": 0.16296295799615929
        },
        "rouge-2": {
          "r": 0.011904761904761904,
          "p": 0.008771929824561403,
          "f": 0.010101005215796668
        },
        "rouge-l": {
          "r": 0.1774193548387097,
          "p": 0.1506849315068493,
          "f": 0.16296295799615929
        }
      }
    },
    {
      "paper_id": "math.CO.math/RT/2503.06975v1",
      "true_abstract": "Given two affine permutations, some results of Lascoux and Deodhar, and\nindependently Jacon-Lecouvey, allow to decide if they are comparable for the\nstrong Bruhat order. These permutations are associated with tuples of core\npartitions, and the preceding problem is equivalent to compare the Young\ndiagrams in each components for the inclusion. Using abaci, we give an easy\nrule to compute these Young diagrams one another. We deduce a procedure to\ncompare, for the Bruhat order, two affine permutations in the window notation.",
      "generated_abstract": "We investigate the problem of determining the dimension of the set of all\n(non-zero) solutions to a linear differential equation. We provide a new\nconstruction of the dimension of a solution set of a linear differential\nequation, which is based on the G\\\"odel number of the solution set of the\nequation. In particular, we prove that the dimension of the solution set of\ncertain linear differential equations with a particular structure is equal to\nthe G\\\"odel number of this solution set. We apply our results to the\nconstruction of a G\\\"odel number for the solution set of certain linear\ndifferential equations with a particular structure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18032786885245902,
          "p": 0.2558139534883721,
          "f": 0.21153845668823976
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.014925373134328358,
          "f": 0.013888883913003327
        },
        "rouge-l": {
          "r": 0.16393442622950818,
          "p": 0.23255813953488372,
          "f": 0.1923076874574705
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.04565v1",
      "true_abstract": "Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive\ninformation to support Multi-Object Tracking (MOT) in capturing spatial and\ntemporal relationships of surrounding objects. However, most MOT algorithms are\ntailored for pinhole images with limited views, impairing their effectiveness\nin panoramic settings. Additionally, panoramic image distortions, such as\nresolution loss, geometric deformation, and uneven lighting, hinder direct\nadaptation of existing MOT methods, leading to significant performance\ndegradation. To address these challenges, we propose OmniTrack, an\nomnidirectional MOT framework that incorporates Tracklet Management to\nintroduce temporal cues, FlexiTrack Instances for object localization and\nassociation, and the CircularStatE Module to alleviate image and geometric\ndistortions. This integration enables tracking in large field-of-view\nscenarios, even under rapid sensor motion. To mitigate the lack of panoramic\nMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic\ndataset collected by a quadruped robot, featuring diverse challenges such as\nwide fields of view, intense motion, and complex environments. Extensive\nexperiments on the public JRDB dataset and the newly introduced QuadTrack\nbenchmark demonstrate the state-of-the-art performance of the proposed\nframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an\nimprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the\nbaseline by 6.81%. The dataset and code will be made publicly available at\nhttps://github.com/xifen523/OmniTrack.",
      "generated_abstract": "Video captioning is the task of generating captions for videos. While many\nresearchers have focused on single-modal video captioning, many challenges\nremain in multi-modal video captioning, including the limited availability of\nmultimodal data, the lack of a comprehensive benchmark, and the lack of\nmulti-modal supervision. To address these challenges, we introduce\nVideo-Captioning-by-Generating-Video-Multi-modal-Supervision (VCV-MS), which\nprovides a unified framework for video captioning across various modalities,\nenabling more effective training and evaluation. The framework consists of\nvideo caption generation modules, multi-modal supervision, and a joint\noptimization strategy, enabling the training of video captioning models on\nmultiple modalities simultaneously. We also introduce a novel Video-Captioning\nby Generating Video-Multi-modal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12658227848101267,
          "p": 0.2702702702702703,
          "f": 0.17241378875891808
        },
        "rouge-2": {
          "r": 0.03827751196172249,
          "p": 0.08080808080808081,
          "f": 0.05194804758580742
        },
        "rouge-l": {
          "r": 0.10759493670886076,
          "p": 0.22972972972972974,
          "f": 0.14655171979340084
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.07997v1",
      "true_abstract": "Autonomous stores leverage advanced sensing technologies to enable\ncashier-less shopping, real-time inventory tracking, and seamless customer\ninteractions. However, these systems face significant challenges, including\nocclusion in vision-based tracking, scalability of sensor deployment, theft\nprevention, and real-time data processing. To address these issues, researchers\nhave explored multi-modal sensing approaches, integrating computer vision,\nRFID, weight sensing, vibration-based detection, and LiDAR to enhance accuracy\nand efficiency. This survey provides a comprehensive review of sensing\ntechnologies used in autonomous retail environments, highlighting their\nstrengths, limitations, and integration strategies. We categorize existing\nsolutions across inventory tracking, environmental monitoring, people-tracking,\nand theft detection, discussing key challenges and emerging trends. Finally, we\noutline future directions for scalable, cost-efficient, and privacy-conscious\nautonomous store systems.",
      "generated_abstract": "The development of high-precision autonomous underwater vehicles (AUVs)\nprovides significant opportunities for exploring the ocean, but also poses\nsignificant challenges. AUVs can be equipped with high-precision sensors,\nenabling precise positioning, but the limited size of AUVs limits their\ndeployment in complex environments. This study proposes a novel AUV swarm\ndeployment strategy using a large-scale autonomous underwater vehicle (LAUV)\nsystem. The proposed strategy uses a LAUV as the central AUV to coordinate the\ndeployment of multiple AUVs, which are equipped with high-precision sensors.\nThe LAUV is equipped with an onboard control system that optimizes the\ndeployment of AUVs and their communication with each other, ensuring the\ncoordinated deployment of AUVs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.1643835616438356,
          "f": 0.14371256992936302
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0851063829787234,
          "p": 0.1095890410958904,
          "f": 0.0958083783125966
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2503.07342v1",
      "true_abstract": "In this work, we introduce a novel variant of the multivariate quadratic\nproblem, which is at the core of one of the most promising post-quantum\nalternatives: multivariate cryptography. In this variant, the solution of a\ngiven multivariate quadratic system must also be regular, i.e. if it is split\ninto multiple blocks of consecutive entries with the same fixed length, then\neach block has only one nonzero entry. We prove the NP-completeness of this\nvariant and show similarities and differences with other computational problems\nused in cryptography. Then we analyze its hardness by reviewing the most common\nsolvers for polynomial systems over finite fields, derive asymptotic formulas\nfor the corresponding complexities and compare the different approaches.",
      "generated_abstract": "The study of structural robustness (SR) in the context of designing\nmonolithic integrated circuits (MICs) is becoming increasingly important.\nRecent studies have shown that SR can be achieved by designing MICs with\nstructural redundancies and introducing redundancy into the design. However,\nexisting studies have focused on the MIC design, and the problem of how to\ndesign the redundancy into the design is still under-explored. In this study,\nwe present a comprehensive study on the design of MICs with structural\nredundancy. We firstly propose a novel design paradigm for MICs with structural\nredundancy, and then we formulate the design problem of MICs with structural\nredundancy. Finally, we propose a novel design method to solve this problem.\nOur method includes three steps: 1) constructing the redundant structure",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19767441860465115,
          "p": 0.22972972972972974,
          "f": 0.21249999502812514
        },
        "rouge-2": {
          "r": 0.018018018018018018,
          "p": 0.01904761904761905,
          "f": 0.018518513522377895
        },
        "rouge-l": {
          "r": 0.1744186046511628,
          "p": 0.20270270270270271,
          "f": 0.18749999502812514
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06638v1",
      "true_abstract": "In the paper the joint optimization of uplink multiuser power and resource\nblock (RB) allocation are studied, where each user has quality of service (QoS)\nconstraints on both long- and short-blocklength transmissions. The objective is\nto minimize the consumption of RBs for meeting the QoS requirements, leading to\na mixed-integer nonlinear programming (MINLP) problem. We resort to deep\nlearning to solve the problem with low inference complexity. To provide a\nperformance benchmark for learning based methods, we propose a hierarchical\nalgorithm to find the global optimal solution in the single-user scenario,\nwhich is then extended to the multiuser scenario. The design of the learning\nmethod, however, is challenging due to the discrete policy to be learned, which\nresults in either vanishing or exploding gradient during neural network\ntraining. We introduce two types of smoothing functions to approximate the\ninvolved discretizing processes and propose a smoothing parameter adaption\nmethod. Another critical challenge lies in guaranteeing the QoS constraints. To\naddress it, we design a nonlinear function to intensify the penalties for minor\nconstraint violations. Simulation results demonstrate the advantages of the\nproposed method in reducing the number of occupied RBs and satisfying QoS\nconstraints reliably.",
      "generated_abstract": "This paper addresses the challenge of estimating the position of a moving\ntarget in an unknown and cluttered environment, leveraging the sparsity of\nacoustic echoes in the acoustic backscatter signal. By leveraging a sparse\nrepresentation of the target's echoes, we design a sparse recovery algorithm\nthat can be applied to the acoustic echoes in the backscatter signal. The\nalgorithm is based on a linear sparse recovery framework that allows for\nefficient implementation and robustness to noise. We demonstrate the\neffectiveness of our proposed framework through simulations and a real-world\nexperiment, highlighting its potential for enhancing signal-to-clutter ratio\n(S/C) estimation and target tracking in cluttered environments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.29411764705882354,
          "f": 0.20202019751045824
        },
        "rouge-2": {
          "r": 0.031914893617021274,
          "p": 0.061855670103092786,
          "f": 0.04210525866765207
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.2647058823529412,
          "f": 0.18181817730843802
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.07328v1",
      "true_abstract": "Reachability Types (RT) are a qualified type system for tracking aliasing and\nseparation in functional and higher-order programming. By formalizing resource\nreachability with a sound static type system, RT enable higher-order\nprogramming patterns with runtime safety and non-interference guarantees.\nHowever, previous RT systems have been based on calculi that restrict cyclic\ndependencies and are shown to be terminating in the absence of built-in\nrecursive constructs. While termination is sometimes a desirable property,\nsimplifying reasoning and ensuring predictable behavior, it implies an\ninability to encode expressive programs involving non-termination and advanced\nrecursive patterns, such as mutual recursion and various fixed-point\ncombinators.\n  In this paper, we address this limitation by extending RT with an expressive\ncyclic reference type that permits the formation of cyclic dependencies through\nthe store, thereby allowing the system to encode recursive programming patterns\nwithout relying on extra built-in constructs. In addition, we redesign\nqualifier typing in the reference introduction rule, allowing separate\nreferences to point to a shared and tracked referent. We formalize the system\nas the $\\lambda^{\\circ}_{<:}$-calculus, with a mechanized soundness proof via\nthe standard progress and preservation lemmas. As a demonstration, we implement\na well-typed fixpoint operator, proving that recursive patterns can be encoded\nusing the novel cyclic reference type.",
      "generated_abstract": "The increasing availability of large language models (LLMs) has inspired a\nparticularly active research agenda on modeling LLMs. Despite the wide range of\napproaches, there is still a lack of a comprehensive and objective evaluation\nframework. In this paper, we propose a novel evaluation method for LLMs that\nprovides a comprehensive assessment of the model's ability to perform\nnatural-language tasks. Our evaluation framework, called LLM-Ora, includes\nfour core components: (1) a natural language processing (NLP) task, (2) a\nmodel, (3) a scoring function, and (4) a set of evaluation metrics. Our\nframework is designed to be broadly applicable, requiring no additional\nconfiguration or expert knowledge. We further provide a detailed evaluation of\nthe LLM-Ora framework on three benchmarks: ChatGPT, OpenAI's GPT-4,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11851851851851852,
          "p": 0.17582417582417584,
          "f": 0.14159291554350398
        },
        "rouge-2": {
          "r": 0.020618556701030927,
          "p": 0.03389830508474576,
          "f": 0.025641020937706316
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.16483516483516483,
          "f": 0.13274335802138004
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.08311v1",
      "true_abstract": "Inference in linear panel data models is complicated by the presence of fixed\neffects when (some of) the regressors are not strictly exogenous. Under\nasymptotics where the number of cross-sectional observations and time periods\ngrow at the same rate, the within-group estimator is consistent but its limit\ndistribution features a bias term. In this paper we show that a panel version\nof the moving block bootstrap, where blocks of adjacent cross-sections are\nresampled with replacement, replicates the limit distribution of the\nwithin-group estimator. Confidence ellipsoids and hypothesis tests based on the\nreverse-percentile bootstrap are thus asymptotically valid without the need to\ntake the presence of bias into account.",
      "generated_abstract": "The increasing availability of large-scale microdata has inspired a\nvariety of methods for estimating a wide range of statistical models,\nparticularly for the analysis of cross-sectional surveys. While these methods\nhave been extensively studied, there is little systematic examination of\ncomparative methodology, particularly in the context of estimating models that\nrequire the use of repeated cross-sectional data. To address this gap, we\nintroduce a novel framework that is tailored to the analysis of repeated\ncross-sectional surveys. We develop a novel methodology for estimating a wide\nrange of statistical models in this setting. We then apply this methodology to\nestimate the effects of environmental subsidies on firm productivity. Our\nfindings highlight the importance of considering the effect of repeated\ncross-sectional data in the analysis of these data, and the importance of\ncomparing alternative methods for estimating",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.19736842105263158,
          "f": 0.19230768731097975
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.008928571428571428,
          "f": 0.009389666374840091
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.13157894736842105,
          "f": 0.12820512320841573
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2411.00749v1",
      "true_abstract": "Accurate survival prediction is essential for personalized cancer treatment.\nHowever, genomic data - often a more powerful predictor than pathology data -\nis costly and inaccessible. We present the cross-modal genomic feature\ntranslation and alignment network for enhanced survival prediction from\nhistopathology images (PathoGen-X). It is a deep learning framework that\nleverages both genomic and imaging data during training, relying solely on\nimaging data at testing. PathoGen-X employs transformer-based networks to align\nand translate image features into the genomic feature space, enhancing weaker\nimaging signals with stronger genomic signals. Unlike other methods, PathoGen-X\ntranslates and aligns features without projecting them to a shared latent space\nand requires fewer paired samples. Evaluated on TCGA-BRCA, TCGA-LUAD, and\nTCGA-GBM datasets, PathoGen-X demonstrates strong survival prediction\nperformance, emphasizing the potential of enriched imaging models for\naccessible cancer prognosis.",
      "generated_abstract": "We present a novel method for non-invasive mapping of the brain's connectome,\nusing a dual-modality approach based on simultaneous magnetic resonance imaging\n(MRI) and functional MRI (fMRI). Our method relies on a novel approach to\nestimate the connectome from the observed fMRI data. This approach is based on\na graph-theoretic framework that characterizes the sparsity of the connectivity\nmatrix in terms of the graph Laplacian. The connectivity matrix is then\nestimated using an alternating optimization procedure. The method is validated\non a large dataset of healthy volunteers. We demonstrate that the estimated\nconnectivity matrix is significantly more sparse than the true one, and that\nthe estimated connectivity matrix is highly correlated with the true one,\nespecially in the deep layers of the brain. Our method can be applied to other\ntypes of brain imag",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20618556701030927,
          "p": 0.25,
          "f": 0.22598869561109525
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.017391304347826087,
          "f": 0.0164609003641059
        },
        "rouge-l": {
          "r": 0.18556701030927836,
          "p": 0.225,
          "f": 0.20338982555459814
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.09307v1",
      "true_abstract": "We consider a broad class of nonlinear integro-differential equations with a\nkernel whose differentiability order is described by a general function $\\phi$.\nThis class includes not only the fractional $p$-Laplace equations, but also\nborderline cases when the fractional order approaches $1$. Under mild\nassumptions on $\\phi$, we establish sharp Sobolev-Poincar\\'e type inequalities\nfor the associated Sobolev spaces, which are connected to a question raised by\nBrezis (Russian Math. Surveys 57:693--708, 2002). Using these inequalities, we\nprove H\\\"older regularity and Harnack inequalities for weak solutions to such\nnonlocal equations. All the estimates in our results remain stable as the\nassociated nonlocal energy functional approaches its local counterpart.",
      "generated_abstract": "This paper introduces a novel approach to the study of the solutions of\nthe nonlinear Schr\\\"odinger equation in a domain $D$ with a general boundary\ncondition for the free propagation of the wave. The boundary conditions are\ndefined on the boundary of $D$ and not only on the boundary of the interior of\n$D$. In addition, the solution of the nonlinear Schr\\\"odinger equation is\ndefined on the whole space $\\mathbb{R}^3$ in the form of a solution of the\nnonlinear wave equation in a domain with a general boundary condition.\n  The problem of the existence and uniqueness of the solutions of the nonlinear\nwave equation in a domain with a general boundary condition is formulated and\nsolved for the case $D=\\mathbb{R}^3$. In this case, the boundary condition is\ndefined on the boundary of $D$. The results of the paper are applied to the\nstudy of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20689655172413793,
          "p": 0.33962264150943394,
          "f": 0.25714285243775514
        },
        "rouge-2": {
          "r": 0.0392156862745098,
          "p": 0.044444444444444446,
          "f": 0.04166666168619851
        },
        "rouge-l": {
          "r": 0.1839080459770115,
          "p": 0.3018867924528302,
          "f": 0.22857142386632662
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mtrl-sci/2503.10373v1",
      "true_abstract": "Topotactic reduction of perovskite oxides offers a powerful approach for\ndiscovering novel phenomena, such as superconducting infinite-layer nickelates\nand polar metallicity, and is commonly accompanied by the emergence of multiple\nvalence states and/or complex crystal fields of transition metals. However,\nunderstanding the complex interplay between crystal chemistry, electronic\nstructure, and physical properties at the spin- and orbital-resolved levels in\nthese reduced systems remains elusive. Here, we combine x-ray absorption\nspectroscopy, resonant inelastic x-ray scattering (RIXS), and density\nfunctional theory calculations to uncover topotactic metal-insulator transition\nand orbital-specific crystal field excitations in brownmillerite\nLa0.67Ca0.33MnO2.5 thin films. We reveal the Mn valence states to be\npredominantly Mn2+/Mn3+, along with their corresponding populations at\noctahedral and tetrahedral sites, which effectively weaken the Mn-O\nhybridization compared to the parent perovskite phase. As a result,\nLa0.67Ca0.33MnO2.5 films exhibit an antiferromagnetic insulating ground state.\nMoreover, by combining the RIXS measurements on selected single-valence\nmanganites, specifically MnO, LaMnO3, and CaMnO3, with orbital- and\nspin-resolved density-of-states calculations, we identify the dd excitations of\noctahedrally and tetrahedrally coordinated Mn2+/Mn3+ ions, directly linking the\nmicroscopic electronic structure to the macroscopic magnetic/electrical\nproperties.",
      "generated_abstract": "The effect of the microstructure of an alloy on its magnetic properties is\nintrinsically linked to the crystallographic state of the alloy. It is well\nestablished that, for alloys with a tetragonal crystal structure, the\nmagnetic ordering temperature is determined by the orientation of the\ncubic lattice planes. In contrast, for alloys with a cubic crystal structure,\nthe ordering temperature is determined by the crystallographic orientation of\nthe individual atoms. This asymmetry in the crystallographic ordering temperature\nhas been interpreted in terms of the difference between the magnetic moment\nassociated with the cubic and the tetragonal axes. However, the existence of a\nsecond crystallographic axis, namely the so-called \"inversion axis\", has\nremained unclear. Here, we show that the inversion axis can play an important\nrole in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14084507042253522,
          "p": 0.273972602739726,
          "f": 0.18604650714288815
        },
        "rouge-2": {
          "r": 0.016483516483516484,
          "p": 0.02857142857142857,
          "f": 0.02090591870485358
        },
        "rouge-l": {
          "r": 0.14084507042253522,
          "p": 0.273972602739726,
          "f": 0.18604650714288815
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/comp-ph/2503.09328v1",
      "true_abstract": "A Schr\\\"odinger bridge is the most probable time-dependent probability\ndistribution that connects an initial probability distribution $w_{i}$ to a\nfinal one $w_{f}$. The problem has been solved and widely used for the case of\nsimple Brownian evolution (non-interacting particles). It is related to the\nproblem of entropy regularized Wasserstein optimal transport. In this article,\nwe generalize Brownian bridges to systems of interacting particles. We derive\nsome equations for the forward and backward single particle ``wave-functions''\nwhich allow to compute the most probable evolution of the single-particle\nprobability between the initial and final distributions.",
      "generated_abstract": "We study the dynamics of a single-domain, rotating, one-dimensional\nfluid, and analyze its steady-state response to external perturbations. We\nfocus on a single rotating domain, with a fixed size, and investigate the\neffect of an external perturbation on the dynamics. We show that, if the\nperturbation is sufficiently strong, the system undergoes a dynamical\ntransition. When the perturbation is sufficiently small, the system enters a\nperiodic state, with a fixed period. We also show that, if the perturbation is\nsufficiently weak, the system remains in a chaotic state. We find that the\ndynamical transition depends on the system's rotation rate and on the\nperturbation strength. We also study the effect of the system's rotational\norder on the dynamics. In particular, we show that, in the case of a\nsingle-domain rotating fluid, the rotational",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.19696969696969696,
          "f": 0.19548871680479407
        },
        "rouge-2": {
          "r": 0.03409090909090909,
          "p": 0.02857142857142857,
          "f": 0.031088077940348187
        },
        "rouge-l": {
          "r": 0.1791044776119403,
          "p": 0.18181818181818182,
          "f": 0.18045112281983164
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.06883v1",
      "true_abstract": "Semantic communication has emerged as a transformative paradigm in\nnext-generation communication systems, leveraging advanced artificial\nintelligence (AI) models to extract and transmit semantic representations for\nefficient information exchange. Nevertheless, the presence of unpredictable\nsemantic noise, such as ambiguity and distortions in transmitted\nrepresentations, often undermines the reliability of received information.\nConventional approaches primarily adopt adversarial training with noise\ninjection to mitigate the adverse effects of noise. However, such methods\nexhibit limited adaptability to varying noise levels and impose additional\ncomputational overhead during model training. To address these challenges, this\npaper proposes Noise-Resilient \\textbf{Se}mantic Communication with\n\\textbf{Hi}gh-and-\\textbf{Lo}w Frequency Decomposition (Se-HiLo) for image\ntransmission. The proposed Se-HiLo incorporates a Finite Scalar Quantization\n(FSQ) based noise-resilient module, which bypasses adversarial training by\nenforcing encoded representations within predefined spaces to enhance noise\nresilience. While FSQ improves robustness, it compromise representational\ndiversity. To alleviate this trade-off, we adopt a transformer-based\nhigh-and-low frequency decomposition module that decouples image\nrepresentations into high-and-low frequency components, mapping them into\nseparate FSQ representation spaces to preserve representational diversity.\nExtensive experiments demonstrate that Se-HiLo achieves superior noise\nresilience and ensures accurate semantic communication across diverse noise\nenvironments.",
      "generated_abstract": "The recent surge of Internet of Things (IoT) devices is driving the\nexplosive growth of the Internet of Medical Things (IoMT). However, the\ncomplex, heterogeneous nature of IoMT poses significant challenges to\nnetworks, as they are exposed to a multitude of challenges such as\nphysical-layer security, network saturation, and dynamic network\nconditions. To address these challenges, this paper introduces the\nNetworked-IoMT (NIMT) model, a novel framework that provides a unified\nperspective on the challenges and solutions of IoMT networks. The NIMT\nmodel establishes a foundation for analyzing the interactions among\nnetworks, devices, and applications, and offers a comprehensive perspective\non the challenges and opportunities of IoMT networks. The paper further\nintroduces the NIMT-based networked security framework to address\nphysical-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12949640287769784,
          "p": 0.23684210526315788,
          "f": 0.16744185589442956
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.05555555555555555,
          "f": 0.0416666619791672
        },
        "rouge-l": {
          "r": 0.12949640287769784,
          "p": 0.23684210526315788,
          "f": 0.16744185589442956
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/MF/2411.11522v1",
      "true_abstract": "This paper presents comparison results and establishes risk bounds for credit\nportfolios within classes of Bernoulli mixture models, assuming conditionally\nindependent defaults that are stochastically increasing with a common risk\nfactor. We provide simple and interpretable conditions for conditional default\nprobabilities that imply a comparison of credit portfolio losses in convex\norder. In the case of threshold models, the ranking of portfolio losses is\nbased on a pointwise comparison of the underlying copulas. Our setting includes\nas special case the well-known Gaussian copula model but allows for general\ntail dependencies, which are crucial for modeling credit portfolio risks.\nMoreover, our results extend the classical parameterized models, such as the\nindustry models CreditMetrics and KMV Portfolio Manager, to a robust setting\nwhere individual parameters or the copula modeling the dependence structure can\nbe ambiguous. A simulation study and a real data example under model\nuncertainty offer evidence supporting the effectiveness of our approach.",
      "generated_abstract": "This paper proposes a novel framework to model the stochastic volatility of\nStochastic Black-Scholes (SBS) models with the stochastic volatility model\n(SVM) as a special case. The SBS model can be seen as a special case of the\ngeneral SBS model when the volatility process is a constant. We show that the\nSVM can be seen as a particular case of the general SBS model. We propose a\nrobust estimator for the SBS model parameters using the SVM, and derive the\nasymptotic properties of the estimated parameters. We also derive the\nasymptotic properties of the SVM estimator, including the asymptotic\ndistribution of the estimator, the asymptotic covariance matrix of the\nestimator, and the asymptotic covariance matrix of the estimator with\nrespect to the model parameters. We further propose an algorithm to find the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2018348623853211,
          "p": 0.3728813559322034,
          "f": 0.26190475734764745
        },
        "rouge-2": {
          "r": 0.04054054054054054,
          "p": 0.061855670103092786,
          "f": 0.04897958705339489
        },
        "rouge-l": {
          "r": 0.1743119266055046,
          "p": 0.3220338983050847,
          "f": 0.22619047163336178
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.19947v1",
      "true_abstract": "Motivated by studying the effects of marriage prospects on students' college\nmajor choices, this paper develops a new econometric test for analyzing the\neffects of an unobservable factor in a setting where this factor potentially\ninfluences both agents' decisions and a binary outcome variable. Our test is\nbuilt upon a flexible copula-based estimation procedure and leverages the\nordered nature of latent utilities of the polychotomous choice model. Using the\nproposed method, we demonstrate that marriage prospects significantly influence\nthe college major choices of college graduates participating in the National\nLongitudinal Study of Youth (97) Survey. Furthermore, we validate the\nrobustness of our findings with alternative tests that use stated marriage\nexpectation measures from our data, thereby demonstrating the applicability and\nvalidity of our testing procedure in real-life scenarios.",
      "generated_abstract": "This study investigates the empirical impact of heterogeneous agent\nsatisfaction on the economic performance of a two-sector economy. The\nagents' satisfaction is modeled as a function of the productivity of the\nsector and the productivity of their firm. We use a panel dataset that\ncovers 19 countries over a 25-year period, from 1996 to 2010. The results show\nthat the productivity of the firm, as well as the productivity of the sector,\nhave a significant impact on the economic performance of the economy. In\naddition, the results suggest that the effect of productivity on the economic\nperformance of the economy is more pronounced for agents with a higher level\nof satisfaction. These findings highlight the importance of understanding the\nimpact of heterogeneous agent satisfaction on economic performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2,
          "f": 0.1666666618055557
        },
        "rouge-2": {
          "r": 0.00819672131147541,
          "p": 0.010101010101010102,
          "f": 0.009049768809814132
        },
        "rouge-l": {
          "r": 0.0989010989010989,
          "p": 0.13846153846153847,
          "f": 0.11538461052350449
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.10283v1",
      "true_abstract": "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
      "generated_abstract": "In this paper, we study the regularity of the solution of the free boundary\nproblems for the compressible Euler and Navier-Stokes equations in a domain\nwith a smooth boundary. The domain is assumed to be a flat plane, and the\nboundary is smooth and simply connected. We prove the regularity of the\nsolutions under the Dirichlet or the Neumann boundary conditions. Moreover, we\nprovide an essential boundary condition for the Neumann problem which is\nequivalent to the Dirichlet condition on the boundary.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2653061224489796,
          "f": 0.18571428116428584
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.0410958904109589,
          "f": 0.028436014431841877
        },
        "rouge-l": {
          "r": 0.0989010989010989,
          "p": 0.1836734693877551,
          "f": 0.1285714240214287
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09767v1",
      "true_abstract": "Classical unsupervised learning methods like clustering and linear\ndimensionality reduction parametrize large-scale geometry when it is discrete\nor linear, while more modern methods from manifold learning find low\ndimensional representation or infer local geometry by constructing a graph on\nthe input data. More recently, topological data analysis popularized the use of\nsimplicial complexes to represent data topology with two main methodologies:\ntopological inference with geometric complexes and large-scale topology\nvisualization with Mapper graphs -- central to these is the nerve construction\nfrom topology, which builds a simplicial complex given a cover of a space by\nsubsets. While successful, these have limitations: geometric complexes scale\npoorly with data size, and Mapper graphs can be hard to tune and only contain\nlow dimensional information. In this paper, we propose to study the problem of\nlearning covers in its own right, and from the perspective of optimization. We\ndescribe a method for learning topologically-faithful covers of geometric\ndatasets, and show that the simplicial complexes thus obtained can outperform\nstandard topological inference approaches in terms of size, and Mapper-type\nalgorithms in terms of representation of large-scale topology.",
      "generated_abstract": "In the era of AI, it is essential for researchers to address the ethical\nproblems that arise from their work. In this paper, we introduce the first\nresearch framework for studying the ethical issues that arise from AI research.\nThis framework is based on the concept of AI research ethics, which defines the\nethical standards that should be followed by researchers while working with AI.\nOur framework consists of three parts: AI research ethics, AI research\nethics-informed approaches, and AI research ethics-driven approaches. We\ndiscuss each part in turn, starting with AI research ethics and then\nconcluding with AI research ethics-informed and AI research ethics-driven\napproaches. We conclude by discussing some of the challenges that exist in\nresearching ethical issues in AI. Finally, we outline some future directions\nfor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1896551724137931,
          "p": 0.3013698630136986,
          "f": 0.23280422806304424
        },
        "rouge-2": {
          "r": 0.028901734104046242,
          "p": 0.046296296296296294,
          "f": 0.03558718387963741
        },
        "rouge-l": {
          "r": 0.16379310344827586,
          "p": 0.2602739726027397,
          "f": 0.2010581963170125
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.04488v1",
      "true_abstract": "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
      "generated_abstract": "We introduce a new notion of a strong Hilbert-Schmidt norm on the class\nof compact operators on Hilbert space. We prove that this norm is equivalent\nto the Hilbert-Schmidt norm on the class of compact operators on a Hilbert\nspace of a certain type, and we give an example of a Hilbert-Schmidt norm on\nthis class which is not equivalent to the strong Hilbert-Schmidt norm. We\nprovide some applications of the strong Hilbert-Schmidt norm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12790697674418605,
          "p": 0.3235294117647059,
          "f": 0.18333332927222232
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.05660377358490566,
          "f": 0.03370786098661837
        },
        "rouge-l": {
          "r": 0.09302325581395349,
          "p": 0.23529411764705882,
          "f": 0.13333332927222236
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/str-el/2503.09717v1",
      "true_abstract": "We clarify the lore that anomaly-free symmetries are either on-site or can be\ntransformed into on-site symmetries. We prove that any finite, internal,\nanomaly-free symmetry in a 1+1d lattice Hamiltonian system can be disentangled\ninto an on-site symmetry by introducing ancillas and applying conjugation via a\nfinite-depth quantum circuit. We provide an explicit construction of the\ndisentangling circuit using Gauss's law operators and emphasize the necessity\nof adding ancillas. Our result establishes the converse to a generalized\nLieb-Schultz-Mattis theorem by demonstrating that any anomaly-free symmetry\nadmits a trivially gapped Hamiltonian.",
      "generated_abstract": "The theory of quasiparticle tunneling in quantum wires is a cornerstone of\nquantum information theory, and it is crucial for applications in quantum\ncomputing and quantum communication. This work focuses on the theory of\nquasiparticle tunneling in quantum wires in the presence of a time-dependent\nmagnetic field. We employ a density functional theory approach to compute the\nquasiparticle tunneling rates in a magnetic field, accounting for both\nmagnetization-driven and magnetic-field-assisted tunneling. We also consider\nthe effects of the tunneling barrier, and show that a gate voltage can be\nutilized to tune the barrier height, which can be tuned from zero to a finite\nvalue by applying a time-dependent magnetic field. We demonstrate that this\ntime-dependent magnetic field can be implemented in quantum computing\napplications. We find that the gate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20967741935483872,
          "p": 0.19117647058823528,
          "f": 0.199999995010651
        },
        "rouge-2": {
          "r": 0.046511627906976744,
          "p": 0.03636363636363636,
          "f": 0.0408163216055816
        },
        "rouge-l": {
          "r": 0.20967741935483872,
          "p": 0.19117647058823528,
          "f": 0.199999995010651
        }
      }
    },
    {
      "paper_id": "cs.CY.econ/GN/2501.19407v2",
      "true_abstract": "Surnames often convey implicit markers of social status, wealth, and lineage,\nshaping perceptions in ways that can perpetuate systemic biases and\nintergenerational inequality. This study is the first of its kind to\ninvestigate whether and how surnames influence AI-driven decision-making,\nfocusing on their effects across key areas such as hiring recommendations,\nleadership appointments, and loan approvals. Using 72,000 evaluations of 600\nsurnames from the United States and Thailand, two countries with distinct\nsociohistorical contexts and surname conventions, we classify names into four\ncategories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our\nfindings show that elite surnames consistently increase AI-generated\nperceptions of power, intelligence, and wealth, which in turn influence\nAI-driven decisions in high-stakes contexts. Mediation analysis reveals\nperceived intelligence as a key mechanism through which surname biases\ninfluence AI decision-making process. While providing objective qualifications\nalongside surnames mitigates most of these biases, it does not eliminate them\nentirely, especially in contexts where candidate credentials are low. These\nfindings highlight the need for fairness-aware algorithms and robust policy\nmeasures to prevent AI systems from reinforcing systemic inequalities tied to\nsurnames, an often-overlooked bias compared to more salient characteristics\nsuch as race and gender. Our work calls for a critical reassessment of\nalgorithmic accountability and its broader societal impact, particularly in\nsystems designed to uphold meritocratic principles while counteracting the\nperpetuation of intergenerational privilege.",
      "generated_abstract": "As the number of people using social media grows, the social graph has become\nmore complex and complex networks have emerged. The development of graph\ncomputing algorithms and graph-based machine learning models has made it\npossible to analyze the structure and properties of social graphs. However,\nthese algorithms and models often fail to account for the complexities of\nsocial networks, leading to biased results. To address this, we propose a\ngraph-based framework that integrates graph theory and machine learning to\ncapture the complexities of social networks. The framework employs a graph\nrepresentation that captures the relationships among nodes and edges, which\nallows it to model social interactions more accurately than traditional\napproaches. By integrating graph theory with machine learning, the framework\nprovides a more accurate and comprehensive understanding of social networks. We\nevaluate the performance of our framework on real-world social network data and\ndemonstrate its ability",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.2,
          "f": 0.13953487917793417
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.014598540145985401,
          "f": 0.011204477062984051
        },
        "rouge-l": {
          "r": 0.10119047619047619,
          "p": 0.18888888888888888,
          "f": 0.13178294119343806
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10299v1",
      "true_abstract": "In this work, we systematically study the interactions of the $S$-wave\n$D^{(*)}\\bar{B}^{(*)}$ systems within the framework of chiral effective field\ntheory in heavy hadron formalism. We calculate the $D^{(*)}\\bar{B}^{(*)}$\neffective potentials up to next-to-leading order, explore the bound state\nformations, and investigate the $D^{(*)}\\bar{B}^{(*)}$ scattering properties\nsuch as scattering rate, scattering length, and effective range. Our results\nshow that all $I=1$ $D^{(*)}\\bar{B}^{(*)}$ potentials are repulsive, preventing\nthe formation of bound states, while the $I=0$ potentials are generally\nattractive. Specifically, we get two important observations: first, the shallow\nbound state is more likely to exist in the $D\\bar{B}[I(J^{P})=0(0^{+})]$ system\nthan in the $D\\bar{B}^{*}[I(J^{P})=0(1^{+})]$ system; second,\n$D^{*}\\bar{B}^{*}[I(J^{P})=0(0^{+})]$ and $D^{*}\\bar{B}^{*}[I(J^{P})=0(1^{+})]$\nsystems possess relatively large binding energies and positive scattering\nlengths, which suggests strong bound state formations in these channels. So the\nattractions in the $D^{*}\\bar{B}^{*}[I=0]$ systems are deeper than those in the\n$D\\bar{B}^{(*)}[I=0]$ systems, thus we strongly recommend the future experiment\nto search for the $D^{*}\\bar{B}^{*}[I=0]$ tetraquark systems. In addition, we\nalso investigate the dependencies of the $D\\bar{B}^{(*)}$ binding energies on\nthe contact low-energy coupling constants (LECs).",
      "generated_abstract": "We investigate the impact of the nonperturbative gluon dynamics in the\nrealistic case of an axially symmetric QGP in a two-flavor QCD-like theory,\nwhere the color-flavor locking is suppressed by the chiral condensate. We show\nthat the nonperturbative effects are crucial in determining the temperature\nand chemical potential of the baryon-number-carrying quarks and the\ncontribution from the gluon dynamics is generally larger than the contribution\nfrom the gluon saturation dynamics. We further discuss the role of the\ngluon saturation in the temperature-dependence of the chemical potential of\nquarks and the baryon-number-carrying quarks in the QGP. Finally, we obtain\nthe critical baryon-chemical-potential and the critical temperature of the\nQGP from the temperature",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.25925925925925924,
          "f": 0.1666666623044219
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.047058823529411764,
          "f": 0.03238865945352388
        },
        "rouge-l": {
          "r": 0.09649122807017543,
          "p": 0.2037037037037037,
          "f": 0.1309523765901362
        }
      }
    },
    {
      "paper_id": "quant-ph.stat/TH/2502.14950v1",
      "true_abstract": "Inferring causal models from observed correlations is a challenging task,\ncrucial to many areas of science. In order to alleviate the effort, it is\nimportant to know whether symmetries in the observations correspond to\nsymmetries in the underlying realization. Via an explicit example, we answer\nthis question in the negative. We use a tripartite probability distribution\nover binary events that is realized by using three (different) independent\nsources of classical randomness. We prove that even removing the condition that\nthe sources distribute systems described by classical physics, the requirements\nthat i) the sources distribute the same physical systems, ii) these physical\nsystems respect relativistic causality, and iii) the correlations are the\nobserved ones, are incompatible.",
      "generated_abstract": "We develop a rigorous theory for the emergence of phase-space foliation in\nlarge quantum systems. We show that the entanglement entropy of the system\nconverges to a limiting form in the thermodynamic limit, which is determined by\nthe Hamiltonian and its eigenstates. We further derive a simple scaling law\nfor the entanglement entropy of any local observable of the system. This law\nis invariant under the action of the Hamiltonian. We also develop a\nperturbative expansion of the entanglement entropy around the thermodynamic\nlimit. This expansion is valid for a broad class of Hamiltonians, including\nclassical, quantum, and mixed systems. We demonstrate that the entropy of a\nlocal observable scales as $S\\propto\\log N$, where $N$ is the number of\nparticles. This scaling holds for both pure and mixed states. We also show that\nthe entropy of any local observable is proportional to the energy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13580246913580246,
          "p": 0.14864864864864866,
          "f": 0.14193547888116564
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.017094017094017096,
          "f": 0.01769911005051437
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.14864864864864866,
          "f": 0.14193547888116564
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2501.00002v2",
      "true_abstract": "In this paper we present a QUBO formulation for the Takuzu game (or Binairo),\nfor the most recent LinkedIn game, Tango, and for its generalizations. We\noptimize the number of variables needed to solve the combinatorial problem,\nmaking it suitable to be solved by quantum devices with fewer resources.",
      "generated_abstract": "In this paper, we consider a scenario where the decision-maker needs to\nchoose a data policy among several policies. In particular, we focus on the\ncase where the data policy is a random variable, and we investigate how this\nrandomness affects the decision-maker's decision. We introduce a novel\nframework, which we call the policy-dependent randomness model, that\ngeneralizes existing models of randomness. We show that in this model, the\ndecision-maker faces an infinite-horizon, mixed-strategy Nash equilibrium\n(NE) problem. This NE problem is non-concave and its solution is not\nmonotone. We characterize the NE under a class of randomness functions that we\ncall \"approximate\". We further characterize the NE under a class of randomness\nfunctions that we call \"exact\". We establish that the approximate NE is the\nunique solution of the problem and that the exact NE",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23255813953488372,
          "p": 0.136986301369863,
          "f": 0.1724137884378717
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.00847457627118644,
          "f": 0.01212120804701698
        },
        "rouge-l": {
          "r": 0.23255813953488372,
          "p": 0.136986301369863,
          "f": 0.1724137884378717
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.12935v1",
      "true_abstract": "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
      "generated_abstract": "Gene regulation is a complex process that involves multiple cellular\nfactors. In this work, we aim to investigate the impact of the transcription\nfactor (TF) TGF-B1 on the TGF-B1 target gene BMP2. BMP2 is an important\ncellular regulator of the skeletal system. We used the yeast two-hybrid (Y2H)\ntechnique to identify the TF binding sites on the promoter of BMP2. Our study\ndemonstrated that the TF binding sites in the promoter of BMP2 are located on\nthree distinct regions, the first region is 5'-GGACTTCTTTCAGAAGTAA-3', the\nsecond region is 5'-TGCAGACCTATGCAGTTT-3', and the third region is 5'-AGAATC\nGTACTGGTTCT",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08843537414965986,
          "p": 0.20634920634920634,
          "f": 0.12380951960952395
        },
        "rouge-2": {
          "r": 0.013824884792626729,
          "p": 0.03529411764705882,
          "f": 0.019867545624096258
        },
        "rouge-l": {
          "r": 0.08163265306122448,
          "p": 0.19047619047619047,
          "f": 0.11428571008571443
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06431v1",
      "true_abstract": "The kidney paired donation (KPD) program provides an innovative solution to\novercome incompatibility challenges in kidney transplants by matching\nincompatible donor-patient pairs and facilitating kidney exchanges. To address\nunequal access to transplant opportunities, there are two widely used fairness\ncriteria: group fairness and individual fairness. However, these criteria do\nnot consider protected patient features, which refer to characteristics legally\nor ethically recognized as needing protection from discrimination, such as race\nand gender. Motivated by the calibration principle in machine learning, we\nintroduce a new fairness criterion: the matching outcome should be\nconditionally independent of the protected feature, given the sensitization\nlevel. We integrate this fairness criterion as a constraint within the KPD\noptimization framework and propose a computationally efficient solution.\nTheoretically, we analyze the associated price of fairness using random graph\nmodels. Empirically, we compare our fairness criterion with group fairness and\nindividual fairness through both simulations and a real-data example.",
      "generated_abstract": "We present a novel method for computing the optimal bandwidth for kernel\nsmoothing. The method is based on a novel and efficient method for constructing\na test statistic, which is a kernel-based version of the Wald test statistic\nand can be computed efficiently using the methods of statistical learning.\nThe test statistic is shown to be asymptotically normal and asymptotically\nlocally asymptotically normal. Furthermore, we show that the optimal bandwidth\nfor kernel smoothing is the median of the asymptotic distribution of the test\nstatistic. We illustrate the method by applying it to the problem of\ncomputing the optimal bandwidth for kernel smoothing of the kernel density\nestimator. We provide a detailed analysis of the asymptotic properties of the\ntest statistic, which is shown to be asymptotically normal and asymptotically\nlocally asymptotically normal. The asymptotic distribution of the test statistic\nis shown",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11504424778761062,
          "p": 0.22413793103448276,
          "f": 0.15204677914298428
        },
        "rouge-2": {
          "r": 0.006896551724137931,
          "p": 0.010309278350515464,
          "f": 0.008264458006628022
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.22413793103448276,
          "f": 0.15204677914298428
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.econ/TH/2501.09778v2",
      "true_abstract": "We propose a disaggregated representation of production through an\nagent-based fund-flow model (NGR-ADAPT) within which inefficiencies, such as\nfactor idleness and production instability, emerge from endogenous frictions.\nThe model incorporates productivity dynamics (learning and depreciation) and is\nextended with time-saving process innovations. Specifically, we assume that\nworkers possess inherent creativity that flourishes during idle periods. The\nfirm, rather than laying off idle workers, is assumed to exploit this potential\nby involving them in the innovation process. Results show that a firm's\norganizational and managerial decisions, the temporal structure of the\nproduction system, the speed at which workers learn and forget, and the pace of\ninnovation are critical factors influencing production efficiency in both the\nshort and long run. The co-evolution of production and innovation processes\nemerges in our model through the two-sided effects of idleness: whereas it\ndrives skill decay it is also a condition for creative thinking that can be\nleveraged for innovation. In doing so, we question the utilization of labour as\nan adjustment variable in a productive organisation. The paper concludes by\ndiscussing potential solutions to this issue and suggesting avenues for future\nresearch.",
      "generated_abstract": "We study a model of resource allocation in a network with heterogeneous\nresources and heterogeneous preferences. The network is divided into two\nregions, each of which can have either zero or non-zero resources. The\npreferences of individuals in the network are encoded by a binary variable\nrepresenting their preference for the resources in each region. We show that\nthe network is connected if and only if all individuals prefer the same\npreferences. We derive the optimal allocation of resources and the optimal\npreferences for each individual in the network. We further show that the\nnetwork is connected if and only if all individuals prefer the same\npreferences. We also derive the optimal allocation of resources and the\noptimal preferences for each individual in the network. We derive the\nconnection between the optimal allocation of resources and the optimal\npreferences of the individuals.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14728682170542637,
          "p": 0.34545454545454546,
          "f": 0.20652173493915416
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.05434782608695652,
          "f": 0.036101078596098506
        },
        "rouge-l": {
          "r": 0.13178294573643412,
          "p": 0.3090909090909091,
          "f": 0.18478260450437156
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.10387v1",
      "true_abstract": "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms.",
      "generated_abstract": "We introduce a novel, self-contained framework for the development of\nnew, high-performance, and scalable neural network architectures. The core\ntechnology behind the framework is the use of a simple but powerful class of\nneural network architectures known as neural networks with linear layers, or\nNLLs. NLLs have the remarkable property that they can be expressed as a\nmultiplication of a linear layer with a neural network (NN) parameterized by a\nlinear layer. This property allows us to efficiently implement the NLL with a\nsimple linear layer and to derive the required computational complexity.\n  The framework provides a comprehensive and self-contained approach for\ndesigning and optimizing NLL-based neural networks. It includes: (1) the\nfoundation of NLL-based neural networks; (2) a general framework for\noptimizing NLL-based neural networks; (3) the implementation of the framework",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1981981981981982,
          "p": 0.2972972972972973,
          "f": 0.23783783303783795
        },
        "rouge-2": {
          "r": 0.029940119760479042,
          "p": 0.043478260869565216,
          "f": 0.03546098807781364
        },
        "rouge-l": {
          "r": 0.18018018018018017,
          "p": 0.2702702702702703,
          "f": 0.21621621141621633
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.10496v1",
      "true_abstract": "Modeling natural phenomena with artificial neural networks (ANNs) often\nprovides highly accurate predictions. However, ANNs often suffer from\nover-parameterization, complicating interpretation and raising uncertainty\nissues. Bayesian neural networks (BNNs) address the latter by representing\nweights as probability distributions, allowing for predictive uncertainty\nevaluation. Latent binary Bayesian neural networks (LBBNNs) further handle\nstructural uncertainty and sparsify models by removing redundant weights. This\narticle advances LBBNNs by enabling covariates to skip to any succeeding layer\nor be excluded, simplifying networks and clarifying input impacts on\npredictions. Ultimately, a linear model or even a constant can be found to be\noptimal for a specific problem at hand. Furthermore, the input-skip LBBNN\napproach reduces network density significantly compared to standard LBBNNs,\nachieving over 99% reduction for small networks and over 99.9% for larger ones,\nwhile still maintaining high predictive accuracy and uncertainty measurement.\nFor example, on MNIST, we reached 97% accuracy and great calibration with just\n935 weights, reaching state-of-the-art for compression of neural networks.\nFurthermore, the proposed method accurately identifies the true covariates and\nadjusts for system non-linearity. The main contribution is the introduction of\nactive paths, enhancing directly designed global and local explanations within\nthe LBBNN framework, that have theoretical guarantees and do not require post\nhoc external tools for explanations.",
      "generated_abstract": "The classical Cram\\'er-Rao lower bound (CRLB) for estimating an unknown\nparameter $\\theta$ is given by\n$\\mathrm{CRLB}(\\theta) = \\mathrm{Var}(\\hat{\\theta}) = \\frac{1}{n}\\sum_{i=1}^n\n\\frac{(\\hat{\\theta}_i - \\theta)^2}{\\sigma^2_i}$, where $\\sigma^2_i$ is the\nstandard deviation of the $i$th observation. We introduce a new lower bound\n$\\mathrm{CRLB}_{\\mathrm{new}}(\\theta)$ that is less conservative than\n$\\mathrm{CRLB}(\\theta)$ for large $n$. This new bound is obtained by\ndecomposing the sample mean of the squared residuals into the sum of two terms:\none term is the sample mean of the squared residuals within the support of the\nestimator and the other term",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07692307692307693,
          "p": 0.2,
          "f": 0.11111110709876558
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.02564102564102564,
          "f": 0.014285710266327658
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.2,
          "f": 0.11111110709876558
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2502.21276v1",
      "true_abstract": "Boosting has emerged as a useful machine learning technique over the past\nthree decades, attracting increased attention. Most advancements in this area,\nhowever, have primarily focused on numerical implementation procedures, often\nlacking rigorous theoretical justifications. Moreover, these approaches are\ngenerally designed for datasets with fully observed data, and their validity\ncan be compromised by the presence of missing observations. In this paper, we\nemploy semiparametric estimation approaches to develop boosting prediction\nmethods for data with missing responses. We explore two strategies for\nadjusting the loss functions to account for missingness effects. The proposed\nmethods are implemented using a functional gradient descent algorithm, and\ntheir theoretical properties, including algorithm convergence and estimator\nconsistency, are rigorously established. Numerical studies demonstrate that the\nproposed methods perform well in finite sample settings.",
      "generated_abstract": "This paper presents a novel approach for the identification of the\neffect of a treatment on the time of death of a patient in a longitudinal\nstudy. The proposed method allows for the identification of the effect of a\ntreatment on the time of death of a patient, even when the treatment is\nassociated with a time-dependent effect of the treatment on the patient's\noutcome. This is achieved by using a model-based approach to estimate the\neffect of the treatment on the time of death of the patient. The proposed\napproach is based on a method proposed by Kearns et al. (2014), who developed a\nmethod for the identification of the effect of a treatment on the time of death\nof a patient. However, the method developed by Kearns et al. (2014) does not\nallow for the identification of the effect of a treatment on the time of death\nof the patient when the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11320754716981132,
          "p": 0.24,
          "f": 0.15384614949046693
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.024096385542168676,
          "f": 0.019230764434635072
        },
        "rouge-l": {
          "r": 0.11320754716981132,
          "p": 0.24,
          "f": 0.15384614949046693
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.07924v1",
      "true_abstract": "We study a fundamental challenge in the economics of innovation: an inventor\nmust reveal details of a new idea to secure compensation or funding, yet such\ndisclosure risks expropriation. We present a model in which a seller (inventor)\nand buyer (investor) bargain over an information good under the threat of\nhold-up. In the classical setting, the seller withholds disclosure to avoid\nmisappropriation, leading to inefficiency. We show that trusted execution\nenvironments (TEEs) combined with AI agents can mitigate and even fully\neliminate this hold-up problem. By delegating the disclosure and payment\ndecisions to tamper-proof programs, the seller can safely reveal the invention\nwithout risking expropriation, achieving full disclosure and an efficient ex\npost transfer. Moreover, even if the invention's value exceeds a threshold that\nTEEs can fully secure, partial disclosure still improves outcomes compared to\nno disclosure. Recognizing that real AI agents are imperfect, we model \"agent\nerrors\" in payments or disclosures and demonstrate that budget caps and\nacceptance thresholds suffice to preserve most of the efficiency gains.\n  Our results imply that cryptographic or hardware-based solutions can function\nas an \"ironclad NDA,\" substantially mitigating the fundamental\ndisclosure-appropriation paradox first identified by Arrow (1962) and Nelson\n(1959). This has far-reaching policy implications for fostering R&D, technology\ntransfer, and collaboration.",
      "generated_abstract": "We study the problem of designing a set of actions that maximizes the expected\noptimality payoff in a stochastic, partially observed, and partially observable\nenvironment. The actions are taken by a single agent and the environment is\ndescribed by a Markov decision process. The agent's information about the\nenvironment is limited to the current state of the environment and a few\ninformation states, while the agent has complete information about the\nenvironment's future state. We consider two distinct settings: a\nsingle-stage and a multi-stage environment. In the single-stage setting, the\nagent's state is observed at time 0, and the environment evolves according to a\nMarkov chain. In the multi-stage setting, the agent observes the current state\nof the environment and one information state at each time step. The\nenvironment's dynamics are described by a stochastic Markov chain. We\ncharacterize the set of actions that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10738255033557047,
          "p": 0.23529411764705882,
          "f": 0.14746543348467808
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.03389830508474576,
          "f": 0.0248447158535559
        },
        "rouge-l": {
          "r": 0.09395973154362416,
          "p": 0.20588235294117646,
          "f": 0.12903225376117577
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.06389v1",
      "true_abstract": "Network estimation has been a critical component of single-cell\ntranscriptomic data analysis, which can provide crucial insights into the\ncomplex interplay among genes, facilitating uncovering the biological basis of\nhuman life at single-cell resolution. Despite notable achievements, existing\nmethodologies often falter in their practicality, primarily due to their narrow\nfocus on simplistic linear relationships and inadequate handling of cellular\nheterogeneity. To bridge these gaps, we propose a joint regularized deep neural\nnetwork method incorporating a Mahalanobis distance-based K-means clustering\n(JRDNN-KM) to estimate multiple networks for various cell subgroups\nsimultaneously, accounting for both unknown cellular heterogeneity and\nzero-inflation and, more importantly, complex nonlinear relationships among\ngenes. We innovatively introduce a selection layer for network construction and\ndevelop homogeneous and heterogeneous hidden layers to accommodate commonality\nand specificity across multiple networks. Through simulations and applications\nto real single-cell transcriptomic data for multiple tissues and species, we\nshow that JRDNN-KM constructs networks with more accuracy and biological\ninterpretability and, meanwhile, identifies more accurate cell subgroups\ncompared to the state-of-the-art methods in the literature. Building on the\nnetwork construction, we further find hub genes with important biological\nimplications and modules with statistical enrichment of biological processes.",
      "generated_abstract": "In recent years, artificial intelligence (AI) has been applied to the\nanalysis of high-dimensional data. However, in most cases, the data are\nsubjected to the assumption of normality, which has been established for a\nlong time. In this paper, we explore the impact of the assumption of normality\non the performance of various AI methods, including neural networks, random\nforests, and deep learning, on various types of high-dimensional data. We\nprovide a comprehensive analysis of the impact of the assumption of normality\non the performance of various AI methods, including neural networks, random\nforests, and deep learning, on various types of high-dimensional data. We\nprovide a comprehensive analysis of the impact of the assumption of normality\non the performance of various AI methods, including neural networks, random\nforests, and deep learning, on various types of high-dimensional data. We\nprovide a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1323529411764706,
          "p": 0.34615384615384615,
          "f": 0.19148935770031697
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.04477611940298507,
          "f": 0.023715411125936052
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.28846153846153844,
          "f": 0.1595744640832957
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "The paper studies the impact of economic policies on the evolution of\nindustrial dynamics. Using the European Union's industrial production data, we\nshow that the introduction of tariffs (i.e. trade barriers) can substantially\nincrease the costs of the production of specific products and the costs of\nmanufacturing processes. This effect is larger for less developed countries.\nThe introduction of trade barriers can also reduce the number of producers in\na given industry, or the average number of workers per unit of output. The\nresults highlight the importance of trade policy in shaping economic\ndynamics and, in particular, in shaping industrial dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.24615384615384617,
          "f": 0.18934910769230784
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.02127659574468085,
          "f": 0.016949147749211354
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.24615384615384617,
          "f": 0.18934910769230784
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2410.21090v1",
      "true_abstract": "One of goals in soft robotics is to achive spontaneous behavior like real\norganisms. To gain a clue to achieve this, we examined the long (16-hour)\nspontaneous exploratory locomotion of snails. The active forager snail, Tegula\nnigerrima, from an intertidal rocky shore was selected to test the general\nhypothesis that nervous systems are inherently near a critical state, which is\nself-organized to drive spontaneous animal behavior. This hypothesis, known as\nthe critical brain hypothesis, was originally proposed for vertebrate species,\nbut it might be applicable to other invertebrate species as well. We first\ninvestigated the power spectra of the speed of locomotion of the snails\n($N=39$). The spectra showed $1/{f^\\alpha}$ fluctuation, which is one of the\nsignatures of self-organized criticality. The $\\alpha$ was estimated to be\nabout 0.9. We further examined whether the spatial and temporal quantities show\nmultiple power-laws and scaling relations, which are rigorous criteria of\ncriticality. Although the satisfaction of these criteria is limited to a\ntruncated region and provides limited evidence to demonstrate the aspect of\nself-organization, the multiple power-laws and the scaling relations were\noverall satisfied. Therefore, these results additionally support the generality\nof the critical brain hypothesis.",
      "generated_abstract": "Recent advances in artificial intelligence (AI) have resulted in the\ndevelopment of a wide range of machine learning (ML) models, many of which\nexhibit promising applications in the field of biomedicine. These models are\noften evaluated using the macro-accuracy metric, which measures the percentage\nof correct predictions, ignoring the individual performance of each individual\nprediction. However, this metric can lead to misinterpretations and is\nprone to overestimation. In this study, we propose a novel metric, the\ndifference-of-mean-error (DME), which accounts for the individual performance\nof each prediction and corrects for the overestimation of macro-accuracy.\nFurthermore, we present a novel benchmark dataset for evaluating DME,\ncomprising 364,000 training samples and 364,000 test samples, enabling",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09523809523809523,
          "p": 0.15789473684210525,
          "f": 0.11881187649446151
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0873015873015873,
          "p": 0.14473684210526316,
          "f": 0.10891088639545163
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/GN/2411.16793v1",
      "true_abstract": "Spatial transcriptomics (ST) provides high-resolution pathological images and\nwhole-transcriptomic expression profiles at individual spots across whole-slide\nscales. This setting makes it an ideal data source to develop multimodal\nfoundation models. Although recent studies attempted to fine-tune visual\nencoders with trainable gene encoders based on spot-level, the absence of a\nwider slide perspective and spatial intrinsic relationships limits their\nability to capture ST-specific insights effectively. Here, we introduce\nST-Align, the first foundation model designed for ST that deeply aligns\nimage-gene pairs by incorporating spatial context, effectively bridging\npathological imaging with genomic features. We design a novel pretraining\nframework with a three-target alignment strategy for ST-Align, enabling (1)\nmulti-scale alignment across image-gene pairs, capturing both spot- and\nniche-level contexts for a comprehensive perspective, and (2) cross-level\nalignment of multimodal insights, connecting localized cellular characteristics\nand broader tissue architecture. Additionally, ST-Align employs specialized\nencoders tailored to distinct ST contexts, followed by an Attention-Based\nFusion Network (ABFN) for enhanced multimodal fusion, effectively merging\ndomain-shared knowledge with ST-specific insights from both pathological and\ngenomic data. We pre-trained ST-Align on 1.3 million spot-niche pairs and\nevaluated its performance through two downstream tasks across six datasets,\ndemonstrating superior zero-shot and few-shot capabilities. ST-Align highlights\nthe potential for reducing the cost of ST and providing valuable insights into\nthe distinction of critical compositions within human tissue.",
      "generated_abstract": "We present a novel approach to learning the latent structure of large\nchimera datasets. Inspired by the fact that the genome of the human is composed\nof two different chromosomes, we propose a novel approach to learn the\nchromosome structures of the human genome from large-scale datasets. This\napproach, which we call Molecular Chimera Embedding (MCE), represents the\ngenome as a molecular graph, where nodes represent genes and edges represent\ninteractions between genes. The MCE method learns a latent space where each\nnode is a gene and each edge is a gene-gene interaction. We demonstrate that\nthe MCE method can learn chromosome structures from a variety of datasets,\nincluding whole-genome datasets, chromosome-scale datasets, and\ngenome-wide-association-study datasets. We also show that the MCE method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10062893081761007,
          "p": 0.2191780821917808,
          "f": 0.1379310301698129
        },
        "rouge-2": {
          "r": 0.004608294930875576,
          "p": 0.009174311926605505,
          "f": 0.006134964873916437
        },
        "rouge-l": {
          "r": 0.09433962264150944,
          "p": 0.2054794520547945,
          "f": 0.12931034051464046
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/space-ph/2503.00705v1",
      "true_abstract": "The first severe (G4) geomagnetic storm of Solar Cycle 25 occurred on 23-24\nApril 2023, following the arrival of a Coronal Mass Ejection (CME) on 23 April.\nThe characteristics of this CME, measured from coronagraphs (speed and mass),\ndid not indicate that it would trigger such an intense geomagnetic storm. In\nthis work, our aim is to understand why this CME led to such a geoeffective\noutcome. Our analysis spans from the source active region to the corona and\ninner heliosphere through 1 au using multiwavelength, multi-viewpoint remote\nsensing observations and in situ data. We find that rotation and possibly\ndeflection of the CME resulted in an axial magnetic field nearly parallel to\nthe ecliptic plane during the Earth encounter, which might explain the storm's\nseverity. Additionally, we find that imaging away from the Sun-Earth line is\ncrucial in hindcasting the CME Time-of-Arrival at Earth. The position (0.39 au)\nand detailed images from the SoloHI telescope onboard the Solar Orbiter\nmission, in combination with SOHO and STEREO images, helped decisively with the\nthree-dimensional (3D) reconstruction of the CME.",
      "generated_abstract": "The Lunar Orbit Insertion (LOI) maneuver, which is performed during the\ndeparture phase of the Lunar Orbit Insertion Mission (LOIM) to insert the\nLunar Orbital Platform (LOP) into a stable orbit around the Moon, is a key\nstep for the mission. A large number of LOI maneuvers are planned, and their\nperformance is assessed through a series of experiments on the Moon. These\nexperiments use a lunar-orbit-keeping spacecraft to fly in the vicinity of the\nLOP. The experiments aim to study the effects of different maneuver parameters\nand conditions on the LOI performance. In this work, we present the results of\nthe first experiment, which uses a lunar-orbit-keeping spacecraft to fly in the\nvicinity of the LOP and performs a series of LOI maneuvers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.140625,
          "p": 0.25,
          "f": 0.17999999539200012
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.038834951456310676,
          "f": 0.029304024605187995
        },
        "rouge-l": {
          "r": 0.1171875,
          "p": 0.20833333333333334,
          "f": 0.14999999539200012
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.16524v2",
      "true_abstract": "This study demonstrates the persistent dominance of identity based voting\nacross democratic systems, using the United States as a primary case and\ncomparative analyses of 19 other democracies as counterfactuals. Drawing solely\non election data from the Roper Center (1976 through recent cycles), we employ\nOLS regression, ANOVA, and correlation tests to show that race remains the\nstrongest predictor of party affiliation in the US (p < 0.001), with White\nvoters favoring Republicans and Black voters consistently supporting Democrats\n(85% since 1988). Income, education, and gender exemplified by gaps like 10\npoints in 2020 further shape voting patterns, yet racial identity predominates.\nComparative evidence from majoritarian (e.g., India), proportional (e.g.,\nGermany through 2025), and hybrid (e.g., South Korea with a 25 point gender\ngap) systems reveals no democracy where issue based voting fully supplants\nidentity based voting. Digital mobilization amplifies this trend globally.\nThese findings underscore identity enduring role in electoral behavior,\nchallenging assumptions of policy driven democratic choice.",
      "generated_abstract": "This paper develops a novel approach to estimating the long-run mean return\nand the mean of the distribution of the return to a stock, using a\nregression-based methodology that includes multiple regressors. The proposed\nmethodology is particularly suited to cases where the number of stocks in a\nportfolio is very large, as is the case in portfolio optimization problems. The\nproposed methodology is also particularly well-suited to cases where the\nunderlying stocks are correlated with each other, as is the case in portfolio\noptimization problems. The methodology is applied to a real data set on\ncompanies in the U.S. stock market. The empirical results show that the\nestimated mean of the distribution of the return to a stock and the mean of the\nreturn to a stock are substantially different from zero. The mean of the\ndistribution of the return to a stock is approximately",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1297709923664122,
          "p": 0.2537313432835821,
          "f": 0.17171716723956748
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.019417475728155338,
          "f": 0.015325665720117166
        },
        "rouge-l": {
          "r": 0.10687022900763359,
          "p": 0.208955223880597,
          "f": 0.14141413693653723
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2409.00780v1",
      "true_abstract": "The main purpose of this work is the derivation of a functional partial\ndifferential equation (FPDE) for the calculations of equity-linked insurance\npolicies, where the payment stream may depend on the whole past history of the\nfinancial asset. To this end, we employ variational techniques from the theory\nof functional It\\^o calculus.",
      "generated_abstract": "We study the problem of estimating the distribution of the maximum loss of a\nasset in a portfolio of assets. Traditionally, this problem is addressed by\nmaximizing the expected loss. However, this approach has two main drawbacks:\nfirst, it is based on a naive assumption that the assets in the portfolio are\nindependent, a key assumption that is not true in practice, and second, it is\nbased on the worst-case scenario, which is not representative of the actual\nportfolio. We propose a novel approach that accounts for this by estimating\nthe distribution of the maximum loss in the portfolio, which is representative\nof the actual portfolio. Our approach involves three steps: (1) estimating the\ndistribution of the maximum loss in the portfolio, (2) estimating the\ndistribution of the maximum loss in each asset, and (3) estimating the\ndistribution of the maximum loss in the portfolio. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21951219512195122,
          "p": 0.14754098360655737,
          "f": 0.17647058342752797
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.0297029702970297,
          "f": 0.039473679751558674
        },
        "rouge-l": {
          "r": 0.1951219512195122,
          "p": 0.13114754098360656,
          "f": 0.15686274029027314
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.21106v1",
      "true_abstract": "Recent advancements in AI and medical imaging offer transformative potential\nin emergency head CT interpretation for reducing assessment times and improving\naccuracy in the face of an increasing request of such scans and a global\nshortage in radiologists. This study introduces a 3D foundation model for\ndetecting diverse neuro-trauma findings with high accuracy and efficiency.\nUsing large language models (LLMs) for automatic labeling, we generated\ncomprehensive multi-label annotations for critical conditions. Our approach\ninvolved pretraining neural networks for hemorrhage subtype segmentation and\nbrain anatomy parcellation, which were integrated into a pretrained\ncomprehensive neuro-trauma detection network through multimodal fine-tuning.\nPerformance evaluation against expert annotations and comparison with CT-CLIP\ndemonstrated strong triage accuracy across major neuro-trauma findings, such as\nhemorrhage and midline shift, as well as less frequent critical conditions such\nas cerebral edema and arterial hyperdensity. The integration of neuro-specific\nfeatures significantly enhanced diagnostic capabilities, achieving an average\nAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundation\nmodels in medical imaging, serving as a benchmark for future AI-assisted\nneuro-trauma diagnostics in emergency radiology.",
      "generated_abstract": "The increasing demand for more personalized and precise medical imaging\nremains a major challenge in the field of medical imaging. While deep learning\nhas demonstrated remarkable success in image analysis, the complexities of\ninherent anatomical variations make it challenging to effectively utilize\ndeep learning in medical image analysis. To address this, we propose a novel\nmulti-modal framework, named MODAL-Net, which integrates multi-modal\ninformation, such as medical images and medical text, for accurate disease\ndiagnosis. Our approach leverages the advantages of both modalities and\nconsiders the potential of combining them. Specifically, we employ the\ncontrastive learning technique to effectively extract and utilize the\nmulti-modal information for disease diagnosis, and propose a novel text-guided\ndisease diagnosis model, which utilizes the text to enhance the learning of\nmulti-modal features. To further improve the model's",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15873015873015872,
          "p": 0.23809523809523808,
          "f": 0.1904761856761906
        },
        "rouge-2": {
          "r": 0.03508771929824561,
          "p": 0.04838709677419355,
          "f": 0.04067796122861305
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.20238095238095238,
          "f": 0.16190475710476204
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.02389v1",
      "true_abstract": "We propose a method for accurately detecting bioacoustic sound events that is\nrobust to overlapping events, a common issue in domains such as ethology,\necology and conservation. While standard methods employ a frame-based,\nmulti-label approach, we introduce an onset-based detection method which we\nname Voxaboxen. It takes inspiration from object detection methods in computer\nvision, but simultaneously takes advantage of recent advances in\nself-supervised audio encoders. For each time window, Voxaboxen predicts\nwhether it contains the start of a vocalization and how long the vocalization\nis. It also does the same in reverse, predicting whether each window contains\nthe end of a vocalization, and how long ago it started. The two resulting sets\nof bounding boxes are then fused using a graph-matching algorithm. We also\nrelease a new dataset designed to measure performance on detecting overlapping\nvocalizations. This consists of recordings of zebra finches annotated with\ntemporally-strong labels and showing frequent overlaps. We test Voxaboxen on\nseven existing data sets and on our new data set. We compare Voxaboxen to\nnatural baselines and existing sound event detection methods and demonstrate\nSotA results. Further experiments show that improvements are robust to frequent\nvocalization overlap.",
      "generated_abstract": "The rapid development of the Internet of Things (IoT) has driven the\nemergence of wireless networks that transmit data over large distances. These\nnetworks are characterized by high-dimensional state variables and complex\nnetwork structures. This paper introduces a novel control framework for\nreal-time network optimization using the multi-agent reinforcement learning\n(MARL) approach. The framework is designed to maximize the network capacity by\nestablishing optimal communication links between network nodes. In this paper,\nthe proposed framework is applied to the wireless networks, which are\ncharacterized by the linear Markovian channel model. Specifically, we consider\nthe case when the channel gains are unknown. To handle this scenario, we\nintroduce a novel channel state information (CSI) estimation method based on\nthe Bayesian inference. The proposed framework is validated through simulation\nusing the real-world network data. The simulation results demonstrate the\neffectiveness",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16153846153846155,
          "p": 0.22340425531914893,
          "f": 0.18749999512914553
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.015384615384615385,
          "f": 0.012658223005129237
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.2127659574468085,
          "f": 0.17857142370057413
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.06790v1",
      "true_abstract": "Recent research applying text-to-image (T2I) diffusion models to real-world\nsuper-resolution (SR) has achieved remarkable success. However, fundamental\nmisalignments between T2I and SR targets result in a dilemma between inference\nspeed and detail fidelity. Specifically, T2I tasks prioritize multi-step\ninversion to synthesize coherent outputs aligned with textual prompts and\nshrink the latent space to reduce generating complexity. Contrariwise, SR tasks\npreserve most information from low-resolution input while solely restoring\nhigh-frequency details, thus necessitating sufficient latent space and fewer\ninference steps. To bridge the gap, we present a one-step diffusion model for\ngenerative detail restoration, GenDR, distilled from a tailored diffusion model\nwith larger latent space. In detail, we train a new SD2.1-VAE16 (0.9B) via\nrepresentation alignment to expand latent space without enlarging the model\nsize. Regarding step-distillation, we propose consistent score identity\ndistillation (CiD) that incorporates SR task-specific loss into score\ndistillation to leverage more SR priors and align the training target.\nFurthermore, we extend CiD with adversarial learning and representation\nalignment (CiDA) to enhance perceptual quality and accelerate training. We also\npolish the pipeline to achieve a more efficient inference. Experimental results\ndemonstrate that GenDR achieves state-of-the-art performance in both\nquantitative metrics and visual fidelity.",
      "generated_abstract": "In this paper, we propose a novel framework for the multi-view image\n(MVI) restoration task, where the input image is comprised of multiple\nsub-images captured from different viewpoints. To address the challenges of\nintrinsic image distortion, viewpoint-specific occlusion, and scene variation,\nwe introduce a viewpoint-aware occlusion model (VOCM) and a viewpoint-specific\nocclusion model (VSCO). The VOCM aims to model the occlusion of the target\nviewpoint while the VSCO captures the occlusion of the entire scene. To\neffectively capture the viewpoint-specific occlusion, we design a viewpoint-aware\noccurrence matrix (VAM) model to integrate the occlusion probability of each\nviewpoint. Furthermore, to effectively model the viewpoint-specific occlusion,\nwe introduce a viewpoint-specific occ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1056338028169014,
          "p": 0.23809523809523808,
          "f": 0.14634145915716845
        },
        "rouge-2": {
          "r": 0.005235602094240838,
          "p": 0.010638297872340425,
          "f": 0.007017539438845506
        },
        "rouge-l": {
          "r": 0.1056338028169014,
          "p": 0.23809523809523808,
          "f": 0.14634145915716845
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2503.03850v1",
      "true_abstract": "Although physics has become increasingly computational, with computing even\nbeing considered the third pillar of physics [1], it is still not well\nintegrated into physics education [2]. Research suggests that integrating\nComputational Thinking (CT) into physics enhances conceptual understanding and\nstrengthens students ability to model and analyze phenomena [3]. Building on\nthis, we designed a didactic sequence for K9 students to foster specific CT\npractices while reinforcing fundamental kinematics concepts. Assessments\nhighlight student's ability to apply CT skills to analyze accelerated motion.\nThis activity can be seamlessly integrated into introductory kinematics\ncourses.",
      "generated_abstract": "The ability to understand and communicate science to a wide audience is\ncrucial for science education. This is particularly true for introductory\nscience courses, where students are expected to understand and explain complex\ntopics to their peers and teachers. In this paper, we introduce a new\nmultimodal, interactive science communication tool that combines video\nrecordings of lectures with a user-driven, real-time scientific visualization\nbased on the VizieR data center. The tool allows students to engage with\nscientific topics, while simultaneously learning how to communicate their\nunderstanding to others. The tool also provides a more interactive and\nimmersive learning experience, as it allows students to directly interact with\nthe scientific visualization. Through a series of user studies, we found that\nstudents enjoyed the tool and were able to effectively communicate their\nunderstanding of scientific concepts to others. Additionally, the scientific\nvisualization allowed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2597402597402597,
          "p": 0.23255813953488372,
          "f": 0.24539876802137842
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.015503875968992248,
          "f": 0.018433174901995357
        },
        "rouge-l": {
          "r": 0.22077922077922077,
          "p": 0.19767441860465115,
          "f": 0.20858895207045816
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04337v1",
      "true_abstract": "In this paper, a compressor system is analyzed in order to show its\ncharacteristics and design a control scheme to improve its efficiency. A\nmathematical technique has been created to forecast the onset of surge and\ninstability in a compressor chart, drawing from the nonlinear Greitzer and\nMoore model. This approach employs the phase plane and Jacobian matrix to\nidentify both stable and unstable regions within the compressor, as well as to\ncapture the limit cycle within the unstable region. A predictive analytical\napproach for anticipating compressor surge and instability is of great\nimportance in system instrumentation and control. State space model is built up\nby nonlinear Greitzer equations. Validation from previous study about especial\ncompressor will be considered for evaluation of mathematic method. Upstream\nflow acts as a disturbance to control loop and controller cannot satisfy\ndesired requirements with flow variances, ergo it is essential that controller\nis adapted to new conditions. Since control signal is linearly related to\nsystem output, a PD controller is used to control compressor system. An\nadaptive PD controller is designed with MRAS method based on a reference model.\nAdaptive controller can stabilize compressor and increase its efficiency in the\npresence of any disturbances. Simulation results shows that an adaptive\ncontroller can provide good performance and convergence in case of speed\nchanges by adapting gain parameters, and adaptive will be compared with normal\nPID. Finally, controller stability is investigated.",
      "generated_abstract": "This paper proposes a novel unified framework for multi-agent systems with\ndiscrete-time stochastic processes and stochastic control. We first reformulate\nthe problem as a stochastic optimal control problem with a single reference\nsystem, where the agents' dynamics are characterized by a set of nonlinear\ncontinuous-time stochastic differential equations. The state of each agent is\nobserved, which leads to a deterministic control problem with a single\nreference system. We then reformulate the control problem as a finite-horizon\ndiscrete-time stochastic optimal control problem with a single reference\nsystem, where the agents' dynamics are characterized by a set of nonlinear\ndiscrete-time stochastic differential equations. The state of each agent is\nobserved, which leads to a finite-horizon optimal control problem with a single\nreference system. We present the well-posedness of the obtained optimal\ncontrol problems and propose an explicit",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11888111888111888,
          "p": 0.29310344827586204,
          "f": 0.1691542247498825
        },
        "rouge-2": {
          "r": 0.004524886877828055,
          "p": 0.012345679012345678,
          "f": 0.006622512630807992
        },
        "rouge-l": {
          "r": 0.11188811188811189,
          "p": 0.27586206896551724,
          "f": 0.15920397599366365
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.15104v2",
      "true_abstract": "In both artificial and biological systems, the centered kernel alignment\n(CKA) has become a widely used tool for quantifying neural representation\nsimilarity. While current CKA estimators typically correct for the effects of\nfinite stimuli sampling, the effects of sampling a subset of neurons are\noverlooked, introducing notable bias in standard experimental scenarios. Here,\nwe provide a theoretical analysis showing how this bias is affected by the\nrepresentation geometry. We then introduce a novel estimator that corrects for\nboth input and feature sampling. We use our method for evaluating both\nbrain-to-brain and model-to-brain alignments and show that it delivers reliable\ncomparisons even with very sparsely sampled neurons. We perform within-animal\nand across-animal comparisons on electrophysiological data from visual cortical\nareas V1, V4, and IT data, and use these as benchmarks to evaluate\nmodel-to-brain alignment. We also apply our method to reveal how object\nrepresentations become progressively disentangled across layers in both\nbiological and artificial systems. These findings underscore the importance of\ncorrecting feature-sampling biases in CKA and demonstrate that our\nbias-corrected estimator provides a more faithful measure of representation\nalignment. The improved estimates increase our understanding of how neural\nactivity is structured across both biological and artificial systems.",
      "generated_abstract": "Recent studies have shown that the majority of the genes and proteins\n(about 70%) in the human genome are under the control of the epigenetic\nmodifications, including histone modifications, methylation and acetylation.\nHowever, the mechanisms of how these epigenetic modifications affect gene\nexpression are still not fully understood. In this study, we firstly analyzed\nthe epigenetic changes of genes and proteins in different cancer types using\nRNA sequencing (RNA-seq) data, and then analyzed the epigenetic changes of\ngene expression in different stages of cancer using a large cancer dataset.\nThe results show that the overall changes of epigenetic modification in\ncancer are higher than those in normal tissues. The methylation rate of\nhistone H3 in cancer is higher than that in normal tissues, and the methylation\nrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12878787878787878,
          "p": 0.23943661971830985,
          "f": 0.1674876801805432
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.02727272727272727,
          "f": 0.020066884981153412
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.22535211267605634,
          "f": 0.15763546343177473
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.10419v1",
      "true_abstract": "In motion simulation, motion cueing algorithms are used for the trajectory\nplanning of the motion simulator platform, where workspace limitations prevent\ndirect reproduction of reference trajectories. Strategies such as motion\nwashout, which return the platform to its center, are crucial in these\nsettings. For serial robotic MSPs with highly nonlinear workspaces, it is\nessential to maximize the efficient utilization of the MSPs kinematic and\ndynamic capabilities. Traditional approaches, including classical washout\nfiltering and linear model predictive control, fail to consider\nplatform-specific, nonlinear properties, while nonlinear model predictive\ncontrol, though comprehensive, imposes high computational demands that hinder\nreal-time, pilot-in-the-loop application without further simplification. To\novercome these limitations, we introduce a novel approach using deep\nreinforcement learning for motion cueing, demonstrated here for the first time\nin a 6-degree-of-freedom setting with full consideration of the MSPs kinematic\nnonlinearities. Previous work by the authors successfully demonstrated the\napplication of DRL to a simplified 2-DOF setup, which did not consider\nkinematic or dynamic constraints. This approach has been extended to all 6 DOF\nby incorporating a complete kinematic model of the MSP into the algorithm, a\ncrucial step for enabling its application on a real motion simulator. The\ntraining of the DRL-MCA is based on Proximal Policy Optimization in an\nactor-critic implementation combined with an automated hyperparameter\noptimization. After detailing the necessary training framework and the\nalgorithm itself, we provide a comprehensive validation, demonstrating that the\nDRL MCA achieves competitive performance against established algorithms.\nMoreover, it generates feasible trajectories by respecting all system\nconstraints and meets all real-time requirements with low...",
      "generated_abstract": "The recent advancements in large language models (LLMs) have opened new\naccessible and intuitive frontiers for automating diverse tasks. However, the\nlimitations of existing LLM-based systems have raised concerns regarding their\nability to handle complex and unstructured data. This study aims to evaluate\nthe performance of LLMs in the area of complex data processing using\ncomputational linguistics as the basis for their evaluation. The study focuses\non four LLMs (Bart, GPT-3.5, GPT-4, and GPT-4.5) and six domains (sports,\nfinance, medicine, physics, chemistry, and art). The study used a mixed-methods\napproach, with a combination of human evaluation and experimental evaluation.\nThe experimental evaluation was conducted using a questionnaire and a\nquestionnaire-based survey. The results show that LLMs have significant\npotential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10404624277456648,
          "p": 0.21176470588235294,
          "f": 0.1395348793026262
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09248554913294797,
          "p": 0.18823529411764706,
          "f": 0.12403100333363394
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/EC/2502.17906v2",
      "true_abstract": "In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.",
      "generated_abstract": "This paper develops a novel approach to predict the volatility of the S&P\nand Nasdaq 100 index returns, leveraging a time-varying volatility model with\nnonlinear stochastic differential equations (SDEs). By incorporating the\ntime-varying volatility model into the SDE framework, our approach provides a\nmore comprehensive understanding of the volatility dynamics of stock returns\nand allows for a deeper analysis of the underlying factors that influence stock\nvolatility. Additionally, the nonlinear SDE model allows for the incorporation\nof nonlinear volatility models into the framework, enhancing the model's\ncapacity to capture complex volatility behaviors. By analyzing the\ntime-varying volatility model, we provide insights into the factors that\ninfluence stock volatility and identify potential strategies for managing\nvolatility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.24615384615384617,
          "f": 0.16580310434105627
        },
        "rouge-2": {
          "r": 0.012121212121212121,
          "p": 0.02,
          "f": 0.015094334923461768
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.24615384615384617,
          "f": 0.16580310434105627
        }
      }
    },
    {
      "paper_id": "math.CV.math/CV/2503.05855v1",
      "true_abstract": "We first study subextensions of m-subharmonic functions in weighted energy\nclasses with given boundary values. The results are used to approximate an\nm-subharmonic function in weighted energy classes with given boundary values by\nan increasing sequence of m-subharmonic functions defined on larger domains.",
      "generated_abstract": "The $n$-dimensional symmetric space $G/H$ is a compact connected\nsymmetric space of non-compact type, where $G$ is a complex reductive group\nacting linearly on a complex manifold $M$ and $H$ is a maximal compact\nsubgroup. In this paper, we study the dimension of $G/H$ and the connected\ncomponent of the identity in $G/H$ as $n$ tends to infinity. In particular, we\nprove that $G/H$ has dimension $n-1$ as $n$ tends to infinity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.13043478260869565,
          "f": 0.15789473206371207
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.13043478260869565,
          "f": 0.15789473206371207
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.07615v1",
      "true_abstract": "Climate change is increasing the frequency and severity of natural disasters\nworldwide. Media coverage of these events may be vital to generate empathy and\nmobilize global populations to address the common threat posed by climate\nchange. Using a dataset of 466 news sources from 123 countries, covering 135\nmillion news articles since 2016, we apply an event study framework to measure\ncross-border media activity following natural disasters. Our results shows that\nwhile media attention rises after disasters, it is heavily skewed towards\ncertain events, notably earthquakes, accidents, and wildfires. In contrast,\nclimatologically salient events such as floods, droughts, or extreme\ntemperatures receive less coverage. This cross-border disaster reporting is\nstrongly related to the number of deaths associated with the event, especially\nwhen the affected populations share strong social ties or genetic similarities\nwith those in the reporting country. Achieving more balanced media coverage\nacross different types of natural disasters may be essential to counteract\nskewed perceptions. Further, fostering closer social connections between\ncountries may enhance empathy and mobilize the resources necessary to confront\nthe global threat of climate change.",
      "generated_abstract": "This paper proposes a novel method to calculate the probability of the\nintergenerational transmission of a disease. We introduce a novel methodology\nbased on the generalized copula that allows us to account for the correlation\nbetween generations in the transmission process. This approach allows us to\nquantify the probability of a specific transmission event, including the\nprobability of transmission of the disease at the current generation. The\nmethodology is applied to the transmission of the HIV/AIDS epidemic and is\nillustrated with historical data from South Africa. The methodology provides\na more accurate estimate of the transmission rates, which is critical in\nestimating the epidemic dynamics, as well as in planning interventions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.23076923076923078,
          "f": 0.15384614940170957
        },
        "rouge-2": {
          "r": 0.011695906432748537,
          "p": 0.02127659574468085,
          "f": 0.015094335044786005
        },
        "rouge-l": {
          "r": 0.1076923076923077,
          "p": 0.2153846153846154,
          "f": 0.1435897391452993
        }
      }
    },
    {
      "paper_id": "math.PR.math/PR/2503.09550v1",
      "true_abstract": "We prove that the limit profile of a sequence of reversible Markov chains\nexhibiting total variation cutoff is a continuous function, under a computable\ncondition involving the spectrum of the transition matrix and the cutoff\nwindow.",
      "generated_abstract": "In this paper, we investigate the convergence of the limit of the sequence\nof iterates\n  $f_n(x,y) := (x+y+n)^n$, $n \\in \\mathbb{N}$, to the solution of the following\nstochastic differential equation\n  $$ d f_n(x,y) = \\frac{1}{n} (f_{n-1}(x,y) - f_{n-2}(x,y))dt + \\sigma\n  (f_{n-1}(x,y),f_{n-2}(x,y))dW(t), $$\nwhere $W(t)$ is a standard Brownian motion. We prove that the sequence\n$(f_n(x,y))_{n \\in \\mathbb{N}}$ converges weakly to the solution of the\nstochastic differential equation\n  $$ d f(x,y) = \\frac{1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.32142857142857145,
          "p": 0.18,
          "f": 0.23076922616699547
        },
        "rouge-2": {
          "r": 0.2,
          "p": 0.11666666666666667,
          "f": 0.14736841639889214
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.14,
          "f": 0.17948717488494426
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.10443v1",
      "true_abstract": "We prove a completely explicit and effective upper bound for the\nN\\'eron--Tate height of rational points of curves of genus at least $2$ over\nnumber fields, provided that they have enough automorphisms with respect to the\nMordell--Weil rank of their jacobian. Our arguments build on Arakelov theory\nfor arithmetic surfaces. Our bounds are practical, and we illustrate this by\nexplicitly computing the rational points of a certain genus $2$ curve whose\njacobian has Mordell--Weil rank $2$.",
      "generated_abstract": "We investigate the relation between the generalized Euler sum and the\ngeneralized sum of products, which is the subject of the classical\nEuler-Riemann-Sulam theorem. We prove that if the coefficients of the\ngenerating function of the generalized Euler sum are positive integers and\nthe coefficients of the generating function of the generalized sum of products\nare integers, then the coefficients of the generating function of the\ngeneralized Euler sum are integers. We also prove that the coefficients of the\ngenerating function of the generalized sum of products are integers if the\ncoefficients of the generating function of the generalized Euler sum are\nintegers, and if the coefficients of the generating function of the generalized\nsum of products are positive integers. We apply our results to the number\nsystems of Fermat and Gauss, and the number systems of Dirichlet, Riemann and\nSulam.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15517241379310345,
          "p": 0.21951219512195122,
          "f": 0.1818181769656159
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.03076923076923077,
          "f": 0.02919707530502509
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.17073170731707318,
          "f": 0.14141413656157553
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.10846v1",
      "true_abstract": "The digitalization of public administration has advanced significantly on a\nglobal scale. Many governments now view digital platforms as essential for\nimproving the delivery of public services and fostering direct communication\nbetween citizens and public institutions. However, this view overlooks the role\nplayed by digital intermediaries significantly shape the provision of\ne-government services. Using Chile as a case study, we analyze these\nintermediaries through a national survey on digitalization, we find five types\nof intermediaries: family members, peers, political figures, bureaucrats, and\ncommunity leaders. The first two classes comprise close intermediaries, while\nthe latter three comprise hierarchical intermediaries. Our findings suggest\nthat all these intermediaries are a critical but underexplored element in the\ndigitalization of public administration.",
      "generated_abstract": "This paper examines the impact of globalization on the labor market and\nestimates the labor market response to the trade balance shock. We propose a\ndynamic model of the labor market with a deterministic labor supply shock. The\nmodel accounts for the interaction between labor supply shocks and the\ntrade balance shock. We show that the labor market response is driven by the\ntrade balance shock and labor supply shock. Furthermore, the trade balance shock\nsignificantly affects the labor market response, especially for low-skilled\nemployees. This paper contributes to the literature on globalization, trade,\nand labor markets by showing how the trade balance shock and labor supply\nshock interact to shape the labor market response.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.23076923076923078,
          "f": 0.1739130387817687
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.012658227848101266,
          "f": 0.01052631093130418
        },
        "rouge-l": {
          "r": 0.13953488372093023,
          "p": 0.23076923076923078,
          "f": 0.1739130387817687
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2407.11453v1",
      "true_abstract": "Telomeres are repetitive sequences of nucleotides at the end of chromosomes,\nwhose evolution over time is intrinsically related to biological ageing. In\nmost cells, with each cell division, telomeres shorten due to the so-called end\nreplication problem, which can lead to replicative senescence and a variety of\nage-related diseases. On the other hand, in certain cells, the presence of the\nenzyme telomerase can lead to the lengthening of telomeres, which may delay or\nprevent the onset of such diseases but can also increase the risk of cancer.In\nthis article, we propose a stochastic representation of this biological model,\nwhich takes into account multiple chromosomes per cell, the effect of\ntelomerase, different cell types and the dependence of the distribution of\ntelomere length on the dynamics of the process. We study theoretical properties\nof this model, including its long-term behaviour. In addition, we investigate\nnumerically the impact of the model parameters on biologically relevant\nquantities, such as the Hayflick limit and the Malthusian parameter of the\npopulation of cells.",
      "generated_abstract": "The emergence of single-cell genomic technologies has transformed the study\nof the complex biology of cells. However, the use of these technologies in\ndisease research remains challenging due to the complexity of biological\nphenotypes, heterogeneity of cell states, and the need to overcome technical\nchallenges. This review aims to provide an overview of the current state of\nthe art in single-cell genomics and disease research, and highlight key\ntechnological developments and challenges that have yet to be resolved. We\ndiscuss the limitations of existing single-cell genomic technologies and\nsuggest how the integration of multiple technologies, including single-cell\ngenomics, single-cell transcriptomics, single-cell proteomics, and single-cell\nepigenomics, can address these challenges. We also highlight the need for\ninnovative technologies and computational methods to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13274336283185842,
          "p": 0.2,
          "f": 0.1595744632893845
        },
        "rouge-2": {
          "r": 0.031446540880503145,
          "p": 0.043859649122807015,
          "f": 0.03663003176589044
        },
        "rouge-l": {
          "r": 0.10619469026548672,
          "p": 0.16,
          "f": 0.12765956967236328
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/OT/2407.05572v2",
      "true_abstract": "This study addresses important issues of traffic congestion and vehicle\nemissions in urban areas by developing a comprehensive mathematical framework\nto evaluate Park-and-Ride (PnR) systems. The proposed approach integrates\nqueueing theory and emissions modeling to simultaneously assess waiting times,\ntravel times, and vehicle emissions under various PnR usage scenarios. The\nmethodology employs a novel combination of Monte Carlo simulation and matrix\ngeometric analytic methods to analyze a queueing network representing PnR\nfacilities and road traffic. A case study of Tsukuba, Japan demonstrates the\nmodel's applicability, revealing potential reductions in social costs related\nto total trip time and emissions through optimized PnR policies. Specifically,\nthe study found that implementing optimal bus frequency and capacity policies\ncould reduce total social costs by up to 30\\% compared to current conditions.\nThis research contributes to the literature by providing a unified framework\nfor evaluating PnR systems that considers both time and environmental costs,\noffering valuable insights for urban planners and policymakers seeking to\nimprove transportation sustainability. The proposed model utilizes a single\nserver queue with a deterministic service time and multiple arrival streams to\nrepresent traffic flow, incorporating both private cars and public buses.\nEmissions are calculated using the Methodologies for Estimating Air Pollutant\nEmissions from Transport (MEET) framework. The social cost of emissions and\ntotal trip time (SCETT) is introduced as a comprehensive metric for evaluating\nPnR system performance.",
      "generated_abstract": "We propose a novel approach for estimating the joint distribution of a\ngenerative model (e.g., a Markov chain) and the corresponding transition\nprobabilities. This approach is based on a novel likelihood function that\ncaptures the joint distribution of the state vector (the model output) and the\ntransition matrix. We demonstrate the applicability of this approach through\nsimulations and a real-world application in image generation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09333333333333334,
          "p": 0.3111111111111111,
          "f": 0.14358974003944783
        },
        "rouge-2": {
          "r": 0.004694835680751174,
          "p": 0.016666666666666666,
          "f": 0.0073260038964714355
        },
        "rouge-l": {
          "r": 0.08,
          "p": 0.26666666666666666,
          "f": 0.12307691952662732
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.10166v1",
      "true_abstract": "With the proliferation of images in online content, language-guided image\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\nencompassing a variety of subtasks with diverse input forms. While the\ndevelopment of large multimodal models (LMMs) has significantly facilitated\nthese tasks, existing approaches often address them in isolation, requiring the\nconstruction of separate systems for each task. This not only increases system\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\nlanguage ambiguity and complex image content, making it difficult for retrieval\nsystems to provide accurate and reliable results. To this end, we propose\nImageScope, a training-free, three-stage framework that leverages collective\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\nthe compositional nature of language, which transforms diverse LGIR tasks into\na generalized text-to-image retrieval process, along with the reasoning of LMMs\nserving as a universal verification to refine the results. To be specific, in\nthe first stage, we improve the robustness of the framework by synthesizing\nsearch intents across varying levels of semantic granularity using\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\nreflect on retrieval results by verifying predicate propositions locally, and\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\ndatasets demonstrate that ImageScope outperforms competitive baselines.\nComprehensive evaluations and ablation studies further confirm the\neffectiveness of our design.",
      "generated_abstract": "This paper introduces a novel approach to automatic speech recognition\n(ASR) using multimodal input: the combination of audio and text. The\nimplementation is based on the Neural Speech Recognition Toolkit (NSTk) and\nemploys the NSTk API to support the integration of text into the ASR pipeline.\nThe approach uses the ASR model provided by the NSTk API and the Speech\nRecognition Engine (SRE) from the OpenSSL library. The NSTk API provides a\nstandardized interface to support a wide range of hardware and software\nplatforms. The SRE, based on the OpenSSL library, provides an interface for\nLinux systems. The ASR model is trained and tested on a dataset of 15,000\nEnglish sentences. The model achieves an F1 score of 98.5% on the test set,\ncompared to an F1 score of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09146341463414634,
          "p": 0.2,
          "f": 0.1255230082456541
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07926829268292683,
          "p": 0.17333333333333334,
          "f": 0.10878660657201396
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2503.06251v1",
      "true_abstract": "Short-term patterns in financial time series form the cornerstone of many\nalgorithmic trading strategies, yet extracting these patterns reliably from\nnoisy market data remains a formidable challenge. In this paper, we propose an\nentropy-assisted framework for identifying high-quality, non-overlapping\npatterns that exhibit consistent behavior over time. We ground our approach in\nthe premise that historical patterns, when accurately clustered and pruned, can\nyield substantial predictive power for short-term price movements. To achieve\nthis, we incorporate an entropy-based measure as a proxy for information gain.\nPatterns that lead to high one-sided movements in historical data, yet retain\nlow local entropy, are more informative in signaling future market direction.\nCompared to conventional clustering techniques such as K-means and Gaussian\nMixture Models (GMM), which often yield biased or unbalanced groupings, our\napproach emphasizes balance over a forced visual boundary, ensuring that\nquality patterns are not lost due to over-segmentation. By emphasizing both\npredictive purity (low local entropy) and historical profitability, our method\nachieves a balanced representation of Buy and Sell patterns, making it better\nsuited for short-term algorithmic trading strategies.",
      "generated_abstract": "We introduce a novel approach to the synthesis of complex financial models\nfrom limited data, leveraging a combination of multi-agent reinforcement\nlearning (MARL) and Bayesian optimization (BO). Our approach, termed\n\\emph{MARL-BO}, consists of two stages: (i) MARL-based policy training to\ngenerate a high-fidelity policy, and (ii) BO-based optimization of the policy\nto ensure that the synthesized model captures the desired behavior. We show\nthat our approach outperforms traditional MARL-based and BO-based methods for\nsynthesizing complex financial models, achieving superior performance across\nmultiple metrics. Furthermore, we demonstrate the potential of our approach to\ngenerate complex financial models in a realistic financial scenario, with\nspecific applications in portfolio optimization, asset pricing, and\nfinancial risk management.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11940298507462686,
          "p": 0.20512820512820512,
          "f": 0.15094339157529382
        },
        "rouge-2": {
          "r": 0.005780346820809248,
          "p": 0.009433962264150943,
          "f": 0.0071684540697092925
        },
        "rouge-l": {
          "r": 0.11940298507462686,
          "p": 0.20512820512820512,
          "f": 0.15094339157529382
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.17096v1",
      "true_abstract": "Estimating market impact and transaction costs of large trades (metaorders)\nis a very important topic in finance. However, using models of price and trade\nbased on public market data provide average price trajectories which are\nqualitatively different from what is observed during real metaorder executions:\nthe price increases linearly, rather than in a concave way, during the\nexecution and the amount of reversion after its end is very limited. We claim\nthat this is a generic phenomenon due to the fact that even sophisticated\nstatistical models are unable to correctly describe the origin of the\nautocorrelation of the order flow. We propose a modified Transient Impact Model\nwhich provides more realistic trajectories by assuming that only a fraction of\nthe metaorder trading triggers market order flow. Interestingly, in our model\nthere is a critical condition on the kernels of the price and order flow\nequations in which market impact becomes permanent.",
      "generated_abstract": "We study the estimation of the efficient frontiers in a continuous-time\nmarket model. We focus on the case of a single asset, where we assume that the\nrisk-neutral measure is Gaussian. In this setting, the efficient frontier is a\ncombination of the mean and covariance of the price process. We propose a\ntwo-step method to construct the efficient frontier using the mean-variance\noptimization. We show that the resulting estimator is unbiased and asymptotically\nnormal, and we propose a test statistic to identify the efficient frontier. We\nalso develop a simulation-based approach to evaluate the efficiency of\nportfolios. We demonstrate the robustness of the method on simulated and real\ndata.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18556701030927836,
          "p": 0.28125,
          "f": 0.22360247968211114
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.07142857142857142,
          "f": 0.05882352456747445
        },
        "rouge-l": {
          "r": 0.14432989690721648,
          "p": 0.21875,
          "f": 0.17391303868832234
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.physics/bio-ph/2503.06239v1",
      "true_abstract": "Interactions between crawling cells, which are essential for many biological\nprocesses, can be quantified by measuring cell-cell collisions. Conventionally,\nexperiments of cell-cell collisions are conducted on two-dimensional flat\nsubstrates, where colliding cells repolarize and move away upon contact with\none another in \"contact inhibition of locomotion\" (CIL). Inspired by recent\nexperiments that show cells on suspended nanofibers have qualitatively\ndifferent CIL behaviors than those on flat substrates, we develop a phase field\nmodel of cell motility and two-cell collisions in fiber geometries. Our model\nincludes cell-cell and cell-fiber adhesion, and a simple positive feedback\nmechanism of cell polarity. We focus on cell collisions on two parallel fibers,\nfinding that larger cell deformability (lower membrane tension), larger\npositive feedback of polarization, and larger fiber spacing promote more\noccurrences of cells walking past one another. We can capture this behavior\nusing a simple linear stability analysis on the cell-cell interface upon\ncollision.",
      "generated_abstract": "We show that the emergence of a new dynamical regime, the so-called\ndynamical phase transition, can be predicted from the basic principles of\nquantum mechanics. The transition occurs when the strength of the interaction\nbetween the two parts of the system, one of which is an oscillator, becomes\nstrong enough to overcome the repulsive force between the oscillator and the\nother oscillator. We provide a simple and intuitive argument for this\nprediction and demonstrate it through a few simple examples. The argument is\nbased on the coupling of the system to a harmonic oscillator, which can be\nunderstood as an analogy between the two parts of the system. We show that the\ndynamical phase transition occurs when the coupling strength between the two\nparts of the system exceeds a critical value. We provide a simple and intuitive\nargument for this prediction and demonstrate it through a few simple\nexamples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17307692307692307,
          "p": 0.2608695652173913,
          "f": 0.20809248075378406
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.028037383177570093,
          "f": 0.023999995103681
        },
        "rouge-l": {
          "r": 0.16346153846153846,
          "p": 0.2463768115942029,
          "f": 0.1965317871121656
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.15422v1",
      "true_abstract": "We study the classical object reallocation problem under strict preferences,\nwith a focus on characterizing \"TTC domains\" -- preference domains on which the\nTop Trading Cycles (TTC) mechanism is the unique mechanism satisfying\nindividual rationality, Pareto efficiency, and strategyproofness. We introduce\na sufficient condition for a domain to be a TTC domain, which we call the\ntop-two condition. This condition requires that, within any subset of objects,\nif two objects can each be most-preferred, they can also be the top-two\nmost-preferred objects (in both possible orders). A weaker version of this\ncondition, applying only to subsets of size three, is shown to be necessary.\nThese results provide a complete characterization of TTC domains for the case\nof three objects, unify prior studies on specific domains such as single-peaked\nand single-dipped preferences, and classify several previously unexplored\ndomains as TTC domains or not.",
      "generated_abstract": "We propose a novel method to identify the equilibria of stochastic\noptimal control problems with a deterministic policy. The method relies on the\nuse of a stochastic optimal control problem with a discrete-time policy and an\narbitrary number of discrete-time stochastic control problems, each with a\ndeterministic policy, to determine the corresponding equilibrium. This\napproach is applied to a standard stochastic optimal control problem with a\ndeterministic policy and to a nonstandard stochastic optimal control problem\nwith a deterministic policy. We prove that the equilibria of the nonstandard\nproblem are the same as those of the standard problem. We provide numerical\nexamples to illustrate the method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.2857142857142857,
          "f": 0.18666666226755566
        },
        "rouge-2": {
          "r": 0.007246376811594203,
          "p": 0.012345679012345678,
          "f": 0.009132415430039283
        },
        "rouge-l": {
          "r": 0.12871287128712872,
          "p": 0.2653061224489796,
          "f": 0.17333332893422235
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2412.18563v3",
      "true_abstract": "Artificial intelligence is transforming financial investment decision-making\nframeworks, with deep reinforcement learning demonstrating substantial\npotential in robo-advisory applications. This paper addresses the limitations\nof traditional portfolio optimization methods in dynamic asset weight\nadjustment through the development of a deep reinforcement learning-based\ndynamic optimization model grounded in practical trading processes. The\nresearch advances two key innovations: first, the introduction of a novel\nSharpe ratio reward function engineered for Actor-Critic deep reinforcement\nlearning algorithms, which ensures stable convergence during training while\nconsistently achieving positive average Sharpe ratios; second, the development\nof an innovative comprehensive approach to portfolio optimization utilizing\ndeep reinforcement learning, which significantly enhances model optimization\ncapability through the integration of random sampling strategies during\ntraining with image-based deep neural network architectures for\nmulti-dimensional financial time series data processing, average Sharpe ratio\nreward functions, and deep reinforcement learning algorithms. The empirical\nanalysis validates the model using randomly selected constituent stocks from\nthe CSI 300 Index, benchmarking against established financial econometric\noptimization models. Backtesting results demonstrate the model's efficacy in\noptimizing portfolio allocation and mitigating investment risk, yielding\nsuperior comprehensive performance metrics.",
      "generated_abstract": "This paper studies the problem of optimal execution in a discrete-time\nmarket where the trading agent has an initial capital to allocate among\ndifferent orders. We focus on the case of a single market maker with a finite\nnumber of orders. We consider an efficient mechanism that orders on the\nbid-ask prices, and our objective is to maximize the agent's expected utility\nunder this mechanism. We provide a new characterization of the optimal\nexecution strategy and a characterization of the optimal market-making\nstrategy. We then use this characterization to study the case of a\nbid-ask-price grid, which is a natural setting for realistic markets. Our\nresults show that the optimal execution strategy is an optimal market-making\nstrategy if and only if the grid is infinite, and that the optimal execution\nstrategy is a Nash equilibrium if and only if the grid is finite. We also\nprovide a characterization of the optimal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11811023622047244,
          "p": 0.2,
          "f": 0.14851484681648872
        },
        "rouge-2": {
          "r": 0.012195121951219513,
          "p": 0.01680672268907563,
          "f": 0.014134270744798094
        },
        "rouge-l": {
          "r": 0.11811023622047244,
          "p": 0.2,
          "f": 0.14851484681648872
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2412.04263v1",
      "true_abstract": "A simple model-free and distribution-free statistic, the functional\nrelationship between the number of \"effective\" degrees of freedom and portfolio\nsize, or N*(N), is used to discriminate between two alternative models for the\ncorrelation of daily cryptocurrency returns within a retail universe of defined\nby the list of tradable assets available to account holders at the Robinhood\nbrokerage. The average pairwise correlation between daily cryptocurrency\nreturns is found to be high (of order 60%) and the data collected supports\ndescription of the cross-section of returns by a simple isotropic correlation\nmodel distinct from a decomposition into a linear factor model with additive\nnoise with high confidence. This description appears to be relatively stable\nthrough time.",
      "generated_abstract": "We introduce a novel approach to the task of stock price prediction using\na reinforcement learning (RL) agent trained using historical market data. The\nagent is equipped with a state representation that incorporates both\nfinancial and non-financial information and is trained to select the best\nactions from a set of predefined strategies. We evaluate the performance of the\nagent on the stock price prediction task, and compare it with a variety of\nrecent RL approaches. The results show that the proposed approach significantly\noutperforms existing methods in terms of both mean and median accuracy,\ndemonstrating the effectiveness of our approach in predicting stock prices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.14705882352941177,
          "f": 0.13698629639331975
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.02,
          "f": 0.01904761405895822
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.1323529411764706,
          "f": 0.12328766625633349
        }
      }
    },
    {
      "paper_id": "stat.ML.q-fin/ST/2502.11310v1",
      "true_abstract": "We tackle the challenges of modeling high-dimensional data sets, particularly\nthose with latent low-dimensional structures hidden within complex, non-linear,\nand noisy relationships. Our approach enables a seamless integration of\nconcepts from non-parametric regression, factor models, and neural networks for\nhigh-dimensional regression. Our approach introduces PCA and Soft PCA layers,\nwhich can be embedded at any stage of a neural network architecture, allowing\nthe model to alternate between factor modeling and non-linear transformations.\nThis flexibility makes our method especially effective for processing\nhierarchical compositional data. We explore ours and other techniques for\nimposing low-rank structures on neural networks and examine how architectural\ndesign impacts model performance. The effectiveness of our method is\ndemonstrated through simulation studies, as well as applications to forecasting\nfuture price movements of equity ETF indices and nowcasting with macroeconomic\ndata.",
      "generated_abstract": "This paper presents a novel approach to estimating the mean and covariance\nof Gaussian random vectors. Our method leverages a novel decomposition of the\nconditional expectation of the sample mean and covariance of random variables\ngiven a subset of variables. We prove that our estimator is consistent and\nhas finite variance for any given sample size. We also show that our estimator\ncan be used to construct confidence sets for the mean and covariance of a\nGaussian random vector. We demonstrate the performance of our method through\nnumerical simulations and compare it with the method proposed in\n\\cite{bai2023gaussian}.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19,
          "p": 0.3220338983050847,
          "f": 0.23899370602428707
        },
        "rouge-2": {
          "r": 0.031007751937984496,
          "p": 0.047058823529411764,
          "f": 0.037383172781466245
        },
        "rouge-l": {
          "r": 0.18,
          "p": 0.3050847457627119,
          "f": 0.22641508967208585
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07527v1",
      "true_abstract": "This paper presents a novel method for real-time lifting-load estimation to\nenhance the control strategies of upper-limb assistive exoskeletons. By\nleveraging cost-effective insole pressure sensors, the proposed system extracts\ndifferential pressure data that minimizes disturbances from variations in body\nweight and sensor placement. Two modeling approaches are explored: a\nchannel-based method that employs traditional regression techniques-Elastic\nNet, Support Vector Regression (SVR), and Multi-Layer Perceptron (MLP)-and a\nmap-based method that utilizes transfer learning with a pre-trained MobileNetV2\nmodel. The experiment is in the preliminary test stage, covering load ranges\nfrom 2 kg to 10 kg in increments of 0.5 kg, and collecting data from three\nsubjects to test the approach. In the Channel-based method, the average\nWeighted Mean Absolute Percentage Error(WMAPE) for three subjects showed that\nthe SVR achieved 13.46%, with the MLP performing similarly. In the Map-based\nmethod, using data from one subject, the Fully Fine-Tuned MobileNetV2 model\nreached a WMAPE of 9.74%. The results indicate that the integration of insole\nsensor technology with advanced machine learning models provides an effective\nsolution for dynamic load estimation, potentially reducing the risks of over-\nand under-compensation in exoskeleton control.",
      "generated_abstract": "This paper addresses the issue of data privacy in wireless networked\nsystems, where users may be subject to both passive and active eavesdropping\nattacks. The main objective is to enhance the privacy of users through a\nprivacy-preserving protocol that can be embedded in a networked system. The\nprivacy protection mechanism is designed to preserve the privacy of users while\nensuring that the system is able to operate effectively. We propose a\nmulti-agent system architecture for wireless networks that integrates\nprivacy-preserving protocols with multi-agent systems. We also propose a\nmulti-agent system architecture for wireless networks that integrates privacy-\npreserving protocols with multi-agent systems. The multi-agent system architecture\nintegrates privacy-preserving protocols with multi-agent systems. We show that\nthe proposed architecture provides a privacy-preserving protocol that\nenhances the privacy of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13533834586466165,
          "p": 0.2857142857142857,
          "f": 0.1836734650255103
        },
        "rouge-2": {
          "r": 0.0273224043715847,
          "p": 0.05154639175257732,
          "f": 0.03571428118596996
        },
        "rouge-l": {
          "r": 0.13533834586466165,
          "p": 0.2857142857142857,
          "f": 0.1836734650255103
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09919v1",
      "true_abstract": "We provide a family of $5$-dimensional prismatoids whose width grows linearly\nin the number of vertices. This provides a new infinite family of\ncounter-examples to the Hirsch conjecture whose excess width grows linearly in\nthe number of vertices, and answers a question of Matschke, Santos and Weibel.",
      "generated_abstract": "We prove that any $n$-vertex graph $G$ has a $2$-coloring that is a\n$2$-coloring of $G$ with the following property: for every $xy \\in E(G)$, if\n$y$ is adjacent to both $x$ and $z$ in $G$, then $y$ is adjacent to exactly two\nvertices in $G$. This result is a generalization of the $K_3$ theorem, a\nclassical result by E. H. Robinson. We also prove that the class of $2$-colorable\ngraphs is closed under all edge-colorings and under vertex-colorings. Finally,\nwe use this class of $2$-colorable graphs to prove the Robinson-Sun theorem,\nwhich is a generalization of the Robinson theorem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2903225806451613,
          "p": 0.14754098360655737,
          "f": 0.1956521694447071
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.25806451612903225,
          "p": 0.13114754098360656,
          "f": 0.1739130390099245
        }
      }
    },
    {
      "paper_id": "hep-ex.physics/data-an/2503.06727v1",
      "true_abstract": "The Precision Reactor Oscillation and Spectrum Experiment, PROSPECT, was a\nsegmented antineutrino detector that successfully operated at the High Flux\nIsotope Reactor in Oak Ridge, TN, during its 2018 run. Despite challenges with\nphotomultiplier tube base failures affecting some segments, innovative machine\nlearning approaches were employed to perform position and energy\nreconstruction, and particle classification. This work highlights the\neffectiveness of convolutional neural networks and graph convolutional networks\nin enhancing data analysis. By leveraging these techniques, a 3.3% increase in\neffective statistics was achieved compared to traditional methods, showcasing\ntheir potential to improve performance. Furthermore, these machine learning\nmethodologies offer promising applications for other segmented particle\ndetectors, underscoring their versatility and impact.",
      "generated_abstract": "The CMS experiment has recently published a study of the energy deposition in\nthe calorimeter from the $pp$ interaction. The study provides a detailed\ndescription of the energy deposition in the calorimeter for a variety of\nincoming particle types. This study is based on a number of assumptions. We\npropose a correction based on the modeling of the particle interaction and\nenergy deposition in the calorimeter. The new model is based on the analysis of\nthe energy deposited in the calorimeter and the measured energy deposited in\nthe calorimeter. The corrected study is based on the updated modeling of the\nparticle interaction and energy deposition in the calorimeter.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11827956989247312,
          "p": 0.2558139534883721,
          "f": 0.16176470155817485
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.014492753623188406,
          "f": 0.011111106383335345
        },
        "rouge-l": {
          "r": 0.10752688172043011,
          "p": 0.23255813953488372,
          "f": 0.1470588192052337
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09794v1",
      "true_abstract": "As Augmented Reality (AR) and Artificial Intelligence (AI) continue to\nconverge, new opportunities emerge for AI agents to actively support human\ncollaboration in immersive environments. While prior research has primarily\nfocused on dyadic human-AI interactions, less attention has been given to\nHuman-AI Teams (HATs) in AR, where AI acts as an adaptive teammate rather than\na static tool. This position paper takes the perspective of team dynamics and\nwork organization to propose that AI agents in AR should not only interact with\nindividuals but also recognize and respond to team-level needs in real time. We\nargue that spatially aware AI agents should dynamically generate the resources\nnecessary for effective collaboration, such as virtual blackboards for\nbrainstorming, mental map models for shared understanding, and memory recall of\nspatial configurations to enhance knowledge retention and task coordination.\nThis approach moves beyond predefined AI assistance toward context-driven AI\ninterventions that optimize team performance and decision-making.",
      "generated_abstract": "The emergence of AI-driven technologies, such as chatbots and virtual\nassistants, has revolutionized customer service by offering a more personalized\nand automated experience. However, these systems can be susceptible to\nmanipulation, which can lead to negative customer experiences and damage brand\nreputation. To address this, we propose a novel framework for detecting\nmanipulative customer service interactions. Our framework leverages the\ninterpretability of natural language models (NLMs) to analyze interactions\nusing contextualized representations. We introduce an attention-based\nrepresentation learning framework for analyzing customer service interactions,\nwhich captures contextual information and enables fine-grained analysis of\ninteractions. Our approach outperforms existing methods in terms of accuracy and\nfine-grained analysis. Additionally, we propose a new evaluation metric,\n`Interaction Similarity', which quantifies how closely interactions align\nwith",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15833333333333333,
          "p": 0.20652173913043478,
          "f": 0.1792452781060877
        },
        "rouge-2": {
          "r": 0.006711409395973154,
          "p": 0.008771929824561403,
          "f": 0.007604557826196986
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16304347826086957,
          "f": 0.14150942904948396
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.cond-mat/soft/2503.09564v1",
      "true_abstract": "Under an externally applied load, granular packings form force chains that\ndepend on the contact network and moduli of the grains. In this work, we\ninvestigate packings of variable modulus (VM) particles, where we can direct\nforce chains by changing the Young's modulus of individual particles within the\npacking on demand. Each VM particle is made of a silicone shell that\nencapsulates a core made of a low-melting-point metallic alloy (Field's metal).\nBy sending an electric current through a co-located copper heater, the Field's\nmetal internal to each particle can be melted via Joule heating, which softens\nthe particle. As the particle cools to room temperature, the alloy solidifies\nand the particle recovers its original modulus. To optimize the mechanical\nresponse of granular packings containing both soft and stiff particles, we\nemploy an evolutionary algorithm coupled with discrete element method\nsimulations to predict the patterns of particle moduli that will yield specific\nforce outputs on the assembly boundaries. The predicted patterns of particle\nmoduli from the simulations were realized in experiments using 2D assemblies of\nVM particles and the force outputs on the assembly boundaries were measured\nusing photoelastic techniques. These studies represent a step towards making\nrobotic granular metamaterials that can dynamically adapt their mechanical\nproperties in response to different environmental conditions or perform\nspecific tasks on demand.",
      "generated_abstract": "We introduce a novel and simple way to generate spin-polarized superfluids\nwith controlled magnetic order. We study a class of models in which the\ninteraction energy between the two components of the spin-polarized superfluid\nis controlled by a parameter that varies from zero to one. This parameter is\nreferred to as a magnetic field-induced chemical potential, and it determines\nthe magnitude of the superfluid fraction. In the limit of zero magnetic\nfield-induced chemical potential, the system behaves as a superfluid with zero\nmagnetic order. In the opposite limit, when the magnetic field-induced chemical\npotential is large enough, a superfluid with nonzero magnetic order appears.\nWe numerically study the properties of the superfluids and discuss their\ninterplay with the magnetic field. We find that the superfluid fraction\ndepends on the magnetic field-induced chemical potential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11510791366906475,
          "p": 0.23880597014925373,
          "f": 0.155339801436045
        },
        "rouge-2": {
          "r": 0.009950248756218905,
          "p": 0.018018018018018018,
          "f": 0.012820508236564753
        },
        "rouge-l": {
          "r": 0.09352517985611511,
          "p": 0.19402985074626866,
          "f": 0.12621358784381204
        }
      }
    },
    {
      "paper_id": "cs.GT.stat/OT/2502.11645v1",
      "true_abstract": "Many real-world multi-agent or multi-task evaluation scenarios can be\nnaturally modelled as normal-form games due to inherent strategic (adversarial,\ncooperative, and mixed motive) interactions. These strategic interactions may\nbe agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or\ncomplementary (e.g. niche finding and specialization). In such a formulation,\nit is the strategies (actions, policies, agents, models, tasks, prompts, etc.)\nthat are rated. However, the rating problem is complicated by redundancy and\ncomplexity of N-player strategic interactions. Repeated or similar strategies\ncan distort ratings for those that counter or complement them. Previous work\nproposed ``clone invariant'' ratings to handle such redundancies, but this was\nlimited to two-player zero-sum (i.e. strictly competitive) interactions. This\nwork introduces the first N-player general-sum clone invariant rating, called\ndeviation ratings, based on coarse correlated equilibria. The rating is\nexplored on several domains including LLMs evaluation.",
      "generated_abstract": "We study the problem of estimating the average treatment effect (ATE) of\na treatment on a random variable $Y$ by maximizing the difference between the\nexpected values of $Y$ under the treatment and the control groups. To do so, we\nintroduce a novel framework for estimating ATE that captures the\nnon-additivity of the expected value of $Y$ under the treatment group. This\nframework is based on the properties of a function $\\beta(X,Y)$, which is a\nfunction of the variables $(X,Y)$, such that $\\beta(X,Y)$ depends only on the\nobservation of $X$ and $Y$. We show that for any function $\\beta$, the\nestimator of ATE is the maximum of the following two estimators:\n$\\hat{\\beta}_1(Y)=\\sup_{X} \\hat{\\beta}(X,Y)$ and\n$\\hat{\\beta}_2(",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11304347826086956,
          "p": 0.2,
          "f": 0.14444443983024705
        },
        "rouge-2": {
          "r": 0.013986013986013986,
          "p": 0.019230769230769232,
          "f": 0.016194327108460877
        },
        "rouge-l": {
          "r": 0.11304347826086956,
          "p": 0.2,
          "f": 0.14444443983024705
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/NI/2503.06785v1",
      "true_abstract": "As reliance on space systems continues to increase, so does the need to\nensure security for them. However, public work in space standards have\nstruggled with defining security protocols that are well tailored to the domain\nand its risks. In this work, we investigate various space networking paradigms\nand security approaches, and identify trade-offs and gaps. Furthermore, we\ndescribe potential existing security protocol approaches that fit well into the\nspace network paradigm in terms of both functionality and security. Finally, we\nestablish future directions for enabling strong security for space\ncommunication.",
      "generated_abstract": "The recent success of reinforcement learning (RL) in various fields has\nprompted the development of RL-based computer vision methods. However, most\nexisting RL-based methods rely on large-scale dataset training, which is\ncomputationally expensive and not practical for real-world applications. In\nthis paper, we propose a lightweight RL-based method named LightRL-DQN, which\nemploys a lightweight DQN architecture with only 10k parameters. This\nlightweight RL-based method achieves comparable or even superior performance\nto the state-of-the-art RL-based methods. We further propose an efficient\ntraining strategy that only requires 10 epochs, which is a significant\nimprovement compared to the existing method that requires 100 epochs. In\naddition, we also present a new benchmark dataset named LightRL-Bench, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22388059701492538,
          "p": 0.18072289156626506,
          "f": 0.199999995056889
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.018867924528301886,
          "f": 0.02061855174407601
        },
        "rouge-l": {
          "r": 0.22388059701492538,
          "p": 0.18072289156626506,
          "f": 0.199999995056889
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01298v2",
      "true_abstract": "Measuring inequality of opportunities has long been a challenging and open\nproblem, primarily due to the limitations associated with individual-level\ndata. In this study, we utilize data obtained from vehicle license plates in a\ncomprehensive survey (17258 vehicles from 6 major cities in China) to evaluate\nthe inequality of opportunities in the country. In our context, we define\ninequality of opportunity as the scenario where relatively expensive vehicles\nhave a higher likelihood of being paired with license plates featuring 'Lucky\nNumbers'. To quantify this, we propose a lucky-number-based opportunity Gini\ncoefficient. Through the calculation of the opportunity Gini coefficient, we\nobserve a significant and positive correlation between opportunity inequality\nand income inequality. Particularly noteworthy is our finding that the\nadvancement of technology, exemplified by the widespread adoption of new energy\nvehicles, can substantially reduce the inequality of opportunity. Taking\nincorporation of a random lottery process before acquiring a motor vehicle in\nBeijing and Shanghai as a natural experiment, our empirical results support the\nargument that, in terms of equality, employing random drawing is a fair and\nequitable approach for allocating scarce resources.",
      "generated_abstract": "The recent outbreak of COVID-19 has shown that pandemics can pose significant\nrisks to global economic and social stability. While some studies have examined\nthe impact of pandemics on the economy, they have typically focused on\nmacroeconomic indicators such as GDP and inflation. This study examines the\nimpact of pandemics on the labour market by considering the impact of COVID-19\non job losses and employment. We use a dynamic panel data model to analyse\njob losses in the Philippines over the period 2018 to 2025. Our findings show\nthat the pandemic led to a significant reduction in the labour force\nparticipation rate and a decline in employment. We also find that the\npandemic-induced job losses were disproportionately concentrated in low-wage\njobs, particularly in the informal sector. These findings",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11666666666666667,
          "p": 0.1686746987951807,
          "f": 0.13793102964886328
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.026785714285714284,
          "f": 0.021052626808003544
        },
        "rouge-l": {
          "r": 0.09166666666666666,
          "p": 0.13253012048192772,
          "f": 0.10837437940255792
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.01378v1",
      "true_abstract": "This paper studies robust forward investment and consumption preferences and\noptimal strategies for a risk-averse and ambiguity-averse agent in an\nincomplete financial market with drift and volatility uncertainties. We focus\non non-zero volatility and constant relative risk aversion (CRRA) forward\npreferences. Given the non-convexity of the Hamiltonian with respect to\nuncertain volatilities, we first construct robust randomized forward\npreferences through endogenous randomization in an auxiliary market. We derive\nthe corresponding optimal and robust investment and consumption strategies.\nFurthermore, we show that such forward preferences and strategies, developed in\nthe auxiliary market, remain optimal and robust in the physical market,\noffering a comprehensive framework for forward investment and consumption under\nmodel uncertainty.",
      "generated_abstract": "This study examines the impact of the COVID-19 pandemic on the behavior of\nHong Kong Stock Market (HKX) and examines whether the pandemic affected the\ndynamics of the market by analyzing the impact of the pandemic on stock\nreturns, stock volatility, and the correlation between stock returns and\nvolatility. The study utilizes a panel data regression model to analyze the\ncorrelation between the stock market and the Hong Kong economy, as well as the\ncorrelation between the stock market and the COVID-19 epidemic in Hong Kong.\nThe study finds that the stock market has a significant impact on the economy,\nas measured by the GDP. The study also finds that the stock market has a\nsignificant impact on the stock volatility, as measured by the VAR. The study\nfinds that the stock market is negatively correlated with the stock market.\nThe study",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19117647058823528,
          "p": 0.23636363636363636,
          "f": 0.21138210887699133
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.021739130434782608,
          "f": 0.021052626583934704
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.18181818181818182,
          "f": 0.16260162107211334
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2412.00986v1",
      "true_abstract": "We study a problem of optimal irreversible investment and emission reduction\nformulated as a nonzero-sum dynamic game between an investor with environmental\npreferences and a firm. The game is set in continuous time on an infinite-time\nhorizon. The firm generates profits with a stochastic dynamics and may spend\npart of its revenues towards emission reduction (e.g., renovating the\ninfrastructure). The firm's objective is to maximize the discounted expectation\nof a function of its profits. The investor participates in the profits and may\ndecide to invest to support the firm's production capacity. The investor uses a\nprofit function which accounts for both financial and environmental factors.\nNash equilibria of the game are obtained via a system of variational\ninequalities. We formulate a general verification theorem for this system in a\ndiffusive setup and construct an explicit solution in the zero-noise limit. Our\nexplicit results and numerical approximations show that both the investor's and\nthe firm's optimal actions are triggered by moving boundaries that increase\nwith the total amount of emission abatement.",
      "generated_abstract": "We consider a general class of stochastic differential games with the\nnon-linear payoff $f(x,y):=\\langle x,y\\rangle$ for $x,y\\in\\mathbb{R}^n$, where\n$n\\geq1$. We consider a large class of games with linear payoffs, namely, the\nclass of non-linear games in the form $f(x,y)=ax+by$, where $a,b>0$ and $x,y\\in\n\\mathbb{R}^n$. We show that for such games the value function and optimal\ncontrol are Lipschitz continuous and the value function is $C^2$-smooth,\nprovided that the initial state is $C^3$-smooth. We also show that for such\ngames the value function and optimal control are Lipschitz continuous and the\nvalue function is $C^2$-smooth, provided that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1619047619047619,
          "p": 0.3617021276595745,
          "f": 0.2236842062543283
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.07462686567164178,
          "f": 0.04310344416802952
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.3191489361702128,
          "f": 0.19736841678064412
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2404.07658v1",
      "true_abstract": "This paper extends the valuation and optimal surrender framework for variable\nannuities with guaranteed minimum benefits in a L\\'evy equity market\nenvironment by incorporating a stochastic interest rate described by the\nHull-White model. This approach frames a more dynamic and realistic financial\nsetting compared to previous literature. We exploit a robust valuation\nmechanism employing a hybrid numerical method that merges tree methods for\ninterest rate modeling with finite difference techniques for the underlying\nasset price. This method is particularly effective for addressing the\ncomplexities of variable annuities, where periodic fees and mortality risks are\nsignificant factors. Our findings reveal the influence of stochastic interest\nrates on the strategic decision-making process concerning the surrender of\nthese financial instruments. Through comprehensive numerical experiments, and\nby comparing our results with those obtained through the Longstaff-Schwartz\nMonte Carlo method, we illustrate how our refined model can guide insurers in\ndesigning contracts that equitably balance the interests of both parties. This\nis particularly relevant in discouraging premature surrenders while adapting to\nthe realistic fluctuations of financial markets. Lastly, a comparative statics\nanalysis with varying interest rate parameters underscores the impact of\ninterest rates on the cost of the optimal surrender strategy, emphasizing the\nimportance of accurately modeling stochastic interest rates.",
      "generated_abstract": "This paper examines the relationship between the price of Bitcoin (BTC) and\nthe price of gold. By examining the long-term price dynamics of the two assets,\nthe paper identifies two distinct patterns that have emerged in the past\ndecade. Specifically, the paper investigates whether these patterns are\nassociated with the onset of major bull and bear markets. By employing\nhigh-frequency data, the paper shows that the onset of bull and bear markets\noccur independently of the price of gold, but that these patterns are strongly\nrelated to the price of BTC. Moreover, the paper finds that the emergence of\nbull and bear markets has a significant impact on the price of gold, with\nbullish events leading to a positive gold price while bearish events result in\na negative gold price. The paper also identifies a distinct correlation between\nthe price of gold and the price",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.2714285714285714,
          "f": 0.18536584916121368
        },
        "rouge-2": {
          "r": 0.02577319587628866,
          "p": 0.044642857142857144,
          "f": 0.0326797339211421
        },
        "rouge-l": {
          "r": 0.1259259259259259,
          "p": 0.24285714285714285,
          "f": 0.16585365403926244
        }
      }
    },
    {
      "paper_id": "math.QA.math/OA/2502.19876v3",
      "true_abstract": "This paper explores Frobenius subalgebra posets within abelian monoidal\ncategories and shows that they form lattices under certain conditions,\nincluding all semisimple tensor categories. Furthermore, it extends Watatani's\ntheorem on the finiteness of intermediate subfactors, proving that these\nlattices are finite under weak positivity constraints, encompassing all\nsemisimple tensor categories as well. The primary tools employed in this paper\nare semisimplification and a concept of formal angle. Additionally, we have\nbroadened several intermediate results, such as the exchange relation theorem\nand Landau's theorem, to apply to abelian monoidal categories. Key applications\nof our findings include a stronger version of the Ino-Watatani result: we show\nthat the finiteness of intermediate C*-algebras holds in a finite-index unital\nirreducible inclusion of C*-algebras without requiring the simple assumption.\nMoreover, for a finite-dimensional semisimple Hopf algebra H, we demonstrate\nthat H* contains a finite number of Frobenius subalgebra objects in Rep(H).\nFinally, we explore a range of applications, including abstract spin chains,\nvertex operator algebras, and speculations on quantum arithmetic involving the\ngeneralization of Ore's theorem, Euler's totient and sigma functions, and RH.",
      "generated_abstract": "We give a complete characterization of the finite-dimensional irreducible\nspin representations of the spin group of the fundamental group of a\nmanifold $M$ with boundary, which is the set of oriented closed orientable\nmanifolds with boundary. We prove that the irreducible representations of the\nspin group are in one-to-one correspondence with the irreducible representations\nof the fundamental group of $M$ and the irreducible representations of the\nfundamental group of $M$ are in one-to-one correspondence with the irreducible\nrepresentations of the spin group. In addition, we prove that the spin group\nis a reductive Lie algebra and that the fundamental group of $M$ is a\nsemisimple Lie algebra. Our proof is based on the representation theory of\npermutation groups.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11570247933884298,
          "p": 0.2978723404255319,
          "f": 0.16666666263676314
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.0410958904109589,
          "f": 0.024896261337098907
        },
        "rouge-l": {
          "r": 0.10743801652892562,
          "p": 0.2765957446808511,
          "f": 0.15476190073200125
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2403.05592v1",
      "true_abstract": "As we keep rapidly advancing toward an era where artificial intelligence is a\nconstant and normative experience for most of us, we must also be aware of what\nthis vision and this progress entail. By first approximating neural connections\nand activities in computer circuits and then creating more and more\nsophisticated versions of this crude approximation, we are now facing an age to\ncome where modern deep learning-based artificial intelligence systems can\nrightly be called thinking machines, and they are sometimes even lauded for\ntheir emergent behavior and black-box approaches. But as we create more\npowerful electronic brains, with billions of neural connections and parameters,\ncan we guarantee that these mammoths built of artificial neurons will be able\nto forget the data that we store in them? If they are at some level like a\nbrain, can the right to be forgotten still be protected while dealing with\nthese AIs? The essential gap between machine learning and the RTBF is explored\nin this article, with a premonition of far-reaching conclusions if the gap is\nnot bridged or reconciled any time soon. The core argument is that deep\nlearning models, due to their structure and size, cannot be expected to forget\nor delete a data as it would be expected from a tabular database, and they\nshould be treated more like a mechanical brain, albeit still in development.",
      "generated_abstract": "This paper explores the use of 3D human pose estimation in real-time\nsystems to enhance the performance of virtual reality (VR) applications.\nSpecifically, we propose an efficient approach for estimating the 3D\nposition of the forearm of a hand in a 3D scene, allowing for the generation of\narbitrary 3D hands. This approach is achieved through the fusion of\nmultiple 3D human pose estimation methods, including RGB-D pose estimation\nmethods, 3D human mesh models, and 3D hand models. Our approach combines\ndifferent pose estimation methods to enhance the precision and accuracy of\nestimated 3D hand poses. Additionally, we integrate hand mesh models with the\nestimated 3D hands to provide a more realistic and natural-looking\nrepresentation of the hand in the VR environment. By integrating different\nmethods for estim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10416666666666667,
          "p": 0.21428571428571427,
          "f": 0.14018691148571943
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.21428571428571427,
          "f": 0.14018691148571943
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.10631v1",
      "true_abstract": "Large Language Models (LLMs) employ three popular training approaches: Masked\nLanguage Models (MLM), Causal Language Models (CLM), and Sequence-to-Sequence\nModels (seq2seq). However, each approach has its strengths and limitations, and\nfaces challenges in addressing specific tasks that require controllable and\nbidirectional generation, such as drug optimization. To address this challenge,\ninspired by the biological processes of growth and evolution, which involve the\nexpansion, shrinking, and mutation of sequences, we introduce ControllableGPT.\nThis initiative represents the first effort to combine the advantages of MLM,\nCLM, and seq2seq into a single unified, controllable GPT framework. It enables\nthe precise management of specific locations and ranges within a sequence,\nallowing for expansion, reduction, or mutation over chosen or random lengths,\nwhile maintaining the integrity of any specified positions or subsequences. In\nthis work, we designed ControllableGPT for drug optimization from the ground\nup, which included proposing the Causally Masked Seq2seq (CMS) objective,\ndeveloping the training corpus, introducing a novel pre-training approach, and\ndevising a unique generation process. We demonstrate the effectiveness and\ncontrollability of ControllableGPT by conducting experiments on drug\noptimization tasks for both viral and cancer benchmarks, surpassing competing\nbaselines.",
      "generated_abstract": "Protein-protein interactions (PPIs) are critical for cellular function and\nprotein design, yet their structure remains poorly understood. Here, we\nrepresent PPIs as protein-ligand interactions, and use a deep learning\napproach to identify PPIs in proteins from the SARS-CoV-2 spike protein. Our\nmethod, named FLIP, produces a large, high-quality dataset of PPIs in\nproteins from SARS-CoV-2, using a simple but effective two-stage training\nprocess: first, we use a neural network to learn a mapping from protein\nsequences to PPIs; then, we train a second neural network to generate\ninteractions between proteins from the same sequence, using the PPIs learned in\nthe first step. FLIP achieves an accuracy of 90.2% on the test set, with 9",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.18292682926829268,
          "f": 0.13824884322453243
        },
        "rouge-2": {
          "r": 0.01092896174863388,
          "p": 0.018867924528301886,
          "f": 0.013840825804769221
        },
        "rouge-l": {
          "r": 0.08148148148148149,
          "p": 0.13414634146341464,
          "f": 0.10138248377752788
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/DB/2503.08717v1",
      "true_abstract": "The ability of tracing states of logistic transportations requires an\nefficient storage and retrieval of the state of logistic transportations and\nlocations of logistic objects. However, the restriction of sharing states and\nlocations of logistic objects across organizations from different countries\nmakes it hard to deploy a centralized database for implementing the\ntraceability in a cross-border logistic system. This paper proposes a semantic\ndata model on Blockchain to represent a logistic process based on the Semantic\nLink Network model where each semantic link represents a logistic\ntransportation of a logistic object between two parties. A state representation\nmodel is designed to represent the states of a logistic transportation with\nsemantic links. It enables the locations of logistic objects to be derived from\nthe link states. A mapping from the semantic links to the blockchain\ntransactions is designed to enable schema of semantic links and states of\nsemantic links to be published in blockchain transactions. To improve the\nefficiency of tracing a path of semantic links on blockchain platform, an\nalgorithm is designed to build shortcuts along the path of semantic links to\nenable a query on the path of a logistic object to reach the target in\nlogarithmic steps on the blockchain platform. A reward-penalty policy is\ndesigned to allow participants to confirm the state of links on blockchain.\nAnalysis and simulation demonstrate the flexibility, effectiveness and the\nefficiency of Semantic Link Network on immutable blockchain for implementing\nlogistic traceability.",
      "generated_abstract": "This paper proposes a novel framework for automatic, scalable and\naccurate detection and classification of data corruption attacks in distributed\nblockchain networks. Unlike previous works, our approach is not limited to a\nspecific attack and focuses on detecting all attacks, including the\nundetected ones. Our framework uses a graph-based detection model to identify\ndata corruption attacks. We develop a novel data corruption attack detection\nmechanism based on the concept of information content. Our model consists of\ntwo main modules: the first one is a graph-based model that uses a pre-trained\nencoder to extract features from the entire graph structure, and the second one\nis a classifier that predicts the type of data corruption attack. Our model\nconsiders various attack types and employs different feature sets for each one\nto enhance its accuracy and robustness. To evaluate the performance of our\nmodel, we developed a large",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.23076923076923078,
          "f": 0.21538461040683773
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.05303030303030303,
          "f": 0.04458598238792703
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.21978021978021978,
          "f": 0.2051282001504275
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.07896v2",
      "true_abstract": "The effect of a negative sectoral shock on GDP depends on how important the\nshocked sector is as a direct and indirect supplier and how easily sectors can\nsubstitute inputs. Past estimates of the parameters that determine these\nqualities in the US have been restrictive: they have not been allowed to vary\nacross industries or across time. This paper uses a novel empirical strategy to\nrelax those restrictions, by exploiting variation in input expenditure share\nshifts within industries rather than across industries. The resulting estimates\nexhibit significant sectoral and temporal heterogeneity, and are dynamically\ncorrelated with weighted patents. In a calibrated GE model of multi-sector\nproduction, this heterogeneity (1) raises[lowers] the GDP effect of negative\nshocks to sectors whose customers are less[more] able to substitute inputs\n(e.g. the GDP effect of \"Chemical products\" shocks rises), (2) raises[lowers]\nthe GDP effect of negative sectoral shocks in years where sectors are\nless[more] able to substitute inputs, and (3) raises[lowers] the GDP effect of\nnegative shocks to sectors as they become more[less] central input suppliers\n(e.g. between 1997 and 2023 the GDP effect of \"Paper products\" shocks fell and\nthe GDP effect of \"Computer and electronic products\" shocks rose due to changes\nin their importance as input suppliers).",
      "generated_abstract": "This paper provides a theoretical framework for analyzing the impact of\nhigh-speed rail (HSR) on urban land use, using a spatial econometric model\nbased on the spatial econometric analysis of urban land use. The model\ncombines the spatial econometric analysis of urban land use with the spatial\neconometric analysis of HSR. The model is then used to analyze the impact of\nHSR on urban land use in the Chinese cities of Beijing, Shanghai, and\nShenzhen. The results show that HSR can significantly improve urban land use\nefficiency, reducing urban land use intensity by 21% in Beijing, 21% in\nShanghai, and 24% in Shenzhen. The findings of this study provide valuable\ninsights into the impact of HSR on urban land use, offering a comprehensive\nperspective on the potential benefits of HSR for urban development.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.25757575757575757,
          "f": 0.18378377919415642
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.020833333333333332,
          "f": 0.014869883885519857
        },
        "rouge-l": {
          "r": 0.13445378151260504,
          "p": 0.24242424242424243,
          "f": 0.17297296838334564
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SY/2503.07935v1",
      "true_abstract": "Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible\ndeployment in 5G and next-generation wireless networks. The performance of such\nwireless networks can be improved by developing new navigation and wireless\nadaptation approaches in digital twins (DTs). However, challenges such as\ncomplex propagation conditions and hardware complexities in real-world\nscenarios introduce a realism gap with the DTs. Moreover, while using real-time\nfull-stack protocols in DTs enables subsequent deployment and testing in a\nreal-world environment, development in DTs requires high computational\ncomplexity and involves a long development time. In this paper, to accelerate\nthe development cycle, we develop a measurement-calibrated Matlab-based\nsimulation framework to replicate performance in a full-stack UAV wireless\nnetwork DT. In particular, we use the DT from the NSF AERPAW platform and\ncompare its reports with those generated by our developed simulation framework\nin wireless networks with similar settings. In both environments, we observe\ncomparable results in terms of RSRP measurement, hence motivating iterative use\nof the developed simulation environment with the DT.",
      "generated_abstract": "In this paper, we propose a multi-agent learning framework for an autonomous\ndriving scenario with a large number of agents. Our approach uses a\nmulti-agent reinforcement learning (MARL) framework to train agents to\nefficiently navigate a large urban environment. We develop a multi-agent\nreinforcement learning framework that combines reinforcement learning and\nmulti-agent cooperative learning to optimize the performance of each agent in\na multi-agent setting. The multi-agent reinforcement learning agent learns to\neffectively navigate the environment and communicate with other agents. The\nmulti-agent cooperative learning agent learns to collaborate with the\nmulti-agent reinforcement learning agent to effectively navigate the\nenvironment. We validate our framework using a simulated environment and a\nreal-world dataset to demonstrate that our framework can effectively navigate\nthe environment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1834862385321101,
          "p": 0.3508771929824561,
          "f": 0.24096385091232408
        },
        "rouge-2": {
          "r": 0.05128205128205128,
          "p": 0.08695652173913043,
          "f": 0.06451612436524488
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.3157894736842105,
          "f": 0.21686746537015542
        }
      }
    },
    {
      "paper_id": "cs.DS.stat/ML/2503.01766v1",
      "true_abstract": "We provide the first $\\widetilde{\\mathcal{O}}\\left(d\\right)$-sample algorithm\nfor sampling from unbounded Gaussian distributions under the constraint of\n$\\left(\\varepsilon, \\delta\\right)$-differential privacy. This is a quadratic\nimprovement over previous results for the same problem, settling an open\nquestion of Ghazi, Hu, Kumar, and Manurangsi.",
      "generated_abstract": "In this paper, we introduce a novel method to estimate the parameter\ndistributions of the multivariate Student's $t$ distribution. Our approach is\nbased on the fact that the multivariate Student's $t$ distribution can be\nrepresented as a product of two independent Student's $t$ distributions, which\nis known as the Student's $t$ decomposition. We prove that the multivariate\nStudent's $t$ distribution is a multivariate Student's $t$ distribution with\nthe same degrees of freedom, and we further show that the multivariate\nStudent's $t$ distribution has the same mean and variance as the corresponding\nStudent's $t$ distribution. Based on these properties, we develop two methods to\nestimate the parameters of the multivariate Student's $t$ distribution. The\nfirst method, referred to as the $h$-approximation method, is based",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24324324324324326,
          "p": 0.15,
          "f": 0.18556700559039233
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.022222222222222223,
          "f": 0.03076922650887633
        },
        "rouge-l": {
          "r": 0.24324324324324326,
          "p": 0.15,
          "f": 0.18556700559039233
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PM/2501.06701v2",
      "true_abstract": "This paper investigates the investment problem of constructing an optimal\nno-short sequential portfolio strategy in a market with a latent dependence\nstructure between asset prices and partly unobservable side information, which\nis often high-dimensional. The results demonstrate that a dynamic strategy,\nwhich forms a portfolio based on perfect knowledge of the dependence structure\nand full market information over time, may not grow at a higher rate infinitely\noften than a constant strategy, which remains invariant over time.\nSpecifically, if the market is stationary, implying that the dependence\nstructure is statistically stable, the growth rate of an optimal dynamic\nstrategy, utilizing the maximum capacity of the entire market information,\nalmost surely decays over time into an equilibrium state, asymptotically\nconverging to the growth rate of a constant strategy.\n  Technically, this work reassesses the common belief that a constant strategy\nonly attains the optimal limiting growth rate of dynamic strategies when the\nmarket process is identically and independently distributed. By analyzing the\ndynamic log-optimal portfolio strategy as the optimal benchmark in a stationary\nmarket with side information, we show that a random optimal constant strategy\nalmost surely exists, even when a limiting growth rate for the dynamic strategy\ndoes not. Consequently, two approaches to learning algorithms for portfolio\nconstruction are discussed, demonstrating the safety of removing side\ninformation from the learning process while still guaranteeing an asymptotic\ngrowth rate comparable to that of the optimal dynamic strategy.",
      "generated_abstract": "This paper studies the optimization of the optimal investment strategy for\nincome-based portfolios with the mean-variance criterion. We consider two\ndistinct types of investment: (i) a constant-weight portfolio, where the\ninvestment strategy is determined by the portfolio weight, and (ii) a\ntime-varying portfolio, where the investment strategy is determined by the\nportfolio weight and the time-varying risk. We characterize the optimal\ninvestment strategy for each of the two types of investment and provide\nnecessary and sufficient conditions for the existence of the optimal\ninvestment strategy. We also provide a portfolio construction technique for\neach of the two types of investment. Finally, we provide a practical\nillustration of the obtained results using the Monte Carlo simulation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.136,
          "p": 0.30357142857142855,
          "f": 0.1878452995940296
        },
        "rouge-2": {
          "r": 0.035,
          "p": 0.08235294117647059,
          "f": 0.04912280283164087
        },
        "rouge-l": {
          "r": 0.128,
          "p": 0.2857142857142857,
          "f": 0.1767955758371235
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/AS/2503.06805v1",
      "true_abstract": "Emotion recognition and sentiment analysis are pivotal tasks in speech and\nlanguage processing, particularly in real-world scenarios involving\nmulti-party, conversational data. This paper presents a multimodal approach to\ntackle these challenges on a well-known dataset. We propose a system that\nintegrates four key modalities/channels using pre-trained models: RoBERTa for\ntext, Wav2Vec2 for speech, a proposed FacialNet for facial expressions, and a\nCNN+Transformer architecture trained from scratch for video analysis. Feature\nembeddings from each modality are concatenated to form a multimodal vector,\nwhich is then used to predict emotion and sentiment labels. The multimodal\nsystem demonstrates superior performance compared to unimodal approaches,\nachieving an accuracy of 66.36% for emotion recognition and 72.15% for\nsentiment analysis.",
      "generated_abstract": "Despite the rapid advancement of deep learning-based speech enhancement\n(SE) models, their application in the real-world still faces significant\nchallenges. The most prominent limitation lies in their limited generalization\nability to diverse speech sources, where the performance deterioration is\nsignificantly more pronounced when transferring from one source to another.\nMoreover, the existing SE models often face severe performance degradation when\nprocessing noisy or highly distorted speech, highlighting the need for\nrobust SE systems that are capable of handling diverse speech conditions.\nDespite the advancement of various SE methods, the problem of speech quality\nassessment (SQA) remains a critical challenge. To address this challenge, we\npropose a novel SQA framework, named Speech Quality Index (SQI), which\nenhances the performance of SE models by quantifying the quality of speech\nsignals. To",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19540229885057472,
          "p": 0.17708333333333334,
          "f": 0.1857923447388696
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.00819672131147541,
          "f": 0.008583685998271424
        },
        "rouge-l": {
          "r": 0.1839080459770115,
          "p": 0.16666666666666666,
          "f": 0.17486338299023577
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/RM/2502.13148v1",
      "true_abstract": "This paper explores key theoretical frameworks instrumental in understanding\nthe relationship between sustainability and institutional investment decisions.\nThe study identifies and analyzes various theories, including Behavioral\nFinance Theory, Modern Portfolio Theory, Risk Management Theory, and others, to\nexplain how sustainability considerations increasingly influence investment\nchoices. By examining these frameworks, the paper highlights how investors\nintegrate Environmental, Social, and Governance (ESG) factors to optimize\nfinancial outcomes and align with broader societal goals.",
      "generated_abstract": "We study the problem of optimal trading under a large number of trading\nconditions. We consider a portfolio consisting of an asset and an unobservable\nfactor and aim to minimize the risk of a portfolio of such assets. The factor\nis correlated with the asset, and the risk is measured by the correlation\nbetween the factor and the asset. We use a non-convex optimization problem to\nsolve the problem. In addition, we also study the problem of optimal trading\nunder a large number of trading conditions with the risk measure defined by\nthe covariance between the factor and the asset. We formulate the problem and\nuse a convex optimization problem to solve it.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11864406779661017,
          "p": 0.14285714285714285,
          "f": 0.12962962467249675
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11864406779661017,
          "p": 0.14285714285714285,
          "f": 0.12962962467249675
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.07778v1",
      "true_abstract": "The future of artificial intelligence (AI) acceleration demands a paradigm\nshift beyond the limitations of purely electronic or photonic architectures.\nPhotonic analog computing delivers unmatched speed and parallelism but\nstruggles with data movement, robustness, and precision. Electronic\nprocessing-in-memory (PIM) enables energy-efficient computing by co-locating\nstorage and computation but suffers from endurance and reconfiguration\nconstraints, limiting it to static weight mapping. Neither approach alone\nachieves the balance needed for adaptive, efficient AI. To break this impasse,\nwe study a hybrid electronic-photonic-PIM computing architecture and introduce\nH3PIMAP, a heterogeneity-aware mapping framework that seamlessly orchestrates\nworkloads across electronic and optical tiers. By optimizing workload\npartitioning through a two-stage multi-objective exploration method, H3PIMAP\nharnesses light speed for high-throughput operations and PIM efficiency for\nmemory-bound tasks. System-level evaluations on language and vision models show\nH3PIMAP achieves a 2.74x energy efficiency improvement and a 3.47x latency\nreduction compared to homogeneous architectures and naive mapping strategies.\nThis proposed framework lays the foundation for hybrid AI accelerators,\nbridging the gap between electronic and photonic computation for\nnext-generation efficiency and scalability.",
      "generated_abstract": "The emergence of advanced Artificial Intelligence (AI) systems and the\nspread of large language models (LLMs) have raised concerns about their\npotential to harm society. These concerns are increasingly being addressed by\nlawmakers and regulators through initiatives such as the European Data\nShield Act. However, despite these efforts, a number of vulnerabilities remain.\nThis paper presents an analysis of the vulnerabilities in the LLMs used in the\nEuropean Data Shield Act. We perform a detailed examination of the vulnerabilities\nin the three LLMs used in the Act: Benevolent AI, XLM-RoBERTa, and OpenAI's\nClever-1.6. We find that each of these LLMs has vulnerabilities related to\ninformation disclosure, privilege escalation, information leakage, and\ndata-leakage. Our analysis provides a compreh",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09923664122137404,
          "p": 0.15853658536585366,
          "f": 0.1220657229641387
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08396946564885496,
          "p": 0.13414634146341464,
          "f": 0.10328638024113403
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.16214v1",
      "true_abstract": "This paper analyzes nonlinearities in the international transmission of\nfinancial shocks originating in the US. To do so, we develop a flexible\nnonlinear multi-country model. Our framework is capable of producing\nasymmetries in the responses to financial shocks for shock size and sign, and\nover time. We show that international reactions to US-based financial shocks\nare asymmetric along these dimensions. Particularly, we find that adverse\nshocks trigger stronger declines in output, inflation, and stock markets than\nbenign shocks. Further, we investigate time variation in the estimated dynamic\neffects and characterize the responsiveness of three major central banks to\nfinancial shocks.",
      "generated_abstract": "This paper introduces a novel method for inference on the parameters of\nstructural models, namely the \\emph{reconstruction of the model} (ROM). In\npractice, the data are typically observed under a model, and the model is\ndesired to fit the observed data. The ROM aims to recover the parameters of the\nmodel that fit the observed data. We show that the ROM is equivalent to a\nconstrained optimization problem with a penalized likelihood. This approach\nallows the ROM to be performed under a wide variety of structural models,\nincluding nonlinear models. We develop an efficient algorithm for the ROM,\nderive the asymptotic properties of the estimator, and demonstrate the\neffectiveness of the method in simulated data. The ROM is also applied to\ninfer the parameters of a structural model in a financial setting, where the\ndata are observed under the Cox-Proport",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.24,
          "f": 0.24489795418575602
        },
        "rouge-2": {
          "r": 0.03260869565217391,
          "p": 0.025423728813559324,
          "f": 0.028571423648073413
        },
        "rouge-l": {
          "r": 0.2361111111111111,
          "p": 0.22666666666666666,
          "f": 0.2312925120088853
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PM/2502.00415v1",
      "true_abstract": "MarketSenseAI is a novel framework for holistic stock analysis which\nleverages Large Language Models (LLMs) to process financial news, historical\nprices, company fundamentals and the macroeconomic environment to support\ndecision making in stock analysis and selection. In this paper, we present the\nlatest advancements on MarketSenseAI, driven by rapid technological expansion\nin LLMs. Through a novel architecture combining Retrieval-Augmented Generation\nand LLM agents, the framework processes SEC filings and earnings calls, while\nenriching macroeconomic analysis through systematic processing of diverse\ninstitutional reports. We demonstrate a significant improvement in fundamental\nanalysis accuracy over the previous version. Empirical evaluation on S\\&P 100\nstocks over two years (2023-2024) shows MarketSenseAI achieving cumulative\nreturns of 125.9% compared to the index return of 73.5%, while maintaining\ncomparable risk profiles. Further validation on S\\&P 500 stocks during 2024\ndemonstrates the framework's scalability, delivering a 33.8% higher Sortino\nratio than the market. This work marks a significant advancement in applying\nLLM technology to financial analysis, offering insights into the robustness of\nLLM-driven investment strategies.",
      "generated_abstract": "We study a model of market makers with one-sided trading costs, where each\nmarket maker is exposed to both the bid and ask prices. The market maker's\ntrading cost is dependent on the bid price, which is itself dependent on the\nask price. The market maker's trading cost is the same as that of a vanilla\nvolatility market maker, but the market maker's bid and ask prices are\ndependent. We show that in the limit of large market makers, the market maker\nachieves a Sharpe ratio that is not necessarily greater than the vanilla\nvolatility market maker. We also show that the market maker's Sharpe ratio\ndecreases with market maker size. This result highlights the importance of\nmarket maker size for trading costs, and the potential for market makers to\nbenefit from a market with a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.23809523809523808,
          "f": 0.15544041011033868
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.027777777777777776,
          "f": 0.021978017195991864
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.23809523809523808,
          "f": 0.15544041011033868
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2412.20570v1",
      "true_abstract": "Characterizing the local voltage distribution within nanophysiological\ndomains, driven by ionic currents through membrane channels, is crucial for\nstudying cellular activity in modern biophysics, yet it presents significant\nexperimental and theoretical challenges. Theoretically, the complexity arises\nfrom the difficulty of solving electro-diffusion equations in three-dimensional\ndomains. Currently, there are no methods available for obtaining asymptotic\ncomputations or approximated solutions of nonlinear equations, and numerically,\nit is challenging to explore solutions across both small and large spatial\nscales. In this work, we develop a method to solve the Poisson-Nernst-Planck\nequations with ionic currents entering and exiting through two narrow, circular\nwindow channels located on the boundary. The inflow through the first window is\ncomposed of a single cation, while the outflow maintains a constant ionic\ndensity satisfying local electro-neutrality conditions. Employing regular\nexpansions and Green's function representations, we derive the ionic profiles\nand voltage drops in both small and large charge regimes. We explore how local\nsurface curvature and window channels size influence voltage dynamics and\nvalidate our theoretical predictions through numerical simulations, assessing\nthe accuracy of our asymptotic computations. These novel relationships between\ncurrent, voltage, concentrations and geometry can enhance the characterization\nof physiological behaviors of nanodomains.",
      "generated_abstract": "The emergence of complex biological systems is driven by the combination of\npotent interactions among molecular entities and the physical constraints\nimposed by their interactions with the environment. The development of\nmachine-learning approaches to biological systems has been hindered by the\ndifficulty in identifying the critical parameters that determine the\nproperties of the resulting networks, often referred to as the ``black box''.\nHere, we present a machine-learning framework that identifies the physical\nconstraints that determine the structure of the resulting biological networks.\nWe demonstrate that the resulting networks can be expressed as a linear\ncombination of the biomolecular entities and the physical constraints, and\nthat these constraints can be identified by the linearity of the resulting\nnetworks. We also present a novel experimental procedure that allows the\nidentification of the physical constraints that determine the structure of the\nresulting networks. This framework offers a more transparent approach to\nunder",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12142857142857143,
          "p": 0.2236842105263158,
          "f": 0.157407402846365
        },
        "rouge-2": {
          "r": 0.010526315789473684,
          "p": 0.017241379310344827,
          "f": 0.013071890717247198
        },
        "rouge-l": {
          "r": 0.11428571428571428,
          "p": 0.21052631578947367,
          "f": 0.14814814358710576
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00467v1",
      "true_abstract": "Recent advancements in convolutional neural network (CNN)-based techniques\nfor remote sensing pansharpening have markedly enhanced image quality. However,\nconventional convolutional modules in these methods have two critical\ndrawbacks. First, the sampling positions in convolution operations are confined\nto a fixed square window. Second, the number of sampling points is preset and\nremains unchanged. Given the diverse object sizes in remote sensing images,\nthese rigid parameters lead to suboptimal feature extraction. To overcome these\nlimitations, we introduce an innovative convolutional module, Adaptive\nRectangular Convolution (ARConv). ARConv adaptively learns both the height and\nwidth of the convolutional kernel and dynamically adjusts the number of\nsampling points based on the learned scale. This approach enables ARConv to\neffectively capture scale-specific features of various objects within an image,\noptimizing kernel sizes and sampling locations. Additionally, we propose ARNet,\na network architecture in which ARConv is the primary convolutional module.\nExtensive evaluations across multiple datasets reveal the superiority of our\nmethod in enhancing pansharpening performance over previous techniques.\nAblation studies and visualization further confirm the efficacy of ARConv.",
      "generated_abstract": "This paper presents a novel framework for the real-time 3D point cloud\nreconstruction of complex industrial scenes. The proposed framework incorporates\na 3D point cloud reconstruction model and a semantic segmentation model to\ngenerate a high-quality 3D model of the scene. This model is trained using\nsimulated data collected from the real-world scenario and is then evaluated on\na real-world dataset. The proposed framework is evaluated on the KITTI-3D\ndataset, which contains 175 scenes from 21 cities. The results demonstrate that\nthe proposed framework can generate high-quality 3D point clouds of complex\nindustrial scenes in real-time. The evaluation also shows that the semantic\nsegmentation model is essential for generating a high-quality 3D point cloud\nmodel of the scene.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09016393442622951,
          "p": 0.1864406779661017,
          "f": 0.12154695693171777
        },
        "rouge-2": {
          "r": 0.011976047904191617,
          "p": 0.02127659574468085,
          "f": 0.015325665889227907
        },
        "rouge-l": {
          "r": 0.08196721311475409,
          "p": 0.1694915254237288,
          "f": 0.1104972331748117
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.02726v1",
      "true_abstract": "Deep learning scaling laws predict how performance improves with increased\nmodel and dataset size. Here we identify measurement noise in data as another\nperformance scaling axis, governed by a distinct logarithmic law. We focus on\nrepresentation learning models of biological single cell genomic data, where a\ndominant source of measurement noise is due to molecular undersampling. We\nintroduce an information-theoretic metric for cellular representation model\nquality, and find that it scales with sampling depth. A single quantitative\nrelationship holds across several model types and across several datasets. We\nshow that the analytical form of this relationship can be derived from a simple\nGaussian noise model, which in turn provides an intuitive interpretation for\nthe scaling law. Finally, we show that the same relationship emerges in image\nclassification models with respect to two types of imaging noise, suggesting\nthat measurement noise scaling may be a general phenomenon. Scaling with noise\ncan serve as a guide in generating and curating data for deep learning models,\nparticularly in fields where measurement quality can vary dramatically between\ndatasets.",
      "generated_abstract": "The aim of this paper is to examine the potential of a novel genetic\nmodel of bacterial metabolism based on the coupling of the growth and\ndissolution of complex organic molecules (COM) and their incorporation into\norganic acids (OAs). These complex organic molecules are considered to be the\nbasis for the biosynthesis of the bacterial cell wall, whereas the incorporation\nof OAs into organic acids is assumed to be a non-enzymatic process. The\ngenetic model of bacterial metabolism is based on the coupling of the growth of\na bacterial population and the incorporation of OAs into organic acids. The\nmodel is evaluated using a simulation approach. The simulation results\ndemonstrate the model's ability to reproduce the growth dynamics of the\npopulation and the incorporation of OAs into organic acids.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10619469026548672,
          "p": 0.21428571428571427,
          "f": 0.14201182988830938
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.031578947368421054,
          "f": 0.02281368359814461
        },
        "rouge-l": {
          "r": 0.10619469026548672,
          "p": 0.21428571428571427,
          "f": 0.14201182988830938
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.10653v1",
      "true_abstract": "This paper proposes a framework for selecting policies that maximize expected\nbenefit in the presence of estimation uncertainty, by controlling for\nestimation risk and incorporating risk aversion. The proposed method explicitly\nbalances the size of the estimated benefit against the uncertainty inherent in\nits estimation, ensuring that chosen policies meet a reporting guarantee,\nnamely that the actual benefit of the implemented policy is guaranteed not to\nfall below the reported estimate with a pre-specified confidence level. This\napproach applies to a variety of settings, including the selection of policy\nrules that allocate individuals to treatments based on observed\ncharacteristics, using both experimental and non-experimental data; and the\nallocation of limited budgets among competing social programs; as well as many\nothers. Across these applications, the framework offers a principled and robust\nmethod for making data-driven policy choices under uncertainty. In broader\nterms, it focuses on policies that are on the efficient decision frontier,\ndescribing policies that offer maximum estimated benefit for a given acceptable\nlevel of estimation risk.",
      "generated_abstract": "We propose a new approach to constructing the optimal treatment effect\n(OTE) for a binary treatment and the average treatment effect (ATE) for a\ncontinuous treatment. The approach combines the use of a propensity score with\na two-step procedure, which first identifies a binary treatment, and then\nconstructs an OTE and an ATE for the binary treatment using the resulting\nidentified binary treatment. The approach is general, and can be applied to\nother types of continuous treatments, such as treatment effect heterogeneity.\nThe procedure is fast and can be implemented with a few lines of code. We\ninvestigate the performance of the procedure in simulated and real data\napplications. The simulation studies show that the proposed method is\neffective, and the real data applications illustrate the usefulness of the\nprocedure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15315315315315314,
          "p": 0.2361111111111111,
          "f": 0.18579234495386557
        },
        "rouge-2": {
          "r": 0.0375,
          "p": 0.05309734513274336,
          "f": 0.04395603910424184
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.20833333333333334,
          "f": 0.16393442145659787
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2501.17490v1",
      "true_abstract": "Leveraging a unique dataset of carbon futures option prices traded on the ICE\nmarket from December 2015 until December 2020, we present the results from an\nunprecedented calibration exercise. Within a multifactor stochastic volatility\nframework with jumps, we employ a three-dimensional pricing kernel compensating\nfor equity and variance components' risk to derive an analytically tractable\nand numerically practical approach to pricing. To the best of our knowledge, we\nare the first to provide an estimate of the equity and variance risk premia for\nthe carbon futures option market. We gain insights into daily option and\nfutures dynamics by exploiting the information from tick-by-tick futures trade\ndata. Decomposing the realized measure of futures volatility into continuous\nand jump components, we employ them as auxiliary variables for estimating\nfutures dynamics via indirect inference. Our approach provides a realistic\ndescription of carbon futures price, volatility, and jump dynamics and an\ninsightful understanding of the carbon option market.",
      "generated_abstract": "We propose a novel approach to the problem of portfolio optimization under\nconstraints on the portfolio size. We consider the problem of constructing a\nportfolio of assets that achieves the lowest possible portfolio weighted average\ncost (WAC) and the minimization of the sum of transaction costs in the\nportfolio. We provide a necessary and sufficient condition for this problem\nto be equivalent to the problem of finding the optimal portfolio in the\nconstrained WAC model. We demonstrate that the proposed approach can be used\nto construct portfolios that outperform traditional approaches. We show that\nour approach is applicable to various scenarios, including portfolio\nconstrained by liquidity, transaction costs, and risk, as well as portfolio\nconstrained by transaction costs and risk. The proposed approach is simple and\neffective, and it can be used in practice to construct portfolios that\noutperform traditional approaches.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.1917808219178082,
          "f": 0.16766466573774622
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.025,
          "f": 0.022988500779496383
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.1917808219178082,
          "f": 0.16766466573774622
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2502.11199v1",
      "true_abstract": "Although the methodology of Design Science Research (DSR) is playing an\nincreasingly important role with the emergence of the \"sciences of the\nartificial\", the validity of the resulting artifacts is occasionally\nquestioned. This paper compares three influential DSR frameworks to assess\ntheir support for artifact validity. Using five essential validity types\n(instrument validity, technical validity, design validity, purpose validity and\ngeneralization), the qualitative analysis reveals that while purpose validity\nis explicitly emphasized, instrument and design validity remain the least\ndeveloped. Their implicit treatment in all frameworks poses a risk of\noverlooked validation, and the absence of mandatory instrument validity can\nlead to invalid artifacts, threatening research credibility. Beyond these\nfindings, the paper contributes (a) a comparative overview of each framework's\nstrengths and weaknesses and (b) a revised DSR framework incorporating all five\nvalidity types with definitions and examples. This ensures systematic artifact\nevaluation and improvement, reinforcing the rigor of DSR.",
      "generated_abstract": "This paper presents a novel approach to synthesize data-driven control\nstrategies for hybrid electric vehicles. Inspired by the concept of\nreinforcement learning, we propose a model predictive control (MPC) framework\nthat combines physics-based vehicle dynamics and a neural network to predict\nthe vehicle's state. The neural network is trained using data collected from\nsimulation, and the MPC controller is then trained to optimize control\nparameters based on the neural network output. The approach is evaluated using\na vehicle model that is used to simulate a hybrid electric vehicle, and the\nresults demonstrate that the neural network-based prediction and MPC\nframework can significantly improve control accuracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.1791044776119403,
          "f": 0.14371257004553784
        },
        "rouge-2": {
          "r": 0.013793103448275862,
          "p": 0.020202020202020204,
          "f": 0.01639343780065984
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.1791044776119403,
          "f": 0.14371257004553784
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/bio-ph/2503.09319v1",
      "true_abstract": "We present PolyMorph, a lightweight standalone C++ program that extends its\npredecessor PolyHoop by a finite-difference solver for multi-component\nreaction-advection-diffusion equations. PolyMorph simulates two integral parts\nof tissue morphogenesis in two dimensions: 1) the mechanics of cellular\ndeformation, growth and proliferation, and 2) transport and reaction of an\narbitrary number of chemical species. Both of these components are\nbidirectionally coupled, allowing cells to base their behavior on local\ninformation on concentrations and flow, and allowing the chemical transport and\nreaction kinetics to depend on spatial information such as the local cell type.\nThis bidirectional feedback makes PolyMorph a versatile tool to study a variety\nof cellular morphogenetic processes such as chemotaxis, cell sorting, tissue\npatterning with morphogen gradients, Turing patterning, and diffusion- or\nsupply-limited growth with sub-cellular resolution.",
      "generated_abstract": "The molecular mechanisms of adhesion and spreading of bacteria on surfaces\nremain largely unknown. Here, we present a microscopic study of the adhesion\nprocess of a bacterium to a substrate, focusing on the role of the\ninterfacial layer between the bacterium and the substrate. We employ a\ntime-dependent boundary-coupled nonlinear Schr\\\"odinger equation (NLS) to\ndescribe the propagation of a bacterium adhering to a flat substrate. By\nintroducing an adhesion potential, we quantify the force required to adhere a\nbacterium to a flat substrate. We find that the adhesion force increases with\nthe adhering bacterium's size and decreases with the adhering bacterium's\nnumber density. Moreover, we show that the adhesion force scales linearly with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13978494623655913,
          "p": 0.20634920634920634,
          "f": 0.16666666185157802
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13978494623655913,
          "p": 0.20634920634920634,
          "f": 0.16666666185157802
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.09740v1",
      "true_abstract": "This paper addresses the challenge of forecasting corporate distress, a\nproblem marked by three key statistical hurdles: (i) right censoring, (ii)\nhigh-dimensional predictors, and (iii) mixed-frequency data. To overcome these\ncomplexities, we introduce a novel high-dimensional censored MIDAS (Mixed Data\nSampling) logistic regression. Our approach handles censoring through inverse\nprobability weighting and achieves accurate estimation with numerous\nmixed-frequency predictors by employing a sparse-group penalty. We establish\nfinite-sample bounds for the estimation error, accounting for censoring, the\nMIDAS approximation error, and heavy tails. The superior performance of the\nmethod is demonstrated through Monte Carlo simulations. Finally, we present an\nextensive application of our methodology to predict the financial distress of\nChinese-listed firms. Our novel procedure is implemented in the R package\n'Survivalml'.",
      "generated_abstract": "We propose a novel approach for estimating the mean and variance of\nthe conditional expectation of a random variable given another random\nvariable. The proposed estimator is a two-step procedure, where the first\nstep is based on the mean and variance of the conditional expectation of the\nrandom variable given another random variable, and the second step uses these\nestimates to estimate the conditional expectation of the random variable given\nanother random variable. We derive a closed-form expression for the second\nstep, and show that the estimator is consistent and asymptotically normal. We\nalso propose a practical algorithm to compute the estimator, and conduct\nsimulation studies to assess its finite sample properties. We demonstrate the\nuse of the proposed estimator in an empirical application to analyze the\neffect of COVID-19 restrictions on employment in the United States.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.2112676056338028,
          "f": 0.17964071367492573
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.037383177570093455,
          "f": 0.035242285765297914
        },
        "rouge-l": {
          "r": 0.14583333333333334,
          "p": 0.19718309859154928,
          "f": 0.16766466577073413
        }
      }
    },
    {
      "paper_id": "cs.LO.cs/LO/2503.06036v1",
      "true_abstract": "Consistent Hoare, Smyth and Plotkin power domains are introduced and\ndiscussed by Yuan and Kou. The consistent algebraic operation $+$ defined by\nthem is a binary partial Scott continuous operation satisfying the requirement:\n$a+b$ exists whenever there exists a $c$ which is greater than $a$ and $b$. We\nextend the consistency to be a categorical concept and obtain an approach to\ngenerating consistent monads from monads on dcpos whose images equipped with\nsome algebraic operations. Then we provide two new power constructions over\ndomains: the consistent Plotkin index power domain and the consistent\nprobabilistic power domain. Moreover, we verify these power constructions are\nfree.",
      "generated_abstract": "This paper presents a novel approach to modeling and reasoning about\nspatial data in multi-agent systems. We propose a novel framework for\nrepresenting spatial data, called Spatial Actor Networks (SANs), which\nintegrates spatial reasoning and multi-agent reasoning into a unified\ncomputational model. Our framework is based on the concept of\nspatial-aware-agents, which are agents that are aware of their spatial\npositions and can actively communicate with each other to exchange information\nabout their spatial locations. By integrating spatial reasoning and multi-agent\nreasoning into a unified computational model, we address two key challenges in\nspatial reasoning: the lack of a universal spatial representation and the\ninability to capture spatial dependencies in multi-agent systems. Our framework\naddresses these challenges by incorporating spatial reasoning and multi-agent\nreasoning into a unified computational model, enabling the modeling and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21333333333333335,
          "p": 0.21052631578947367,
          "f": 0.2119205248015439
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.01834862385321101,
          "f": 0.01913875099013431
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.19736842105263158,
          "f": 0.19867549168896115
        }
      }
    },
    {
      "paper_id": "quant-ph.cs/CC/2503.03600v1",
      "true_abstract": "Bosonic quantum systems operate in an infinite-dimensional Hilbert space,\nunlike discrete-variable quantum systems. This distinct mathematical structure\nleads to fundamental differences in quantum information processing, such as an\nexponentially greater complexity of state tomography [MMB+24] or a factoring\nalgorithm in constant space [BCCRK24]. Yet, it remains unclear whether this\nstructural difference of bosonic systems may also translate to a practical\ncomputational advantage over finite-dimensional quantum computers. Here we take\na step towards answering this question by showing that universal bosonic\nquantum computations can be simulated in exponential time on a classical\ncomputer, significantly improving the best previous upper bound requiring\nexponential memory [CJMM24]. In complexity-theoretic terms, we improve the best\nupper bound on $\\textsf{CVBQP}$ from $\\textsf{EXPSPACE}$ to $\\textsf{EXP}$.\nThis result is achieved using a simulation strategy based on finite energy\ncutoffs and approximate coherent state decompositions. While we propose ways to\npotentially refine this bound, we also present arguments supporting the\nplausibility of an exponential computational advantage of bosonic quantum\ncomputers over their discrete-variable counterparts. Furthermore, we emphasize\nthe role of circuit energy as a resource and discuss why it may act as the\nfundamental bottleneck in realizing this advantage in practical\nimplementations.",
      "generated_abstract": "We investigate the computational complexity of sampling the state space of\na general class of dynamical systems. In this class, the state space is\ncontinuous and the system dynamics are time-independent, periodic, or\ntime-varying. We provide an explicit construction of a randomized algorithm that\ncan efficiently sample the state space of each of these systems with high\nprobability. We also provide an algorithm that samples the state space of\nperiodic systems with high probability, and an algorithm that samples the state\nspace of time-varying systems with high probability. In addition, we provide an\nalgorithm that samples the state space of a class of non-reversible\ntime-independent systems with high probability, and an algorithm that samples\nthe state space of a class of non-reversible time-varying systems with high\nprobability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14615384615384616,
          "p": 0.38,
          "f": 0.2111111070987655
        },
        "rouge-2": {
          "r": 0.005405405405405406,
          "p": 0.0136986301369863,
          "f": 0.007751933926749314
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.36,
          "f": 0.19999999598765442
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.06711v1",
      "true_abstract": "This paper touches on several interaction points of semigroups and\nconstructions from category theory: An adjunction is established between\ncategories with selected arrows and semigroups. Regular semigroups are\ncharacterized by split epi - split mono factorization of the Karoubi envelope.\nWe investigate how semigroupads (monads without requirement of unit\ntransformation) map semigroups to semigroups and ensure certain properties\nprovided they hold on meta level.",
      "generated_abstract": "In this paper, we study the properties of certain linear operators on\nhermitian space, such as Fredholm and self-adjointness. We show that the\noperators of the type $A^T-B$ and $A-B^T$ are Fredholm and self-adjoint\noperators, respectively. We also prove that the operators $A+B$ and $A^T+B$\nare Fredholm operators, but they are not self-adjoint. Furthermore, we show\nthat the operators $A+B$ and $A^T+B$ are not Fredholm operators, but they are\nself-adjoint.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.24324324324324326,
          "f": 0.19780219297669374
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.018867924528301886,
          "f": 0.017391299378451328
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.1891891891891892,
          "f": 0.15384614902064983
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.06327v1",
      "true_abstract": "We investigate the return-forecasting and volatility-forecasting power of\nintraday on-chain flow data for BTC, ETH, and USDT, and the associated option\nstrategies. First, we find that USDT net inflow into cryptocurrency exchanges\npositively forecasts future returns of both BTC and ETH, with the strongest\neffect at the 1-hour frequency. Second, we find that ETH net inflow into\ncryptocurrency exchanges negatively forecasts future returns of ETH. Third, we\nfind that BTC net inflow into cryptocurrency exchanges does not significantly\nforecast future returns of BTC. Finally, we confirm that selling 0DTE ETH call\noptions is a profitable trading strategy when the net inflow into\ncryptocurrency exchanges is high. Our study lends new insights into the\nemerging literature that studies the on-chain activities and their\nasset-pricing impact in the cryptocurrency market.",
      "generated_abstract": "We propose a novel approach to modeling and interpreting the effects of\nrandomized experiments on outcomes. Our framework is based on the idea of\n\"natural experiments\", which arise from the fact that the treatment assignment\nis random, but the outcome is not. In our approach, we estimate a model of\noutcomes under treatment assignment that is fully specified by the treatment\nassignment, but the model of the treatment effect is not specified. We show\nthat under some assumptions, the natural-experiment-based model of the\ntreatment effect can be interpreted as the difference-in-differences estimator.\nWe apply our approach to a natural experiment in which the government\nadministers a universal basic income.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.18461538461538463,
          "f": 0.17021276098787802
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.18461538461538463,
          "f": 0.17021276098787802
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/ME/2503.06401v1",
      "true_abstract": "Distribution-as-response regression problems are gaining wider attention,\nespecially within biomedical settings where observation-rich patient specific\ndata sets are available, such as feature densities in CT scans (Petersen et\nal., 2021) actigraphy (Ghosal et al., 2023), and continuous glucose monitoring\n(Coulter et al., 2024; Matabuena et al., 2021). To accommodate the complex\nstructure of such problems, Petersen and M\\\"uller (2019) proposed a regression\nframework called Fr\\'echet regression which allows non-Euclidean responses,\nincluding distributional responses. This regression framework was further\nextended for variable selection by Tucker et al. (2023), and Coulter et al.\n(2024) (arXiv:2403.00922 [stat.AP]) developed a fast variable selection\nalgorithm for the specific setting of univariate distributional responses\nequipped with the 2-Wasserstein metric (2-Wasserstein space). We present\n\"fastfrechet\", an R package providing fast implementation of these Fr\\'echet\nregression and variable selection methods in 2-Wasserstein space, with\nresampling tools for automatic variable selection. \"fastfrechet\" makes\ndistribution-based Fr\\'echet regression with resampling-supplemented variable\nselection readily available and highly scalable to large data sets, such as the\nUK Biobank (Doherty et al., 2017).",
      "generated_abstract": "The paper investigates the problem of estimating the average mean displacement\nof a population of point processes. We develop a new approach based on the\nLaplace transform of the empirical measure, which allows for the estimation of\nthe mean and the variance of the population mean displacement. The proposed\nestimator is based on a bootstrap procedure, which is computationally\neconomical, and is shown to be consistent and asymptotically normal. A\nsimulation study is performed to assess the finite-sample performance of the\nestimator. The proposed approach is applied to the estimation of the average\nmean displacement of a sample of $10^3$ random walks in $10^3$ realisations,\nshowing that the proposed estimator performs well in terms of the bias and the\nvariance of the estimator. A real-life example is also considered to assess the\nperformance of the estimator in practice.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09322033898305085,
          "p": 0.16176470588235295,
          "f": 0.11827956525378676
        },
        "rouge-2": {
          "r": 0.012903225806451613,
          "p": 0.018018018018018018,
          "f": 0.015037589121772162
        },
        "rouge-l": {
          "r": 0.09322033898305085,
          "p": 0.16176470588235295,
          "f": 0.11827956525378676
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09127v1",
      "true_abstract": "This research presents Spiritus, an AI-assisted creation tool designed to\nstreamline 2D character animation creation while enhancing creative\nflexibility. By integrating natural language processing and diffusion models,\nusers can efficiently transform natural language descriptions into personalized\n2D characters and animations. The system employs automated segmentation,\nlayered costume techniques, and dynamic mesh-skeleton binding solutions to\nsupport flexible adaptation of complex costumes and additional components.\nSpiritus further achieves real-time animation generation and efficient\nanimation resource reuse between characters through the integration of BVH data\nand motion diffusion models. Experimental results demonstrate Spiritus's\neffectiveness in reducing technical barriers, enhancing creative freedom, and\nsupporting resource universality. Future work will focus on optimizing user\nexperience and further exploring the system's human-computer collaboration\npotential.",
      "generated_abstract": "The Internet of Things (IoT) is transforming the way we live, work, and\nperform everyday tasks. This paradigm shift is opening up new opportunities for\nusers to make informed decisions based on their individual needs and preferences.\nHowever, it also poses significant challenges, such as privacy risks and\nunintended consequences. To address these challenges, this paper introduces a\nnew framework for analyzing and designing personalized user interfaces\n(UIs) for the Internet of Things (IoT). The framework emphasizes the\ninteraction between a user and the IoT environment, encompassing the user's\nphysical, cognitive, and social factors. It also integrates user-centric design\nconcepts, such as privacy, security, usability, and empathy, to improve the\nuser experience and foster a more human-centered approach to design",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.13793103448275862,
          "f": 0.13114753599570028
        },
        "rouge-2": {
          "r": 0.017391304347826087,
          "p": 0.017391304347826087,
          "f": 0.017391299347827526
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.13793103448275862,
          "f": 0.13114753599570028
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2410.12648v1",
      "true_abstract": "Fingerprints, otherwise known as dermatoglyphs, are most commonly thought of\nin the context of identification, but have myriad other roles in human biology.\nThey are formed by the restricted ability of ridges and furrows of the\nepidermis to flatten. The patterns these ridges and furrows make can be\nrepresented as 2D fingerprints, but also as 3D structures with cross-sectional\nshapes that may add new levels of detail to identification, forensic, and\nbehavioral uses/studies. Surface metrology techniques better allow for the\nquantification of these features, though it is unclear what tool and what scale\nis most appropriate. A Sensofar S Neox white light reflectance confocal\nmicroscope and a Gelsight Mobile 2 were used to independently measure the\nsurface roughness of the fingerprints of four individuals from preserved\ncadaveric remains. Scale-sensitive fractal analyses (SSFA) were performed on\nthe data from the S Neox (a small area), Gelsight (a larger area), and the same\nGelsight datasets cropped down to the size of the S Neox scan size. Though\nfewer SSFA parameters identified differences between individuals from the\nsmaller, extracted Gelsight area, all three forms of measurement found\nsignificant differences between some individuals from the study. No significant\ndifferences were found that differ between fingers themselves. Though only an\ninitial step, these data suggest that a variety of surface metrology techniques\nmay be useful in differentiating individuals.",
      "generated_abstract": "In this study, we present a novel approach for the identification of\nnon-canonical protein-protein interactions (NCPPIs) based on the\ncomprehensive analysis of the human protein-protein interaction network. The\nNCPPIs are defined as the interactions that are significantly different from\nother interactions. These interactions are identified through a multi-layer\nanalysis of the protein-protein interaction network. To identify NCPPIs, we\napply the non-negative matrix factorization (NMF) approach to the protein-protein\ninteraction network. We demonstrate that the NMF approach can be used to\nidentify NCPPIs with high accuracy. The performance of the NMF method is\nevaluated through the use of three different metrics. The results demonstrate\nthat the NMF method is able to detect NCPPIs with high accuracy. We also\nprovide an example of the application of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15172413793103448,
          "p": 0.3142857142857143,
          "f": 0.20465115839913478
        },
        "rouge-2": {
          "r": 0.028846153846153848,
          "p": 0.05714285714285714,
          "f": 0.03833865368841216
        },
        "rouge-l": {
          "r": 0.1310344827586207,
          "p": 0.2714285714285714,
          "f": 0.17674418165494873
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/DS/2503.07769v1",
      "true_abstract": "We study the problem of computing the diameter and the mean distance of a\ncontinuous graph, i.e., a connected graph where all points along the edges,\ninstead of only the vertices, must be taken into account. It is known that for\ncontinuous graphs with $m$ edges these values can be computed in roughly\n$O(m^2)$ time. In this paper, we use geometric techniques to obtain\nsubquadratic time algorithms to compute the diameter and the mean distance of a\ncontinuous graph for two well-established classes of sparse graphs. We show\nthat the diameter and the mean distance of a continuous graph of treewidth at\nmost $k$ can be computed in $O(n\\log^{O(k)} n)$ time, where $n$ is the number\nof vertices in the graph. We also show that computing the diameter and mean\ndistance of a continuous planar graph with $n$ vertices and $F$ faces takes\n$O(n F \\log n)$ time.",
      "generated_abstract": "We present a simple and practical approach to improving the performance of\nalgorithms that use recursion, particularly when recursion is applied to\ncomputations that are difficult to parallelize. Our approach relies on\ndistributed data structures, and is inspired by the work of Kadane,\nKnuth, and Sedgewick, who have used the technique to improve the performance\nof various algorithms. Our approach differs in several ways from that of\nKadane, Knuth, and Sedgewick. First, our approach is not restricted to\nsingle-machine systems, and we do not require that the recursion be parallel\nwithin each machine. Second, our approach is not restricted to algorithms that\ncan be represented as a single, recursive function. Third, our approach\nincorporates the use of data structures that are not part of the standard\nlibrary. We present the basic algorithm and data structures, as well as\nsuitable applications of the algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1686746987951807,
          "p": 0.1728395061728395,
          "f": 0.17073170231781692
        },
        "rouge-2": {
          "r": 0.01652892561983471,
          "p": 0.016,
          "f": 0.016260157602949517
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.14814814814814814,
          "f": 0.1463414584153779
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/CP/2412.16067v1",
      "true_abstract": "We use modifications of the Adams method and very fast and accurate\nsinh-acceleration method of the Fourier inversion (iFT) (S.Boyarchenko and\nLevendorski\\u{i}, IJTAF 2019, v.22) to evaluate prices of vanilla options; for\noptions of moderate and long maturities and strikes not very far from the spot,\nthousands of prices can be calculated in several msec. with relative errors of\nthe order of 0.5\\% and smaller running Matlab on a Mac with moderate\ncharacteristics. We demonstrate that for the calibrated set of parameters in\nEuch and Rosenbaum, Math. Finance 2019, v. 29, the correct implied volatility\nsurface is significantly flatter and fits the data very poorly, hence, the\ncalibration results in op.cit. is an example of the {\\em ghost calibration}\n(M.Boyarchenko and Levendorki\\u{i}, Quantitative Finance 2015, v. 15): the\nerrors of the model and numerical method almost cancel one another. We explain\nhow calibration errors of this sort are generated by each of popular versions\nof numerical realizations of iFT (Carr-Madan, Lipton-Lewis and COS methods)\nwith prefixed parameters of a numerical method, resulting in spurious\nvolatility smiles and skews. We suggest a general {\\em Conformal Bootstrap\nprinciple} which allows one to avoid ghost calibration errors. We outline\nschemes of application of Conformal Bootstrap principle and the method of the\npaper to the design of accurate and fast calibration procedures.",
      "generated_abstract": "In this paper, we study the optimal consumption and investment strategies for\na portfolio of stocks and a portfolio of bonds in a portfolio-adjusted risk-free\nrisk-free rate model with the mean-variance criterion. Our main focus is on the\nportfolio with the mean-variance criterion. We develop a new method to solve\nthe portfolio problem with the mean-variance criterion. Our method is based on\nthe convex decomposition method. We prove that the resulting optimization\nproblem has a unique solution. Then we analyze the optimality of the solution\nand provide sufficient conditions for the optimality of the solution.\nMoreover, we provide a sufficient condition for the optimal strategy to be\nconvex. Our theoretical analysis is based on the results in the literature on\nthe optimal consumption and investment strategies for portfolios with the\nexpected utility criterion. In addition, we analyze the performance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.2727272727272727,
          "f": 0.17910447320115852
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.027777777777777776,
          "f": 0.018867920042720893
        },
        "rouge-l": {
          "r": 0.1259259259259259,
          "p": 0.25757575757575757,
          "f": 0.1691542244449396
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00818v1",
      "true_abstract": "Sample size determination is crucial in experimental design, especially in\ntraffic and transport research. Frequentist statistics require a fixed sample\nsize determined by power analysis, which cannot be adjusted once the experiment\nstarts. Bayesian sample size determination, with proper priors, offers an\nalternative. Bayesian optional stopping (BOS) allows experiments to stop when\nstatistical targets are met. We introduce predictive Bayesian optional stopping\n(pBOS), combining BOS with Bayesian rehearsal simulations to predict future\ndata and stop experiments if targets are unlikely to be met within resource\nconstraints. We identified and corrected a bias in predictions using multiple\nlinear regression. pBOS shows up to 118% better cost benefit than traditional\nBOS and is more efficient than frequentist methods. pBOS allows researchers to,\nunder certain conditions, stop experiments when resources are insufficient or\nwhen enough data is collected, optimizing resource use and cost savings.",
      "generated_abstract": "In this paper, we present a novel approach to the problem of predicting\nquantitative and qualitative data based on the joint distribution of the\npredictors. In particular, we propose to use the multidimensional scaling\n(MDS) technique to obtain a low-dimensional representation of the data. This\ntechnique is widely used in a number of fields, including econometrics,\nstatistics, and computer science. In the present work, we use MDS to project\nthe data into a low-dimensional space, where the predictors are embedded.\nThis enables us to estimate the joint distribution of the predictors and use\nthe resulting predictor distributions as the predictors in the regression model\n(e.g., linear, logistic, or multiple regression). The predictor distributions\nare used to predict the response variables, which are then used to generate\npredictive models. The predictor distributions can also be used to identify\nimportant predictors",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15841584158415842,
          "p": 0.1927710843373494,
          "f": 0.17391303852611073
        },
        "rouge-2": {
          "r": 0.007407407407407408,
          "p": 0.008064516129032258,
          "f": 0.007722002731029895
        },
        "rouge-l": {
          "r": 0.13861386138613863,
          "p": 0.1686746987951807,
          "f": 0.15217390809132816
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.17455v1",
      "true_abstract": "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
      "generated_abstract": "The concept of a biological brain has been a prominent part of human\nhuman cultural narratives for millennia. It has been a central theme in\nmedicine, philosophy, and art. This paper explores the origins of this\nconcept. It discusses the historical development of the concept, its\ncontemporary usage in popular culture, and its contemporary applications. We\nalso examine the implications of this concept for the future of humanity. We\nconclude by presenting a framework for understanding the biological brain as a\nhierarchical system composed of multiple neural subsystems. This framework\nprovides a framework for understanding the neurobiology of human consciousness\nand provides a foundation for designing future neurotechnologies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14606741573033707,
          "p": 0.20634920634920634,
          "f": 0.17105262672524252
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.010526315789473684,
          "f": 0.009708732894243283
        },
        "rouge-l": {
          "r": 0.12359550561797752,
          "p": 0.1746031746031746,
          "f": 0.1447368372515583
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2502.19397v1",
      "true_abstract": "In chemical reaction network theory, ordinary differential equations are used\nto model the temporal change of chemical species concentration. As the\nfunctional form of these ordinary differential equations systems is derived\nfrom an empirical model of the reaction network, it may be incomplete. Our\napproach aims to elucidate these hidden insights in the reaction network by\ncombining dynamic modelling with deep learning in the form of neural ordinary\ndifferential equations. Our contributions not only help to identify the\nshortcomings of existing empirical models but also assist the design of future\nreaction networks.",
      "generated_abstract": "This study presents a novel approach for analyzing the impact of gene\ntranscripts on protein expression. Our methodology integrates protein sequence\nand transcriptome information to identify the most likely gene products that\nexert a direct influence on protein expression. We demonstrate the utility of\nthis approach in analyzing the effects of genetic variations on protein\nexpression. The results reveal that genetic variation at the Delta-like-4\n(Dll4) locus has a significant impact on the expression of the protein\nDll4, which plays a role in the regulation of inflammatory processes and\ninflammation-related diseases. Our findings highlight the potential of this\napproach to provide new insights into the molecular mechanisms underlying\ndisease pathologies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.11267605633802817,
          "f": 0.12030074690259503
        },
        "rouge-2": {
          "r": 0.04819277108433735,
          "p": 0.0392156862745098,
          "f": 0.04324323829598303
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.11267605633802817,
          "f": 0.12030074690259503
        }
      }
    },
    {
      "paper_id": "math.PR.q-fin/MF/2412.16436v1",
      "true_abstract": "We consider a microstructure foundation for rough volatility models driven by\nPoisson random measures. In our model the volatility is driven by self-exciting\narrivals of market orders as well as self-exciting arrivals of limit orders and\ncancellations. The impact of market order on future order arrivals is captured\nby a Hawkes kernel with power law decay, and is hence persistent. The impact of\nlimit orders on future order arrivals is temporary, yet possibly long-lived.\nAfter suitable scaling the volatility process converges to a fractional Heston\nmodel driven by an additional Poisson random measure. The random measure\ngenerates occasional spikes and clusters of spikes in the volatility process.\nOur results are based on novel existence and uniqueness of solutions results\nfor stochastic path-dependent Volterra equations driven by Poisson random\nmeasures.",
      "generated_abstract": "We consider the linear-quadratic Nash equilibrium problem in the\nassumption of a finite-dimensional state space and a discrete-time setting.\nUnder the assumption of a unique and strong solution of the associated\nlinear-quadratic Gauss-Markov process, we derive a necessary and sufficient\ncondition for the existence of the Nash equilibrium. We also derive a\nnecessary and sufficient condition for the existence of the Nash equilibrium\nwith a specific form of the quadratic cost function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.2631578947368421,
          "f": 0.17699114597854188
        },
        "rouge-2": {
          "r": 0.018867924528301886,
          "p": 0.03773584905660377,
          "f": 0.025157228259958853
        },
        "rouge-l": {
          "r": 0.10666666666666667,
          "p": 0.21052631578947367,
          "f": 0.14159291589004636
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03648v1",
      "true_abstract": "The work aims to propose a new nonlinear characteristics model for a wideband\nradio amplifier of variable supply voltage. An extended Rapp model proposal is\npresented. The proposed model has been verified by measurements of three\ndifferent amplifiers. This model can be used to design frontend-aware 6G\nsystems.\n  --\n  Praca ma na celu zaproponowanie nowego modelu dla nieliniowej charakterystyki\nwzmacniacza radiowego ze zmiennym napi\\k{e}ciem zasilania pracuj\\k{a}cym w\nszerokim zakresie cz\\k{e}stotliwo\\'sci. Przedstawiona zosta{\\l}a propozycja\nrozszerzonego modelu Rappa. Zaproponowany model zweryfikowano na podstawie\npomiar\\'ow charakterystyk trzech r\\'o\\.znych wzmacniaczy. Model ten mo\\.ze\nby\\'c wykorzystany do projektowania system\\'ow 6G \"\\'swiadomych\"\nniedoskona{\\l}o\\'sci uk{\\l}ad\\'ow wej\\'sciowo-wyj\\'sciowych.",
      "generated_abstract": "This paper studies the design of optimal linear-time-in-memory (LTIM)\nvector quantizers (VQs) for compressive sensing (CS) by considering the\ninformation-theoretic capacity of the compressed signal model. The VQ is\nconstructed by minimizing a Lagrangian dual problem, which is a non-convex\noptimization problem. This dual problem is a linear program (LP) whose\nsolution depends on the Lagrangian multipliers. The complexity of the dual\nproblem is proportional to the number of VQs. In this paper, we propose an\nalternating optimization (AO) algorithm for the dual problem to accelerate its\nsolution, which consists of an iterative reweighted least squares (RWLS)\nalgorithm and an iterative least square (ILS) algorithm. We show that the\ndual problem can be reformulated as a linear program (LP)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14606741573033707,
          "p": 0.16883116883116883,
          "f": 0.15662650105022516
        },
        "rouge-2": {
          "r": 0.01,
          "p": 0.009433962264150943,
          "f": 0.00970873286832192
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.16883116883116883,
          "f": 0.15662650105022516
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06376v1",
      "true_abstract": "Over-the-air federated learning (OTA-FL) offers an exciting new direction\nover classical FL by averaging model weights using the physics of analog signal\npropagation. Since each participant broadcasts its model weights concurrently\nin time and frequency, this paradigm conserves communication bandwidth and\nmodel upload latency. Despite its potential, there is no prior large-scale\ndemonstration on a real-world experimental platform. This paper proves for the\nfirst time that OTA-FL can be deployed in a cellular network setting within the\nconstraints of a 5G compliant waveform. To achieve this, we identify challenges\ncaused by multi-path fading effects, thermal noise at the radio devices, and\nmaintaining highly precise synchronization across multiple clients to perform\ncoherent OTA combining. To address these challenges, we propose a unified\nframework for real-time channel estimation, model weight to OFDM symbol mapping\nand dual-layer synchronization interface to perform OTA model training. We\nexperimentally validate OTA-FL using two relevant applications - Channel\nEstimation and Object Classification, at a large-scale on ORBIT Testbed and a\nportable setup respectively, along with analyzing the benefits from the\nperspective of a telecom operator. Under specific experimental conditions,\nOTA-FL achieves equivalent model performance, supplemented with 43 times\nimprovement in spectrum utilization and 7 times improvement in energy\nefficiency over classical FL when considering 5 nodes.",
      "generated_abstract": "The integration of cognitive radio (CR) technology in wireless networks\nrequires the development of novel algorithms that support dynamic spectrum\nmanagement (DSM) and provide adaptive resource allocation to maximize the\nperformance of both users and CRs. The traditional approach is to optimize\nspectrum sharing among users and CRs, which can be a complex process due to\nthe diversity of users and CRs, the interference between users and CRs, and the\ncomplex spectrum sharing rules. In this paper, we propose an optimized\nspectrum sharing algorithm that maximizes the expected utility of users and CRs\nwhile simultaneously minimizing the total cost of spectrum sharing. We\npropose a novel dynamic CR policy that integrates the user's utility and\nCRs' utility into the CR policy. We derive the optimal CR policy and its\ncorresponding optimal spectrum sharing policy, which is used to derive the\noptimal spectrum sharing policy. Finally, the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11464968152866242,
          "p": 0.225,
          "f": 0.15189872970499754
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.023622047244094488,
          "f": 0.01829267818132931
        },
        "rouge-l": {
          "r": 0.11464968152866242,
          "p": 0.225,
          "f": 0.15189872970499754
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.04924v2",
      "true_abstract": "The integration of artificial intelligence (AI) into the workplace is\nadvancing rapidly, necessitating robust metrics to evaluate its tangible impact\non the labour market. Existing measures of AI occupational exposure largely\nfocus on AI's theoretical potential to substitute or complement human labour on\nthe basis of technical feasibility, providing limited insight into actual\nadoption and offering inadequate guidance for policymakers. To address this\ngap, we introduce the AI Startup Exposure (AISE) index-a novel metric based on\noccupational descriptions from O*NET and AI applications developed by startups\nfunded by the Y Combinator accelerator. Our findings indicate that while\nhigh-skilled professions are theoretically highly exposed according to\nconventional metrics, they are heterogeneously targeted by startups. Roles\ninvolving routine organizational tasks-such as data analysis and office\nmanagement-display significant exposure, while occupations involving tasks that\nare less amenable to AI automation due to ethical or high-stakes, more than\nfeasibility, considerations -- such as judges or surgeons -- present lower AISE\nscores. By focusing on venture-backed AI applications, our approach offers a\nnuanced perspective on how AI is reshaping the labour market. It challenges the\nconventional assumption that high-skilled jobs uniformly face high AI risks,\nhighlighting instead the role of today's AI players' societal\ndesirability-driven and market-oriented choices as critical determinants of AI\nexposure. Contrary to fears of widespread job displacement, our findings\nsuggest that AI adoption will be gradual and shaped by social factors as much\nas by the technical feasibility of AI applications. This framework provides a\ndynamic, forward-looking tool for policymakers and stakeholders to monitor AI's\nevolving impact and navigate the changing labour landscape.",
      "generated_abstract": "This paper develops a novel approach to measure the influence of the media on\nthe political behavior of citizens. We use the method of regression discontinuity\ndesigned to identify the moment at which the media influence is maximum. The\nresults of the empirical analysis show that the media have a statistically\nsignificant effect on the political behavior of the population. However, this\neffect is significantly reduced when the media are viewed as a type of\ninformation channel that can be measured by a proxy, such as the level of\nmedia literacy or the quality of media content. Our findings suggest that the\nmedia have a significant impact on political behavior, but this impact is\nsignificantly reduced when we measure the effect of the media through the\nmeasures of the population.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14444444444444443,
          "p": 0.37142857142857144,
          "f": 0.2079999959680001
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.06542056074766354,
          "f": 0.03899721030035504
        },
        "rouge-l": {
          "r": 0.12777777777777777,
          "p": 0.32857142857142857,
          "f": 0.18399999596800007
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/PF/2503.08973v1",
      "true_abstract": "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets.",
      "generated_abstract": "In this paper, we introduce a novel model of data processing in the cloud\ninstead of the well-known single-node model. We use the framework of\ntopological data analysis (TDA) to describe the data flow and compute in the\ncloud. This allows us to view the cloud as a data processing network that\nincludes a network of data-processing nodes. The cloud's role is to provide\nresources and compute, such as memory, processing power, and communication\ncapabilities. We use the network model to describe the data flow and compute in\nthe cloud. We show that the data flow in the cloud is equivalent to the flow\nin a network of data-processing nodes. We then describe the data flow in the\ncloud using TDA and show that the data flow in the cloud can be described as a\ntopological data analysis. We show that the data flow in the cloud can be\ndescribed as a flow of data, called a flow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10218978102189781,
          "p": 0.23333333333333334,
          "f": 0.1421319754593008
        },
        "rouge-2": {
          "r": 0.0051813471502590676,
          "p": 0.009708737864077669,
          "f": 0.006756752219004143
        },
        "rouge-l": {
          "r": 0.10218978102189781,
          "p": 0.23333333333333334,
          "f": 0.1421319754593008
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.07905v1",
      "true_abstract": "We present the results of the first multi-event study of the normalized\nreconnection rate integrating events spanning the three primary regimes of\nreconnection observed by the Magnetospheric Multiscale (MMS) mission. We\nutilize a new method for determining the normalized reconnection rate with\nfewer sources of uncertainty by estimating the diffusion region aspect ratio\nwith magnetic field gradients, which are very well measured by MMS. After\ndemonstrating our technique is valid in the guide field and asymmetric regimes\nof reconnection, we investigate any relationships between the normalized rate\non guide field, upstream magnetic field variability, and magnetic field and\ndensity asymmetry. Our results suggest that under typical magnetospheric\nconditions, the normalized reconnection rate is constant, which may be\nsignificant in predicting the terrestrial effects of space weather by providing\ninsight into the efficiency of solar wind-magnetospheric coupling.",
      "generated_abstract": "The development of autonomous spacecraft to explore the solar system is\nrequired for long-term exploration and mission extension. To enable autonomous\nspacecraft, the Space Situational Awareness (SSA) system is required to\ndetect, track, and classify potential threats to spacecraft. In this paper, we\npropose a novel SSA system based on the deep learning (DL) approach, which\nintegrates the deep learning-based SSA module and a novel DL-based SSA\nclassification module. The DL-based SSA module leverages a large language\nmodel (LLM) to classify the SSA threats to spacecraft and generate SSA alerts.\nThe DL-based SSA classification module uses a convolutional neural network to\nclassify the SSA threats to spacecraft into various categories, such as\ncruise, launch, collision, debris, and anomaly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.16901408450704225,
          "f": 0.14723925888667258
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11956521739130435,
          "p": 0.15492957746478872,
          "f": 0.13496932023636588
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2410.13265v2",
      "true_abstract": "An automated market maker where the price can cross the zero bound into the\nnegative price domain with applications in electricity, energy, and derivatives\nmarkets is presented. A unique feature involves the ability to swap both\nnegatively and positively priced assets between one another, which unlike\ntraditional markets requires a numeraire in the form of a currency. Model\nextensions to skew and concentrate liquidity are shown. The liquidity\nfingerprint, payoff, and invariant are compared to the Black-Scholes covered\ncall and the Logarithmic Market Scoring Rule invariants.",
      "generated_abstract": "This paper studies the problem of portfolio selection for an agent with\nattention mechanism. The attention mechanism enables the agent to select the\nasset with the highest attention score from a pool of assets. In addition, we\nintroduce a novel attention mechanism that is applicable to both single-asset\nand portfolio selection problems. Our main contribution is to provide a\ntheoretical framework for the optimal portfolio selection problem under\nattention mechanism. In addition, we also consider the portfolio selection\nproblem under the attention mechanism. Our results show that the optimal\nportfolio depends on the attention score. We further demonstrate that the\nattention mechanism can be used to construct a portfolio which outperforms the\noptimal portfolio. In addition, we investigate the optimal portfolio for\nportfolio selection problem under the attention mechanism. Our results show that\nthe optimal portfolio depends on the attention score and the asset returns. We\nalso demonstrate that the attention mechanism can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.1935483870967742,
          "f": 0.18461537962603566
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.009615384615384616,
          "f": 0.010582005632543396
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.16129032258064516,
          "f": 0.1538461488568049
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2403.03862v1",
      "true_abstract": "In December 2023 the Florida State Seminoles became the first Power 5 school\nto have an undefeated season and miss selection for the College Football\nPlayoff. In order to assess this decision, we employed an Elo ratings model to\nrank the teams and found that the selection committee's decision was justified\nand that Florida State were not one of the four best teams in college football\nin that season (ranking only 11th!). We extended this analysis to all other\nyears of the CFP and found that the top four teams by Elo ratings differ\ngreatly from the four teams selected in almost every year of the CFP's\nexistence. Furthermore, we found that there have been more egregious\nnon-selections including when Alabama was ranked first by Elo ratings in 2022\nand were not selected. The analysis suggests that the current criteria are too\nsubjective and a ratings model should be implemented to provide transparency\nfor the sport, its teams, and its fans.",
      "generated_abstract": "In the context of social science, causal inference is the study of the\nexplanatory relationships between variables. This study aims to explore the\ninterplay between causal inference and data augmentation by analyzing the\ncombination of these two methods in the context of predictive modeling. This\nstudy examines the potential of data augmentation in improving the generalizability\nof predictive models. The results show that data augmentation can be a\npotential solution for improving the generalizability of predictive models. In\naddition, this study provides a framework for the development of future\nresearch in the field of data augmentation and causal inference.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13131313131313133,
          "p": 0.24528301886792453,
          "f": 0.17105262703687688
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.024390243902439025,
          "f": 0.017699110420550977
        },
        "rouge-l": {
          "r": 0.13131313131313133,
          "p": 0.24528301886792453,
          "f": 0.17105262703687688
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.07935v1",
      "true_abstract": "Unmanned aerial vehicles (UAVs) enhance coverage and provide flexible\ndeployment in 5G and next-generation wireless networks. The performance of such\nwireless networks can be improved by developing new navigation and wireless\nadaptation approaches in digital twins (DTs). However, challenges such as\ncomplex propagation conditions and hardware complexities in real-world\nscenarios introduce a realism gap with the DTs. Moreover, while using real-time\nfull-stack protocols in DTs enables subsequent deployment and testing in a\nreal-world environment, development in DTs requires high computational\ncomplexity and involves a long development time. In this paper, to accelerate\nthe development cycle, we develop a measurement-calibrated Matlab-based\nsimulation framework to replicate performance in a full-stack UAV wireless\nnetwork DT. In particular, we use the DT from the NSF AERPAW platform and\ncompare its reports with those generated by our developed simulation framework\nin wireless networks with similar settings. In both environments, we observe\ncomparable results in terms of RSRP measurement, hence motivating iterative use\nof the developed simulation environment with the DT.",
      "generated_abstract": "In this paper, we present a method for efficiently detecting and classifying\nfingerprints using a combination of convolutional neural networks (CNNs) and\na graph neural network (GNN). The method is based on the idea of using a\ngraph to represent the fingerprints and then classifying them using a GNN.\nThe graph is constructed from the fingerprints and the edges are drawn based\non the similarity between the fingerprints. The GNN is trained on the graph\nstructure to identify the fingerprints. We use two fingerprints to demonstrate\nthe proposed method. The first is a fingerprint of a human hand and the second\nis a fingerprint of a mouse. The experiments show that the proposed method\nrepresents the fingerprints well and can be used to classify them. We also\npresent some preliminary results on the detection of fingerprints.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1651376146788991,
          "p": 0.2608695652173913,
          "f": 0.20224718626372942
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.034482758620689655,
          "f": 0.029411759814014653
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.2608695652173913,
          "f": 0.20224718626372942
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/IT/2503.08632v1",
      "true_abstract": "This study investigates secret-key generation for device authentication using\nphysical identifiers, such as responses from physical unclonable functions\n(PUFs). The system includes two legitimate terminals (encoder and decoder) and\nan eavesdropper (Eve), each with access to different measurements of the\nidentifier. From the device identifier, the encoder generates a secret key,\nwhich is securely stored in a private database, along with helper data that is\nsaved in a public database accessible by the decoder for key reconstruction.\nEve, who also has access to the public database, may use both her own\nmeasurements and the helper data to attempt to estimate the secret key and\nidentifier. Our setup focuses on authentication scenarios where channel\nstatistics are uncertain, with the involved parties employing multiple antennas\nto enhance signal reception. Our contributions include deriving inner and outer\nbounds on the optimal trade-off among secret-key, storage, and privacy-leakage\nrates for general discrete sources, and showing that these bounds are tight for\nGaussian sources.",
      "generated_abstract": "In this paper, we propose a novel technique for real-time image generation\nbased on the combination of self-supervised learning (SSL) and conditional\ngenerative adversarial networks (C-GANs). The SSL method is used to obtain\nhigh-quality image representations, while the C-GAN is employed to generate\nimagery conditioned on a latent vector. We demonstrate that this approach\nproduces image sequences that are visually pleasing and realistic, even in\nthe presence of large outliers. To further improve the quality of the generated\nimages, we introduce a novel method for quantifying the difference between\ndifferent image representations. This approach enables us to select the most\nrelevant features for the generation process, thus ensuring that the\ngeneration process is only based on the most relevant features. We validate our\nmethod on two datasets: ImageNet and COCO-Stuff. The results show that our\nmethod significantly out",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1391304347826087,
          "p": 0.17391304347826086,
          "f": 0.15458936704240492
        },
        "rouge-2": {
          "r": 0.012903225806451613,
          "p": 0.015503875968992248,
          "f": 0.014084502084161633
        },
        "rouge-l": {
          "r": 0.12173913043478261,
          "p": 0.15217391304347827,
          "f": 0.13526569554482037
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.05360v1",
      "true_abstract": "Sandqvist's base-extension semantics (B-eS) for intuitionistic sentential\nlogic grounds meaning relative to bases (rather than, say, models), which are\narbitrary sets of permitted inferences over sentences. While his soundness\nproof is standard, his completeness proof, is quite unusual. It closely\nparallels a method introduced much earlier by Mints, who developed a\nresolution-based approach to intuitionistic logic using a systematic\ntranslation of formulas into sentential counterparts. In this short note, we\nhighlight the connection between these two approaches and show that the\nsoundness and completeness of B-eS follow directly from Mints' theorem. While\nthe result is modest, it reinforces the relevance of proof-search to\nproof-theoretic semantics and suggests that resolution methods have a deeper\nconceptual role in constructive reasoning than is often acknowledged.",
      "generated_abstract": "We give a new proof of the fact that the set of all positive integers is\nsubmodular. In the proof, we show that if an arbitrary subset $S$ of $X$ is\npartially satisfying a certain submodular function, then it is also satisfying a\nsubmodular function with a different partial satisfaction. We use this to show\nthat if $S$ is a submodular set, then $S$ is a submodular set with a\nsubmodular set. We also prove that if $S$ is a submodular set with a submodular\nset, then $S$ is a submodular set. We give a simple proof of our main result,\nwhich is a submodular function theorem for the set of all positive integers. We\nalso prove a submodular function theorem for a submodular set.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17525773195876287,
          "p": 0.3469387755102041,
          "f": 0.23287670786920628
        },
        "rouge-2": {
          "r": 0.01652892561983471,
          "p": 0.02564102564102564,
          "f": 0.020100497746017642
        },
        "rouge-l": {
          "r": 0.15463917525773196,
          "p": 0.30612244897959184,
          "f": 0.20547944759523373
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.05807v1",
      "true_abstract": "This paper introduces a novel multi-stage decision-making model that\nintegrates hypothesis testing and dynamic programming algorithms to address\ncomplex decision-making scenarios.Initially,we develop a sampling inspection\nscheme that controls for both Type I and Type II errors using a simple random\nsampling method without replacement,ensuring the randomness and\nrepresentativeness of the sample while minimizing selection bias.Through the\napplication of hypothesis testing theory,a hypothesis testing model concerning\nthe defect rate is established,and formulas for the approximate distribution of\nthe sample defect rate and the minimum sample size required under two different\nscenarios are derived. Subsequently,a multi-stage dynamic programming decision\nmodel is constructed.This involves defining the state transition functions and\nstage-specific objective functions,followed by obtaining six optimal decision\nstrategies under various conditions through backward recursion.The results\ndemonstrate the model's potent capability for multi-stage decision-making and\nits high interpretability,offering significant advantages in practical\napplications.",
      "generated_abstract": "The rapid advancement of deep learning (DL) and high-performance computing (HPC)\nenable the development of large language models (LLMs) capable of producing\nhigh-quality text, enabling the development of text-based robotic systems.\nHowever, these systems face several challenges, including the inability to\nobtain high-quality text for some tasks, the lack of text-based interaction\ncapabilities, and the difficulty of understanding complex text. To address\nthese challenges, we propose a novel text-based robotic system, TextBot, that\nutilizes a text-based interface and a hierarchical neural network for text\ninteraction. TextBot is capable of generating high-quality text in various\nformats, including text-based image captions, text-based audio commentary, and\ntext-based video summaries. Additionally, TextBot enables the system to\nunderstand complex text,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13861386138613863,
          "p": 0.18666666666666668,
          "f": 0.159090904200026
        },
        "rouge-2": {
          "r": 0.014705882352941176,
          "p": 0.018518518518518517,
          "f": 0.016393437688794817
        },
        "rouge-l": {
          "r": 0.12871287128712872,
          "p": 0.17333333333333334,
          "f": 0.14772726783638962
        }
      }
    },
    {
      "paper_id": "eess.SP.math/FA/2503.10274v1",
      "true_abstract": "This paper devotes to combine the chirp basis function transformation and\nsymplectic coordinates transformation to yield a novel Wigner distribution (WD)\nassociated with the linear canonical transform (LCT), named as the symplectic\nWD in the LCT domain (SWDL). It incorporates the merits of the symplectic WD\n(SWD) and the WD in the LCT domain (WDL), achieving stronger capability in the\nlinear frequency-modulated (LFM) signal frequency rate feature extraction while\nmaintaining the same level of computational complexity. Some essential\nproperties of the SWDL are derived, including marginal distributions, energy\nconservations, unique reconstruction, Moyal formula, complex conjugate\nsymmetry, time reversal symmetry, scaling property, time translation property,\nfrequency modulation property, and time translation and frequency modulation\nproperty. Heisenberg's uncertainty principles of the SWDL are formulated,\ngiving rise to three kinds of lower bounds attainable respectively by Gaussian\nenveloped complex exponential signal, Gaussian signal and Gaussian enveloped\nchirp signal. The optimal symplectic matrices corresponding to the highest\ntime-frequency resolution are generated by solving the lower bound optimization\n(minimization) problem. The time-frequency resolution of the SWDL is compared\nwith those of the SWD and WDL to demonstrate its superiority in LFM signals\ntime-frequency energy concentration. A synthesis example is also carried out to\nverify the feasibility and reliability of the theoretical analysis.",
      "generated_abstract": "We investigate the performance of a wireless multiple-input multiple-output\n(MIMO) system with multiple transmit antennas (ATs) and a single receive\nantenna (Rx). Specifically, we consider a system with $K$ users, $M$ ATs, and a\nsingle Rx. The system is subject to the constraint that there exists a\npermutation of the $K$ users, such that the Rx can simultaneously receive all\nthe $K$ users. We derive the achievable rate region for the system with\nmultiple transmit antennas and a single receive antenna, where the\ncorresponding Rx can simultaneously receive all the $K$ users. We characterize\nthe achievable rate region for the system with multiple transmit antennas and\na single receive antenna, where the corresponding Rx can simultaneously\nreceive only a subset of the $K$ users. Our results demonstrate that the\nsystem with multiple transmit antennas",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08396946564885496,
          "p": 0.19298245614035087,
          "f": 0.11702127237041665
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.024390243902439025,
          "f": 0.014925368887281229
        },
        "rouge-l": {
          "r": 0.061068702290076333,
          "p": 0.14035087719298245,
          "f": 0.08510637875339541
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.09860v1",
      "true_abstract": "Molecular discovery has brought great benefits to the chemical industry.\nVarious molecule design techniques are developed to identify molecules with\ndesirable properties. Traditional optimization methods, such as genetic\nalgorithms, continue to achieve state-of-the-art results across multiple\nmolecular design benchmarks. However, these techniques rely solely on random\nwalk exploration, which hinders both the quality of the final solution and the\nconvergence speed. To address this limitation, we propose a novel approach\ncalled Gradient Genetic Algorithm (Gradient GA), which incorporates gradient\ninformation from the objective function into genetic algorithms. Instead of\nrandom exploration, each proposed sample iteratively progresses toward an\noptimal solution by following the gradient direction. We achieve this by\ndesigning a differentiable objective function parameterized by a neural network\nand utilizing the Discrete Langevin Proposal to enable gradient guidance in\ndiscrete molecular spaces. Experimental results demonstrate that our method\nsignificantly improves both convergence speed and solution quality,\noutperforming cutting-edge techniques. For example, it achieves up to a 25%\nimprovement in the top-10 score over the vanilla genetic algorithm. The code is\npublicly available at https://github.com/debadyuti23/GradientGA.",
      "generated_abstract": "The application of artificial intelligence (AI) to biological systems has\npromised significant advances in scientific discovery. However, this\ndiscovery has been hindered by the lack of a rigorous framework for\nsystematic AI research. In this paper, we propose a framework for systematic\nAI research in biology, emphasizing the need to incorporate rigorous methodology\nand ethical considerations. We outline the key principles of systematic AI\nresearch and highlight the importance of evidence-based decision-making in\nAI development. We also provide a roadmap for advancing AI research in biology\nby identifying key challenges, opportunities, and best practices. This\nframework will help to ensure that AI research is conducted ethically,\ntransparently, and efficiently, enhancing the potential of AI to benefit\nbiological research and discovery.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12686567164179105,
          "p": 0.22972972972972974,
          "f": 0.16346153387758888
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.027522935779816515,
          "f": 0.021276591002214223
        },
        "rouge-l": {
          "r": 0.12686567164179105,
          "p": 0.22972972972972974,
          "f": 0.16346153387758888
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2503.09615v1",
      "true_abstract": "We proceed to the canonical quantization of the complex scalar field without\nmaking use of its real and imaginary parts. Our motivation is to formally\nconnect, as tightly as possible, the quantum-field notions of particle and\nantiparticle - most prominently represented, formally, by the creation and\nannihilation operators - to the initial classical field theory - whose main\nformal object is the field amplitude at a given spacetime point. Our point of\nview is that doing this via the use of the real and imaginary parts of the\nfield is not satisfying. The derivation demands to consider, just before\nquantization, the field and its complex conjugate as independent fields, which\nyields a system of two copies of independent complex scalar fields. One then\nproceeds to quantization with these two copies, which leads to the introduction\nof two families of creation and annihilation operators, corresponding to\nparticles on the one hand, and antiparticles on the other hand. One realizes\nthat having two such families is the only hope for being able to \"invert\" the\ndefinitions of the creation and annihilation in terms of the Fourier quantized\nfields, so as to obtain an expression of the direct-space fields in terms of\nthese creation and annihilation operators, because the real-field condition\nused in the case of a real scalar field does not hold for a complex scalar\nfield. This hope is then met by introducing the complex-conjugate constraint at\nthe quantum level, that is, that the second independent field copy is actually\nthe complex conjugate of the first. All standard results are then recovered in\na rigorous and purely deductive way. While we reckon our derivation exists in\nthe literature, we have not found it.",
      "generated_abstract": "The present study investigates the dynamics of the rotating, non-rotating,\nand rotating neutron stars in the context of the relativistic general relativity\nand the hydrodynamics. The equations of motion are derived and numerically\nsolved. The results show that the rotation of the neutron star has a significant\ninfluence on the evolution of the star. The rotating neutron stars have a\nmore stable gravitational wave emission. The rotating neutron stars are\ndifferent from the non-rotating ones in their structure, and the rotating\nneutron stars have a different structure in the core and crust. The rotating\nneutron stars have a different evolution path in the gravitational wave\nemission. The rotating neutron stars have a more stable gravitational wave\nemission.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0738255033557047,
          "p": 0.21153846153846154,
          "f": 0.10945273248285946
        },
        "rouge-2": {
          "r": 0.016194331983805668,
          "p": 0.04878048780487805,
          "f": 0.024316105680103315
        },
        "rouge-l": {
          "r": 0.0738255033557047,
          "p": 0.21153846153846154,
          "f": 0.10945273248285946
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20852v1",
      "true_abstract": "Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the\nchallenges such as long scan times and expensive equipment by enhancing image\nresolution from low-quality inputs acquired in shorter scan times in clinical\nsettings. However, current SR techniques still have problems such as limited\nability to capture both local and global static patterns effectively and\nefficiently. To address these limitations, we propose Delta-WKV, a novel MRI\nsuper-resolution model that combines Meta-in-Context Learning (MiCL) with the\nDelta rule to better recognize both local and global patterns in MRI images.\nThis approach allows Delta-WKV to adjust weights dynamically during inference,\nimproving pattern recognition with fewer parameters and less computational\neffort, without using state-space modeling. Additionally, inspired by\nReceptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional\nscanning mechanism with time-mixing and channel-mixing structures to capture\nlong-range dependencies while maintaining high-frequency details. Tests on the\nIXI and fastMRI datasets show that Delta-WKV outperforms existing methods,\nimproving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and\ninference times by over 15\\%. These results demonstrate its efficiency and\npotential for clinical use with large datasets and high-resolution imaging.",
      "generated_abstract": "This paper proposes a new approach to automatic segmentation of breast\nchiasm using a hybrid approach that combines multi-level feature fusion and\nmulti-scale context modeling. The proposed method leverages the advantages of\ncontrastive learning and self-supervised learning to enhance the robustness and\nreproducibility of the segmentation model, while ensuring that the model can\nhandle complex anatomical variations. To improve the generalization ability of\nthe model, a novel chiasm segmentation dataset containing diverse anatomical\nvariations is constructed. The dataset is used to evaluate the model's\nperformance in segmenting chiasm in various anatomical scenarios. The results\ndemonstrate that the proposed method outperforms the existing state-of-the-art\nmethods in terms of both precision and recall. The performance of the proposed\nmethod is further verified through ablation studies, which demonstrates that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14788732394366197,
          "p": 0.26582278481012656,
          "f": 0.19004524427509684
        },
        "rouge-2": {
          "r": 0.016853932584269662,
          "p": 0.025423728813559324,
          "f": 0.020270265475713332
        },
        "rouge-l": {
          "r": 0.14084507042253522,
          "p": 0.25316455696202533,
          "f": 0.18099547051944073
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2408.06683v1",
      "true_abstract": "The mechanical properties within living cells play a critical role in the\nadaptive regulation of their biological functions upon environmental and\ninternal stimuli. While these properties exhibit nonequilibrium dynamics due to\nthe thermal and nonthermal forces that universally coexist in\nactin-myosin-active proliferative cells, quantifying them within such complex\nsystems remains challenging. Here, we develop a nonequilibrium framework that\ncombines fluorescence correlation spectroscopy (FCS) measurements of\nintracellular diffusion with nonequilibrium theory to quantitatively analyze\ncell-specific nonthermal driving forces and cellular adaptability. Our results\nreveal that intracellular particle diffusion is influenced not only by common\nthermal forces but also by nonthermal forces generated by approximately 10-100\nmotor proteins. Furthermore, we derive a physical parameter that quantitatively\nassesses the sensitivity of intracellular particle responses to these\nnonthermal forces, showing that systems with more active diffusion exhibit\nhigher response sensitivity. Our work highlights the biological fluctuations\narising from multiple interacting elements, advancing the understanding of the\ncomplex mechanical properties within living cells.",
      "generated_abstract": "The development of new and improved diagnostic methods for neurological\ndiseases is an important area of research. These methods should be able to\ndetect subtle, but clinically important, changes in brain function, such as\ncerebral spasm, and should be able to identify the underlying cause of these\nchanges. We aim to develop a new method that will allow for the detection of\ncerebral spasm in a clinical setting. We have designed a novel imaging system\nthat is capable of detecting cerebral spasm in real time. This system consists\nof a light sheet microscope and a multiphoton laser scanning microscope. The\nlight sheet microscope is used to capture images of the brain, while the\nmultiphoton laser scanning microscope is used to stimulate the spasm. By\ncapturing and stimulating the spasm, the system",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12264150943396226,
          "p": 0.16883116883116883,
          "f": 0.1420764978578042
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.017094017094017096,
          "f": 0.014981268484620563
        },
        "rouge-l": {
          "r": 0.12264150943396226,
          "p": 0.16883116883116883,
          "f": 0.1420764978578042
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.03860v1",
      "true_abstract": "Covariate balancing is a popular technique for controlling confounding in\nobservational studies. It finds weights for the treatment group which are close\nto uniform, but make the group's covariate means (approximately) equal to those\nof the entire sample. A crucial question is: how approximate should the\nbalancing be, in order to minimize the error of the final estimate? Current\nguidance is derived from heuristic or asymptotic analyses, which are\nuninformative when the size of the sample is small compared to the number of\ncovariates. This paper presents the first rigorous, nonasymptotic analysis of\ncovariate balancing; specifically, we use PAC-Bayesian techniques to derive\nvalid, finite-sample confidence intervals for the treatment effect. More\ngenerally, we prove these guarantees for a flexible form of covariate balancing\nwhere the regularization parameters weighting the tradeoff between bias\n(imbalance) and variance (divergence from uniform) are optimized, not fixed.\nThis gives rise to a new balancing algorithm which empirically delivers\nsuperior adaptivity. Our overall contribution is to make covariate balancing a\nmore reliable method for causal inference.",
      "generated_abstract": "This paper presents a novel methodology for estimating the posterior\ndistribution of the population covariance matrix. Specifically, we propose a\nmethod for estimating the posterior distribution of the population covariance\nmatrix using the kernel density estimator and the kernel trick. This\nmethodology is particularly useful when the population covariance matrix is\nunknown. We demonstrate the methodology through Monte Carlo simulations and\npresent results from a real-world dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092436974789916,
          "p": 0.3170731707317073,
          "f": 0.16249999618828134
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.09433962264150944,
          "f": 0.04651162419210414
        },
        "rouge-l": {
          "r": 0.1092436974789916,
          "p": 0.3170731707317073,
          "f": 0.16249999618828134
        }
      }
    },
    {
      "paper_id": "cs.CY.q-fin/EC/2503.00632v1",
      "true_abstract": "Improving social welfare is a complex challenge requiring policymakers to\noptimize objectives across multiple time horizons. Evaluating the impact of\nsuch policies presents a fundamental challenge, as those that appear suboptimal\nin the short run may yield significant long-term benefits. We tackle this\nchallenge by analyzing the long-term dynamics of two prominent policy\nframeworks: Rawlsian policies, which prioritize those with the greatest need,\nand utilitarian policies, which maximize immediate welfare gains. Conventional\nwisdom suggests these policies are at odds, as Rawlsian policies are assumed to\ncome at the cost of reducing the average social welfare, which their\nutilitarian counterparts directly optimize. We challenge this assumption by\nanalyzing these policies in a sequential decision-making framework where\nindividuals' welfare levels stochastically decay over time, and policymakers\ncan intervene to prevent this decay. Under reasonable assumptions, we prove\nthat interventions following Rawlsian policies can outperform utilitarian\npolicies in the long run, even when the latter dominate in the short run. We\ncharacterize the exact conditions under which Rawlsian policies can outperform\nutilitarian policies. We further illustrate our theoretical findings using\nsimulations, which highlight the risks of evaluating policies based solely on\ntheir short-term effects. Our results underscore the necessity of considering\nlong-term horizons in designing and evaluating welfare policies; the true\nefficacy of even well-established policies may only emerge over time.",
      "generated_abstract": "The proliferation of personal finance apps has increased consumer\ntraffic, but their privacy risks remain unaddressed. We introduce a novel\napproach to assessing user privacy risk in financial apps by integrating\nexisting privacy-preserving techniques and applying them to a comprehensive\ndataset of 240 million user interactions. Our findings demonstrate that\nsensitive user data, such as financial transactions, is transmitted\nunencrypted in an unprotected manner. In particular, we uncover that\napplications transmit sensitive data such as account balances, card numbers, and\ntransaction details without the appropriate encryption. Our findings highlight the\nneed for stronger privacy protections to mitigate privacy risks associated\nwith financial apps. We also provide recommendations for improving privacy\nprotections in these apps.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.23529411764705882,
          "f": 0.1818181770764464
        },
        "rouge-2": {
          "r": 0.0049504950495049506,
          "p": 0.009259259259259259,
          "f": 0.00645160836295845
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.23529411764705882,
          "f": 0.1818181770764464
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04908v1",
      "true_abstract": "This paper presents a novel dissipativity-based distributed droop-free\ncontrol approach for voltage regulation and current sharing in DC microgrids\n(MGs) comprised of an interconnected set of distributed generators (DGs),\nloads, and power lines. First, we describe the closed-loop DC MG as a networked\nsystem where the DGs and lines (i.e., subsystems) are interconnected via a\nstatic interconnection matrix. This interconnection matrix demonstrates how the\ninputs, outputs, and disturbances of DGs and lines are connected in a DC MG.\nEach DG has a local controller and a distributed global controller. To design\nthe controller, we use the dissipativity properties of the subsystems and\nformulate a linear matrix inequality (LMI) problem. To support the feasibility\nof this problem, we identify a set of necessary local and global conditions\nthat we then enforce in a specifically developed LMI-based controller design\nprocess. In contrast to existing DC MG control solutions, our approach proposes\na unified framework for co-designing the distributed controller and\ncommunication topology. As the co-design process is LMI-based, it can be\nefficiently implemented and evaluated. The effectiveness of the proposed\nsolution can be verified by simulating an islanded DC MG in a MATLAB/Simulink\nenvironment under different scenarios, such as load changes and topological\nconstraint changes, and then comparing the performance with a recent droop\ncontrol algorithm.",
      "generated_abstract": "This paper investigates the optimal control of a class of fluid-structure\nsystems, which includes the full three-dimensional Navier-Stokes equations and\na nonlinear, time-dependent nonlinearity in the form of a nonlinear\ndissipative damping term. The model is derived in two steps. First, we\nconsider the full three-dimensional Navier-Stokes equations and derive the\ncontrol laws for the velocity, the pressure, and the nonlinear damping term.\nThen, we introduce a dissipative damping term, and derive the control laws\nfor the new system. The control laws are based on the Lyapunov function and\nthe Lyapunov-Krasovskii functional. The stability of the control laws is\nproved. A numerical example is presented to illustrate the effectiveness of\nthe proposed control laws.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.2857142857142857,
          "f": 0.18181817747933895
        },
        "rouge-2": {
          "r": 0.029556650246305417,
          "p": 0.06451612903225806,
          "f": 0.04054053623105415
        },
        "rouge-l": {
          "r": 0.1259259259259259,
          "p": 0.2698412698412698,
          "f": 0.17171716737832882
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.01080v1",
      "true_abstract": "We introduce a new dynamic factor correlation model with a novel\nvariation-free parametrization of factor loadings. The model is applicable to\nhigh dimensions and can accommodate time-varying correlations, heterogeneous\nheavy-tailed distributions, and dependent idiosyncratic shocks, such as those\nobserved in returns on stocks in the same subindustry. We apply the model to a\n\"small universe\" with 12 asset returns and to a \"large universe\" with 323 asset\nreturns. The former facilitates a comprehensive empirical analysis and\ncomparisons and the latter demonstrates the flexibility and scalability of the\nmodel.",
      "generated_abstract": "We consider the problem of estimating the mean of a conditional distribution,\nunder the assumption that the mean of the underlying distribution is known.\nIn this paper, we present a novel estimation method based on a two-step\nprocedure: First, we estimate the conditional distribution by means of a\nmaximum likelihood estimator (MLE), and then we estimate the mean by means of\na maximum likelihood estimator (MLE) of the conditional distribution.\nSpecifically, we first obtain a bound on the difference between the conditional\nand the unconditional means by using the maximum likelihood estimator for the\nconditional distribution. We then use this bound to obtain an estimator of the\nmean of the conditional distribution. Our method is based on a novel\ncombination of the maximum likelihood estimator for the conditional distribution\nand the maximum likelihood estimator for the unconditional mean. We derive a\nconsistent estimator for the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15254237288135594,
          "p": 0.16071428571428573,
          "f": 0.1565217341338376
        },
        "rouge-2": {
          "r": 0.03614457831325301,
          "p": 0.030612244897959183,
          "f": 0.03314916630505859
        },
        "rouge-l": {
          "r": 0.15254237288135594,
          "p": 0.16071428571428573,
          "f": 0.1565217341338376
        }
      }
    },
    {
      "paper_id": "hep-th.math/QA/2503.10469v1",
      "true_abstract": "We introduce a novel machine learning based framework for discovering\nintegrable models. Our approach first employs a synchronized ensemble of neural\nnetworks to find high-precision numerical solution to the Yang-Baxter equation\nwithin a specified class. Then, using an auxiliary system of algebraic\nequations, [Q_2, Q_3] = 0, and the numerical value of the Hamiltonian obtained\nvia deep learning as a seed, we reconstruct the entire Hamiltonian family,\nforming an algebraic variety. We illustrate our presentation with three- and\nfour-dimensional spin chains of difference form with local interactions.\nRemarkably, all discovered Hamiltonian families form rational varieties.",
      "generated_abstract": "We construct a new approach for the study of large $N$ conformal field\ntheory. The key feature of our construction is that it can be applied to\ninfinite families of models, allowing us to obtain the partition function of\nthe model with the highest number of conformal dimensions $D$, by solving the\n$D$-th order Bethe Ansatz equations for the lowest number of bosonic\nrepresentations, which are the lowest order in the $D$-th order series.\nSpecifically, we consider the $D$-th order Bethe Ansatz equations for the\npartition function of the $D$-th order $N$-species $q$-deformed CFT. The\nsolutions for the lowest $D$th order $N$-species $q$-deformed CFT are obtained\nby applying the $D$-th order Bethe Ansatz equations to the $D$-th order $N$-species\n$q$-deformed C",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.18461538461538463,
          "f": 0.17142856645408178
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.022727272727272728,
          "f": 0.02197801698345724
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.15384615384615385,
          "f": 0.14285713788265325
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/ao-ph/2503.04227v1",
      "true_abstract": "We present the first extensive analysis of K/Ka-band ranging post-fit\nresiduals of an official Level-2 product, characterised as Line-of-Sight\nGravity Differences (LGD), which exhibit and showcase interesting sub-monthly\ngeophysical signals. These residuals, provided by CSR, were derived from the\ndifference between spherical harmonic coefficient least-squares fits and\nreduced Level-1B range-rate observations. We classified the geophysical signals\ninto four distinct categories: oceanic, meteorological, hydrological, and solid\nEarth, focusing primarily on the first three categories in this study. In our\nexamination of oceanic processes, we identified notable mass anomalies in the\nArgentine basin, specifically within the Zapiola Rise, where persistent\nremnants of the rotating dipole-like modes are evident in the LGD post-fit\nresiduals. Our analysis extended to the Gulf of Carpentaria and Australia\nduring the 2013 Oswald cyclone, revealing significant LGD residual anomalies\nthat correlate with cyclone tracking and precipitation data. Additionally, we\ninvestigated the monsoon seasons in Bangladesh, particularly from June to\nSeptember 2007, where we observed peaks in sub-monthly variability. These\nfindings were further validated by demonstrating high spatial and temporal\ncorrelations between gridded LGD residuals and ITSG-Grace2018 daily solutions.\nGiven that these anomalies are associated with significant mass change\nphenomena, it is essential to integrate the post-fit residuals into a\nhigh-frequency mass change framework, with the purpose of providing enhanced\nspatial resolution compared to conventional Kalman-filtered methods.",
      "generated_abstract": "This study investigates the effects of surface deformation on the thermal\nenergy storage and release in permafrost soils, particularly in areas with\nhighly deformable soils. The thermal energy storage and release of permafrost\nsoils is influenced by both the physical properties of the soil and the\nsurface deformation. The soil thermal energy storage capacity of a permafrost\nsoil is mainly determined by its thermal diffusivity. A 2-D numerical\nsimulation was conducted to investigate the effects of surface deformation on\nthe thermal energy storage and release of permafrost soils. The results show\nthat the thermal energy storage capacity of permafrost soils is significantly\naffected by both the surface deformation and the thermal diffusivity. The\nsurface deformation of the soil increases the thermal energy storage capacity,\nwhich is mainly due to the increase of thermal diffusivity. The thermal\nenergy storage capacity of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09032258064516129,
          "p": 0.2545454545454545,
          "f": 0.1333333294671203
        },
        "rouge-2": {
          "r": 0.014218009478672985,
          "p": 0.03409090909090909,
          "f": 0.020066885478239354
        },
        "rouge-l": {
          "r": 0.07096774193548387,
          "p": 0.2,
          "f": 0.10476190089569175
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.10009v1",
      "true_abstract": "We introduce a double/debiased machine learning (DML) estimator for the\nimpulse response function (IRF) in settings where a time series of interest is\nsubjected to multiple discrete treatments, assigned over time, which can have a\ncausal effect on future outcomes. The proposed estimator can rely on fully\nnonparametric relations between treatment and outcome variables, opening up the\npossibility to use flexible machine learning approaches to estimate IRFs. To\nthis end, we extend the theory of DML from an i.i.d. to a time series setting\nand show that the proposed DML estimator for the IRF is consistent and\nasymptotically normally distributed at the parametric rate, allowing for\nsemiparametric inference for dynamic effects in a time series setting. The\nproperties of the estimator are validated numerically in finite samples by\napplying it to learn the IRF in the presence of serial dependence in both the\nconfounder and observation innovation processes. We also illustrate the\nmethodology empirically by applying it to the estimation of the effects of\nmacroeconomic shocks.",
      "generated_abstract": "We propose a novel method for estimating a vector of stochastic\ntrajectories of an unknown state variable, which is often referred to as the\nvector autoregressive (VAR) process. The method is based on the observation\nthat a vector of stochastic trajectories can be viewed as a linear combination\nof a vector of deterministic trajectories. We then show that the VAR\nprocess can be expressed as a linear combination of deterministic trajectories,\nand we further show that the VAR process can be expressed as a linear\ncombination of deterministic trajectories and stochastic trajectories.\nTherefore, the VAR process can be viewed as a linear combination of deterministic\ntrajectories. We derive the asymptotic normality of the VAR process, and we\ndemonstrate that the asymptotic normality of the VAR process is equivalent to\nthe asymptotic normality of the deterministic trajectories.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1559633027522936,
          "p": 0.3333333333333333,
          "f": 0.21249999565703134
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.04938271604938271,
          "f": 0.034042548673970725
        },
        "rouge-l": {
          "r": 0.14678899082568808,
          "p": 0.3137254901960784,
          "f": 0.19999999565703136
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/MF/2411.12375v3",
      "true_abstract": "In this paper, we introduce a novel pricing model for Uniswap V3, built upon\nstochastic processes and the Martingale Stopping Theorem. This model\ninnovatively frames the valuation of positions within Uniswap V3. We further\nconduct a numerical analysis and examine the sensitivities through Greek risk\nmeasures to elucidate the model's implications. The results underscore the\nmodel's significant academic contribution and its practical applicability for\nUniswap liquidity providers, particularly in assessing risk exposure and\nguiding hedging strategies.",
      "generated_abstract": "This paper extends the concept of multi-agent reinforcement learning to\nmulti-period investment problems. We develop a model for investors with\ndiffering time horizons, focusing on the allocation of portfolios among them.\nWe also introduce a mechanism to allocate investments among the agents, with\nthe goal of maximizing the expected return. We show that this mechanism\nprovides a consistent, first-order optimality condition for the optimal\nportfolio allocation and the optimal allocation of the investments. We derive\nthe closed-form solutions for the optimal investment allocation and optimal\nportfolio allocation. We also discuss the effect of time horizon on the\nperformance of the optimal allocation and the optimal portfolio allocation. We\nalso analyze the convergence of the optimal allocation and the optimal portfolio\nallocation under different time horizons and show that they converge to the\noptimal allocation and the optimal portfolio allocation as the time horizon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1774193548387097,
          "p": 0.16923076923076924,
          "f": 0.17322834145948307
        },
        "rouge-2": {
          "r": 0.0410958904109589,
          "p": 0.02830188679245283,
          "f": 0.033519548242564916
        },
        "rouge-l": {
          "r": 0.1774193548387097,
          "p": 0.16923076923076924,
          "f": 0.17322834145948307
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10484v1",
      "true_abstract": "Existing quadrupedal locomotion learning paradigms usually rely on extensive\ndomain randomization to alleviate the sim2real gap and enhance robustness. It\ntrains policies with a wide range of environment parameters and sensor noises\nto perform reliably under uncertainty. However, since optimal performance under\nideal conditions often conflicts with the need to handle worst-case scenarios,\nthere is a trade-off between optimality and robustness. This trade-off forces\nthe learned policy to prioritize stability in diverse and challenging\nconditions over efficiency and accuracy in ideal ones, leading to overly\nconservative behaviors that sacrifice peak performance. In this paper, we\npropose a two-stage framework that mitigates this trade-off by integrating\npolicy learning with imagined transitions. This framework enhances the\nconventional reinforcement learning (RL) approach by incorporating imagined\ntransitions as demonstrative inputs. These imagined transitions are derived\nfrom an optimal policy and a dynamics model operating within an idealized\nsetting. Our findings indicate that this approach significantly mitigates the\ndomain randomization-induced negative impact of existing RL algorithms. It\nleads to accelerated training, reduced tracking errors within the distribution,\nand enhanced robustness outside the distribution.",
      "generated_abstract": "This paper proposes a novel method for the generation of robotic\narm trajectories for the safe manipulation of two objects in a two-arm robotic\ngripper, with a particular focus on multi-object manipulation. The method\nemploys a neural network to generate trajectories for the robotic arm, which\nis used for object manipulation. The neural network is trained using data\ncollected using the Simulator of the Universe (SoU), and tested on real-world\ndata collected from the real robotic arm. The proposed method is compared to\nstate-of-the-art approaches for multi-object manipulation, and it is found to\nbe effective in achieving the same task, with the proposed method achieving\ncomparable performance to the state-of-the-art methods in terms of object\npositioning accuracy. The method is also evaluated on a robotic manipulator\nthat is equipped with a single",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11290322580645161,
          "p": 0.19444444444444445,
          "f": 0.1428571382090797
        },
        "rouge-2": {
          "r": 0.011428571428571429,
          "p": 0.016666666666666666,
          "f": 0.013559317207700653
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.16666666666666666,
          "f": 0.1224489749437736
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2411.07986v2",
      "true_abstract": "A fundamental question in the field of molecular computation is what\ncomputational tasks a biochemical system can carry out. In this work, we focus\non the problem of finding the maximum likelihood estimate (MLE) for log-affine\nmodels. We revisit a construction due to Gopalkrishnan of a mass-action system\nwith the MLE as its unique positive steady state, which is based on choosing a\nbasis for the kernel of the design matrix of the model. We extend this\nconstruction to allow for any finite spanning set of the kernel, and explore\nhow the choice of spanning set influences the dynamics of the resulting\nnetwork, including the existence of boundary steady states, the deficiency of\nthe network, and the rate of convergence. In particular, we prove that using a\nMarkov basis as the spanning set guarantees global stability of the MLE steady\nstate.",
      "generated_abstract": "The study of the evolution of the genome is one of the most challenging\nproblems in theoretical biology, due to the complexity of the physical laws\nthat govern biological systems. One of the main challenges in the study of the\nevolution of the genome is that the information in the genome is encoded in the\nsequence of nucleotides, while the evolution of the genome is affected by\nchanges in the sequence. The emergence of the genome is a complex process,\ncharacterized by the presence of various evolutionary mechanisms, including\nmutation, selection, and recombination, which influence the evolution of the\ngenome. This paper explores the evolution of the genome using the framework of\nthe coalescent theory. The main idea behind this framework is that the genome\nis a continuous object, and mutations occur at random locations within the\ngenome. The coalescent theory is a statistical model that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1590909090909091,
          "p": 0.2,
          "f": 0.17721518493831132
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.02912621359223301,
          "f": 0.025531909969761023
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.15714285714285714,
          "f": 0.13924050139400754
        }
      }
    },
    {
      "paper_id": "physics.app-ph.physics/app-ph/2503.10139v1",
      "true_abstract": "We present an optomechanical device platform for characterization of optical,\nthermal, and rheological properties of fluids on the micron scale. A suspended\nsilicon microdisk resonator with a vibrating mass of 100 fg and an effective\nmeasurement volume of less than a pL is used to monitor properties of different\nfluids at rest. By employing analytical models for thermo-optical effects,\nthermal diffusion and fluid-structure interactions, our platform determines the\nrefractive index, thermal conductivity, viscosity, density and compressibility\nof the fluid, in a compact measurement setup. A single measurement takes as\nshort as 70 microseconds, and the employed power can be less than 100\nmicrowatts, guaranteeing measurement at rest and in thermal equilibrium.",
      "generated_abstract": "The theory of the quantum Hall effect is based on the quantization of the\nvolume\n  current density. Here we propose a different approach, based on the\nquantization of the Hall conductivity, which leads to the same result. We show\nthat the quantum Hall effect can be explained using a single parameter, the\nratio of the Hall conductivity to the electrical conductivity. We further\nderive the relation between the ratio of the Hall conductivity to the electrical\nconductivity and the ratio of the Hall mobility to the electrical mobility,\nwhich is not directly related to the quantum Hall effect. We show that the\nratio of the Hall conductivity to the electrical conductivity is\nnon-universal, which may explain the non-universal Hall effect.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.28846153846153844,
          "f": 0.23076922596923088
        },
        "rouge-2": {
          "r": 0.037383177570093455,
          "p": 0.05333333333333334,
          "f": 0.04395603911061519
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.19230769230769232,
          "f": 0.15384614904615398
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.04265v1",
      "true_abstract": "Regression discontinuity (RD) designs typically identify the treatment effect\nat a single cutoff point. But when and how can we learn about treatment effects\naway from the cutoff? This paper addresses this question within a\nmultiple-cutoff RD framework. We begin by examining the plausibility of the\nconstant bias assumption proposed by Cattaneo, Keele, Titiunik, and\nVazquez-Bare (2021) through the lens of rational decision-making behavior,\nwhich suggests that a kind of similarity between groups and whether individuals\ncan influence the running variable are important factors. We then introduce an\nalternative set of assumptions and propose a broadly applicable partial\nidentification strategy. The potential applicability and usefulness of the\nproposed bounds are illustrated through two empirical examples.",
      "generated_abstract": "This paper investigates the role of prior knowledge in the identification of\ncause-effect relationships using a simple randomised controlled trial (RCT)\ndesign. We consider a scenario where a treatment is associated with a\npotentially harmful outcome, which may be prevented through a counterfactual\ntreatment. In this scenario, we first examine the identification of the\ncausal effect of the treatment in the presence of a fixed effect. Then, we\npropose a methodology to identify the causal effect of the treatment in the\npresence of a random effect. We demonstrate that the identified causal effect\nis always zero in this scenario, and that a fixed effect is always preferred\nover a random effect. We also show that the identified causal effect can be\nidentified in the presence of an intervention that prevents the outcome from\noccurring. Finally, we propose a methodology to identify the causal effect of\nthe treatment in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21978021978021978,
          "p": 0.28169014084507044,
          "f": 0.24691357532312158
        },
        "rouge-2": {
          "r": 0.05309734513274336,
          "p": 0.05714285714285714,
          "f": 0.055045866566366924
        },
        "rouge-l": {
          "r": 0.1978021978021978,
          "p": 0.2535211267605634,
          "f": 0.22222221729843022
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.03114v1",
      "true_abstract": "This study constructs a novel analytical general equilibrium model to compare\nenvironmental policies in a setting where oligopolistic energy firms engage in\nthird-degree price discrimination across residential consumers and industrial\nfirms. Closed-form solutions demonstrate the impact on prices and quantities.\nThe resulting welfare change is decomposed across three distortions: output,\nprice discrimination, and externality. This study finds that the output\ndistortion and price discrimination welfare effects generally move in opposite\ndirections under policies such as an emission tax or a two-part instrument.\nNumerical analysis compares policies and finds scenarios where the output\ndistortion and price discrimination welfare changes fully offset and thus\nleaves the net welfare gain of the externality correction. In this way,\nenvironmental policy can be designed to mitigate output distortion welfare\nconcerns when firms have market power.",
      "generated_abstract": "This paper investigates the impact of the COVID-19 pandemic on the\nconcentration of the U.S. economy. I use a time-varying elasticity of\nsubstitution model to estimate the impact of the pandemic on the growth of\nindustries and employment. I find that the pandemic reduced employment growth\nand increased concentration of employment in the U.S. economy. The reduction in\nemployment growth and the increase in concentration of employment are\nlargest for manufacturing and information industries, and are less pronounced\nfor services and construction industries. The findings suggest that the\npandemic reduced the level of labor market volatility and increased the\nconcentration of employment in the U.S. economy. This finding suggests that\nthe pandemic reduced the volatility of the labor market and increased the\nconcentration of employment in the U.S.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.25,
          "f": 0.18181817719008275
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.0449438202247191,
          "f": 0.03827750707172518
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.25,
          "f": 0.18181817719008275
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.02642v1",
      "true_abstract": "In both machine learning and in computational neuroscience, plasticity in\nfunctional neural networks is frequently expressed as gradient descent on a\ncost. Often, this imposes symmetry constraints that are difficult to reconcile\nwith local computation, as is required for biological networks or neuromorphic\nhardware. For example, wake-sleep learning in networks characterized by\nBoltzmann distributions builds on the assumption of symmetric connectivity.\nSimilarly, the error backpropagation algorithm is notoriously plagued by the\nweight transport problem between the representation and the error stream.\nExisting solutions such as feedback alignment tend to circumvent the problem by\ndeferring to the robustness of these algorithms to weight asymmetry. However,\nthey are known to scale poorly with network size and depth. We introduce\nspike-based alignment learning (SAL), a complementary learning rule for spiking\nneural networks, which uses spike timing statistics to extract and correct the\nasymmetry between effective reciprocal connections. Apart from being\nspike-based and fully local, our proposed mechanism takes advantage of noise.\nBased on an interplay between Hebbian and anti-Hebbian plasticity, synapses can\nthereby recover the true local gradient. This also alleviates discrepancies\nthat arise from neuron and synapse variability -- an omnipresent property of\nphysical neuronal networks. We demonstrate the efficacy of our mechanism using\ndifferent spiking network models. First, we show how SAL can significantly\nimprove convergence to the target distribution in probabilistic spiking\nnetworks as compared to Hebbian plasticity alone. Second, in neuronal\nhierarchies based on cortical microcircuits, we show how our proposed mechanism\neffectively enables the alignment of feedback weights to the forward pathway,\nthus allowing the backpropagation of correct feedback errors.",
      "generated_abstract": "The complex and multifaceted nature of biological systems is reflected in\ntheir computational representations. In this paper, we review the current\nstate-of-the-art in computational approaches to biological systems, focusing\non their computational representation and their potential for biological\napplication. We discuss the advantages and limitations of these approaches,\nincluding the potential of emerging technologies, such as artificial intelligence\nand quantum computing, to advance our understanding of biological systems. We\nalso discuss the challenges and opportunities for future research, including\nthe need for more diverse and equitable approaches to computational\nrepresentation, the need for better integration of computational and experimental\ndata, and the need for more comprehensive models of biological systems.\nAdditionally, we highlight the need for more effective communication and\ninterpretation of computational results, as well as the need for improved\ndiversity and equity in the biological research community. This review provides",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13609467455621302,
          "p": 0.2911392405063291,
          "f": 0.18548386662623578
        },
        "rouge-2": {
          "r": 0.023622047244094488,
          "p": 0.04918032786885246,
          "f": 0.03191488923325094
        },
        "rouge-l": {
          "r": 0.11834319526627218,
          "p": 0.25316455696202533,
          "f": 0.16129031823913903
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.physics/soc-ph/2503.08418v1",
      "true_abstract": "Human mobility, a pivotal aspect of urban dynamics, displays a profound and\nmultifaceted relationship with urban sustainability. Despite considerable\nefforts analyzing mobility patterns over decades, the ranking dynamics of urban\nmobility has received limited attention. This study aims to contribute to the\nfield by investigating changes in rank and size of hourly inflows to various\nlocations across 60 Chinese cities throughout the day. We find that the\nrank-size distribution of hourly inflows over the course of the day is stable\nacross cities. To uncover the microdynamics beneath the stable aggregate\ndistribution amidst shifting location inflows, we analyzed consecutive-hour\ninflow size and ranking variations. Our findings reveal a dichotomy: locations\nwith higher daily average inflow display a clear monotonic trend, with more\npronounced increases or decreases in consecutive-hour inflow. In contrast,\nranking variations exhibit a non-monotonic pattern, distinguished by the\nstability of not only the top and bottom rankings but also those in\nmoderately-inflowed locations. Finally, we compare ranking dynamics across\ncities using a ranking metric, the rank turnover. The results advance our\nunderstanding of urban mobility dynamics, providing a basis for applications in\nurban planning and traffic engineering.",
      "generated_abstract": "The spatial and temporal dynamics of food insecurity are complex,\nspanning multiple scales and influencing diverse human behaviors and\nsocietal responses. This study investigates the emergence of spatially\nexplicit, temporal patterns of food insecurity across three regions in\nNorthern India using a novel Bayesian data-driven approach. We analyze the\nspatial dynamics of the food insecurity index (FII) and its temporal\nvariation across three districts in Haryana, Uttar Pradesh, and Punjab. Our\nfindings reveal a complex spatial structure of the food insecurity index\n(FII) with significant heterogeneity across districts, particularly in Haryana\nand Uttar Pradesh. The spatial dynamics of FII are also characterized by high\nvolatility, with significant differences in their temporal evolution across\ndistricts. Our analysis highlights the need for a nuanced understanding of\ntempor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17073170731707318,
          "p": 0.2692307692307692,
          "f": 0.20895521913120974
        },
        "rouge-2": {
          "r": 0.04519774011299435,
          "p": 0.07272727272727272,
          "f": 0.05574912419235432
        },
        "rouge-l": {
          "r": 0.15447154471544716,
          "p": 0.24358974358974358,
          "f": 0.18905472161877193
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.09924v1",
      "true_abstract": "This paper discusses the possibility of applying the velocity averaging\ntheorems in [F. Golse, P.-L. Lions, B. Perthame, R. Sentis: J. Funct. Anal.\n76(1):110--125, 1988] to the Wigner equation governing the quantum evolution of\nthe Wigner transform of quantum density operators. Our first main results\naddress the case of the Wigner function of a special class of density operators\nassociated to mixed states, whose Hilbert-Schmidt norm is of order\n$\\hbar^{d/2}$, where $d$ is the space dimension and $\\hbar$ the reduced Planck\nconstant. In space dimension $d=1$, we prove that the density function belongs\nto the Sobolev space $H^s(\\mathbb R)$ for some $s>0$. In the case of pure\nstates, we first obtain a characterization of the Wigner transform of rank-one\nquantum density operators, and apply this characterization (1) to analyze a\nrather general setting in which velocity averaging cannot apply to the Wigner\nfunctions of a family of rank-one density operators whose evolution is governed\nby the von Neumann equation, and (2) to obtain a quick derivation of Madelung's\nsystem of quantum hydrodynamic equations. This derivation provides a physical\nexplanation of one key assumption used in the proof of the negative result (1)\ndescribed above.",
      "generated_abstract": "We investigate the $q$-expansion of the product of two $q$-Pochhammer symbols\nand of a $q$-hypergeometric series. We show that the $q$-expansion of the\nproduct of two $q$-Pochhammer symbols and of a $q$-hypergeometric series is a\n$q$-hypergeometric series.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05263157894736842,
          "p": 0.375,
          "f": 0.09230769014911247
        },
        "rouge-2": {
          "r": 0.01744186046511628,
          "p": 0.14285714285714285,
          "f": 0.031088080962173602
        },
        "rouge-l": {
          "r": 0.05263157894736842,
          "p": 0.375,
          "f": 0.09230769014911247
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10194v1",
      "true_abstract": "This paper describes novel algorithms for the identification of\n(almost-)resonant behavior in scattering problems. Our methods, relying on\nrational approximation, aim at building surrogate models of what we call \"field\namplification\", defined as the norm of the solution operator of the scattering\nproblem, which we express through boundary-integral equations. To provide our\ntechniques with theoretical foundations, we first derive results linking the\nfield amplification to the spectral properties of the operator that defines the\nscattering problem. Such results are then used to justify the use of rational\napproximation in the surrogate-modeling task. Some of our proposed methods\napply rational approximation in a \"standard\" way, building a rational\napproximant for either the solution operator directly or, in the interest of\ncomputational efficiency, for a randomly \"sketched\" version of it. Our other\n\"hybrid\" approaches are more innovative, combining\nrational-approximation-assisted root-finding with approximation using radial\nbasis functions. Three key features of our methods are that (i) they are\nagnostic of the strategy used to discretize the scattering problem, (ii) they\ndo not require any computations involving non-real wavenumbers, and (iii) they\ncan adjust to different settings through the use of adaptive sampling\nstrategies. We carry out some numerical experiments involving 2D scatterers to\ncompare our approaches. In our tests, two of our approaches (one standard, one\nhybrid) emerge as the best performers, with one or the other being preferable,\ndepending on whether emphasis is placed on accuracy or efficiency.",
      "generated_abstract": "We investigate the problem of finding the optimal initial condition for a\nnonlinear fluid-structure interaction system. We formulate the problem as a\nconstrained minimization problem and propose a numerical algorithm to find the\nminimum. We analyze the algorithm and derive the convergence rate of the\niterative procedure. We compare the convergence rate of the proposed algorithm\nwith the standard Newton method and the iterative quasi-Newton method, and\ndemonstrate that the proposed algorithm converges at a faster rate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1038961038961039,
          "p": 0.35555555555555557,
          "f": 0.16080401660059102
        },
        "rouge-2": {
          "r": 0.009216589861751152,
          "p": 0.029411764705882353,
          "f": 0.014035084085935071
        },
        "rouge-l": {
          "r": 0.07792207792207792,
          "p": 0.26666666666666666,
          "f": 0.12060301157546538
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.08026v1",
      "true_abstract": "A principal uses payments conditioned on stochastic outcomes of a team\nproject to elicit costly effort from the team members. We develop a multi-agent\ngeneralization of a classic first-order approach to contract optimization by\nleveraging methods from network games. The main results characterize the\noptimal allocation of incentive pay across agents and outcomes. Incentive\noptimality requires equalizing, across agents, a product of (i) individual\nproductivity (ii) organizational centrality and (iii) responsiveness to\nmonetary incentives.",
      "generated_abstract": "We study the design of optimal public policies to allocate resources between\ndifferent goods. The goods are grouped into two classes, with different\npreferences among them. The first class includes a scarce good, while the\nsecond includes a non-scarce good. The preferences of the first class are\ncontinuous and non-negative, while the preferences of the second class are\ndiscrete and non-negative. We propose a novel policy design procedure that\nguarantees that the sum of the preferences of the first class is at least as\nlarge as the sum of the preferences of the second class. This policy design\nprocedure is based on a dynamic programming approach, which is motivated by\nthe theory of optimal transport. We provide a complete analysis of the\npolicies that the procedure generates. We also derive the conditions under which\nthe procedure generates policies that are optimal with respect to their\nutility. We further provide an explicit formulation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.12987012987012986,
          "f": 0.1459853965368428
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.12987012987012986,
          "f": 0.1459853965368428
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.08288v1",
      "true_abstract": "We study numerical regularities for complexes over noncommutative noetherian\nlocally finite $\\mathbb{N}$-graded algebras $A$ such as CM (cm)-regularity, Tor\n(tor)-regularity (Ext (ext)-regularity) and Ex (ex)-regularity, which are the\nsupremum or infimum degrees of some associated canonical complexes. We show\nthat for any right bounded complex $X$ with finitely generated cohomologies,\nthe supremum degree of $R\\underline{\\text{Hom}}_A(X, A_0)$ coincides with the\nopposite of the infimum degree of $X$ if $A_0$ is semisimple. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the\nCM-regularity of $X$ coincides with the supremum degree of\n$R\\underline{\\text{Hom}}_A(A_0,X)$ for any left bounded complex $X$ with\nfinitely generated cohomologies.\n  Several inequalities concerning the numerical regularities and the supremum\nor infimum degree of derived Hom or derived tensor complexes are given for\nnoncommutative noetherian locally finite $\\mathbb{N}$-graded algebras. Some of\nthese are generalizations of J\\o rgensen's results on the inequalities between\nthe CM-regularity and Tor-regularity, some are new even in the connected graded\ncase. Conditions are given under which the inequalities become equalities by\nestablishing two technical lemmas.\n  Following Kirkman, Won and Zhang, we also use the numerical AS-regularity\n(resp. little AS-regularity) to study Artin-Schelter regular property\n(finite-dimensional property) for noetherian $\\mathbb{N}$-graded algebras. We\nprove that the numerical AS-regularity of $A$ is zero if and only if that $A$\nis an $\\mathbb{N}$-graded AS-regular algebra under some mild conditions, which\ngeneralizes a result of Dong-Wu and a result of Kirkman-Won-Zhang. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the little\nAS-regularity of $A$ is zero if and only if $A$ is finite-dimensional.",
      "generated_abstract": "In this paper, we consider the problem of estimating the mean of a random\nvector by solving a system of linear equations with constraints. We derive a\nclosed-form solution for the problem under the assumption that the constraint\nmatrix is symmetric positive definite. We prove that the solution has a\nlogarithmic growth in the number of variables, and we provide an efficient\nnumerical method to compute it. Furthermore, we show that the solution is\napproximately optimal in terms of the least squares error. We apply the\nmethod to solve the problem arising in the estimation of the mean of the\nlogarithm of a random variable. The application to the problem arising in the\nestimation of the mean of the logarithm of a random variable is discussed.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14960629921259844,
          "p": 0.2835820895522388,
          "f": 0.1958762841380594
        },
        "rouge-2": {
          "r": 0.035,
          "p": 0.07216494845360824,
          "f": 0.04713804273940342
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.2537313432835821,
          "f": 0.17525772743702847
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.09647v1",
      "true_abstract": "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
      "generated_abstract": "The rapid advancements in deep learning and the development of large language\nmodel (LLM) have significantly transformed various fields, including natural\nlanguage processing (NLP) and robotics. While recent research has shown that\nLLMs can be employed to enhance the efficiency of robotic systems, the\ndifficulty in interpreting their outputs has not been fully addressed. In this\npaper, we propose a novel framework, named GNN-LLM, to facilitate the\nunderstanding of LLM outputs. Specifically, we integrate Graph Neural Networks\n(GNNs) with Large Language Models (LLMs) to capture semantic and structure\ninformation of the input data. By integrating the two models, GNN-LLM effectively\ncaptures the relationship between data and LLM outputs, facilitating the\nunderstanding of LLM outputs. Additionally, we introduce a GNN-LLM\nframework-based reasoning module to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1625,
          "p": 0.13978494623655913,
          "f": 0.1502890123692741
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.025423728813559324,
          "f": 0.028169009142807632
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.12903225806451613,
          "f": 0.1387283187276556
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10447v1",
      "true_abstract": "Audio-visual speech recognition (AVSR) has become critical for enhancing\nspeech recognition in noisy environments by integrating both auditory and\nvisual modalities. However, existing AVSR systems struggle to scale up without\ncompromising computational efficiency. In this study, we introduce MoHAVE\n(Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework\ndesigned to address these scalability constraints. By leveraging a\nMixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific\nexpert groups, ensuring dynamic adaptation to various audio-visual inputs with\nminimal computational overhead. Key contributions of MoHAVE include: (1) a\nsparse MoE framework that efficiently scales AVSR model capacity, (2) a\nhierarchical gating mechanism that dynamically utilizes the expert groups based\non input context, enhancing adaptability and robustness, and (3) remarkable\nperformance across robust AVSR benchmarks, including LRS3 and MuAViC\ntranscription and translation tasks, setting a new standard for scalable speech\nrecognition systems.",
      "generated_abstract": "We consider the problem of training a deep neural network (DNN) for speech\nrecognition in noisy environments with a few samples of audio. We propose a\nnovel training procedure that trains a DNN to generate samples from a\nspecified speech distribution, using only a few samples of audio. This\napproach enables the training of DNNs that are trained on a limited amount of\naudio, yet can be deployed for real-world speech recognition in noisy\nenvironments. We demonstrate the effectiveness of our approach on the\nChallenge-Hearing-01 dataset, where we train a DNN to generate speech\nsamples from a speech distribution and test the trained DNN on a real-world\ndataset, achieving an F1-score of 0.988 and a Kullback-Leibler divergence of\n0.0009. The code and data are available at https://github.com/V",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14678899082568808,
          "p": 0.21333333333333335,
          "f": 0.17391303864898405
        },
        "rouge-2": {
          "r": 0.03759398496240601,
          "p": 0.043859649122807015,
          "f": 0.040485824989100576
        },
        "rouge-l": {
          "r": 0.13761467889908258,
          "p": 0.2,
          "f": 0.16304347343159278
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.01992v1",
      "true_abstract": "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.",
      "generated_abstract": "We study the market impact of a large-scale trading event, where the\nmarket maker simultaneously trades the underlying asset and a new, market-\ndependent option on the same underlying asset. The market impact of the\ntrading event is evaluated by the difference in the option's price between the\nmoment the option is bought and the moment the option is sold. This difference\nis known as the market impact. We show that the market impact is determined by\nthe market's price process and the market maker's optimal trading strategy.\nSpecifically, for a market with a continuous time-varying price process, the\nmarket impact is the difference in the option's price between the moment the\noption is bought and the moment the option is sold. For a market with a\ndiscrete time-varying price process, the market impact is the difference in\nthe option's price between the moment the option is bought",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2545454545454545,
          "f": 0.18300653134264608
        },
        "rouge-2": {
          "r": 0.00847457627118644,
          "p": 0.011363636363636364,
          "f": 0.009708732970122175
        },
        "rouge-l": {
          "r": 0.1326530612244898,
          "p": 0.23636363636363636,
          "f": 0.16993463591780952
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.09177v1",
      "true_abstract": "We generalize the notions of composition series and composition factors for\nprofinite groups, and prove a profinite version of the Jordan-Holder Theorem.\nWe apply this to prove a Galois Theorem for infinite prosolvable extensions. In\naddition, we investigate the connection between the abstract and topological\ncomposition factors of a nonstrongly complete profinite group.",
      "generated_abstract": "We prove that any finite dimensional Hilbert space $H$ has a finite-dimensional\n$A_1$-module of the same dimension. This is an analogue of a theorem of\nMatsumoto which asserts that any finite dimensional $A_1$-module is isomorphic\nto a sum of copies of $\\mathbb C$. We also prove that any finite dimensional\n$A_1$-module can be generated by a finite number of vectors. We also show that\nthe rank of any finite dimensional $A_1$-module is a sum of powers of the\nnumber of generators. Finally, we prove that the Hilbert--Schmidt norm of any\nfinite dimensional $A_1$-module is a sum of powers of the norm of the\ngenerators.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.15217391304347827,
          "f": 0.1728395012650512
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.028169014084507043,
          "f": 0.033057846390274606
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.15217391304347827,
          "f": 0.1728395012650512
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.12638v2",
      "true_abstract": "3D molecule generation is crucial for drug discovery and material design.\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\npretrained molecule LM for 1D molecule generation, and subsequently predicts\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\nNExT-Mol's performance by scaling up the LM's model size, refining the\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\nNotably, our 1D molecule LM significantly outperforms baselines in\ndistributional similarity while ensuring validity, and our 3D diffusion model\nachieves leading performances in conformer prediction. Given these improvements\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\navailable at https://github.com/acharkq/NExT-Mol.",
      "generated_abstract": "In this paper, we propose a method to generate high-quality cell lineage\ntrajectories in the 2D and 3D space from the 2D cell lineage trajectories.\nSpecifically, we introduce a novel cell lineage clustering method, which\ncombines the CellLineageCluster algorithm and the Probabilistic Graphical\nModel (PGM) to cluster the cells and the lineages. The clustering is performed\nusing the k-Means algorithm with different number of clusters. Then, the\nclusters are connected by lineage edges. We apply the proposed method to\ngenerate cell lineage trajectories in the 2D space. We present the\ncomputational complexity of our method, and we evaluate its performance by\ncomparing it with the CellLineageCluster algorithm. Finally, we compare the\nresults of our method with the results of the CellLineageCluster algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.144,
          "p": 0.2727272727272727,
          "f": 0.1884816708697679
        },
        "rouge-2": {
          "r": 0.022099447513812154,
          "p": 0.039603960396039604,
          "f": 0.028368789728636122
        },
        "rouge-l": {
          "r": 0.144,
          "p": 0.2727272727272727,
          "f": 0.1884816708697679
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.02383v1",
      "true_abstract": "This paper investigates strategic investments needed to mitigate transition\nrisks, particularly focusing on sectors significantly impacted by the shift to\na low-carbon economy. It emphasizes the importance of tailored sector-specific\nstrategies and the role of government interventions, such as carbon taxes and\nsubsidies, in shaping corporate behavior. In providing a multi-period\nframework, this paper evaluates the economic and operational trade-offs\ncompanies face under four various decarbonization scenarios: immediate, quick,\nslow, and no transitions. The analysis provides practical insights for both\npolicymakers and business leaders, demonstrating how regulatory frameworks and\nstrategic investments can be aligned to manage transition risks while\noptimizing long-term sustainability effectively. The findings contribute to a\ndeeper understanding of the economic impacts of regulatory policies and offer a\ncomprehensive framework to navigate the complexities of transitioning to a\nlow-carbon economy.",
      "generated_abstract": "This study examines the relationship between the U.S. Federal Reserve's\n(Fed) interest rate policy and the U.S. equity market by employing a\ndynamic-regression approach. The Fed's decision-making process is characterized\nby a multi-step process that includes policy rule formulation, implementation\nand policy adjustments. The Fed's decision-making process is also characterized\nby a set of underlying variables that include monetary policy indicators,\neconomic conditions, and financial market conditions. The Fed's interest rate\npolicy decision is then assessed by comparing the expected return on the\nequity market to the actual realized return on the equity market. The study\nexplores the Fed's interest rate policy decisions during the U.S. financial\ncrisis, the U.S. economic recovery, and the U.S. stock market's performance\nduring the COVID",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10101010101010101,
          "p": 0.15151515151515152,
          "f": 0.1212121164121214
        },
        "rouge-2": {
          "r": 0.008,
          "p": 0.01020408163265306,
          "f": 0.00896860493877083
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.13636363636363635,
          "f": 0.1090909042909093
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10383v1",
      "true_abstract": "In the DEAP-3600 dark matter search experiment, precise reconstruction of the\npositions of scattering events in liquid argon is key for background rejection\nand defining a fiducial volume that enhances dark matter candidate events\nidentification. This paper describes three distinct position reconstruction\nalgorithms employed by DEAP-3600, leveraging the spatial and temporal\ninformation provided by photomultipliers surrounding a spherical liquid argon\nvessel. Two of these methods are maximum-likelihood algorithms: the first uses\nthe spatial distribution of detected photoelectrons, while the second\nincorporates timing information from the detected scintillation light.\nAdditionally, a machine learning approach based on the pattern of photoelectron\ncounts across the photomultipliers is explored.",
      "generated_abstract": "The R&D of the Next Generation Accelerator (NGA) for the International\nH-Bridge experiment (IHEP/NA68) has been continuously progressing in China.\nThe detector design and the test bench of the NGA accelerator are in progress.\nThe test bench has been assembled and tested in the NGA Accelerator Test\nBench (NATB), which is located in the NGA Test Facility (NGATF). The NATB\nhas been used for the accelerator test, the beam test, the beam test of the\naccelerator test bench, the beam test of the proton accelerator test bench,\nand the beam test of the electron accelerator test bench. The test results of\nthe beam test of the accelerator test bench are reported here. The beam test\nof the accelerator test bench is a test of the accelerator and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10256410256410256,
          "p": 0.16326530612244897,
          "f": 0.12598424722921464
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.012048192771084338,
          "f": 0.010869560265243277
        },
        "rouge-l": {
          "r": 0.08974358974358974,
          "p": 0.14285714285714285,
          "f": 0.11023621573315166
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.03312v1",
      "true_abstract": "In this paper, we conduct a large-scale field experiment to investigate the\nmanipulability of prediction markets. The main experiment involves randomly\nshocking prices across 817 separate markets; we then collect hourly price data\nto examine whether the effects of these shocks persist over time. We find that\nprediction markets can be manipulated: the effects of our trades are visible\neven 60 days after they have occurred. However, as predicted by our model, the\neffects of the manipulations somewhat fade over time. Markets with more\ntraders, greater trading volume, and an external source of probability\nestimates are harder to manipulate.",
      "generated_abstract": "This study investigates the impact of the Covid-19 pandemic on the\ntime preference of the Chinese consumers. We use a large-scale household panel\ndata set to examine the effects of the pandemic on the time preferences of\nhouseholds. The results show that the time preferences of Chinese consumers have\nbeen affected by the pandemic. Specifically, the time preferences of consumers\nhave become more aggressive and risk averse during the pandemic. The results\nalso indicate that the consumption patterns of Chinese consumers have been\naffected by the pandemic, especially during the lockdown period. The results\nalso suggest that the time preferences of Chinese consumers have become more\naggressive and risk averse during the pandemic. The results show that the\ntime preferences of Chinese consumers have become more aggressive and risk averse\nduring the pandemic. The results also indicate that the consumption patterns",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20253164556962025,
          "p": 0.3137254901960784,
          "f": 0.24615384138579888
        },
        "rouge-2": {
          "r": 0.05434782608695652,
          "p": 0.07246376811594203,
          "f": 0.06211179634427722
        },
        "rouge-l": {
          "r": 0.189873417721519,
          "p": 0.29411764705882354,
          "f": 0.23076922600118355
        }
      }
    },
    {
      "paper_id": "math.RT.math/QA/2503.10394v1",
      "true_abstract": "This article investigates the two-parameter quantum matrix algebra at roots\nof unity. In the roots of unity setting, this algebra becomes a Polynomial\nIdentity (PI) algebra and it is known that simple modules over such algebra are\nfinite-dimensional with dimension at most the PI degree. We determine the\ncenter, compute the PI degree, and classify simple modules for two-parameter\nquantum matrix algebra, up to isomorphism, over an algebraically closed field\nof arbitrary characteristics.",
      "generated_abstract": "The set of rational points on an algebraic variety $X$ is a topological space\nthat is a topological manifold if and only if $X$ is smooth. This is a\nresult that dates back to Gauss and Euler, and it has been the starting point\nfor a plethora of classical and modern results. In this article, we present a\nnovel proof of this fundamental result, focusing on the case of smooth\nvarieties.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24528301886792453,
          "p": 0.26,
          "f": 0.25242717947026116
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.015151515151515152,
          "f": 0.015151510151516803
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.2,
          "f": 0.19417475228579523
        }
      }
    },
    {
      "paper_id": "math.RA.math/RA/2503.05337v2",
      "true_abstract": "We classify all two-dimensional simple algebras over an algebraically closed\nfield. For each two-dimensional algebra $\\mathcal{A}$ with an infinite group of\nautomorphisms we describe a minimal (with respect to inclusion) generating set\nfor the algebra of invariants of the $m$-tuples of $\\mathcal{A}$ in\ncharacteristic zero case. As a consequence, we show that in characteristic zero\ncase Artin-Procesi-Iltyakov Equality holds for all two-dimensional simple\nalgebras with an infinite group of automorphisms. We also consider\nnondegenerate invariant bilinear forms over two-dimensional algebras.",
      "generated_abstract": "We study a class of non-linear systems on the sphere and on the tetrahedron\nwith a non-linear interaction term between the two degrees of freedom. The\ninteraction term is of the form $\\sum_{i=1}^{m} \\theta_{i} \\left(\n\\mathbf{u}_{i}^{*} \\mathbf{u}_{i} - \\gamma_{i} \\mathbf{I} \\right)$, where\n$\\mathbf{u}_{i} \\in \\mathbb{R}^{3}$ is the $i$-th degree of freedom, and the\n$\\gamma_{i}$'s are real constants. The system is non-linearly stabilized via\na pair of $\\gamma$-Lagrange multiplier constraints. We consider two\napproaches: the first approach is a linearization of the system at the\nequilibrium points of the system, and the second approach is a linearization\nof the system at the equilibria",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.09836065573770492,
          "f": 0.10714285218271706
        },
        "rouge-2": {
          "r": 0.014705882352941176,
          "p": 0.011363636363636364,
          "f": 0.01282050790269748
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.09836065573770492,
          "f": 0.10714285218271706
        }
      }
    },
    {
      "paper_id": "econ.EM.q-fin/ST/2502.15458v1",
      "true_abstract": "Network connections, both across and within markets, are central in countless\neconomic contexts. In recent decades, a large literature has developed and\napplied flexible methods for measuring network connectedness and its evolution,\nbased on variance decompositions from vector autoregressions (VARs), as in\nDiebold and Yilmaz (2014). Those VARs are, however, typically identified using\nfull orthogonalization (Sims, 1980), or no orthogonalization (Koop, Pesaran,\nand Potter, 1996; Pesaran and Shin, 1998), which, although useful, are special\nand extreme cases of a more general framework that we develop in this paper. In\nparticular, we allow network nodes to be connected in \"clusters\", such as asset\nclasses, industries, regions, etc., where shocks are orthogonal across clusters\n(Sims style orthogonalized identification) but correlated within clusters\n(Koop-Pesaran-Potter-Shin style generalized identification), so that the\nordering of network nodes is relevant across clusters but irrelevant within\nclusters. After developing the clustered connectedness framework, we apply it\nin a detailed empirical exploration of sixteen country equity markets spanning\nthree global regions.",
      "generated_abstract": "We develop a novel, efficient, and interpretable methodology for\nexact model selection in empirical finance. We propose a novel score-based\napproach that efficiently quantifies the performance of model alternatives. We\nalso introduce a novel method for modeling model selection, which captures the\ninformation contained in the scores. We further develop a novel estimation\nprocedure for the model selection scores that enables us to obtain a consistent\nand efficient estimator. We apply our methods to a data set that contains\ninformation about the performance of different asset management strategies and\nseveral model alternatives. We find that our model selection approach\nsignificantly outperforms traditional methods for model selection.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10236220472440945,
          "p": 0.20967741935483872,
          "f": 0.13756613315752653
        },
        "rouge-2": {
          "r": 0.006289308176100629,
          "p": 0.010526315789473684,
          "f": 0.007874011065474916
        },
        "rouge-l": {
          "r": 0.10236220472440945,
          "p": 0.20967741935483872,
          "f": 0.13756613315752653
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.03026v2",
      "true_abstract": "When can interventions in markets be designed to increase surplus robustly --\ni.e., with high probability -- accounting for uncertainty due to imprecise\ninformation about economic primitives? In a setting with many strategic firms,\neach possessing some market power, we present conditions for such interventions\nto exist. The key condition, recoverable structure, requires large-scale\ncomplementarities among families of products. The analysis works by decomposing\nthe incidence of interventions in terms of principal components of a Slutsky\nmatrix. Under recoverable structure, a noisy signal of this matrix reveals\nenough about these principal components to design robust interventions. Our\nresults demonstrate the usefulness of spectral methods for analyzing\nimperfectly observed strategic interactions with many agents.",
      "generated_abstract": "In this paper, we propose a framework for learning and optimizing the\nparameterized\n  optimal control problem with a time-varying cost function. We formulate the\noptimal control problem as a non-convex optimization problem and show that the\nproposed algorithm is guaranteed to converge to a solution. We then provide an\nefficient algorithm for the proposed problem. Our theoretical analysis\ndemonstrates that the proposed algorithm achieves the optimal convergence\nrate and has a simple and efficient implementation. In addition, we propose a\ntime-varying cost control method to efficiently optimize the state of a\nmulti-agent system. We show that the proposed algorithm converges to the\noptimal solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12790697674418605,
          "p": 0.19298245614035087,
          "f": 0.15384614905178753
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.011494252873563218,
          "f": 0.01015227933211608
        },
        "rouge-l": {
          "r": 0.12790697674418605,
          "p": 0.19298245614035087,
          "f": 0.15384614905178753
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.05601v2",
      "true_abstract": "This paper proposes a Matrix Error Correction Model to identify cointegration\nrelations in matrix-valued time series. We hereby allow separate cointegrating\nrelations along the rows and columns of the matrix-valued time series and use\ninformation criteria to select the cointegration ranks. Through Monte Carlo\nsimulations and a macroeconomic application, we demonstrate that our approach\nprovides a reliable estimation of the number of cointegrating relationships.",
      "generated_abstract": "This paper develops a novel method for testing the causal effect of a\nindividual treatment on a dependent variable under instrumental variables\n(IVs) using the non-identical instrumental variables (NID) method. The\nassumption that the difference in the observed outcome of the individual\ntreatment depends on the difference in the observed outcome of the IV only\nholds under a special class of IVs called ``NID-identical.'' We derive the\nconditional NID-identical distribution of the IV and derive the resulting\nconditional NID-identical distribution of the observed outcome. We develop a\nmethod to test the causal effect of the individual treatment on the dependent\nvariable under the NID-identical distribution of the IV. We derive the\nconditional NID-identical distribution of the conditional causal effect of the\nindividual treatment on the dependent variable and develop a method to test the\ncausal effect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.19607843137254902,
          "f": 0.2020201970247935
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.025,
          "f": 0.02857142367347023
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.19607843137254902,
          "f": 0.2020201970247935
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.10431v1",
      "true_abstract": "Deep learning methods for point tracking are applicable in 2D\nechocardiography, but do not yet take advantage of domain specifics that enable\nextremely fast and efficient configurations. We developed MyoTracker, a\nlow-complexity architecture (0.3M parameters) for point tracking in\nechocardiography. It builds on the CoTracker2 architecture by simplifying its\ncomponents and extending the temporal context to provide point predictions for\nthe entire sequence in a single step. We applied MyoTracker to the right\nventricular (RV) myocardium in RV-focused recordings and compared the results\nwith those of CoTracker2 and EchoTracker, another specialized point tracking\narchitecture for echocardiography. MyoTracker achieved the lowest average point\ntrajectory error at 2.00 $\\pm$ 0.53 mm. Calculating RV Free Wall Strain (RV\nFWS) using MyoTracker's point predictions resulted in a -0.3$\\%$ bias with\n95$\\%$ limits of agreement from -6.1$\\%$ to 5.4$\\%$ compared to reference\nvalues from commercial software. This range falls within the interobserver\nvariability reported in previous studies. The limits of agreement were wider\nfor both CoTracker2 and EchoTracker, worse than the interobserver variability.\nAt inference, MyoTracker used 67$\\%$ less GPU memory than CoTracker2 and 84$\\%$\nless than EchoTracker on large sequences (100 frames). MyoTracker was 74 times\nfaster during inference than CoTracker2 and 11 times faster than EchoTracker\nwith our setup. Maintaining the entire sequence in the temporal context was the\ngreatest contributor to MyoTracker's accuracy. Slight additional gains can be\nmade by re-enabling iterative refinement, at the cost of longer processing\ntime.",
      "generated_abstract": "In this paper, we propose a new unified framework for 3D object segmentation\nand 3D multi-view stereo (MVS) reconstruction. Our framework enables the\nintegration of multi-view stereo and 3D object segmentation into a unified\nframework, which allows for a more robust and generalizable model. Specifically,\nwe introduce a multi-view stereo loss that encourages the segmentation model to\nalign the predicted bounding boxes with the ground truth. We also introduce a\n3D object segmentation loss that encourages the model to segment the object\ninside the camera frame. We further introduce a multi-view stereo reconstruction\nloss that encourages the segmentation model to align the predicted bounding\nboxes with the ground truth. To ensure that the model can effectively handle\nunseen 3D objects, we propose a novel multi-view stereo reconstruction loss,\nwhich",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06172839506172839,
          "p": 0.16666666666666666,
          "f": 0.09009008614560524
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.05555555555555555,
          "p": 0.15,
          "f": 0.08108107713659625
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-lat/2503.08847v1",
      "true_abstract": "The concept of nucleon radii plays a central role in our understanding of the\ninternal structure of protons and neutrons, providing critical insights into\nthe non-perturbative regime of quantum chromodynamics (QCD). While the charge\nradius is often interpreted as the ``size\" of the nucleon, this interpretation\nis an oversimplification that overlooks the multifaceted nature of nucleon\nstructure. This paper provides a comprehensive overview of the different\nnucleon radii, including the charge and magnetic radii, the axial radius, and\nthe emerging concepts of mechanical and mass radii. We discuss the definitions\nas well as the experimental, theoretical and phenomenological determinations of\nthese radii, highlighting their distinct physical origins and implications. By\nsynthesizing recent experimental results and theoretical advancements, we\nemphasize that each radius reflects a specific aspect of the nucleon's internal\nstructure, such as its electric charge distribution, magnetic properties, weak\ninteractions, or internal mechanical stress. In particular, we address the\ncommon but misleading interpretation of the proton radius as a simple measure\nof its size, underscoring the nuanced and context-dependent nature of nucleon\nradii. Through this exploration, we aim to clarify the roles of these radii in\ncharacterizing nucleon structure and to identify open questions that remain to\nbe addressed. This work contributes to a deeper understanding of the nucleon\nand its significance in the broader context of particle and nuclear physics.",
      "generated_abstract": "We study the behavior of the mass of the $\\eta_c$ meson in a wide range of\nhigher order QCD corrections. We find that the mass of the $\\eta_c$ meson\nexhibits a logarithmic behavior with respect to the coupling constant at large\nvalues of the coupling constant. The dependence of the mass of the $\\eta_c$\nmeson on the coupling constant is studied by calculating the mass of the\n$\\eta_c$ meson in different QCD orders, and the mass of the $\\eta_c$ meson is\nfound to increase with increasing the coupling constant. We also calculate the\n$\\eta_c$ meson mass in the limit of zero coupling constant and finite coupling\nconstant. The mass of the $\\eta_c$ meson exhibits a logarithmic behavior with\nrespect to the coupling constant at large values of the coupling constant. The\ndependence of the mass",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.2608695652173913,
          "f": 0.13483145684130804
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.0410958904109589,
          "f": 0.02142856757372518
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.2391304347826087,
          "f": 0.12359550178512825
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.06607v1",
      "true_abstract": "We prove that any complex local representation of the flat virtual braid\ngroup, $FVB_2$, into $GL_2(\\mathbb{C})$, for $n\\geq 2$, has one of the types\n$\\lambda_i: FVB_2 \\rightarrow GL_2(\\mathbb{C})$, $1\\leq i\\leq 12$. We find\nnecessary and sufficient conditions that guarantee the irreducibility of\nrepresentations of type $\\lambda_i$, $1\\leq i\\leq 5$, and we prove that\nrepresentations of type $\\lambda_i$, $6\\leq i\\leq 12$, are reducible. Regarding\nfaithfulness, we find necessary and sufficient conditions for representations\nof type $\\lambda_6$ or $\\lambda_7$ to be faithful. Moreover, we give sufficient\nconditions for representations of type $\\lambda_1$, $\\lambda_2$, or $\\lambda_4$\nto be unfaithful, and we show that representations of type $\\lambda_i$, $i=3,\n5, 8, 9, 10, 11, 12$ are unfaithful. We prove that any complex homogeneous\nlocal representations of the flat virtual braid group, $FVB_n$, into\n$GL_{n}(\\mathbb{C})$, for $n\\geq 2$, has one of the types $\\gamma_i: FVB_n\n\\rightarrow GL_n(\\mathbb{C})$, $i=1, 2$. We then prove that representations of\ntype $\\gamma_1: FVB_n \\rightarrow GL_n(\\mathbb{C})$ are reducible for $n\\geq\n6$, while representations of type $\\gamma_2: FVB_n \\rightarrow\nGL_n(\\mathbb{C})$ are reducible for $n\\geq 3$. Then, we show that\nrepresentations of type $\\gamma_1$ are unfaithful for $n\\geq 3$ and that\nrepresentations of type $\\gamma_2$ are unfaithful if $y=b$. Furthermore, we\nprove that any complex homogeneous local representation of the flat virtual\nbraid group, $FVB_n$, into $GL_{n+1}(\\mathbb{C})$, for all $n\\geq 4$, has one\nof the types $\\delta_i: FVB_n \\rightarrow GL_{n+1}(\\mathbb{C})$, $1\\leq i\\leq\n8$. We prove that these representations are reducible for $n\\geq 10$. Then, we\nshow that representations of types $\\delta_i$, $i\\neq 5, 6$, are unfaithful,\nwhile representations of types $\\delta_5$ or $\\delta_6$ are unfaithful if\n$x=y$.",
      "generated_abstract": "We give a simple and effective proof of a generalization of the classical\nperimeter theorem. We focus on the case when $S$ is a simple closed curve in\n$\\mathbb{R}^2$, and we show that, for each $n \\in \\mathbb{N}$, there exists a\nconstant $\\gamma_n > 0$ such that for each closed curve $\\Gamma$ in\n$\\mathbb{R}^2$ we have\n$\\operatorname{per}(\\Gamma) \\le \\gamma_n \\operatorname{per}(S)$. We also show that, for\neach closed curve $\\Gamma$ in $\\mathbb{R}^2$ with $\\operatorname{per}(\\Gamma)\n\\ge 2$, there exists a constant $C > 0$ such that\n$\\operatorname{per}(\\Gamma) \\le C \\operatorname{per}(S)$. We also discuss applications of\nour results to the study of the Hausdorff dimension of closed curves in\n$\\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10891089108910891,
          "p": 0.1774193548387097,
          "f": 0.13496932043961024
        },
        "rouge-2": {
          "r": 0.01875,
          "p": 0.03529411764705882,
          "f": 0.024489791386922952
        },
        "rouge-l": {
          "r": 0.10891089108910891,
          "p": 0.1774193548387097,
          "f": 0.13496932043961024
        }
      }
    },
    {
      "paper_id": "math.ST.math/MP/2503.08808v1",
      "true_abstract": "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
      "generated_abstract": "In this paper, we study the asymptotic behavior of the $k$-th Lyapunov\nspectrum of the Markov chain on a graph. Specifically, we determine the\nspectral gap, the eigenvalues of the Lyapunov matrix, and the associated\nLyapunov exponent of the Markov chain on a graph when the number of vertices\n$n$ and the diameter of the graph $d$ are fixed. In particular, we prove that\nthe Lyapunov spectrum of the Markov chain on a graph is contained in the\ninterval $[-1, 1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1016949152542373,
          "p": 0.27906976744186046,
          "f": 0.14906831906639414
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.01639344262295082,
          "f": 0.008968605891131685
        },
        "rouge-l": {
          "r": 0.09322033898305085,
          "p": 0.2558139534883721,
          "f": 0.13664595881794697
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2411.16373v1",
      "true_abstract": "Slow-fast dynamics are intrinsically related to complex phenomena, and are\nresponsible for many of the homeostatic dynamics that keep biological systems\nhealthfully functioning. We study a discrete-time membrane potential model that\ncan generate a diverse set of spiking behavior depending on the choice of\nslow-fast time scales, from fast spiking to bursting, or plateau action\npotentials -- also known as cardiac spikes, since they are characteristic in\nheart myocytes. The plateau of cardiac spikes may lose stability, generating\nearly or delayed afterdepolarizations (EAD and DAD, respectively), both of\nwhich are related to cardiac arrhythmia. We show the periodicity changes along\nthe transition from the healthy action potentials to these impaired spikes. We\nshow that while EADs are mainly periodic attractors, DAD usually comes with\nchaos. EADs are found inside shrimps -- isoperiodic structures of the parameter\nspace. However, in our system, the shrimps have an internal structure made of\nmultiple periodicities, revealing a complete devil's staircase. Understanding\nthe periodicity of plateau attractors in slow-fast systems could come in handy\nto unveil the features of heart myocytes behavior that are linked to cardiac\narrhythmias.",
      "generated_abstract": "Recent progress in high-throughput single-cell RNA-seq technology has\nopened up new avenues for studying complex biological systems. However, the\nhigh-throughput nature of single-cell data often presents a challenge for\nanalyzing and interpreting the data. In this paper, we propose a novel\nmethodology for identifying regulatory circuits from high-throughput single-cell\ndata. Our methodology relies on a novel thresholding procedure, which we call\nthresholding with locality-sensitive hashing (TLSH), for identifying the\nregulatory elements within the gene expression profiles. We demonstrate the\nproposed methodology on two case studies: (i) a single-cell RNA-seq analysis of\nT-cell receptor signaling, and (ii) a single-cell RNA-seq analysis of\nmicroRNA-mediated regulation of the immune response",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12295081967213115,
          "p": 0.20833333333333334,
          "f": 0.1546391705898609
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.020618556701030927,
          "f": 0.014760143005134806
        },
        "rouge-l": {
          "r": 0.11475409836065574,
          "p": 0.19444444444444445,
          "f": 0.14432989223934545
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.10505v1",
      "true_abstract": "We compute the $K_1$-group of ultraproducts of unital, simple $C^*$-algebras\nwith unique trace and strict comparison. As an application, we prove that the\nreduced free group $C^*$-algebras $C^*_r(F_m)$ and $C^*_r(F_n)$ are\nelementarily equivalent (i.e., have isomorphic ultrapowers) if and only if $m =\nn$. This settles in the negative the $C^*$-algebraic analogue of Tarski's 1945\nproblem for groups.",
      "generated_abstract": "In this paper, we prove a new type of $\\mathbb{P}\\mathcal{O}_{\\mathbb{P}^3}$-invariants\nfor the universal family of hyperplane sections of $\\mathbb{P}^3$. We obtain\ntwo new $\\mathbb{P}\\mathcal{O}_{\\mathbb{P}^3}$-invariants for the\n$\\mathbb{P}\\mathcal{O}_{\\mathbb{P}^3}$-bundle $\\mathcal{P}_{\\mathbb{P}^3}$ on\n$\\mathbb{P}^3$. One of them is a new $\\mathbb{P}\\mathcal{O}_{\\mathbb{P}^3}$-invariant\nfor $\\mathcal{P}_{\\mathbb{P}^3}$, and the other is a new $\\mathbb{P}\\mathcal{O}_{\\mathbb{P}^3}$-invariant\nfor the $\\mathbb{P}\\mathcal{O}_{\\mathbb{P}^3}$-bundle $\\mathcal{P}_{\\mathbb{P}^3/k}$ over a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13725490196078433,
          "p": 0.21875,
          "f": 0.1686746940571928
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.023809523809523808,
          "f": 0.019801975339673775
        },
        "rouge-l": {
          "r": 0.13725490196078433,
          "p": 0.21875,
          "f": 0.1686746940571928
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/GN/2503.02692v1",
      "true_abstract": "To improve stock trend predictions and support personalized investment\ndecisions, this paper proposes FinArena, a novel Human-Agent collaboration\nframework. Inspired by the mixture of experts (MoE) approach, FinArena combines\nmultimodal financial data analysis with user interaction. The human module\nfeatures an interactive interface that captures individual risk preferences,\nallowing personalized investment strategies. The machine module utilizes a\nLarge Language Model-based (LLM-based) multi-agent system to integrate diverse\ndata sources, such as stock prices, news articles, and financial statements. To\naddress hallucinations in LLMs, FinArena employs the adaptive\nRetrieval-Augmented Generative (RAG) method for processing unstructured news\ndata. Finally, a universal expert agent makes investment decisions based on the\nfeatures extracted from multimodal data and investors' individual risk\npreferences. Extensive experiments show that FinArena surpasses both\ntraditional and state-of-the-art benchmarks in stock trend prediction and\nyields promising results in trading simulations across various risk profiles.\nThese findings highlight FinArena's potential to enhance investment outcomes by\naligning strategic insights with personalized risk considerations.",
      "generated_abstract": "The 2010s have seen the rapid emergence of the gig economy, a workforce\nmodel that has been increasingly adopted in recent years. This paper\ninvestigates the impact of the gig economy on workers' economic well-being\nand explores the role of digital platforms in this emerging workforce model.\nUsing a novel multilevel model, we demonstrate that the adoption of the gig\neconomy increases the risk of unemployment among workers, whereas the\nadoption of digital platforms improves their financial well-being. Furthermore,\nthe adoption of the gig economy increases the risk of unemployment among\nworkers, whereas the adoption of digital platforms improves their financial\nwell-being. Additionally, the findings suggest that the gig economy is more\nlikely to emerge in economies with high levels of digitalization. This study\ncontributes to the literature by exploring the complex interplay",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14049586776859505,
          "p": 0.22972972972972974,
          "f": 0.17435896964944128
        },
        "rouge-2": {
          "r": 0.0064516129032258064,
          "p": 0.010309278350515464,
          "f": 0.007936503201375966
        },
        "rouge-l": {
          "r": 0.12396694214876033,
          "p": 0.20270270270270271,
          "f": 0.15384614913662079
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.10361v1",
      "true_abstract": "We study the stochastic gravitational wave background sourced by a network of\ncosmic superstrings and demonstrate that incorporating higher-mass string\nspecies, beyond the fundamental string, is crucial for accurately modeling the\nresulting gravitational wave spectrum across frequencies ranging from nanohertz\nto kilohertz. Using the multi-tension velocity-dependent one-scale model to\nevolve the cosmic superstring network, we perform several fits to the NANOGrav\n15-year dataset and obtain expectation values for the fundamental string\ntension, string coupling and effective size of compact extra dimensions. We\nfind that the cosmic superstring best-fits are comparable in likelihood to\nSupermassive Black Hole models, thought by many to be the leading candidate\nexplanation of the signal. The implications of the best-fit spectra are\ndiscussed within the context of future gravitational wave experiments. We\nobtain expectation values for the fundamental string tension of\n$\\log_{10}(G\\mu_1)=-11.5^{+0.3}_{-0.3}$($-11.6^{+0.2}_{-0.3}$) for\ngravitational waves originating from large cuspy (kinky) cosmic superstring\nloops and $\\log_{10}(G\\mu_1)=-9.7^{+0.7}_{-0.7}$($-9.9^{+1.0}_{-0.5}$) for\nsmall cuspy (kinky) loops. We also place $2\\sigma$ upper bounds on the string\ncoupling, finding $g_s<0.65$ in all cases, and comment on the implication of\nour results for the effective size of the compact extra dimensions.",
      "generated_abstract": "We present the first measurement of the mass of the dark matter halo of the\ngalaxy\n  M87 (VLBA, ALMA, NIR) using a Bayesian approach that incorporates the\ninformation of the observed VLBA parallax and the NIR magnitudes. Our results\nshow that the mass of the dark matter halo of M87 is $M_{\\rm DM} =\n1.61^{+0.03}_{-0.04}\\times 10^{14} M_{\\odot}$, a value that is consistent with\nthe previous value from the VLBA parallax, but much smaller than the value\nmeasured by the Hubble Space Telescope and the ground-based Very Large Telescopes\n(VLT). This result is also in good agreement with the dark matter mass obtained\nby the NIR parallax, which is $M_{\\rm DM}",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09448818897637795,
          "p": 0.17391304347826086,
          "f": 0.12244897502967532
        },
        "rouge-2": {
          "r": 0.011299435028248588,
          "p": 0.02127659574468085,
          "f": 0.01476014307049335
        },
        "rouge-l": {
          "r": 0.09448818897637795,
          "p": 0.17391304347826086,
          "f": 0.12244897502967532
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.10407v1",
      "true_abstract": "The cloud computing model enables the on-demand provisioning of computing\nresources, reducing manual management, increasing efficiency, and improving\nenvironmental impact. Software architects now play a strategic role in\ndesigning and deploying elasticity policies for automated resource management.\nHowever, creating policies that meet performance and cost objectives is\ncomplex. Existing approaches, often relying on formal models like Queueing\nTheory, require advanced skills and lack specific methods for representing\nelasticity within architectural models. This paper introduces an architectural\nview type for modeling and simulating elasticity, supported by the Scaling\nPolicy Definition (SPD) modeling language, a visual notation, and precise\nsimulation semantics. The view type is integrated into the Palladio ecosystem,\nproviding both conceptual and tool-based support. We evaluate the approach\nthrough two single-case experiments and a user study. In the first experiment,\nsimulations of elasticity policies demonstrate sufficient accuracy when\ncompared to load tests, showing the utility of simulations for evaluating\nelasticity. The second experiment confirms feasibility for larger applications,\nthough with increased simulation times. The user study shows that participants\ncompleted 90% of tasks, rated the usability at 71%, and achieved an average\nscore of 76% in nearly half the allocated time. However, the empirical evidence\nsuggests that modeling with this architectural view requires more time than\nmodeling control flow, resource environments, or usage profiles, despite its\nbenefits for elasticity policy design and evaluation.",
      "generated_abstract": "The popularity of deep learning in the last decade has made it a crucial\ntool for real-world applications. However, the increasing complexity of\napplications often leads to overfitting, which can be mitigated by the\nintroduction of regularization. One popular regularization technique is the\nL2 penalty. In this paper, we consider a more general regularization method,\nthe L1 penalty, which has been successfully applied to the regression task. We\npropose a new regularization technique, namely the L1 penalty with a\nregularization parameter, which can be interpreted as a weighted average of the\nL1 penalty. The proposed method has been applied to regression and classification\nproblems and demonstrated to perform better than the L1 penalty on these tasks.\nWe have also analyzed the performance of the proposed method on the\nCIFAR-10 dataset. Our experiments show that the proposed method is more\nstable than the L1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.147239263803681,
          "p": 0.27586206896551724,
          "f": 0.19199999546208013
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.015748031496062992,
          "f": 0.011594198246420686
        },
        "rouge-l": {
          "r": 0.147239263803681,
          "p": 0.27586206896551724,
          "f": 0.19199999546208013
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.03151v1",
      "true_abstract": "Subset selection is central to many wireless communication problems,\nincluding link scheduling, power allocation, and spectrum management. However,\nthese problems are often NP-complete, because of which heuristic algorithms\napplied to solve these problems struggle with scalability in large-scale\nsettings. To address this, we propose a determinantal point process-based\nlearning (DPPL) framework for efficiently solving general subset selection\nproblems in massive networks. The key idea is to model the optimal subset as a\nrealization of a determinantal point process (DPP), which balances the\ntrade-off between quality (signal strength) and similarity (mutual\ninterference) by enforcing negative correlation in the selection of {\\em\nsimilar} links (those that create significant mutual interference). However,\nconventional methods for constructing similarity matrices in DPP impose\ndecomposability and symmetry constraints that often do not hold in practice. To\novercome this, we introduce a new method based on the Gershgorin Circle Theorem\nfor constructing valid similarity matrices. The effectiveness of the proposed\napproach is demonstrated by applying it to two canonical wireless network\nsettings: an ad hoc network in 2D and a cellular network serving drones in 3D.\nSimulation results show that DPPL selects near-optimal subsets that maximize\nnetwork sum-rate while significantly reducing computational complexity compared\nto traditional optimization methods, demonstrating its scalability for\nlarge-scale networks.",
      "generated_abstract": "This paper addresses the problem of time-frequency estimation for the\nmixed-norm optimal control problem (MNOCP) in the presence of noisy\nmeasurements. The problem involves a stochastic control system with state\nfeedback and a control law, which is subject to noise. The optimal control\nproblem is solved using the optimal control theory, and the resulting optimal\ncontrol law is then used to control the stochastic system to obtain the\nestimated state. The estimation error is quantified using the $L^2$ norm of\nthe difference between the estimated and actual states. The proposed estimation\nmethod is shown to be robust and reliable. Numerical experiments demonstrate\nthat the proposed method is effective in estimating the state of the stochastic\nsystem, especially when the noise is high.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10884353741496598,
          "p": 0.24615384615384617,
          "f": 0.15094339197445722
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.044642857142857144,
          "f": 0.032051277449047344
        },
        "rouge-l": {
          "r": 0.10204081632653061,
          "p": 0.23076923076923078,
          "f": 0.14150942971030628
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CR/2503.10320v1",
      "true_abstract": "Cellular Automata (CA) are commonly investigated as a particular type of\ndynamical systems, defined by shift-invariant local rules. In this paper, we\nconsider instead CA as algebraic systems, focusing on the combinatorial designs\ninduced by their short-term behavior. Specifically, we review the main results\npublished in the literature concerning the construction of mutually orthogonal\nLatin squares via bipermutive CA, considering both the linear and nonlinear\ncases. We then survey some significant applications of these results to\ncryptography, and conclude with a discussion of open problems to be addressed\nin future research on CA-based combinatorial designs.",
      "generated_abstract": "We prove that every connected, strongly connected, and regular graph is\ngraph-homotopy equivalent to a tree. We also show that the class of graph-homotopy\nequivalent graphs is closed under certain restrictions on the graph. We show\nthat the class of graphs that can be constructed using a finite number of\nvertices and edges, but not all of their automorphisms, is not closed under\ngraph-homotopy equivalence. We give a simple example of a graph that is not\ngraph-homotopy equivalent to a tree.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11842105263157894,
          "p": 0.20930232558139536,
          "f": 0.15126049958618754
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.014492753623188406,
          "f": 0.012345674122086987
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.18604651162790697,
          "f": 0.13445377689711194
        }
      }
    },
    {
      "paper_id": "cs.CE.econ/TH/2503.00201v1",
      "true_abstract": "This paper demonstrates that Automated Market Maker (AMM) based markets, such\nas those using constant product formulas (e.g., Uniswap), are inherently\npath-dependent. We prove mathematically that the sequence of operations in AMMs\ndetermines the final state, challenging the notion that market prices solely\nreflect information. This property has profound implications for decentralized\nprediction markets that rely on AMMs for price discovery, as it demonstrates\nthey cannot function as pure \"truth machines.\" Using both mathematical proofs\nand empirical evidence from ETH/USDC pools, we show that AMM-based markets\nincorporate historical path information beyond the current market beliefs. Our\nfindings contribute to the understanding of market efficiency, mechanism\ndesign, and the interpretation of prices in decentralized finance systems.",
      "generated_abstract": "This paper proposes a novel framework for optimizing the design of\nincentive compatible rewards for agents in multi-agent systems. The framework\nis based on the idea that agents' preferences can be understood as a\ncombination of utility functions and reward functions. It enables agents to\nexpress their preferences using the combination of the utility functions and\nreward functions. Furthermore, agents' preferences can be expressed in terms\nof the reward functions, making the design of the incentive compatible reward\nframework more flexible. The framework can be applied to various scenarios,\nincluding multi-agent reinforcement learning, multi-agent coordination, and\nmulti-agent decision-making. The framework offers a practical approach for\noptimizing the design of incentive compatible rewards in multi-agent systems.\nIt provides a framework for designing incentive compatible rewards that is\nflexible and practical. The framework can be used to optimize the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15053763440860216,
          "p": 0.2222222222222222,
          "f": 0.17948717467209088
        },
        "rouge-2": {
          "r": 0.008547008547008548,
          "p": 0.009523809523809525,
          "f": 0.009009004023620973
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.2222222222222222,
          "f": 0.17948717467209088
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2405.07102v3",
      "true_abstract": "Instrumental variables (IV) are a commonly used tool to estimate causal\neffects from non-randomized data. An archetype of an IV is a randomized trial\nwith non-compliance where the randomized treatment assignment serves as an IV\nfor the non-ignorable treatment received. Under a monotonicity assumption, a\nvalid IV non-parametrically identifies the average treatment effect among a\nnon-identified, latent complier subgroup, whose generalizability is often under\ndebate. In many studies, there could exist multiple versions of an IV, for\ninstance, different nudges to take the same treatment in different study sites\nin a multicentre clinical trial. These different versions of an IV may result\nin different compliance rates and offer a unique opportunity to study IV\nestimates' generalizability. In this article, we introduce a novel nested IV\nassumption and study identification of the average treatment effect among two\nlatent subgroups: always-compliers and switchers, who are defined based on the\njoint potential treatment received under two versions of a binary IV. We derive\nthe efficient influence function for the SWitcher Average Treatment Effect\n(SWATE) under a non-parametric model and propose efficient estimators. We then\npropose formal statistical tests of the generalizability of IV estimates under\nthe nested IV framework. We apply the proposed method to the Prostate, Lung,\nColorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal\neffect of colorectal cancer screening and its generalizability.",
      "generated_abstract": "This paper introduces the novel concept of a ``tour of the field'' as an\npart of a data-driven decision-making framework. It is based on the\nobservation that there is a wealth of data about the field of economics, but\nthere are also data gaps. In our experience, the most challenging areas of\neconomics for data collection are those that are relatively well-understood\nand have been studied in depth by other researchers. The tour of the field\nprovides a mechanism to identify the gaps and to collect the data needed to\nfill them. The tour of the field also provides a mechanism to evaluate the\npotential of other researchers to collect data on the topic, thereby\nenhancing the data availability for other researchers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.2753623188405797,
          "f": 0.1862745053272781
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.05660377358490566,
          "f": 0.038338653667589255
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.2608695652173913,
          "f": 0.17647058375865063
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10060v1",
      "true_abstract": "This paper investigates the resource allocation design for a pinching antenna\n(PA)-assisted multiuser multiple-input single-output (MISO) non-orthogonal\nmultiple access (NOMA) system featuring multiple dielectric waveguides. To\nenhance model accuracy, we propose a novel frequency-dependent power\nattenuation model for dielectric waveguides in PA-assisted systems. By jointly\noptimizing the precoder vector and the PA placement, we aim to maximize the\nsystem's sum-rate while accounting for the power attenuation across dielectric\nwaveguides. The design is formulated as a non-convex optimization problem. To\neffectively address the problem at hand, we introduce an alternating\noptimization-based algorithm to obtain a suboptimal solution in polynomial\ntime. Our results demonstrate that the proposed PA-assisted system not only\nsignificantly outperforms the conventional system but also surpasses a naive\nPA-assisted system that disregards power attenuation. The performance gain\ncompared to the naive PA-assisted system becomes more pronounced at high\ncarrier frequencies, emphasizing the importance of considering power\nattenuation in system design.",
      "generated_abstract": "In this paper, we present a novel methodology for the design of digital\ninterconnects for high-speed serial interconnects (SSI), which can be used to\ncreate a flexible and robust solution for the design of SSI interconnects. The\napproach we propose is based on the use of a low-density parity check (LDPC)\ncode to enhance the error correction capability of the SSI. We propose a\ncombination of the LDPC code and the Dual-State Error Correction (DSEC) code,\nwhich we refer to as the LDPC-DSEC (LDPC-D) code, to enhance the error\ncorrection capability of the SSI. This approach is based on the fact that the\nLDPC-DSEC (LDPC-D) code is the same as the DSEC code but with a DSEC code",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.2857142857142857,
          "f": 0.21818181346115711
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.061224489795918366,
          "f": 0.049999995168056026
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2698412698412698,
          "f": 0.206060601339945
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CR/2503.09712v1",
      "true_abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data.",
      "generated_abstract": "Recent advances in deep learning have led to significant improvements in\nresearch in the field of computer vision, particularly in the area of\nobject-centric classification. However, existing approaches often face challenges\nsuch as data scarcity, high computational costs, and inadequate model\ngeneralization. To address these limitations, we propose a novel framework,\nObject-Centric Attention-guided Network (O-CAN), which integrates\nattention-guided self-attention and attention-guided position-aware self-attention\nmodules. These modules enable the model to better capture the complex\ninteractions between objects and their environments, enabling it to more\neffectively identify and classify objects. Additionally, O-CAN incorporates\nobject-centric attention, which enables the model to focus on specific parts of\nthe object, effectively improving its ability to recognize specific objects.\nExperimental results on the COCO",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20134228187919462,
          "p": 0.33707865168539325,
          "f": 0.2521008356539087
        },
        "rouge-2": {
          "r": 0.02512562814070352,
          "p": 0.043478260869565216,
          "f": 0.03184712911578631
        },
        "rouge-l": {
          "r": 0.18120805369127516,
          "p": 0.30337078651685395,
          "f": 0.22689075162029526
        }
      }
    },
    {
      "paper_id": "math.NT.math/NT/2503.08800v1",
      "true_abstract": "We prove the Fontaine-Plamondon conjecture and show that there are precisely\n$4400$ and $26952$ positive integral $E_7$-friezes and $E_8$-friezes\nrespectively, completing the enumerative classification of all positive\nintegral friezes of Dynkin type. In general, we count positive integral friezes\nof rank $n$ by determining the positive integral points on a $n$-dimensional\nsingular affine variety. This gives new Diophantine proofs of the enumeration\ntheorems for friezes of the other Dynkin types, which were previously proved\nusing discrete geometry, algebraic combinatorics, and the theory of cluster\nalgebras.",
      "generated_abstract": "We introduce an approach to study the relation between the arithmetic\n$K$-theory of a number field and its Jacobson radical. We show that the\nhomology of the Jacobson radical is a direct sum of the arithmetic $K$-theory of\nthe number field. In particular, we prove that the arithmetic $K$-theory of a\nnumber field is a direct sum of its Jacobson radical. We also provide a new\ncharacterization of the Jacobson radical in terms of the arithmetic $K$-theory\nof the number field. We also prove that the arithmetic $K$-theory of a number\nfield is a direct sum of the Jacobson radical if and only if the residue field\nis a finite field of characteristic $p$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1746031746031746,
          "p": 0.2682926829268293,
          "f": 0.21153845676220426
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.031746031746031744,
          "f": 0.02857142362142943
        },
        "rouge-l": {
          "r": 0.1746031746031746,
          "p": 0.2682926829268293,
          "f": 0.21153845676220426
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.19869v1",
      "true_abstract": "This study investigates event-related desynchronization (ERD) phenomena\nduring motor imagery and actual movement. Using sLORETA software, we analyzed\nthe cortical current source density distributions in Mu and Beta frequency\nbands for 33 subjects during rest, motor imagery, and actual movement\nconditions. The results were normalized for analysis. Using sLORETA's\nstatistical tools, paired t-tests were conducted to compare the normalized\ncurrent source density results between rest and motor imagery, rest and actual\nmovement, and motor imagery and actual movement conditions in both frequency\nbands. The findings revealed: In both Mu and Beta frequency bands, during motor\nimagery, significant ERD (P<0.01) was observed in the salience network,\nsupplementary motor area, primary motor area, premotor cortex, primary\nsomatosensory cortex, and parietofrontal mirror neuron system. During actual\nmovement, significant ERD (P<0.05) was observed in the primary somatosensory\ncortex, primary motor area, and parietofrontal mirror neuron system in both\nfrequency bands. Comparing motor imagery to actual movement, the current source\ndensity in the primary somatosensory cortex and parietofrontal mirror neuron\nsystem was higher during motor imagery, though this difference was not\nstatistically significant (P>0.05). This paper analyzes the factors\ncontributing to these statistical results and proposes preliminary solutions.",
      "generated_abstract": "The advent of high-throughput sequencing has revolutionized the study of\ngenomic variation. In this paper, we present a novel approach for genotype\nprediction from sequencing data based on a diffusion model. We use the\ninformation in the sequencing data to approximate the diffusion process that\ngoverns the evolution of the genetic variations in the population. We demonstrate\nthat our approach performs well in a variety of settings, including simulated\ndata and real-world data from the Human Genome Diversity Project (HGDP) and\nfrom the 1000 Genomes Project. We also present a Bayesian inference method for\nthe parameter estimates of the diffusion model, and we discuss how our\napproach can be used for other applications, such as estimating mutation rates\nfrom sequence data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10752688172043011,
          "p": 0.1282051282051282,
          "f": 0.11695905936595895
        },
        "rouge-2": {
          "r": 0.006944444444444444,
          "p": 0.008849557522123894,
          "f": 0.007782096240067313
        },
        "rouge-l": {
          "r": 0.10752688172043011,
          "p": 0.1282051282051282,
          "f": 0.11695905936595895
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2412.05430v1",
      "true_abstract": "Recent advances in self-supervised models for natural language, vision, and\nprotein sequences have inspired the development of large genomic DNA language\nmodels (DNALMs). These models aim to learn generalizable representations of\ndiverse DNA elements, potentially enabling various genomic prediction,\ninterpretation and design tasks. Despite their potential, existing benchmarks\ndo not adequately assess the capabilities of DNALMs on key downstream\napplications involving an important class of non-coding DNA elements critical\nfor regulating gene activity. In this study, we introduce DART-Eval, a suite of\nrepresentative benchmarks specifically focused on regulatory DNA to evaluate\nmodel performance across zero-shot, probed, and fine-tuned scenarios against\ncontemporary ab initio models as baselines. Our benchmarks target biologically\nmeaningful downstream tasks such as functional sequence feature discovery,\npredicting cell-type specific regulatory activity, and counterfactual\nprediction of the impacts of genetic variants. We find that current DNALMs\nexhibit inconsistent performance and do not offer compelling gains over\nalternative baseline models for most tasks, while requiring significantly more\ncomputational resources. We discuss potentially promising modeling, data\ncuration, and evaluation strategies for the next generation of DNALMs. Our code\nis available at https://github.com/kundajelab/DART-Eval.",
      "generated_abstract": "The increasing availability of high-throughput omics data has made it\nimpractical to rely solely on traditional statistical tests to assess the\nrelevance of gene-disease associations. Traditional tests are also limited in\nthat they are only applicable to a single gene-disease association at a time,\nwhile many gene-disease associations have multiple biological mechanisms that\ncan be tested simultaneously. We introduce a novel approach for testing the\nrelevance of gene-disease associations, based on the concept of pathway\ninterference. This approach enables us to test the relevance of multiple\ngene-disease associations simultaneously, while accounting for the biological\nmechanisms that underlie these associations. We demonstrate the utility of\npathway interference testing through an example involving the HIF-1A gene and\nhypoxia.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1347517730496454,
          "p": 0.25,
          "f": 0.1751152028218906
        },
        "rouge-2": {
          "r": 0.0111731843575419,
          "p": 0.019230769230769232,
          "f": 0.014134270969547908
        },
        "rouge-l": {
          "r": 0.12056737588652482,
          "p": 0.2236842105263158,
          "f": 0.1566820230983883
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.17412v1",
      "true_abstract": "Gaussian multiplicative chaos (GMC) is a canonical random fractal measure\nobtained by exponentiating log-correlated Gaussian processes, first constructed\nin the seminal work of Kahane (1985). Since then it has served as an important\nbuilding block in constructions of quantum field theories and Liouville quantum\ngravity. However, in many natural settings, non-Gaussian log-correlated\nprocesses arise. In this paper, we investigate the universality of GMC through\nan invariance principle. We consider the model of a random Fourier series, a\nprocess known to be log-correlated. While the Gaussian Fourier series has been\na classical object of study, recently, the non-Gaussian counterpart was\ninvestigated and the associated multiplicative chaos constructed by Junnila in\n2016. We show that the Gaussian and non-Gaussian variables can be coupled so\nthat the associated chaos measures are almost surely mutually absolutely\ncontinuous throughout the entire sub-critical regime. This solves the main open\nproblem from Kim and Kriechbaum (2024) who had earlier established such a\nresult for a part of the regime. The main ingredient is a new high dimensional\nCLT for a sum of independent (but not i.i.d.) random vectors belonging to rank\none subspaces with error bounds involving the isotropic properties of the\ncovariance matrix of the sum, which we expect will find other applications. The\nproof relies on a path-wise analysis of Skorokhod embeddings as well as a\nperturbative result about square roots of positive semi-definite matrices\nwhich, surprisingly, appears to be new.",
      "generated_abstract": "We study the problem of learning a model of the form $X \\sim P_{D}$ from a\ndata sample $D$ with a fixed $P_D$. For instance, in applications one often\nencounters the situation where $D$ is drawn from a given distribution $P$ and\n$X$ is observed from a distribution $Q$ that is close to $P$. The main\nquestion is to find the best possible sample complexity $\\tilde O(\\sqrt{n\n\\log n})$ for learning $P_D$ from $D$ under the assumption that $Q$ is a\nconcentration of a certain class of distributions. We give a sharp upper bound\nfor this sample complexity in the case where $P$ and $Q$ are both supported on\nthe unit ball of $\\mathbb{R}^d$ and $d \\ge 1$. In the case $d=1$, we provide an\nexample of a distribution $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.30864197530864196,
          "f": 0.20325202810331158
        },
        "rouge-2": {
          "r": 0.026200873362445413,
          "p": 0.04918032786885246,
          "f": 0.03418802965268199
        },
        "rouge-l": {
          "r": 0.12727272727272726,
          "p": 0.25925925925925924,
          "f": 0.17073170290005957
        }
      }
    },
    {
      "paper_id": "math-ph.math-ph/2503.09558v1",
      "true_abstract": "For a given graph $G$, Budzik, Gaiotto, Kulp, Wang, Williams, Wu, Yu, and the\nfirst author studied a ''topological'' differential form $\\alpha_G$, which\nexpresses violations of BRST-closedness of a quantum field theory along a\nsingle topological direction. In a seemingly unrelated context, Brown, Panzer,\nand the second author studied a ''Pfaffian'' differential form $\\phi_G$, which\nis used to construct cohomology classes of the odd commutative graph complex.\nWe give an explicit combinatorial proof that $\\alpha_G$ coincides with\n$\\phi_G$. We also discuss the equivalence of several properties of these forms,\nwhich had been established independently for both contexts in previous work.",
      "generated_abstract": "We study the problem of constructing a local-in-time solution to the 3D nonlinear\nreaction-diffusion system\n\\begin{align}\n  \\partial_t u - \\Delta u + \\nabla\\cdot(u\\nabla u) = |u|^{p-2}u\\nabla^2\n  u-f(u) \\qquad \\text{in} \\quad (0,T)\\times\\Omegaf, \\label{n6cz4cQLBJ4MNmI6F08S2Il8C0JQYiUlI1YkKoiubVtfGuOaMx580}\n  \\\\\n  \\frac{\\BTHYERWB u}{\\BTHYERWB N} = 0 \\qquad \\text{in} \\quad (0,T)\\times\\Omegaf,\n  \\label{",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06329113924050633,
          "p": 0.1388888888888889,
          "f": 0.08695651743818548
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06329113924050633,
          "p": 0.1388888888888889,
          "f": 0.08695651743818548
        }
      }
    },
    {
      "paper_id": "math.CO.math/IT/2503.08948v1",
      "true_abstract": "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.",
      "generated_abstract": "In this paper we study the $L^2$-$L^2$ and $L^2$-$L^\\infty$ bounds of\nthe norm of a linear operator $T: H^2 \\rightarrow L^2$ with respect to its\nHilbert-Schmidt norm $\\|\\cdot\\|_{HS}$ and its spectral norm $\\|\\cdot\\|_{2}$,\nrespectively. We prove that the norm of $T$ satisfies the $L^2$-$L^2$ and\n$L^2$-$L^\\infty$ bounds if and only if the operator $T$ is bounded on the\nHilbert-Schmidt norm $\\|\\cdot\\|_{HS}$ and on the spectral norm $\\|\\cdot\\|_{2}$,\nrespectively. We also prove the $L^2$-$L^\\infty$ bound for $T$ in this case.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.34375,
          "p": 0.2558139534883721,
          "f": 0.29333332844088894
        },
        "rouge-2": {
          "r": 0.05405405405405406,
          "p": 0.03125,
          "f": 0.03960395575335806
        },
        "rouge-l": {
          "r": 0.3125,
          "p": 0.23255813953488372,
          "f": 0.2666666617742223
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.18868v1",
      "true_abstract": "This study examines the relationship between income inequality, gender, and\nschool completion rates in Malaysia using machine learning techniques. The\ndataset utilized is from the Malaysia's Public Sector Open Data Portal,\ncovering the period 2016-2022. The analysis employs various machine learning\ntechniques, including K-means clustering, ARIMA modeling, Random Forest\nregression, and Prophet for time series forecasting. These models are used to\nidentify patterns, trends, and anomalies in the data, and to predict future\nschool completion rates. Key findings reveal significant disparities in school\ncompletion rates across states, genders, and income levels. The analysis also\nidentifies clusters of states with similar completion rates, suggesting\npotential regional factors influencing educational outcomes. Furthermore, time\nseries forecasting models accurately predict future completion rates,\nhighlighting the importance of ongoing monitoring and intervention strategies.\nThe study concludes with recommendations for policymakers and educators to\naddress the observed disparities and improve school completion rates in\nMalaysia. These recommendations include targeted interventions for specific\nstates and demographic groups, investment in early childhood education, and\naddressing the impact of income inequality on educational opportunities. The\nfindings of this study contribute to the understanding of the factors\ninfluencing school completion in Malaysia and provide valuable insights for\npolicymakers and educators to develop effective strategies to improve\neducational outcomes.",
      "generated_abstract": "This study investigates the determinants of the performance of small and\nsmall and medium-sized enterprises (SMEs) in the oil and gas sector in Algeria\nusing a panel data set comprising 293 observations over the period 2016-2023.\nWe employ a panel vector error correction model to identify the impact of\ndeterminants on SMEs' performance. Our findings reveal that the economic\nenvironment, access to finance, and entrepreneurial skills are the main\ndeterminants of SMEs' performance. This study highlights the importance of\nimproving the economic environment, promoting access to finance, and\ndeveloping entrepreneurial skills for SMEs. These findings offer valuable\ninsights for policymakers in Algeria and other oil- and gas-rich countries in\nthe region. Future research should focus on exam",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1693548387096774,
          "p": 0.2876712328767123,
          "f": 0.21319796487825
        },
        "rouge-2": {
          "r": 0.07027027027027027,
          "p": 0.1262135922330097,
          "f": 0.09027777318311174
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.273972602739726,
          "f": 0.2030456806142906
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2502.02619v1",
      "true_abstract": "This paper introduces a novel agent-based approach for enhancing existing\nportfolio strategies using Proximal Policy Optimization (PPO). Rather than\nfocusing solely on traditional portfolio construction, our approach aims to\nimprove an already high-performing strategy through dynamic rebalancing driven\nby PPO and Oracle agents. Our target is to enhance the traditional 60/40\nbenchmark (60% stocks, 40% bonds) by employing the Regret-based Sharpe reward\nfunction. To address the impact of transaction fee frictions and prevent signal\nloss, we develop a transaction cost scheduler. We introduce a future-looking\nreward function and employ synthetic data training through a circular block\nbootstrap method to facilitate the learning of generalizable allocation\nstrategies. We focus on two key evaluation measures: return and maximum\ndrawdown. Given the high stochasticity of financial markets, we train 20\nindependent agents each period and evaluate their average performance against\nthe benchmark. Our method not only enhances the performance of the existing\nportfolio strategy through strategic rebalancing but also demonstrates strong\nresults compared to other baselines.",
      "generated_abstract": "This paper introduces a new class of stochastic processes, called Markovian\nnon-Gaussian processes (MNGPs), which we characterize as a class of Markovian\ndiffusion processes. MNGPs are a generalization of diffusion processes and\ninclude some of the most popular stochastic processes such as Brownian motion and\nGaussian processes. The key feature of MNGPs is the presence of a drift term\nwhich is non-Gaussian and does not satisfy the usual Markov property. We show\nthat MNGPs are a generalization of standard Gaussian processes and of\nGaussian-Markov processes. We derive a general representation for the\ndrift-diffusion structure of MNGPs and give a characterization of the MNGP\nstructure. This characterization allows us to explicitly compute the drift and\ndiffusion terms for any MNGP. We also show that MNGPs are character",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12396694214876033,
          "p": 0.22058823529411764,
          "f": 0.15873015412334493
        },
        "rouge-2": {
          "r": 0.025157232704402517,
          "p": 0.03571428571428571,
          "f": 0.02952029035334566
        },
        "rouge-l": {
          "r": 0.11570247933884298,
          "p": 0.20588235294117646,
          "f": 0.14814814354133438
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.04246v1",
      "true_abstract": "Bayesian inference has many advantages for complex models. However, standard\nMonte Carlo methods for summarizing the posterior can be computationally\ndemanding, and it is attractive to consider optimization-based variational\napproximations. Our work considers Gaussian approximations with sparse\nprecision matrices which are tractable to optimize in high-dimensional\nproblems. Although the optimal Gaussian approximation is usually defined as the\none closest to the target posterior in Kullback-Leibler divergence, it is\nuseful to consider other divergences when the Gaussian assumption is crude, in\norder to capture important features of the posterior for a given application.\nOur work studies the weighted Fisher divergence, which focuses on gradient\ndifferences between the target posterior and its approximation, with the Fisher\nand score-based divergences being special cases. We make three main\ncontributions. First, we compare approximations for weighted Fisher divergences\nunder mean-field assumptions for both Gaussian and non-Gaussian targets with\nKullback-Leibler approximations. Second, we go beyond mean-field and consider\napproximations with sparse precision matrices reflecting posterior conditional\nindependence structure for hierarchical models. Using stochastic gradient\ndescent to enforce sparsity, we develop two approaches to minimize the weighted\nFisher divergence, based on the reparametrization trick and a batch\napproximation of the objective. Finally, we examine the performance of our\nmethods for examples involving logistic regression, generalized linear mixed\nmodels and stochastic volatility models.",
      "generated_abstract": "In this work we present a novel method for the analysis of time series data\nwith a continuous-time Markov process (CTMP) using the method of moments. We\nfocus on the analysis of the variance of a CTMP and its application to\nidentifying the underlying state transition matrix. We derive the first order\nconditions of the process and show how the variance of the process can be\ncalculated using these conditions. We provide a simple algorithm to estimate\nthe state transition matrix using these conditions. We also discuss the\npotential application of this method to the analysis of a wide range of\nstatistical problems. Finally, we present a numerical example and discuss the\napplications of our method to the analysis of the daily closing prices of\nS&P500 stocks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1450381679389313,
          "p": 0.2835820895522388,
          "f": 0.1919191874415877
        },
        "rouge-2": {
          "r": 0.03535353535353535,
          "p": 0.06796116504854369,
          "f": 0.046511623405040114
        },
        "rouge-l": {
          "r": 0.13740458015267176,
          "p": 0.26865671641791045,
          "f": 0.18181817734057762
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09865v1",
      "true_abstract": "Remote driving of vehicles is gaining in importance in the transportation\nsector, especially when Automated Driving Systems (ADSs) reach the limits of\ntheir system boundaries. This study investigates the challenges faced by human\nRemote Drivers (RDs) during remote driving, particularly focusing on the\nidentification and classification of human performance-related challenges\nthrough a comprehensive analysis of real-world remote driving data Las Vegas.\nFor this purpose, a total of 183 RD performance-related Safety Driver (SD)\ninterventions were analyzed and classified using an introduced severity\nclassification. As it is essential to prevent the need for SD interventions,\nthis study identified and analyzed harsh driving events to detect an increased\nlikelihood of interventions by the SD. In addition, the results of the\nsubjective RD questionnaire are used to evaluate whether the objective metrics\nfrom SD interventions and harsh driving events can also be confirmed by the RDs\nand whether additional challenges can be uncovered. The analysis reveals\nlearning curves, showing a significant decrease in SD interventions as RD\nexperience increases. Early phases of remote driving experience, especially\nbelow 200 km of experience, showed the highest frequency of safety-related\nevents, including braking too late for traffic signs and responding impatiently\nto other traffic participants. Over time, RDs follow defined rules for\nimproving their control, with experience leading to less harsh braking,\nacceleration, and steering maneuvers. The study contributes to understanding\nthe requirements of RDS, emphasizing the importance of targeted training to\naddress human performance limitations. It further highlights the need for\nsystem improvements to address challenges like latency and the limited haptic\nfeedback replaced by visual feedback, which affect the RDs' perception and\nvehicle control.",
      "generated_abstract": "We consider a scenario where a multi-agent system must determine an optimal\nselection of its actions, based on a dynamic set of constraints, which are\nsatisfied by a set of target states. The set of constraints is unknown and the\nsystem must make an initial selection, which it then updates in real-time. The\nsystem must also determine an optimal trajectory that is feasible for the\nconstraints. The problem is referred to as the constrained optimal\nselection-trajectory problem. We propose a framework for solving this problem\nby using a dynamic programming approach. We prove that the problem is\nundecidable, and that the optimal solution can be computed in polynomial time.\nWe also present an algorithm for solving the problem that uses a novel\napproach, which we call the sub-optimal sub-graph method. This method is\ncomputationally efficient and is provably optimal for the case where the\nconstraints are simple, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1329479768786127,
          "p": 0.2804878048780488,
          "f": 0.1803921524995003
        },
        "rouge-2": {
          "r": 0.007662835249042145,
          "p": 0.014925373134328358,
          "f": 0.010126577795355291
        },
        "rouge-l": {
          "r": 0.12716763005780346,
          "p": 0.2682926829268293,
          "f": 0.17254901524459834
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.07461v1",
      "true_abstract": "We study the optimal management of a photovoltaic system's battery owned by a\nself-consumption group that aims to minimize energy consumption costs. We\nassume that the photovoltaic system is composed of a photovoltaic panel and a\nbattery, where the photovoltaic panel produces energy according to a certain\nstochastic process. The management of the battery is the responsibility of a\ngroup administrator, who makes the joint decision to either store part of the\nphotovoltaic energy production and sell the remaining energy at the electricity\nspot price, or discharge part of the energy stored in the battery and sell it\nin the electricity market. Inspired by European Union and Italian legislation,\nwhich promote incentives for energy transition and renewable energy production,\nwe assume that the group receives a monetary incentive for the virtual\nself-consumed energy, defined as the minimum between the power bought from the\ngrid to satisfy the group's power demand and the energy sold to the market. In\nthis case, the energy sold by the group is a mix of part of the photovoltaic\nproduction that is not stored and part of the energy discharged from the\nbattery. We model the problem as a stochastic optimal control problem, where\nthe optimal strategy is the joint charge-discharge decision that minimizes the\ngroup's energy consumption costs. We find the solution numerically by applying\na finite difference scheme to solve the Hamilton-Jacobi-Bellman equation\nassociated with the value function of the optimal control problem.",
      "generated_abstract": "This paper explores how technology and policy changes have affected the\ntrade in commodities and the geographical patterns of production, consumption,\nand trade. We examine the impact of technological developments on the\ndistribution of production, consumption, and trade, and the extent to which\nthese developments have affected the global trade in commodities. The paper\nassesses the impact of technological advancements on production, consumption,\nand trade in commodities, and the impact of these developments on the\ndistribution of production, consumption, and trade. Additionally, the paper\nexplores the impact of technological developments on the distribution of\nproduction, consumption, and trade, and the extent to which these developments\nhave affected the global trade in commodities. The paper explores the impact of\ntechnological developments on production, consumption, and trade in commodities\nand the impact of these developments on the distribution of production",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08695652173913043,
          "p": 0.2631578947368421,
          "f": 0.1307189505147594
        },
        "rouge-2": {
          "r": 0.005,
          "p": 0.017241379310344827,
          "f": 0.007751934499130225
        },
        "rouge-l": {
          "r": 0.06086956521739131,
          "p": 0.18421052631578946,
          "f": 0.09150326424024963
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/EM/2501.07615v1",
      "true_abstract": "Climate change is increasing the frequency and severity of natural disasters\nworldwide. Media coverage of these events may be vital to generate empathy and\nmobilize global populations to address the common threat posed by climate\nchange. Using a dataset of 466 news sources from 123 countries, covering 135\nmillion news articles since 2016, we apply an event study framework to measure\ncross-border media activity following natural disasters. Our results shows that\nwhile media attention rises after disasters, it is heavily skewed towards\ncertain events, notably earthquakes, accidents, and wildfires. In contrast,\nclimatologically salient events such as floods, droughts, or extreme\ntemperatures receive less coverage. This cross-border disaster reporting is\nstrongly related to the number of deaths associated with the event, especially\nwhen the affected populations share strong social ties or genetic similarities\nwith those in the reporting country. Achieving more balanced media coverage\nacross different types of natural disasters may be essential to counteract\nskewed perceptions. Further, fostering closer social connections between\ncountries may enhance empathy and mobilize the resources necessary to confront\nthe global threat of climate change.",
      "generated_abstract": "We develop a new measure of economic performance that integrates\nthe contribution of productivity growth with the impact of public investment.\nUsing a large-scale dataset of 50 countries over 2000-2020, we compare the\nperformance of countries with and without public investment. We find that\ninvestment growth is crucial for a country's economic performance, but public\ninvestment can have a positive or negative effect, depending on the circumstances\nof the country. We identify three broad groups of countries: those with high\nproductivity growth and low investment, those with high productivity growth and\nhigh investment, and those with low productivity growth and high investment. We\nalso show that the impact of public investment on economic growth varies across\ncountries. This suggests that a country's economic performance may be more\nsensitive to public investment than previously thought.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16153846153846155,
          "p": 0.30434782608695654,
          "f": 0.21105527185172104
        },
        "rouge-2": {
          "r": 0.023391812865497075,
          "p": 0.03571428571428571,
          "f": 0.028268546454070655
        },
        "rouge-l": {
          "r": 0.14615384615384616,
          "p": 0.2753623188405797,
          "f": 0.1909547693391582
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.03883v1",
      "true_abstract": "To advance the understanding of cellular metabolisms and control\nbatch-to-batch variations in cell culture processes, a multi-scale mechanistic\nmodel with a bottom-up and top-down structure was developed to simulate the\ndynamics of cell culture process undergoing metabolic state transitions. This\nmodel integrates interactions at the molecular, cellular, and macro-kinetic\nlevels, accounting for inherent variations in metabolic state transitions of\nindividual cells. By incorporating both online (e.g., oxygen uptake, pH) and\noffline measurements (e.g., viable cell density, metabolite concentrations),\nthe proposed mechanistic model enables accurate long-term prediction of cell\nculture trajectories and provides reliable prediction intervals quantifying\nbatch-to-batch variations. This work can guide optimal design of experiments\nand robust process control to improve yield and production stability.\nAdditionally, the proposed multi-scale model has a modular design enables\nflexible in silico simulations and extrapolation across diverse conditions,\nproviding a robust prediction framework for scalable and flexible\nbiomanufacturing applications.",
      "generated_abstract": "The study of genetic networks, including gene regulatory networks, protein\nnetworks, and complex systems, is essential for understanding the function of\nmolecular systems and complex biological systems. The objective of this paper\nis to provide a framework for analyzing genetic networks using a combination of\nmathematical modeling and statistical analysis. The framework is based on the\nwell-known stochastic differential equation (SDE) modeling approach, which\nis commonly used to describe the evolution of gene regulatory networks. By\nintroducing a novel concept of local connectivity, the paper provides a\nframework for analyzing gene regulatory networks and protein networks using\nSDE modeling. This framework allows for a more nuanced analysis of gene\nregulatory networks, as it allows for the consideration of local regulatory\neffects and the influence of gene regulatory networks on other gene regulatory\nnetworks. Additionally, the framework provides a framework for analyz",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12244897959183673,
          "p": 0.17391304347826086,
          "f": 0.14371257000107585
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.01680672268907563,
          "f": 0.015564197361960136
        },
        "rouge-l": {
          "r": 0.11224489795918367,
          "p": 0.15942028985507245,
          "f": 0.13173652209688425
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/space-ph/2503.00620v1",
      "true_abstract": "This study explores the generation of Electrostatic (ES) Electron\nKelvin-Helmholtz instability (EKHI) in collisionless plasma with a\nstep-function electron velocity shear akin to that developed in the electron\ndiffusion region in magnetic reconnection. In incompressible plasma, ES EKHI\ndoesn't arise in any velocity shear profile due to the decoupling of the\nelectric potential from the electron momentum equation. Instead a fluid-like\nKelvin-Helmholtz instability (KHI) can arise. However, in compressible plasma,\nthe compressibility couples the electric potential with the electron dynamics,\nleading to the emergence of a new ES mode EKHI on Debye length $\\lambda_{De}$,\naccompanied by the co-generation of an electron acoustic-like wave. The minimum\nthreshold of ES EKHI is $\\Delta \\mathbf{U}> 2c_{se}$, i.e., the electron\nvelocity shear larger than twice the electron acoustic speed $c_{se}$. The\ncorresponding growth rate is $Im(\\omega) = ((\\Delta \\mathbf{U}/c_{se})^2 -\n4)^{1/2} \\omega_{pe}$, where $\\omega_{pe}$ is the electron plasma frequency.",
      "generated_abstract": "In this paper, we present a novel approach for modeling the radiation\ndynamics of a magnetized plasma in a gravitational field. The model is based on\nthe energy-momentum tensor of the Maxwell-Einstein-Vlasov system, which is\ncalculated by means of the method of moments. The new model allows for the\nsimultaneous treatment of the radiation and the plasma dynamics, providing\nrealistic radiation distributions and plasma structures. In addition, the\nmethod of moments is used to perform the numerical integration of the\nenergy-momentum tensor, which allows for accurate calculations of the\nradiation spectra and to accurately predict the evolution of the radiation\nfields during the propagation of electromagnetic waves. In our model, the\ngravitational field is represented by a perfect fluid, which allows us to\ncalculate the gravitational wave radiation spectrum. The model is applied to\nthe radiation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14432989690721648,
          "p": 0.19444444444444445,
          "f": 0.1656804684821961
        },
        "rouge-2": {
          "r": 0.022556390977443608,
          "p": 0.02586206896551724,
          "f": 0.024096380565475778
        },
        "rouge-l": {
          "r": 0.12371134020618557,
          "p": 0.16666666666666666,
          "f": 0.14201182942894175
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.physics/comp-ph/2503.10263v1",
      "true_abstract": "A new parallelized simulation code is presented, which uses a Monte Carlo\nmethod to determine particle spectra in the KATRIN source. Reaction chains are\ngenerated from the decay of tritium within the source. The code includes all\nrelevant processes: elastic scattering, ionization, excitation (electric,\nvibrational, rotational), recombination and various clustering processes. The\nmain emphasis of the code is the calculation of particle spectra and particle\ndensities and currents at specific points within the source. It features a new\ntechnique to determine these quantities. It also calculates target fields for\nthe interaction of particles with each other as it is needed for recombination\nprocesses. The code has been designed for the KATRIN experiment but is easily\nadapt-able for other tritium based experiments like Project 8. Geometry and\nbackground tritium gas flow can be given as user input. The code is\nparallelized using MPI and writes output using HDF5. Input to the simulation is\nread from a JSON description.",
      "generated_abstract": "This article presents a novel methodology for constructing a two-dimensional\n(2D) periodic structure for a 1D-input, 1D-output model. By exploiting the\nuniversality of the hyperbolic tangent function, this method allows for the\nrealization of a periodic structure that can be tuned to any desired frequency\nby varying the value of the input voltage. The resulting periodic structure\nyields a 2D network that can be used to generate and analyze complex waveforms\nin the absence of any nonlinearity. The generated waveforms can be used to\ngenerate and analyze any desired frequency response in the absence of any\nnonlinearity. In addition, the periodic structure is also capable of\ngenerating and analyzing any desired frequency response. Furthermore, the\ngenerated waveforms and frequency responses can be used to generate and analyze\nany desired frequency response. In addition, the periodic structure is also\ncapable of generating",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14563106796116504,
          "p": 0.23076923076923078,
          "f": 0.17857142382723934
        },
        "rouge-2": {
          "r": 0.027586206896551724,
          "p": 0.04081632653061224,
          "f": 0.032921805886637234
        },
        "rouge-l": {
          "r": 0.13592233009708737,
          "p": 0.2153846153846154,
          "f": 0.16666666192247745
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.08452v1",
      "true_abstract": "We propose Knowledge-Aware Preprocessing (KAP), a two-stage preprocessing\nframework tailored for Traditional Chinese non-narrative documents, designed to\nenhance retrieval accuracy in Hybrid Retrieval systems. Hybrid Retrieval, which\nintegrates Sparse Retrieval (e.g., BM25) and Dense Retrieval (e.g., vector\nembeddings), has become a widely adopted approach for improving search\neffectiveness. However, its performance heavily depends on the quality of input\ntext, which is often degraded when dealing with non-narrative documents such as\nPDFs containing financial statements, contractual clauses, and tables. KAP\naddresses these challenges by integrating Multimodal Large Language Models\n(MLLMs) with LLM-driven post-OCR processing, refining extracted text to reduce\nOCR noise, restore table structures, and optimize text format. By ensuring\nbetter compatibility with Hybrid Retrieval, KAP improves the accuracy of both\nSparse and Dense Retrieval methods without modifying the retrieval architecture\nitself.",
      "generated_abstract": "We introduce the first formalism for the representation and analysis of\nnon-interactive temporal logic (NTTL). NTTL is a language of temporal\npredicates that expresses temporal relations between events and can be used to\ndefine temporal reasoning tasks. We present a novel axiomatization of NTTL and\nshow that it is equivalent to the standard form of NTTL. We prove that\nnon-interactive temporal logic with refinement (NTTLR) is expressible in the\nformalism, and we present a refinement semantics for NTTLR that extends\npreviously known semantics for NTTL. We also show that NTTLR is decidable, and\nthat it can be reduced to NTTL. Finally, we present a decidable refinement\nsemantics for NTTLR and show that it is sound and complete with respect to the\nformalism. This completeness result answers",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09345794392523364,
          "p": 0.15625,
          "f": 0.11695905964365122
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08411214953271028,
          "p": 0.140625,
          "f": 0.1052631532109027
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2503.03248v1",
      "true_abstract": "We introduce and study a new theoretical concept of $\\textit{spectral pair}$\nfor a Schr\\\"{o}dinger operator $H$ in $L^2(\\mathbb{R}_{+})$ with a bounded\n$\\textit{complex-valued}$ potential. The spectral pair consists of a measure\nand a complex-valued function, both of which are defined on $\\mathbb{R}_{+}$.\nWe show that in many ways, the spectral pair generalises the classical spectral\nmeasure to the non-self-adjoint case. First, extending the classical\nBorg-Marchenko theorem, we prove a uniqueness result: the spectral pair\nuniquely determines the potential. Second, we derive asymptotic formulas for\nthe spectral pair in the spirit of the classical result of Marchenko. In the\ncase of real-valued potentials, we relate the spectral pair to the spectral\nmeasure of $H$. Lastly, we provide formulas for the spectral pair at a~simple\neigenvalue of $|H|$.",
      "generated_abstract": "We prove the conjecture of P. A. Shnirelman that for every $n \\geq 1$ and every\n$d \\geq 2$ there exists a positive constant $C(n,d)$ such that for every\n$m \\leq C(n,d) n^2$ there exists a non-isomorphic, non-hyperbolic $d$-regular\ngraph on $m$ vertices. The theorem also implies that for every $d \\geq 2$ there\nexists a non-isomorphic, non-hyperbolic $d$-regular graph on $n$ vertices for\nevery $n \\geq 1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14084507042253522,
          "p": 0.25,
          "f": 0.18018017557016489
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14084507042253522,
          "p": 0.25,
          "f": 0.18018017557016489
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.04851v2",
      "true_abstract": "In protein engineering, while computational models are increasingly used to\npredict mutation effects, their evaluations primarily rely on high-throughput\ndeep mutational scanning (DMS) experiments that use surrogate readouts, which\nmay not adequately capture the complex biochemical properties of interest. Many\nproteins and their functions cannot be assessed through high-throughput methods\ndue to technical limitations or the nature of the desired properties, and this\nis particularly true for the real industrial application scenario. Therefore,\nthe desired testing datasets, will be small-size (~10-100) experimental data\nfor each protein, and involve as many proteins as possible and as many\nproperties as possible, which is, however, lacking. Here, we present\nVenusMutHub, a comprehensive benchmark study using 905 small-scale experimental\ndatasets curated from published literature and public databases, spanning 527\nproteins across diverse functional properties including stability, activity,\nbinding affinity, and selectivity. These datasets feature direct biochemical\nmeasurements rather than surrogate readouts, providing a more rigorous\nassessment of model performance in predicting mutations that affect specific\nmolecular functions. We evaluate 23 computational models across various\nmethodological paradigms, such as sequence-based, structure-informed and\nevolutionary approaches. This benchmark provides practical guidance for\nselecting appropriate prediction methods in protein engineering applications\nwhere accurate prediction of specific functional properties is crucial.",
      "generated_abstract": "The analysis of the population dynamics of a cellular population is\na central problem in biology, with many applications in evolutionary\nbiology, developmental biology, and disease research. This work introduces a\nnovel approach for studying population dynamics, based on the concept of\ndynamical systems. The key insight is that a cellular population can be\nviewed as a dynamical system with a continuous-time dynamics. The dynamics\ncan be discretized into discrete-time dynamics, which can be solved using\nstandard methods. The discrete-time dynamics can then be integrated into a\ncontinuous-time dynamics, which can be solved using standard methods. This\napproach allows us to analyze the population dynamics in a new way. We show\nthat the population dynamics can be understood as the evolution of an\ninteracting pair of population dynamics, where the dynamics of one population\ndepends on the dynamics of the other. We then show how to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13815789473684212,
          "p": 0.29577464788732394,
          "f": 0.18834080283456348
        },
        "rouge-2": {
          "r": 0.00510204081632653,
          "p": 0.008333333333333333,
          "f": 0.006329109213270611
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.28169014084507044,
          "f": 0.17937219296909257
        }
      }
    },
    {
      "paper_id": "cs.SI.cs/SI/2503.09725v1",
      "true_abstract": "Avian Influenza Virus (AIV) poses significant threats to the poultry\nindustry, humans, domestic animals, and wildlife health worldwide. Monitoring\nthis infectious disease is important for rapid and effective response to\npotential outbreaks. Conventional avian influenza surveillance systems have\nexhibited limitations in providing timely alerts for potential outbreaks. This\nstudy aimed to examine the idea of using online activity on social media, and\nGoogle searches to improve the identification of AIV in the early stage of an\noutbreak in a region. To this end, to evaluate the feasibility of this\napproach, we collected historical data on online user activities from X\n(formerly known as Twitter) and Google Trends and assessed the statistical\ncorrelation of activities in a region with the AIV outbreak officially reported\ncase numbers. In order to mitigate the effect of the noisy content on the\noutbreak identification process, large language models were utilized to filter\nout the relevant online activity on X that could be indicative of an outbreak.\nAdditionally, we conducted trend analysis on the selected internet-based data\nsources in terms of their timeliness and statistical significance in\nidentifying AIV outbreaks. Moreover, we performed an ablation study using\nautoregressive forecasting models to identify the contribution of X and Google\nTrends in predicting AIV outbreaks. The experimental findings illustrate that\nonline activity on social media and search engine trends can detect avian\ninfluenza outbreaks, providing alerts earlier compared to official reports.\nThis study suggests that real-time analysis of social media outlets and Google\nsearch trends can be used in avian influenza outbreak early warning systems,\nsupporting epidemiologists and animal health professionals in informed\ndecision-making.",
      "generated_abstract": "The rapid rise of blockchain technologies has fueled the emergence of\nblockchain-based systems. The emergence of these systems has presented new\nchallenges in terms of data security and privacy. Blockchain systems are\noften deployed in a cloud environment, which presents its own unique security\nand privacy challenges. In this paper, we study the security and privacy of\nblockchain systems deployed in a cloud environment, focusing on the security\nand privacy of the blockchain database and the blockchain network. To address\nthese issues, we propose a novel architecture, called Blockchain in the Cloud\n(BiTC), which integrates blockchain systems with traditional cloud services\nand security mechanisms. Our approach allows for the integration of blockchain\nsystems with traditional cloud services, thereby increasing the scalability of\nblockchain systems. Additionally, we propose a novel architecture, called\nBlockchain in the Cloud (BiTC), which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12101910828025478,
          "p": 0.2835820895522388,
          "f": 0.16964285295001602
        },
        "rouge-2": {
          "r": 0.028688524590163935,
          "p": 0.06862745098039216,
          "f": 0.040462423587824946
        },
        "rouge-l": {
          "r": 0.12101910828025478,
          "p": 0.2835820895522388,
          "f": 0.16964285295001602
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.14154v1",
      "true_abstract": "In allocating objects via lotteries, it is common to consider ordinal rules\nthat rely solely on how agents rank degenerate lotteries. While ordinality is\noften imposed due to cognitive or informational constraints, we provide another\njustification from an axiomatic perspective: for three-agent problems, the\ncombination of efficiency, strategy-proofness, non-bossiness, and a weak form\nof continuity collectively implies ordinality.",
      "generated_abstract": "We consider a network of agents, where each agent has a single type of\ninterest, and a single agent can be interested in a single type of interest.\nAgents can only interact with each other through exogenous events. We study the\nsituation where agents can only choose between the two types of interest,\nindependently of each other. We show that, in this case, the only Nash\nequilibrium is the one where the agents are indifferent between the two types of\ninterest. This is not surprising, as this is the only case where the\ninterest-dependent exogenous events can be eliminated. We also show that the\nexistence of the Nash equilibrium is not guaranteed in the presence of\ninteraction effects. However, we show that the situation can be improved to\nsituations where the agents are indifferent between the two types of interest\nonly in a certain range of the strength of interaction effects.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.14705882352941177,
          "f": 0.16393442129535088
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.008928571428571428,
          "f": 0.0118343150561972
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.1323529411764706,
          "f": 0.14754097867240004
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.12731v1",
      "true_abstract": "We address counterfactual analysis in empirical models of games with\npartially identified parameters, and multiple equilibria and/or randomized\nstrategies, by constructing and analyzing the counterfactual predictive\ndistribution set (CPDS). This framework accommodates various outcomes of\ninterest, including behavioral and welfare outcomes. It allows a variety of\nchanges to the environment to generate the counterfactual, including\nmodifications of the utility functions, the distribution of utility\ndeterminants, the number of decision makers, and the solution concept. We use a\nBayesian approach to summarize statistical uncertainty. We establish conditions\nunder which the population CPDS is sharp from the point of view of\nidentification. We also establish conditions under which the posterior CPDS is\nconsistent if the posterior distribution for the underlying model parameter is\nconsistent. Consequently, our results can be employed to conduct counterfactual\nanalysis after a preliminary step of identifying and estimating the underlying\nmodel parameter based on the existing literature. Our consistency results\ninvolve the development of a new general theory for Bayesian consistency of\nposterior distributions for mappings of sets. Although we primarily focus on a\nmodel of a strategic game, our approach is applicable to other structural\nmodels with similar features.",
      "generated_abstract": "We consider the case in which there is a hidden random parameter that determines\nthe mean of a random vector, and the distribution of the mean is unknown. We\nintroduce a novel class of nonparametric methods for estimating this hidden\nparameter. We derive an efficient estimator, and derive an oracle estimator\nthat is optimal in the sense that the relative error between the estimator and\nthe oracle is minimized. We illustrate the effectiveness of our method in\nsimulation studies and in a real-data analysis of the stock market. Our\napproach is general and applicable to a wide range of models, including those\nin which the mean and the covariance matrix are unknown.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17094017094017094,
          "p": 0.31746031746031744,
          "f": 0.22222221767222233
        },
        "rouge-2": {
          "r": 0.0446927374301676,
          "p": 0.07692307692307693,
          "f": 0.056537097824670444
        },
        "rouge-l": {
          "r": 0.1623931623931624,
          "p": 0.30158730158730157,
          "f": 0.2111111065611112
        }
      }
    },
    {
      "paper_id": "math.AP.physics/class-ph/2503.10300v1",
      "true_abstract": "We derive a new approach to analyze the coupling of linear Boussinesq and\nSaint-Venant shallow water wave equations in the case where the interface\nremains at a constant position in space. We propose a one-way coupling model as\na reference, which allows us to obtain an analytical solution, prove the\nwell-posedness of the original coupled model and compute what we call the\ncoupling error-a quantity that depends solely on the choice of transmission\nconditions at the interface. We prove that this coupling error is\nasymptotically small for a certain class of data and discuss its role as a\nproxy for the full error with respect to the 3D water wave problem.\nAdditionally, we highlight that this error can be easily computed in other\nscenarios. We show that the coupling error consists of reflected waves and\nargue that this explains some previously unexplained spurious oscillations\nreported in the literature. Finally, we prove the well-posedness of the\nhalf-line linear Boussinesq problem.",
      "generated_abstract": "We study the gravitational wave (GW) emission of a neutron star at the\ntime of its merger with a black hole, considering the inspiral and merger\nregimes. We adopt a non-rotating and a non-spinning NS, and perform a\ntwo-dimensional (2D) numerical simulation of the GW emission of the NS,\nincluding its spin, and the NS-black hole system. We also consider the\nspinning NS, where we show that the GW emission of the NS is dominated by its\nspin, rather than by its spin-orbit coupling with the NS-black hole system.\nWe also discuss the GW emission of the black hole after the merger, and the\nGW emission of the NS-black hole system after the merger. We conclude that the\nspin of the NS significantly contributes to the GW emission of the NS-black\nhole system after",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17525773195876287,
          "p": 0.2833333333333333,
          "f": 0.2165605048318391
        },
        "rouge-2": {
          "r": 0.034722222222222224,
          "p": 0.052083333333333336,
          "f": 0.04166666186666723
        },
        "rouge-l": {
          "r": 0.12371134020618557,
          "p": 0.2,
          "f": 0.15286623731591562
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/ST/2503.00603v1",
      "true_abstract": "Signature methods have been widely and effectively used as a tool for feature\nextraction in statistical learning methods, notably in mathematical finance.\nThey lack, however, interpretability: in the general case, it is unclear why\nsignatures actually work. The present article aims to address this issue\ndirectly, by introducing and developing the concept of signature perturbations.\nIn particular, we construct a regular perturbation of the signature of the term\nstructure of log prices for various commodities, in terms of the convenience\nyield. Our perturbation expansion and rigorous convergence estimates help\nexplain the success of signature-based classification of commodities markets\naccording to their term structure, with the volatility of the convenience yield\nas the major discriminant.",
      "generated_abstract": "We consider a portfolio of stocks that is expected to return a mean-variance\ntarget. We study the problem of determining the optimal portfolio allocation\nwhen the target is a quadratic function of the stock returns. We propose an\nalgorithm that uses a randomized step-size gradient descent algorithm, where the\nstepsizes are chosen as a function of the portfolio's expected return. We show\nthat the proposed algorithm converges to a stationary point of the objective\nfunction. We show that the proposed algorithm is stable under mild assumptions\non the objective function. We also show that the proposed algorithm has a\nproperly selected step-size. We provide theoretical guarantees on the\nperformance of the proposed algorithm. We also provide numerical experiments\ndemonstrating the performance of the proposed algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07058823529411765,
          "p": 0.09836065573770492,
          "f": 0.08219177595702787
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.020202020202020204,
          "f": 0.019323666507037623
        },
        "rouge-l": {
          "r": 0.07058823529411765,
          "p": 0.09836065573770492,
          "f": 0.08219177595702787
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.02397v1",
      "true_abstract": "Many data problems contain some reference or normal conditions, upon which to\ncompare newly collected data. This scenario occurs in data collected as part of\nclinical trials to detect adverse events, or for measuring climate change\nagainst historical norms. The data is typically multivariate, and often the\nnormal ranges are specified by a multivariate normal distribution. The work\npresented in this paper develops methods to compare the new sample against the\nreference distribution with high-dimensional visualisation. It uses a\nprojection pursuit guided tour to produce a sequence of low-dimensional\nprojections steered towards those where the new sample is most different from\nthe reference. A new projection pursuit index is defined for this purpose. The\ntour visualisation also includes drawing of the projected ellipse, which is\ncomputed analytically, corresponding to the reference distribution. The methods\nare implemented in the R package, tourr.",
      "generated_abstract": "In a real-world setting, a decision-maker may be faced with a trade-off\nbetween two options, each with a different expected value. This trade-off can\nbe expressed in the form of a utility function, and the decision-maker is\ninterested in determining the value of the utility function. The value of the\nutility function can be expressed as the expected value of the utility function\nunder the null hypothesis, which is the probability of the utility function\nhitting a particular level. This paper considers the problem of determining the\nvalue of the utility function in the context of a decision-maker with\nnon-probability beliefs, and provides a framework for determining the value of\nthe utility function. This framework is based on the concept of the value of\nthe utility function, which is defined as the value of the utility function\nunder the null hypothesis. We derive the value of the utility function for\nspecific cases, including",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16304347826086957,
          "p": 0.25,
          "f": 0.19736841627423835
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.039603960396039604,
          "f": 0.034334759037006356
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.23333333333333334,
          "f": 0.18421052153739625
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00945v1",
      "true_abstract": "Deep learning-based computer-aided diagnosis (CAD) of medical images requires\nlarge datasets. However, the lack of large publicly available labeled datasets\nlimits the development of deep learning-based CAD systems. Generative\nAdversarial Networks (GANs), in particular, CycleGAN, can be used to generate\nnew cross-domain images without paired training data. However, most\nCycleGAN-based synthesis methods lack the potential to overcome alignment and\nasymmetry between the input and generated data. We propose a two-stage\ntechnique for the synthesis of abdominal MRI using cross-modality translation\nof abdominal CT. We show that the synthetic data can help improve the\nperformance of the liver segmentation network. We increase the number of\nabdominal MRI images through cross-modality image transformation of unpaired CT\nimages using a CycleGAN inspired deformation invariant network called EssNet.\nSubsequently, we combine the synthetic MRI images with the original MRI images\nand use them to improve the accuracy of the U-Net on a liver segmentation task.\nWe train the U-Net on real MRI images and then on real and synthetic MRI\nimages. Consequently, by comparing both scenarios, we achieve an improvement in\nthe performance of U-Net. In summary, the improvement achieved in the\nIntersection over Union (IoU) is 1.17%. The results show potential to address\nthe data scarcity challenge in medical imaging.",
      "generated_abstract": "Diffusion models have revolutionized image generation, but their ability to\nmodel natural surfaces and textures has remained limited, in part due to\ndifficulty in capturing complex surface geometries. This work presents a\nmethodology for training diffusion models to generate natural surface\nrepresentations. Our approach employs a hybrid diffusion model to generate\nsurfaces using diffusion maps and a 3D surface model to guide the diffusion\nprocess. We introduce a new objective that leverages the 3D surface model to\npenalize diffusion maps that do not follow the surface geometry. This objective\nenables the model to learn natural surface representations while preserving\nsurface integrity. Our approach demonstrates significant improvements in\nrepresenting natural surfaces, particularly in areas like texture detail and\ntexture variation. This work paves the way for more sophisticated and realistic\ntexture synthesis and surface generation applications in 3D medical imaging.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13008130081300814,
          "p": 0.1927710843373494,
          "f": 0.15533980101376207
        },
        "rouge-2": {
          "r": 0.0106951871657754,
          "p": 0.015748031496062992,
          "f": 0.01273884868574972
        },
        "rouge-l": {
          "r": 0.12195121951219512,
          "p": 0.18072289156626506,
          "f": 0.14563106314968438
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.14934v1",
      "true_abstract": "Molecular docking that predicts the bound structures of small molecules\n(ligands) to their protein targets, plays a vital role in drug discovery.\nHowever, existing docking methods often face limitations: they either overlook\ncrucial structural changes by assuming protein rigidity or suffer from low\ncomputational efficiency due to their reliance on generative models for\nstructure sampling. To address these challenges, we propose FABFlex, a fast and\naccurate regression-based multi-task learning model designed for realistic\nblind flexible docking scenarios, where proteins exhibit flexibility and\nbinding pocket sites are unknown (blind). Specifically, FABFlex's architecture\ncomprises three specialized modules working in concert: (1) A pocket prediction\nmodule that identifies potential binding sites, addressing the challenges\ninherent in blind docking scenarios. (2) A ligand docking module that predicts\nthe bound (holo) structures of ligands from their unbound (apo) states. (3) A\npocket docking module that forecasts the holo structures of protein pockets\nfrom their apo conformations. Notably, FABFlex incorporates an iterative update\nmechanism that serves as a conduit between the ligand and pocket docking\nmodules, enabling continuous structural refinements. This approach effectively\nintegrates the three subtasks of blind flexible docking-pocket identification,\nligand conformation prediction, and protein flexibility modeling-into a\nunified, coherent framework. Extensive experiments on public benchmark datasets\ndemonstrate that FABFlex not only achieves superior effectiveness in predicting\naccurate binding modes but also exhibits a significant speed advantage (208\n$\\times$) compared to existing state-of-the-art methods. Our code is released\nat https://github.com/tmlr-group/FABFlex.",
      "generated_abstract": "We present a novel method to generate realistic and high-resolution\nmedical images based on a physically motivated model of tissue dynamics. The\nmethod uses a deep generative model, trained on real images, to generate\nsynthetic images that resemble the appearance of actual tissues. The model\ncaptures the complex interactions between tissue properties, such as\nstiffness, viscosity, and permeability, and the dynamics of cells,\nmicro-flows, and cellular aggregates, producing realistic images that accurately\nrepresent the properties of biological tissues. The method is illustrated with\ntwo case studies: (1) the generation of realistic images of skin, using a\nmulti-scale, physically motivated model of tissue dynamics, and (2) the\ngeneration of realistic images of blood vessels, using a novel model of\nvascular dynamics based on a combination of physically",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08771929824561403,
          "p": 0.2112676056338028,
          "f": 0.1239669380025273
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08187134502923976,
          "p": 0.19718309859154928,
          "f": 0.11570247519260994
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.09339v1",
      "true_abstract": "We report a single-frequency, narrow-linewidth semiconductor pulsed laser\nbased on pump current modulation and optical injection locking technique. A\nmonolithic non-planar ring oscillator laser is employed as the seed source to\nguarantee the single-frequency narrow-linewidth performance. Simultaneously,\npulse operation is achieved by directly modulating the pump current of the\nsemiconductor laser. The single-frequency pulsed laser (SFPL) has achieved a\npulse repetition rate of 50 kHz-1 MHz, a pulse duration ranging from 120 ns to\na quasi-continuous state, and a peak power of 160 mW. Moreover, the SFPL has\nreached a pulsed laser linewidth as narrow as 905 Hz, optical spectrum\nsignal-to-noise ratio of better than 65 dB at a center wavelength of 1064.45\nnm. Such extremely narrow-linewidth, repetition-rate and pulse-width tunable\nSFPL has great potential for applications in coherent LIDAR, metrology, remote\nsensing, and nonlinear frequency conversion.",
      "generated_abstract": "The nonlinear optics of semiconductor quantum dots (QDs) is a rapidly\nexpanding field of research, driven by their potential for quantum computing,\nspintronics, and sensing applications. However, the photonic properties of\nthese materials are highly nonlinear, and conventional techniques for measuring\nthe nonlinear response are often limited by the inability to operate at low\ntemperatures. In this work, we present a method for measuring the photonic\nproperties of QDs in the presence of an optical field at low temperatures.\nFocusing on the quantum-dot-quantum-dot (QD-QD) system, we introduce a\nnonlinear susceptibility that describes the response of the QDs to an\noptical field. Using this susceptibility, we can measure the response of the\nQDs to a nonlinear optical field in a temperature-controlled mic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.23943661971830985,
          "f": 0.19999999513564024
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.028846153846153848,
          "f": 0.025423723883942352
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.2112676056338028,
          "f": 0.17647058337093438
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.20745v1",
      "true_abstract": "Exposure to high ambient temperatures is a significant driver of preventable\nmortality, with non-linear health effects and elevated risks in specific\nregions. To capture this complexity and account for spatial dependencies across\nsmall areas, we propose a Bayesian framework that integrates non-linear\nfunctions with the Besag, York, and Mollie (BYM2) model. Applying this\nframework to all-cause mortality data in Switzerland, we quantified spatial\ninequalities in heat-related mortality. We retrieved daily all-cause mortality\nat small areas (2,145 municipalities) for people older than 65 years from the\nSwiss Federal Office of Public Health and daily mean temperature at\n1km$\\times$1km grid from the Swiss Federal Office of Meteorology. By fully\npropagating uncertainties, we derived key epidemiological metrics, including\nheat-related excess mortality and minimum mortality temperature (MMT).\nHeat-related excess mortality rates were higher in northern Switzerland, while\nlower MMTs were observed in mountainous regions. Further, we explored the role\nof the proportion of individuals older than 85 years, green space, average\ntemperature, deprivation, urbanicity, and language regions in explaining these\ndiscrepancies. We found that spatial disparities in heat-related excess\nmortality were primarily driven by population age distribution, green space,\nand vulnerabilities associated with elevated temperature exposure.",
      "generated_abstract": "A common approach for detecting and classifying complex patterns in data is\nto apply a neural network trained on a small dataset to predict the class of\nthe target data. However, this approach is only effective if the target data\nare drawn from the same distribution as the training data. We show that, in\nmany situations, this is not the case. We introduce a new model,\nRandom-Projection Neural Networks (RPNNs), which generalize the well-known\nrandom projection neural network (RPNN) to arbitrary dimensions, and we show\nthat it is possible to train RPNNs on a dataset from a different distribution\nthan the training dataset. We demonstrate the effectiveness of RPNNs in\nclassifying and detecting complex patterns by applying them to two applications:\n1) detecting and classifying patterns in the evolution of bacterial populations\nand 2) detecting and classifying patterns in the evolution of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.2125,
          "f": 0.1642512029872344
        },
        "rouge-2": {
          "r": 0.0111731843575419,
          "p": 0.016,
          "f": 0.013157889894608775
        },
        "rouge-l": {
          "r": 0.11811023622047244,
          "p": 0.1875,
          "f": 0.14492753148964985
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.17463v1",
      "true_abstract": "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
      "generated_abstract": "This paper provides a novel statistical framework for predictive\nmodel selection and uncertainty quantification in the context of\nhigh-dimensional linear regression models. We propose a novel approach that\nintegrates uncertainty quantification with model selection and addressing the\nchallenges of the model-selection criterion in high-dimensional settings. We\nalso introduce a novel model-selection criterion that takes into account the\npredicted values of the response. We provide theoretical guarantees for the\nproposed model selection criterion and its associated inference. We apply the\nproposed framework to a series of empirical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19402985074626866,
          "p": 0.24528301886792453,
          "f": 0.21666666173472232
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.025974025974025976,
          "f": 0.024096380568298312
        },
        "rouge-l": {
          "r": 0.13432835820895522,
          "p": 0.16981132075471697,
          "f": 0.1499999950680557
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.03515v1",
      "true_abstract": "We consider Inverse Optimal Stopping (IOS) problem where, based on stopped\nexpert trajectories, one aims to recover the optimal stopping region through\ncontinuation and stopping gain functions approximation. The uniqueness of the\nstopping region allows the use of IOS in real-world applications with safety\nconcerns. While current state-of-the-art inverse reinforcement learning methods\nrecover both a Q-function and the corresponding optimal policy, they fail to\naccount for specific challenges posed by optimal stopping problems. These\ninclude data sparsity near the stopping region, non-Markovian nature of the\ncontinuation gain, a proper treatment of boundary conditions, the need for a\nstable offline approach for risk-sensitive applications, and a lack of a\nquality evaluation metric. These challenges are addressed with the proposed\nDynamics-Aware Offline Inverse Q-Learning for Optimal Stopping (DO-IQS), which\nincorporates temporal information by approximating the cumulative continuation\ngain together with the world dynamics and the Q-function without querying to\nthe environment. Moreover, a confidence-based oversampling approach is proposed\nto treat the data sparsity problem. We demonstrate the performance of our\nmodels on real and artificial data including an optimal intervention for\ncritical events problem.",
      "generated_abstract": "This paper presents an efficient and accurate approach for predicting the\npotential of a new drug using a large-scale drug-disease relationship database.\nIn the field of drug discovery, the identification of potential therapeutic\nagents is a critical step for the development of new drugs. In this work, we\npropose a novel approach for predicting the potential of a new drug using a\nlarge-scale drug-disease relationship database. The proposed method uses the\nrelationship between drug and disease to predict the potential of a new drug.\nThe method consists of three key steps: (1) selecting the best drug-disease\nrelationship, (2) constructing a drug-disease relationship database based on\nthese selected relationships, and (3) training a neural network to predict the\npotential of a new drug. The proposed method is validated using a large-scale\ndrug-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11864406779661017,
          "p": 0.21212121212121213,
          "f": 0.15217390844281678
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.030927835051546393,
          "f": 0.022222217618382296
        },
        "rouge-l": {
          "r": 0.11864406779661017,
          "p": 0.21212121212121213,
          "f": 0.15217390844281678
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.18875v1",
      "true_abstract": "We study competitive equilibria in exchange economies when a continuum of\ngoods is conflated into a finite set of commodities. The design of conflation\nchoices affects the allocation of scarce resources among agents, by\nconstraining trading opportunities and shifting competitive pressures. We\ndemonstrate the consequences on relative prices, trading positions, and\nwelfare.",
      "generated_abstract": "We present a novel and simple model of anonymization and re-identification\nof individuals that is a generalization of the widely studied\nre-identification-based model of anonymous data. The model is based on the\nidea that individuals can be identified by aggregating their behavioral data\nand by analyzing the aggregate data. We show that the re-identification rate\ncan be controlled by choosing the size of the aggregate. We then introduce a\nre-identification attack that requires only the ability to compute a\ncombination of aggregate data and the individual's behavioral data. We prove\nthat, if the re-identification rate is sufficiently high, the attack is\nimpossible. Furthermore, we show that, under certain assumptions, the\nattacker's reward can be minimized if the re-identification rate is sufficiently\nlow. Finally, we provide a simple and general argument for why, under certain\nassumptions,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20930232558139536,
          "p": 0.13043478260869565,
          "f": 0.16071428098373738
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.20930232558139536,
          "p": 0.13043478260869565,
          "f": 0.16071428098373738
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/chem-ph/2503.09670v1",
      "true_abstract": "Solving challenging problems in quantum chemistry is one of the leading\npromised applications of quantum computers. Within the quantum algorithms\nproposed for problems in excited state quantum chemistry, subspace-based\nquantum algorithms, including quantum subspace expansion (QSE), quantum\nequation of motion (qEOM) and quantum self-consistent equation-of-motion\n(q-sc-EOM), are promising for pre-fault-tolerant quantum devices. The working\nequation of QSE and qEOM requires solving a generalized eigenvalue equation\nwith associated matrix elements measured on a quantum computer. Our\nquantitative analysis of the QSE method shows that the errors in eigenvalues\nincrease drastically with an increase in the condition number of the overlap\nmatrix when a generalized eigenvalue equation is solved in the presence of\nstatistical sampling errors. This makes such methods unstable to errors that\nare unavoidable when using quantum computers. Further, at very high condition\nnumbers of overlap matrix, the QSE's working equation could not be solved\nwithout any additional steps in the presence of sampling errors as it becomes\nill-conditioned. It was possible to use the thresholding technique in this case\nto solve the equation, but the solutions achieved had missing excited states,\nwhich may be a problem for future chemical studies. We also show that\nexcited-state methods that have an eigenvalue equation as the working equation,\nsuch as q-sc-EOM, do not have such problems and could be suitable candidates\nfor excited-state quantum chemistry calculations using quantum computers.",
      "generated_abstract": "A simple and robust method for measuring the polarizability of atoms and\ngas molecules is presented. The polarizability is defined as the change in\nelectric dipole moment per unit mass when a magnetic field is applied. The\nmethod uses an atomic beam of an excited state of a light transition that\ncouples to a spin-polarized electromagnetic field. The polarizability can be\nmeasured by analyzing the light scattered by the beam. The method is demonstrated\nfor 1s2p electrons in helium and for 1s2p2 electrons in helium-like atoms. The\npolarizabilities of these two electrons are measured to be 1.38 and 1.43\nmicro-evs/nm, respectively, which is in excellent agreement with theoretical\ncalculations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16793893129770993,
          "p": 0.30985915492957744,
          "f": 0.21782177761935112
        },
        "rouge-2": {
          "r": 0.014423076923076924,
          "p": 0.028037383177570093,
          "f": 0.019047614561653867
        },
        "rouge-l": {
          "r": 0.16793893129770993,
          "p": 0.30985915492957744,
          "f": 0.21782177761935112
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.16355v1",
      "true_abstract": "We study monotonicity testing of high-dimensional distributions on\n$\\{-1,1\\}^n$ in the model of subcube conditioning, suggested and studied by\nCanonne, Ron, and Servedio~\\cite{CRS15} and Bhattacharyya and\nChakraborty~\\cite{BC18}. Previous work shows that the \\emph{sample complexity}\nof monotonicity testing must be exponential in $n$ (Rubinfeld,\nVasilian~\\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld,\nYodpinyanee~\\cite{AGPRY19}). We show that the subcube \\emph{query complexity}\nis $\\tilde{\\Theta}(n/\\varepsilon^2)$, by proving nearly matching upper and\nlower bounds. Our work is the first to use directed isoperimetric inequalities\n(developed for function monotonicity testing) for analyzing a distribution\ntesting algorithm. Along the way, we generalize an inequality of Khot, Minzer,\nand Safra~\\cite{KMS18} to real-valued functions on $\\{-1,1\\}^n$.\n  We also study uniformity testing of distributions that are promised to be\nmonotone, a problem introduced by Rubinfeld, Servedio~\\cite{RS09} , using\nsubcube conditioning. We show that the query complexity is\n$\\tilde{\\Theta}(\\sqrt{n}/\\varepsilon^2)$. Our work proves the lower bound,\nwhich matches (up to poly-logarithmic factors) the uniformity testing upper\nbound for general distributions (Canonne, Chen, Kamath, Levi,\nWaingarten~\\cite{CCKLW21}). Hence, we show that monotonicity does not help,\nbeyond logarithmic factors, in testing uniformity of distributions with subcube\nconditional queries.",
      "generated_abstract": "We present a simple approach for computing the empirical spectral\ndecomposition of a discrete-time stochastic matrix. This is done by\nconstructing a randomized version of the method of orthogonal polynomials.\nUnlike previous methods, this approach is applicable to any discrete-time\nstochastic matrix, not just a matrix whose entries follow a Gaussian\ndistribution. Additionally, this approach provides an explicit expression for\nthe spectral decomposition, which is useful in analyzing the\ndiscrepancy-based spectral estimator. We also derive a convergence rate for the\nempirical spectral decomposition, which is guaranteed to converge to the\ntrue spectral decomposition at a rate that is independent of the sampling\nfidelity. Finally, we present numerical experiments to illustrate the\nefficiency and accuracy of our method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.2465753424657534,
          "f": 0.1925133642243131
        },
        "rouge-2": {
          "r": 0.005917159763313609,
          "p": 0.009433962264150943,
          "f": 0.007272722535143581
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.2465753424657534,
          "f": 0.1925133642243131
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.09174v1",
      "true_abstract": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF), in a wireless setup comprising a small continuous linear\nintelligent antenna array in the near field of a large one. The framework\nallows for any orientations between the arrays and any positions in a\ntwo-dimensional space assuming that the transmitting array is placed at the\norigin. Therefore, apart from the length of the two continuous arrays, four key\nparameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveiled key\nsystem-level insights.",
      "generated_abstract": "The increasing demand for real-time information processing in wireless\nnetworks necessitates the development of efficient techniques for compressing\nmessages. However, it is challenging to maintain high compression ratios while\npreserving the integrity of the original information. In this paper, we propose\na novel scheme to compress messages with low distortion while preserving the\noriginal content. The proposed scheme uses the block-level scheme for\ncompression and a novel compression method for the block-level scheme. We\npropose a new compression method for the block-level scheme that uses a\nmulti-level coding scheme, which reduces the computational complexity of the\ncompression algorithm. To evaluate the performance of the proposed scheme, we\ncompare it with three other schemes: the BPSK scheme, the QPSK scheme, and the\ncombination of the BPSK scheme and QPSK scheme. Simulation results show that\nthe proposed scheme has superior compression performance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.22666666666666666,
          "f": 0.18085105903406531
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.01652892561983471,
          "f": 0.013745699609123007
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.21333333333333335,
          "f": 0.1702127611617249
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.10065v1",
      "true_abstract": "This paper is the first to propose valid inference tools, based on\nself-normalization, in time series expected shortfall regressions. In doing so,\nwe propose a novel two-step estimator for expected shortfall regressions which\nis based on convex optimization in both steps (rendering computation easy) and\nit only requires minimization of quantile losses and squared error losses\n(methods for both of which are implemented in every standard statistical\ncomputing package). As a corollary, we also derive self-normalized inference\ntools in time series quantile regressions. Extant methods, based on a bootstrap\nor direct estimation of the long-run variance, are computationally more\ninvolved, require the choice of tuning parameters and have serious size\ndistortions when the regression errors are strongly serially dependent. In\ncontrast, our inference tools only require estimates of the quantile regression\nparameters that are computed on an expanding window and are correctly sized.\nSimulations show the advantageous finite-sample properties of our methods.\nFinally, two applications to stock return predictability and to Growth-at-Risk\ndemonstrate the practical usefulness of the developed inference tools.",
      "generated_abstract": "In this paper, we propose a novel method for causal inference with heterogeneous\nsubpopulations, including both treatments and controls. The method utilizes\nsemiparametric kernel methods to construct a causal graph, enabling the\nidentification of treatment effects and covariate effects across subpopulations.\nThis approach is applicable to settings with multiple levels of\nheterogeneity, including a single-level categorical variable, a multilevel\ncontinuous variable, and a multilevel categorical variable with continuous\noutcomes. We demonstrate the method's effectiveness through simulation\nstudies and a real-world data application. The simulations demonstrate that the\nmethod provides a more efficient way to identify causal effects across\nheterogeneous subpopulations compared to existing methods, while also\nenhancing the identification of treatment effects in single-level categorical\nvariables and continuous outcomes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.27631578947368424,
          "f": 0.2234042505024899
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.03669724770642202,
          "f": 0.029739772131397526
        },
        "rouge-l": {
          "r": 0.16964285714285715,
          "p": 0.25,
          "f": 0.20212765475780908
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08451v1",
      "true_abstract": "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
      "generated_abstract": "This paper introduces a novel framework to facilitate the development of\napplications in the context of the Blockchain Technology. The framework\nincorporates a set of design patterns to enhance the development of Blockchain\napplications. The framework is designed to facilitate the development of\nblockchain applications by providing a foundation for the development of\nblockchain applications. The framework is designed to facilitate the\ndevelopment of blockchain applications by providing a foundation for the\ndevelopment of blockchain applications. The framework provides a set of\ndesign patterns that are intended to facilitate the development of blockchain\napplications. The framework provides a set of design patterns that are intended\nto facilitate the development of blockchain applications. The framework provides\na set of design patterns that are intended to facilitate the development of\nblockchain applications. The framework provides a set of design patterns that\nare intended to facilitate the development of blockchain applications. The\nframework provides a set",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07051282051282051,
          "p": 0.3333333333333333,
          "f": 0.11640211351977835
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0641025641025641,
          "p": 0.30303030303030304,
          "f": 0.10582010293776778
        }
      }
    },
    {
      "paper_id": "math-ph.math/SP/2502.17290v1",
      "true_abstract": "The two-dimensional magnetic Laplacian is considered. We calculate the\nleading term of the splitting between the first two eigenvalues of the operator\nin the semiclassical limit under the assumption that the magnetic field does\nnot vanish and has two symmetric magnetic wells with respect to the coordinate\naxes. This is the first result of quantum tunneling between purely magnetic\nwells under generic assumptions. The proof, which strongly relies on microlocal\nanalysis, reveals a purely magnetic Agmon distance between the wells.\nSurprisingly, it is discovered that the exponential decay of the eigenfunctions\naway from the magnetic wells is not crucial to derive the tunneling formula.\nThe key is a microlocal exponential decay inside the characteristic manifold,\nwith respect to the variable quantizing the classical center guide motion.",
      "generated_abstract": "We present a novel approach to the problem of estimating the size of the\ncomputational budget needed to solve a given nonlinear optimization problem,\nwhich is the subject of this article. The proposed method is based on the\ncombination of the classical Monte Carlo sampling technique with the\nGibbs-Kubo formula. The numerical experiments conducted in the paper show that\nthe proposed method is efficient in estimating the computational budget. We\nalso compare the efficiency of the proposed method with the methods based on\nthe Monte Carlo sampling and the stochastic gradient descent algorithm. The\nresults of the numerical experiments are in good agreement with the results of\nthe corresponding theoretical studies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19736842105263158,
          "p": 0.24193548387096775,
          "f": 0.21739129939928598
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.07692307692307693,
          "f": 0.0689655122948874
        },
        "rouge-l": {
          "r": 0.18421052631578946,
          "p": 0.22580645161290322,
          "f": 0.2028985457760976
        }
      }
    },
    {
      "paper_id": "eess.SP.cs/SD/2503.09349v1",
      "true_abstract": "Correlation-based auditory attention decoding (AAD) algorithms exploit neural\ntracking mechanisms to determine listener attention among competing speech\nsources via, e.g., electroencephalography signals. The correlation coefficients\nbetween the decoded neural responses and encoded speech stimuli of the\ndifferent speakers then serve as AAD decision variables. A critical trade-off\nexists between the temporal resolution (the decision window length used to\ncompute these correlations) and the AAD accuracy. This trade-off is typically\ncharacterized by evaluating AAD accuracy across multiple window lengths,\nleading to the performance curve. We propose a novel method to model this\ntrade-off curve using labeled correlations from only a single decision window\nlength. Our approach models the (un)attended correlations with a normal\ndistribution after applying the Fisher transformation, enabling accurate AAD\naccuracy prediction across different window lengths. We validate the method on\ntwo distinct AAD implementations: a linear decoder and the non-linear VLAAI\ndeep neural network, evaluated on separate datasets. Results show consistently\nlow modeling errors of approximately 2 percent points, with 94% of true\naccuracies falling within estimated 95%-confidence intervals. The proposed\nmethod enables efficient performance curve modeling without extensive\nmulti-window length evaluation, facilitating practical applications in, e.g.,\nperformance tracking in neuro-steered hearing devices to continuously adapt the\nsystem parameters over time.",
      "generated_abstract": "In this paper, we consider the problem of learning a classifier that separates\nthe input points into two disjoint sets by optimizing the sum of squared\ndistances (SSD) between the points. The classifier is trained on the points of\nthe training set, while the points in the test set are treated as outliers.\nPrevious works have demonstrated that this problem is NP-hard and\nexponentially hard to approximate. We propose an approach based on a\nconvex-concave transformation to transform this problem into a simpler\nproblem, where the transformation is efficient to compute. In this transformed\nproblem, we can use any convex or concave function to represent the classifier.\nWe provide an exact and an efficient approximate solution for this transformed\nproblem. Furthermore, we propose a novel approach to learn the transformation\nfunction that is efficient to compute. We use this transformation function to\nlearn a new classifier, and we compare",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1342281879194631,
          "p": 0.24390243902439024,
          "f": 0.17316016858079883
        },
        "rouge-2": {
          "r": 0.030456852791878174,
          "p": 0.04411764705882353,
          "f": 0.036036031203817084
        },
        "rouge-l": {
          "r": 0.12080536912751678,
          "p": 0.21951219512195122,
          "f": 0.15584415126478154
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.01376v1",
      "true_abstract": "Structure-Based Drug Design (SBDD) has revolutionized drug discovery by\nenabling the rational design of molecules for specific protein targets. Despite\nsignificant advancements in improving docking scores, advanced 3D-SBDD\ngenerative models still face challenges in producing drug-like candidates that\nmeet medicinal chemistry standards and pharmacokinetic requirements. These\nlimitations arise from their inherent focus on molecular interactions, often\nneglecting critical aspects of drug-likeness. To address these shortcomings, we\nintroduce the Collaborative Intelligence Drug Design (CIDD) framework, which\ncombines the structural precision of 3D-SBDD models with the chemical reasoning\ncapabilities of large language models (LLMs). CIDD begins by generating\nsupporting molecules with 3D-SBDD models and then refines these molecules\nthrough LLM-supported modules to enhance drug-likeness and structural\nreasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a\nremarkable success ratio of 37.94%, significantly outperforming the previous\nstate-of-the-art benchmark of 15.72%. Although improving molecular interactions\nand drug-likeness is often seen as a trade-off, CIDD uniquely achieves a\nbalanced improvement in both by leveraging the complementary strengths of\ndifferent models, offering a robust and innovative pathway for designing\ntherapeutically promising drug candidates.",
      "generated_abstract": "Genome-wide association studies (GWAS) have identified 40 loci associated\nwith multiple sclerosis (MS). These loci are involved in multiple pathways,\nincluding inflammation, immunity, and autoimmunity. In this study, we examine\nthe role of these loci in MS pathogenesis and the mechanisms through which\nthey influence immune and inflammatory processes. We used data from the\nMultiple Sclerosis Genetics Consortium (MSGC) to identify genetic variants\nassociated with MS risk. We then analyzed the pathways in which these variants\naffect immune and inflammatory processes. We found that genetic variants at 35\nloci are associated with increased risk of developing MS. We also identified\npathways that were significantly affected by these variants. Pathways affected\nby these variants include the immune response, inflammation, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12030075187969924,
          "p": 0.21621621621621623,
          "f": 0.15458936738687032
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12030075187969924,
          "p": 0.21621621621621623,
          "f": 0.15458936738687032
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2501.13143v1",
      "true_abstract": "We introduce a model-free preference under ambiguity, as a primitive trait of\nbehavior, which we apply once as well as repeatedly. Its single and double\napplication yield simple, easily interpretable definitions of ambiguity\naversion and ambiguity prudence. We derive their implications within canonical\nmodels for decision under risk and ambiguity. We establish in particular that\nour new definition of ambiguity prudence is equivalent to a positive third\nderivative of: (i) the capacity in the Choquet expected utility model, (ii) the\ndual conjugate of the divergence function under variational divergence\npreferences and (iii) the ambiguity attitude function in the smooth ambiguity\nmodel. We show that our definition of ambiguity prudent behavior may be\nnaturally linked to an optimal insurance problem under ambiguity.",
      "generated_abstract": "We propose a novel approach for predicting the return of a stock based on\ntheir price and volume trends. The proposed model, called the Trend-Volume\n(TV) model, takes into account both the price and volume trends. The TV\nmodel is designed to capture the interplay between the two key drivers of\nreturns, and it is expected to provide more accurate predictions than the\ntraditional approach. We evaluate the performance of the proposed model using\nthe historical stock data of the Shanghai Stock Exchange (SSE). The results\nshow that the TV model outperforms the traditional approach, with a higher\nabsolute error and a lower root mean squared error (RMSE) for predicting the\nreturns of the stocks. This suggests that the TV model is more effective at\ncapturing the interplay between price and volume trends, which are key\ncharacteristics of stock returns. Furthermore, the TV model shows potential\napplications in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1927710843373494,
          "p": 0.1839080459770115,
          "f": 0.18823528912041537
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.015748031496062992,
          "f": 0.016666661683682044
        },
        "rouge-l": {
          "r": 0.1566265060240964,
          "p": 0.14942528735632185,
          "f": 0.1529411714733566
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.01296v1",
      "true_abstract": "The separating Noether number $\\beta_{\\mathrm{sep}}(G)$ of a finite group $G$\nis the minimal positive integer $d$ such that for every $G$-module $V$ there is\na separating set of degree $\\leq d$. In this manuscript, we investigate the\nseparating Noether number $\\beta_{\\mathrm{sep}}(G)$. Among others, we obtain\nthe exact value of $\\beta_{\\mathrm{sep}}(G)$ for finite abelian groups $G$,\nwhen either $G$ is a $p$-group or $\\mathsf r(G)\\in \\{3,5\\}$.",
      "generated_abstract": "We prove a formula for the intersection of the set of all quadratic forms of\na certain type with a linear space, and we show that it coincides with the\nconjugate of the set of all quadratic forms of a certain type.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.2916666666666667,
          "f": 0.19718309411624688
        },
        "rouge-2": {
          "r": 0.03389830508474576,
          "p": 0.06451612903225806,
          "f": 0.04444443992839552
        },
        "rouge-l": {
          "r": 0.1276595744680851,
          "p": 0.25,
          "f": 0.16901408003173987
        }
      }
    },
    {
      "paper_id": "physics.chem-ph.physics/chem-ph/2503.10038v1",
      "true_abstract": "We synthesize MoS$_{2}$ atomic layer flakes at different growth conditions to\ntailor S-terminated and Mo-terminated edge defect states that are investigated\nfor their ferromagnetic response. We leverage quantum weak measurement\nprinciples to construct a spin Hall effect of light-based magneto-optic Kerr\neffect (SHEL-MOKE) setup to sense the ultra-small magnetic response from the\nsynthesized atomic layers. Our findings demonstrate that Mo-terminated edge\nstates are the primary source of ferromagnetic response from MoS$_{2}$ flakes,\nwhich is consistent with X-ray photoelectron, Raman and photoluminescence\nspectroscopic results. In the process, we demonstrate SHEL-MOKE to be a robust\ntechnique to investigate ultra weak properties in novel atomic-scale materials.",
      "generated_abstract": "The study of liquid crystals (LCs) is of fundamental importance in many\nparticular fields of science and engineering. This study aims to investigate the\nphase behavior of the new LC, 2,6-diisocyanatotetramethylene-bis(4-ethylpiperazin-\n1-yl)oxy-3,5-diphenyl-1,4-dihydro-1,4-dioxane (DTPA) in an ethanolic medium.\nIn this study, the effect of the temperature and the concentration of the\nsolvent on the phase behavior of DTPA has been investigated using both DSC and\nRaman spectroscopy techniques. The results show that the DTPA exhibits a\nspontaneous transition from a monolayer to a mono- and di-layer structure at\nlow temperatures (20-2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.24615384615384617,
          "f": 0.2237762188175462
        },
        "rouge-2": {
          "r": 0.020202020202020204,
          "p": 0.023809523809523808,
          "f": 0.021857918530862008
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2,
          "f": 0.18181817685950424
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2502.14228v1",
      "true_abstract": "The umbilical cord plays a critical role in delivering nutrients and oxygen\nfrom the placenta to the fetus through the umbilical vein, while the two\numbilical arteries carry deoxygenated blood with waste products back to the\nplacenta. Although solute exchange in the placenta has been extensively\nstudied, exchange within the cord tissue has not been investigated. Here, we\nexplore the hypothesis that the coiled structure of the umbilical cord could\nstrengthen diffusive coupling between the arteries and the vein, resulting in a\nfunctional shunt. We calculate the diffusion of solutes, such as oxygen, and\nheat in the umbilical cord to quantify how this shunt is affected by vascular\nconfiguration within the cord. We demonstrate that the shunt is enhanced by\ncoiling and vessel proximity. Furthermore, our model predicts that typical\nvascular configurations of the human cord tend to minimise shunting, which\ncould otherwise disrupt thermal regulation of the fetus. We also show that the\nexchange, amplified by coiling, can provide additional oxygen supply to the\ncord tissue surrounding the umbilical vessels.",
      "generated_abstract": "This article presents a methodology to model the spatiotemporal dynamics of\npangenome-based metagenomic data. The method is based on the integration of\nspatiotemporal patterns in pangenome-based metagenomic data, leveraging the\nemerging field of spatially structured genomics (SSG). In this context, we\npropose a method to identify spatial patterns within a metagenomic data set.\nThese patterns are then used to model the spatiotemporal dynamics of the\nindividual metagenome within a particular geographical area. The methodology\nenables the identification of spatiotemporal patterns within pangenome-based\nmetagenomic data, which can be exploited for the generation of spatially\nstructured models of metagenomic data. This approach provides a framework for\nanalyzing the evolution of metagenomic communities over time and spatially,\nenabling the identification",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13592233009708737,
          "p": 0.20588235294117646,
          "f": 0.16374268526794583
        },
        "rouge-2": {
          "r": 0.006666666666666667,
          "p": 0.01020408163265306,
          "f": 0.008064511348858192
        },
        "rouge-l": {
          "r": 0.1262135922330097,
          "p": 0.19117647058823528,
          "f": 0.1520467788351973
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2405.20522v1",
      "true_abstract": "This project is a collaboration between industry and academia to delve into\nFinance Social Networks, specifically the Board of Directors of public\ncompanies. Knowing the connections between Directors and Executives in\ndifferent companies can generate powerful stories and meaningful insights on\ninvestments. A proof of concept in the form of a Data Visualization tool\nreveals its strength in investigating corporate governance and sustainability,\nas well as in the partnership between industry and academic institutions.",
      "generated_abstract": "This paper introduces a novel methodology for analyzing the impact of\ndifferent financial market regulation in the context of the 2008 financial\ncrisis. We focus on the impact of the European Union's Markets in Financial\nInstruments Directive (MiFID) on the performance of the Spanish stock market,\nthrough a detailed analysis of the impact of the regulation on the stock\nprices of the three largest Spanish companies. We find that the implementation\nof MiFID had a significant impact on the market, with the regulation having a\ndramatic impact on the prices of the three companies, with a 20% drop in the\nprice of Santander, a 40% drop in the price of BBVA and a 10% drop in the price\nof Bankinter. We also find that the implementation of MiFID had a positive\nimpact on the stock market,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.14754098360655737,
          "f": 0.15652173414896045
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.010309278350515464,
          "f": 0.011976043034890286
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.14754098360655737,
          "f": 0.15652173414896045
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.20438v1",
      "true_abstract": "The financial sector, a pivotal force in economic development, increasingly\nuses the intelligent technologies such as natural language processing to\nenhance data processing and insight extraction. This research paper through a\nreview process of the time span of 2018-2023 explores the use of text mining as\nnatural language processing techniques in various components of the financial\nsystem including asset pricing, corporate finance, derivatives, risk\nmanagement, and public finance and highlights the need to address the specific\nproblems in the discussion section. We notice that most of the research\nmaterials combined probabilistic with vector-space models, and text-data with\nnumerical ones. The most used technique regarding information processing is the\ninformation classification technique and the most used algorithms include the\nlong-short term memory and bidirectional encoder models. The research noticed\nthat new specific algorithms are developed and the focus of the financial\nsystem is mainly on asset pricing component. The research also proposes a path\nfrom engineering perspective for researchers who need to analyze financial\ntext. The challenges regarding text mining perspective such as data quality,\ncontext-adaption and model interpretability need to be solved so to integrate\nadvanced natural language processing models and techniques in enhancing\nfinancial analysis and prediction. Keywords: Financial System (FS), Natural\nLanguage Processing (NLP), Software and Text Engineering, Probabilistic,\nVector-Space, Models, Techniques, TextData, Financial Analysis.",
      "generated_abstract": "In this paper, we propose a new method for evaluating the quality of\nsubmodular maximization problems that is based on the concept of\nreinforcement learning. Our method provides a framework for analyzing the\nefficiency of different optimization algorithms for submodular maximization\nproblems. We evaluate the performance of two different reinforcement learning\nalgorithms: Q-learning and DQN. The Q-learning algorithm is well-known and is\nwell-suited for handling submodular maximization problems. The DQN algorithm\nis a recent algorithm that has been shown to outperform Q-learning in\nimproving the performance of reinforcement learning in a variety of domains. We\nconduct extensive experiments to compare the performance of Q-learning and\nDQN in solving submodular maximization problems. Our results show that DQN\noutperforms Q-learning, particularly in scenarios where the submodular",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11029411764705882,
          "p": 0.22727272727272727,
          "f": 0.14851484708558
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.22727272727272727,
          "f": 0.14851484708558
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02349v1",
      "true_abstract": "Sequential change-point detection for time series is widely used in data\nmonitoring in practice. In this work, we focus on sequential change-point\ndetection on high-order compositional time series models. Under the regularity\nconditions, we prove that a process following the generalized Beta AR(p) model\nwith exogenous variables is stationary and ergodic. We develop a nonparametric\nsequential change-point detection method for the generalized Beta AR(p) model,\nwhich does not rely on any strong assumptions about the sources of the change\npoints. We show that the power of the test converges to one given that the\namount of initial observations is large enough. We apply the nonparametric\nmethod to a rate of automobile crashes with alcohol involved, which is recorded\nmonthly from January 2010 to December 2020; the exogenous variable is the price\nlevel of alcoholic beverages, which has a change point around August 2019. We\nfit a generalized Beta AR(p) model to the crash rate sequence, and we use the\nnonparametric sequential change-point detection method to successfully detect\nthe change point.",
      "generated_abstract": "Recent advancements in deep learning have demonstrated that they can be\nused to model complex nonlinear relationships between variables. However,\nthese models often suffer from high computational costs, making them\ninappropriate for large-scale data analysis. To address these issues, we\npropose a novel approach for the inference of multivariate nonlinear\nrelationships based on kernel methods. Specifically, we develop a kernel\napproach to the estimation of the multivariate nonlinear relationship and\nestimate its parameters by solving a high-dimensional optimization problem. We\npropose two different kernel methods: the Gaussian process (GP) kernel and the\nradial basis function (RBF) kernel. We show that these kernels are well suited\nfor the estimation of multivariate nonlinear relationships, especially in\nhigh-dimensional settings. We validate the performance of our methodology on\nreal-world datasets and demonstrate its effectiveness in analyzing the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.21739130434782608,
          "f": 0.20833332834201398
        },
        "rouge-2": {
          "r": 0.040268456375838924,
          "p": 0.048,
          "f": 0.04379561547631789
        },
        "rouge-l": {
          "r": 0.19,
          "p": 0.20652173913043478,
          "f": 0.19791666167534736
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/QM/2503.02685v1",
      "true_abstract": "Precise parcellation of functional networks (FNs) of early developing human\nbrain is the fundamental basis for identifying biomarker of developmental\ndisorders and understanding functional development. Resting-state fMRI\n(rs-fMRI) enables in vivo exploration of functional changes, but adult FN\nparcellations cannot be directly applied to the neonates due to incomplete\nnetwork maturation. No standardized neonatal functional atlas is currently\navailable. To solve this fundamental issue, we propose TReND, a novel and fully\nautomated self-supervised transformer-autoencoder framework that integrates\nregularized nonnegative matrix factorization (RNMF) to unveil the FNs in\nneonates. TReND effectively disentangles spatiotemporal features in voxel-wise\nrs-fMRI data. The framework integrates confidence-adaptive masks into\ntransformer self-attention layers to mitigate noise influence. A self\nsupervised decoder acts as a regulator to refine the encoder's latent\nembeddings, which serve as reliable temporal features. For spatial coherence,\nwe incorporate brain surface-based geodesic distances as spatial encodings\nalong with functional connectivity from temporal features. The TReND clustering\napproach processes these features under sparsity and smoothness constraints,\nproducing robust and biologically plausible parcellations. We extensively\nvalidated our TReND framework on three different rs-fMRI datasets: simulated,\ndHCP and HCP-YA against comparable traditional feature extraction and\nclustering techniques. Our results demonstrated the superiority of the TReND\nframework in the delineation of neonate FNs with significantly better spatial\ncontiguity and functional homogeneity. Collectively, we established TReND, a\nnovel and robust framework, for neonatal FN delineation. TReND-derived neonatal\nFNs could serve as a neonatal functional atlas for perinatal populations in\nhealth and disease.",
      "generated_abstract": "This paper presents the first systematic review of the literature on the\nphenomenon of human-animal interaction in the form of animal care,\ncomplementary medicine, and animal welfare. The research focuses on the\naccommodation of animals in human environments, with a particular emphasis on\ntheir health, welfare, and the ethical and legal aspects of their care. This\nstudy includes a scoping review of the scientific literature, followed by a\nsystematic review of the empirical research conducted in the last 10 years,\nincluding both quantitative and qualitative studies. The review results show that\nthe research on the phenomenon of human-animal interaction has advanced\nsignificantly, particularly in the last decade. The review identifies a wide\nrange of themes and topics, including animal welfare, animal-human\ninteractions, animal-human interactions, animal-human interactions in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06626506024096386,
          "p": 0.1527777777777778,
          "f": 0.09243697056987521
        },
        "rouge-2": {
          "r": 0.008620689655172414,
          "p": 0.01834862385321101,
          "f": 0.011730200929130187
        },
        "rouge-l": {
          "r": 0.06626506024096386,
          "p": 0.1527777777777778,
          "f": 0.09243697056987521
        }
      }
    },
    {
      "paper_id": "math.ST.econ/EM/2502.20917v1",
      "true_abstract": "We examine the location characteristics of a conditional selective confidence\ninterval based on the polyhedral method. This interval is constructed from the\ndistribution of a test statistic conditional upon the event of statistical\nsignificance. In the case of a one-sided test, the behavior of the interval\nvaries depending on whether the parameter is highly significant or only\nmarginally significant. When the parameter is highly significant, the interval\nis similar to the usual confidence interval derived without considering\nselection. However, when the parameter is only marginally significant, the\ninterval falls into an extreme range and deviates greatly from the estimated\nvalue of the parameter. In contrast, an interval conditional on two-sided\nsignificance does not yield extreme results, although it may exclude the\nestimated parameter value.",
      "generated_abstract": "The emergence of non-trivial, yet unobserved characteristics in data,\nsuch as the temporal and spatial aspects of human behaviour, has long been\nrecognised as a challenge in economic modelling. Traditional economic models\ntypically assume that these characteristics are fully observable and are\nknown to the researcher, and that they are fixed and unchanging. However, the\nreal world often presents a more complex picture, where characteristics are\npartially observable and can change over time. In this paper, we propose a\ndynamic model for non-observed heterogeneity, which allows for the presence of\ndynamic non-observed characteristics and is suitable for modelling\neconomic-related data. The model includes two distinct dynamic components, the\ndynamic component and the stochastic component. The dynamic component incorporates\na dynamic random walk process, while the stochastic component is characterised\nby an autoregressive (AR) model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14084507042253522,
          "p": 0.11363636363636363,
          "f": 0.12578615857917033
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.007874015748031496,
          "f": 0.00858368602774329
        },
        "rouge-l": {
          "r": 0.14084507042253522,
          "p": 0.11363636363636363,
          "f": 0.12578615857917033
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/PM/2412.11019v1",
      "true_abstract": "The domain of hedge fund investments is undergoing significant\ntransformation, influenced by the rapid expansion of data availability and the\nadvancement of analytical technologies. This study explores the enhancement of\nhedge fund investment performance through the integration of machine learning\ntechniques, the application of PolyModel feature selection, and the analysis of\nfund size. We address three critical questions: (1) the effect of machine\nlearning on trading performance, (2) the role of PolyModel feature selection in\nfund selection and performance, and (3) the comparative reliability of larger\nversus smaller funds.\n  Our findings offer compelling insights. We observe that while machine\nlearning techniques enhance cumulative returns, they also increase annual\nvolatility, indicating variability in performance. PolyModel feature selection\nproves to be a robust strategy, with approaches that utilize a comprehensive\nset of features for fund selection outperforming more selective methodologies.\nNotably, Long-Term Stability (LTS) effectively manages portfolio volatility\nwhile delivering favorable returns. Contrary to popular belief, our results\nsuggest that larger funds do not consistently yield better investment outcomes,\nchallenging the assumption of their inherent reliability.\n  This research highlights the transformative impact of data-driven approaches\nin the hedge fund investment arena and provides valuable implications for\ninvestors and asset managers. By leveraging machine learning and PolyModel\nfeature selection, investors can enhance portfolio optimization and reassess\nthe dependability of larger funds, leading to more informed investment\nstrategies.",
      "generated_abstract": "We introduce a novel framework to model the impact of algorithmic trading\non financial markets and asset prices. The framework combines the\nMarkov-Perron method with a nonparametric approach to the estimation of the\nMarkov-Perron polynomial. The methodology is applied to the US equity market\nfrom 1984 to 2021, where it provides a new and more robust methodology to\nquantify the impact of algorithmic trading on asset prices. The results show\nthat algorithmic trading has a significant impact on the market dynamics,\nparticularly during periods of extreme volatility. The estimated nonparametric\nMarkov-Perron polynomial is consistent with previous empirical studies and\nprovides a more robust methodology for quantifying the impact of algorithmic\ntrading on asset prices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14583333333333334,
          "p": 0.3442622950819672,
          "f": 0.20487804460011907
        },
        "rouge-2": {
          "r": 0.014563106796116505,
          "p": 0.03260869565217391,
          "f": 0.020134223919644163
        },
        "rouge-l": {
          "r": 0.1388888888888889,
          "p": 0.32786885245901637,
          "f": 0.19512194703914346
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.07327v1",
      "true_abstract": "Multilinear Principal Component Analysis (MPCA) is an important tool for\nanalyzing tensor data. It performs dimension reduction similar to PCA for\nmultivariate data. However, standard MPCA is sensitive to outliers. It is\nhighly influenced by observations deviating from the bulk of the data, called\ncasewise outliers, as well as by individual outlying cells in the tensors,\nso-called cellwise outliers. This latter type of outlier is highly likely to\noccur in tensor data, as tensors typically consist of many cells. This paper\nintroduces a novel robust MPCA method that can handle both types of outliers\nsimultaneously, and can cope with missing values as well. This method uses a\nsingle loss function to reduce the influence of both casewise and cellwise\noutliers. The solution that minimizes this loss function is computed using an\niteratively reweighted least squares algorithm with a robust initialization.\nGraphical diagnostic tools are also proposed to identify the different types of\noutliers that have been found by the new robust MPCA method. The performance of\nthe method and associated graphical displays is assessed through simulations\nand illustrated on two real datasets.",
      "generated_abstract": "The use of Bayesian inference in medical research is a cornerstone of the\nmodel-based paradigm. However, it is often challenging to compute the posterior\ndistributions of key parameters. For example, when fitting a model, a model\nparameter's posterior is the probability of the parameter given the model,\nwhereas when estimating the parameters, the posterior is the probability of\nthe parameters given the observed data. The problem of estimating the posterior\nof a parameter is known as the posterior estimation problem. We introduce a\nnovel framework for the Bayesian estimation of a model parameter, termed\n\"model-free posterior estimation\", which is based on a Bayesian inference\nframework. Our framework is general and can be applied to any model-free\nestimator. We show that our framework has two key advantages: first, the\nposterior estimation of a parameter is a well-defined mathematical object and\ncan be estimated by any model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.23684210526315788,
          "f": 0.18652849263497018
        },
        "rouge-2": {
          "r": 0.01744186046511628,
          "p": 0.024793388429752067,
          "f": 0.020477810851146763
        },
        "rouge-l": {
          "r": 0.1452991452991453,
          "p": 0.2236842105263158,
          "f": 0.17616579833445206
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/SC/2307.10289v1",
      "true_abstract": "This is the second part of the previous review. In the previous review we\nsuspected that Orai3 channels were involved in lung cancer and more precisely\nin several cancers. Here we confirm that calcium dysregulation is important for\ncancer development. in this paper we show that Orai3 is an upstream activator\nof AKT and we prove that AKT is involved in chemoresistance in NSCLC.",
      "generated_abstract": "This paper presents a novel approach to predict the survival probability of\nKirchhoff cells, which is a fundamental problem in cancer research. We propose\nan evolutionary model that integrates mutation, recombination, and\nmutation-induced recombination. This model takes into account the potential\neffect of mutation on survival probability and incorporates the potential\ninfluence of mutation-induced recombination on survival probability. The\nevolutionary dynamics of the model is driven by a selection process that\ncombines fitness and survival probability. This selection process is based on\nthe fitness of the individual cells and the survival probability of the\nindividuals, which is the probability that the cell will survive for the next\ngeneration. The survival probability is dependent on the survival probability\nof the individuals and the survival probability of the population. To\nquantify the fitness of the individuals,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.16923076923076924,
          "f": 0.20754716506763987
        },
        "rouge-2": {
          "r": 0.03389830508474576,
          "p": 0.01904761904761905,
          "f": 0.02439023929580694
        },
        "rouge-l": {
          "r": 0.2682926829268293,
          "p": 0.16923076923076924,
          "f": 0.20754716506763987
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2502.15781v1",
      "true_abstract": "Digital innovation in education has revolutionized teaching and learning\nprocesses, demanding a rethink of pedagogical competence among educators. This\nstudy evaluates the preparation of instructors to use digital technologies into\ntheir educational practices. The study used a mixed-methods approach,\nintegrating both qualitative interviews and quantitative surveys to evaluate\nteachers' institutional support systems, beliefs, and technical proficiency.\nThe results show that even while a large number of educators acknowledge the\nbenefits of digital tools, problems including poor professional development and\nchange aversion still exist. In order to improve digital pedagogical\npreparation, the study emphasizes the necessity of focused training initiatives\nand encouraging institutional regulations. There is discussion on the\nimplications for educational institutions and policymakers.",
      "generated_abstract": "We introduce a new approach to the teaching of quantum mechanics to\n students. We propose a method for the teaching of the Heisenberg Uncertainty\n Principle using the concept of a 'quantum clock', which is a quantum object\n that is constantly changing. This approach is based on the observation that\n the uncertainty of a quantum clock can be reduced by making it smaller. We\n show that this can be used to reduce the uncertainty of the electron spin\n within a single electron. We then apply this concept to the Heisenberg\n Uncertainty Principle, and we show that the quantum clock concept can be used\n to reduce the uncertainty of a single electron within a single electron. We\n conclude that this approach to the teaching of quantum mechanics can be\n applied to any concept that can be reduced by making a quantum object smaller.\n We apply this concept to the Heisenberg Uncertainty Principle, and we show that\n the quantum clock concept can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.22807017543859648,
          "f": 0.17931034005612378
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.020202020202020204,
          "f": 0.01886791955010811
        },
        "rouge-l": {
          "r": 0.14772727272727273,
          "p": 0.22807017543859648,
          "f": 0.17931034005612378
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09769v1",
      "true_abstract": "The Standard Model of cosmology, $\\Lambda$CDM, while enormously successful,\nis currently unable to account for several cosmological anomalies the most\nprominent of which are in the measurements of the Hubble parameter and $S_8$.\nAdditionally, the inclusion of the cosmological constant is theoretically\nunappealing. This has lead to extensions of the model such as the use of fluid\nequations for interacting dark matter and dark energy which, however, are ad\nhoc since they do not appear to arise from a Lagrangian. Recently, we have\nproposed $\\mathcal{Q}_{\\rm CDM}$ as an alternative to $\\Lambda$CDM which is a\ndynamical model of a quintessence field interacting with dark matter within a\nfield theoretic approach. In this approach, we analyze the effect of the dark\nmatter mass, the dark matter-dark energy interaction strength and the dark\nmatter self-interaction on the cosmological parameters. Further, within\n$\\mathcal{Q}_{\\rm CDM}$ we investigate the possible alleviation of the Hubble\ntension and the $S_8$ anomaly and the nature of dark energy.",
      "generated_abstract": "We present the first cosmological constraints on the gravitational lensing\nrelevance of the dark matter halo of the supermassive black hole at the center\nof the galaxy M87, using data from the Dark Energy Camera Legacy Survey (DECaLS)\nand the DES Y1 Data Release 2 (DES Y1 DR2). We find that the lensing\nrelevance of M87's dark matter halo is of the order of $10^{-5}-10^{-3}$ at\nredshifts $z=0.1-0.9$. This is comparable to the relevance of the halo of the\nsupermassive black hole at the center of the Milky Way (MW), but at higher\nredshifts. We also find that the lensing relevance of M87's dark matter halo\ndepends on the mass of the lens,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11458333333333333,
          "p": 0.18333333333333332,
          "f": 0.14102563629191336
        },
        "rouge-2": {
          "r": 0.03496503496503497,
          "p": 0.05952380952380952,
          "f": 0.04405285877389482
        },
        "rouge-l": {
          "r": 0.11458333333333333,
          "p": 0.18333333333333332,
          "f": 0.14102563629191336
        }
      }
    },
    {
      "paper_id": "nlin.AO.nlin/AO/2503.07585v1",
      "true_abstract": "In this paper we explore the effects of instantaneous stochastic resetting on\na planar slow-fast dynamical system of the form $\\dot{x}=f(x)-y$ and\n$\\dot{y}=\\epsilon (x-y)$ with $0<\\epsilon \\ll 1$. We assume that only the fast\nvariable $x(t)$ resets to its initial state $x_0$ at a random sequence of times\ngenerated from a Poisson process of rate $r$. Fixing the slow variable, we\ndetermine the parameterized probability density $p(x,t|y)$, which is the\nsolution to a modified Liouville equation. We then show how for $r\\gg \\epsilon$\nthe slow dynamics can be approximated by the averaged equation\n$dy/d\\tau=\\E[x|y]-y$ where $\\tau=\\epsilon t$, $\\E[x|y]=\\int x p^*(x|y)dx$ and\n$p^*(x|y)=\\lim_{t\\rightarrow \\infty}p(x,t|y)$. We illustrate the theory for\n$f(x)$ given by the cubic function of the FitzHugh-Nagumo equation. We find\nthat the slow variable typically converges to an $r$-dependent fixed point\n$y^*$ that is a solution of the equation $y^*=\\E[x|y^*]$. Finally, we\nnumerically explore deviations from averaging theory when $r=O(\\epsilon)$.",
      "generated_abstract": "In this paper, we propose a novel framework for the treatment of\nthe stochastic heat equation (SHE) using a stochastic variational method.\nIn this framework, the SHE is reformulated as a stochastic stochastic\nStokes-type system, and the Stokes equation is solved in a mean-field\napproximation. By employing the stochastic variational method, we derive a\nproper stochastic regularity condition for the SHE, which can be efficiently\ncomputed in a finite number of steps. Furthermore, we present two different\nsolutions of the SHE by applying the proposed method: (i) the solution of the\nSTS-type system is obtained by a fixed point argument, and (ii) the solution of\nthe SHE is obtained by a stochastic mean-field approximation. We illustrate the\nproposed methods by considering a stochastic thermal fluid model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19811320754716982,
          "p": 0.2876712328767123,
          "f": 0.2346368666783185
        },
        "rouge-2": {
          "r": 0.055944055944055944,
          "p": 0.07692307692307693,
          "f": 0.0647773230598768
        },
        "rouge-l": {
          "r": 0.1792452830188679,
          "p": 0.2602739726027397,
          "f": 0.21229049796323468
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/GL/2211.09554v1",
      "true_abstract": "It is essential to discuss the role, difficulties, and opportunities\nconcerning people of different gender in the field of software engineering\nresearch, education, and industry. Although some literature reviews address\nsoftware engineering and gender, it is still unclear how research and practices\nin Asia exist for handling gender aspects in software development and\nengineering. We conducted a systematic literature review to grasp the\ncomprehensive view of gender research and practices in Asia. We analyzed the 32\nidentified papers concerning countries and publication years among 463\npublications. Researchers and practitioners from various organizations actively\nwork on gender research and practices in some countries, including China,\nIndia, and Turkey. We identified topics and classified them into seven\ncategories varying from personal mental health and team building to\norganization. Future research directions include investigating the synergy\nbetween (regional) gender aspects and cultural concerns and considering\npossible contributions and dependency among different topics to have a solid\nfoundation for accelerating further research and getting actionable practices.",
      "generated_abstract": "The design of efficient parallel algorithms for linear algebra problems is\na long-standing and challenging problem. In this paper, we consider the\noptimization of the conjugate gradient (CG) algorithm, which is widely used in\nnumerical linear algebra. In the CG algorithm, a starting vector is iteratively\nupdated to minimize a given objective function. This objective function can be\nany linear function of the current iterate, such as the residual of the\niterative process. We present a novel parallel algorithm for the CG algorithm\nthat uses a block-diagonal decomposition of the matrix A. Our approach\nincorporates the block-diagonal structure into the iterative process, which\nallows for the efficient parallelization of the algorithm. Our algorithm\nachieves linear speedup over the serial version of the CG algorithm, which is\na state-of-the-art. Our experiments on various types of matrixes demonstrate\nthat our algorithm out",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.1411764705882353,
          "f": 0.1269841220346577
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10576923076923077,
          "p": 0.12941176470588237,
          "f": 0.11640211145264713
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2502.18231v1",
      "true_abstract": "1. Strong and weak notions of erasure are distinguished according to whether\nthe single erasure procedure does or does not leave the environment in the same\nstate independently of the pre-erasure state.\n  2. Purely thermodynamic considerations show that strong erasure cannot be\ndissipationless.\n  3. The main source of entropy creation in erasure processes at molecular\nscales is the entropy that must be created to suppress thermal fluctuations\n(\"noise\").\n  4. A phase space analysis recovers no minimum entropy cost for weak erasure\nand a positive minimum entropy cost for strong erasure.\n  5. An information entropy term has been attributed mistakenly to pre-erasure\nstates in the Gibbs formalism through the neglect of an additive constant in\nthe \"-k sum p log p\" Gibbs entropy formula.",
      "generated_abstract": "The idea that physics is ultimately a human enterprise, and that the\ncomputational tools and theoretical techniques that we now take for granted\nwere developed by human beings, has been largely ignored by the history of\nphysics. However, in this paper we show that this idea is central to the history\nof physics, and that the development of the physical theory of electromagnetism\nwas the product of a process of human creativity that was closely tied to\nhuman agency. The history of physics is the history of human beings making\ndecisions about what to do with the newly discovered laws of nature. The\nhistorical development of the theory of electromagnetism is a particularly\nimportant case study because the discovery of the law of induction, which\ndetermined the laws of electromagnetic induction and of electromagnetic\nresonance, was the product of a very particular decision by the early 19th\ncentury physicist Michael Faraday",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1511627906976744,
          "p": 0.1566265060240964,
          "f": 0.1538461488477296
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.03125,
          "f": 0.03278688025799592
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.1566265060240964,
          "f": 0.1538461488477296
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2503.00336v1",
      "true_abstract": "Continuing professional development for teachers in the physical sciences is\ncrucial to maintaining high-quality instruction, especially when addressing\nmodern physics. Nevertheless, the teaching of these topics often relies on\ntheoretical models that may seem abstract and removed from practical\napplications. In this context, research in astrophysics provides many valuable\ninsights into the nature of light and its fundamental properties, such as\ncontinuous and discrete spectra, blackbody radiation, and atomic orbitals. This\npaper, aimed at both high school and university-level physics teachers,\nexamines the peculiarities of the emission and absorption spectra of various\ntypes of astronomical objects and demonstrates how spectroscopy is applied in\nastrophysics research. From this perspective, the study conceptually\nillustrates how astrophysicists, by measuring light spectra, determine the\ncomposition, physical properties, origin, and evolution of celestial bodies\nand, by extension, of the universe as a whole. By understanding not only the\ntheory but also the direct applications of astronomical spectroscopy, teachers\nwill be better prepared to guide their students, thereby showcasing the true\nvalue of modern physics in the real world.",
      "generated_abstract": "This article presents a brief overview of the role of education in the\ndevelopment of quantum physics, and specifically, how quantum theory and\nquantum mechanics were first developed in the early 20th century. We will\ndiscuss the history of the development of quantum theory, its foundations, and\nhow it has been used to develop quantum physics and the quantum revolution. We\nwill also discuss the role of education in the development of quantum theory\nand quantum physics. We will discuss the role of education in the development\nof quantum theory and quantum physics, and will discuss how education has\nbeen instrumental in the development of quantum theory and quantum physics. We\nwill also discuss how education has been instrumental in the development of\nquantum theory and quantum physics, and how education has been instrumental in\nthe development of quantum theory and quantum physics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.3333333333333333,
          "f": 0.16666666291666676
        },
        "rouge-2": {
          "r": 0.011976047904191617,
          "p": 0.03125,
          "f": 0.017316013310096315
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.2857142857142857,
          "f": 0.14285713910714296
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.07126v1",
      "true_abstract": "We propose to relax traditional axioms in decision theory by incorporating a\nmeasurement, or degree, of satisfaction. For example, if the independence axiom\nof expected utility theory is violated, we can measure the size of the\nviolation. This measure allows us to derive an approximation guarantee for a\nutility representation that aligns with the unmodified version of the axiom.\nAlmost satisfying the axiom implies, then, a utility that is near a utility\nrepresentation. We develop specific examples drawn from expected utility theory\nunder risk and uncertainty.",
      "generated_abstract": "We study the equilibrium prices and quantity of output of a firm operating\nunder a non-linear demand function. We develop a novel approach to model the\nnon-linear demand function. This approach is based on a generalization of\nMas-Colell-Van Tharissen model that we term the Multidimensional\nMulti-dimensional Model (MMDM). We establish that the equilibrium prices and\nquantities satisfy the Hodrick-Prescott filter (HPF) equation, and we provide\nan explicit closed-form expression for the HPF equation. We also show that the\nMMDM has a unique steady state, and we characterize the steady state in terms\nof the equilibrium prices and quantities. We show that the steady state price\nand quantity satisfy the HPF equation, and we derive an explicit closed-form\nexpression for the HPF equation. We establish the long-run stability of the\nsteady state price and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26666666666666666,
          "p": 0.24615384615384617,
          "f": 0.2559999950080001
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.030612244897959183,
          "f": 0.03409090415547592
        },
        "rouge-l": {
          "r": 0.21666666666666667,
          "p": 0.2,
          "f": 0.20799999500800012
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.07843v1",
      "true_abstract": "We show that compact locally symmetric Lorentz manifolds are geodesically\ncomplete.",
      "generated_abstract": "We consider the moduli space of stable rank one maps of a smooth projective\ncurve with a fixed normal crossing divisor. We prove that it is smooth,\nprojective, and geometrically connected. We also construct a natural\n$\\mathbb{C}$-linear vector bundle on this space, called the bundle of rank one\nmaps, and we prove that this bundle is integrable and has torsion-free\n$H^0(\\mathbb{C} E_1)$-module. We then show that this bundle is a line bundle\non the moduli space of stable rank one maps. In particular, we obtain a\ngeometric interpretation of the line bundle on the moduli space of stable rank\none maps constructed by Ehresmann in 1968. We also show that the line bundle\non the moduli space of stable rank one maps constructed by Ehresmann is the\ncanonical line bundle.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2727272727272727,
          "p": 0.05084745762711865,
          "f": 0.08571428306530622
        },
        "rouge-2": {
          "r": 0.1,
          "p": 0.011235955056179775,
          "f": 0.020202018385879157
        },
        "rouge-l": {
          "r": 0.2727272727272727,
          "p": 0.05084745762711865,
          "f": 0.08571428306530622
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.q-bio/MN/2501.00983v1",
      "true_abstract": "We study Hopfield networks with non-reciprocal coupling inducing switches\nbetween memory patterns. Dynamical phase transitions occur between phases of no\nmemory retrieval, retrieval of multiple point-attractors, and limit-cycle\nattractors. The limit cycle phase is bounded by two critical regions: a Hopf\nbifurcation line and a fold bifurcation line, each with unique dynamical\ncritical exponents and sensitivity to perturbations. A Master Equation approach\nnumerically verifies the critical behavior predicted analytically. We discuss\nhow these networks could model biological processes near a critical threshold\nof cyclic instability evolving through multi-step transitions.",
      "generated_abstract": "In recent years, there has been a growing interest in exploring the\ndiscrete structure of biological networks using graph theory. The idea is to\nunderstand how the network topology affects its function and performance. The\nmain challenge in this area is the lack of suitable methods for analyzing the\ntopology of networks. In this work, we propose a novel method for analyzing the\ntopology of networks. We show that, for any graph, there is a unique pair of\nvectors $\\mathbf{x}$ and $\\mathbf{y}$ such that $\\mathbf{x}\\mathbf{x}^T=\\mathbf{I}$\nand $\\mathbf{y}\\mathbf{y}^T=\\mathbf{I}$. Then, we introduce a new graph\nstructure that is characterized by a set of two parameters: a positive\nparameter $\\lambda$ and a negative parameter $\\gamma$. This parameterization\nallows us to define a new metric that is invariant to the sign",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.17073170731707318,
          "f": 0.18181817683926477
        },
        "rouge-2": {
          "r": 0.011363636363636364,
          "p": 0.008403361344537815,
          "f": 0.009661830860932714
        },
        "rouge-l": {
          "r": 0.18055555555555555,
          "p": 0.15853658536585366,
          "f": 0.1688311638522518
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/TH/2502.06387v1",
      "true_abstract": "Human-annotated preference data play an important role in aligning large\nlanguage models (LLMs). In this paper, we investigate the questions of\nassessing the performance of human annotators and incentivizing them to provide\nhigh-quality annotations. The quality assessment of language/text annotation\nfaces two challenges: (i) the intrinsic heterogeneity among annotators, which\nprevents the classic methods that assume the underlying existence of a true\nlabel; and (ii) the unclear relationship between the annotation quality and the\nperformance of downstream tasks, which excludes the possibility of inferring\nthe annotators' behavior based on the model performance trained from the\nannotation data. Then we formulate a principal-agent model to characterize the\nbehaviors of and the interactions between the company and the human annotators.\nThe model rationalizes a practical mechanism of a bonus scheme to incentivize\nannotators which benefits both parties and it underscores the importance of the\njoint presence of an assessment system and a proper contract scheme. From a\ntechnical perspective, our analysis extends the existing literature on the\nprincipal-agent model by considering a continuous action space for the agent.\nWe show the gap between the first-best and the second-best solutions (under the\ncontinuous action space) is of $\\Theta(1/\\sqrt{n \\log n})$ for the binary\ncontracts and $\\Theta(1/n)$ for the linear contracts, where $n$ is the number\nof samples used for performance assessment; this contrasts with the known\nresult of $\\exp(-\\Theta(n))$ for the binary contracts when the action space is\ndiscrete. Throughout the paper, we use real preference annotation data to\naccompany our discussions.",
      "generated_abstract": "We consider the problem of learning a classifier for a discrete choice model\nwith unknown parameters using only observed data. We introduce a new\nclassifier-based estimator that combines a randomized estimator with a\nclassification algorithm and show that it achieves a minimax optimal rate of\n$\\sqrt{\\frac{2}{\\kappa}\\log(d/\\delta)}$ for any $\\kappa > 0$. We demonstrate\nthe practical utility of our estimator in a wide range of applications,\nincluding estimating the mean and variance of the classifier, the classifier's\ninteraction with the baseline choice model, and the classifier's performance\nacross different choice models. We also consider the case where the\nclassifier-based estimator is based on a randomized estimator for the mean and\nclassifier, and show that this estimator also achieves minimax optimal rate\nunder the same assumptions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15753424657534246,
          "p": 0.323943661971831,
          "f": 0.21198156241754984
        },
        "rouge-2": {
          "r": 0.022026431718061675,
          "p": 0.044642857142857144,
          "f": 0.029498520649142254
        },
        "rouge-l": {
          "r": 0.136986301369863,
          "p": 0.28169014084507044,
          "f": 0.1843317928322964
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/AP/2502.16988v1",
      "true_abstract": "A dynamic treatment regime is a sequence of treatment decision rules tailored\nto an individual's evolving status over time. In precision medicine, much focus\nhas been placed on finding an optimal dynamic treatment regime which, if\nfollowed by everyone in the population, would yield the best outcome on\naverage; and extensive investigation has been conducted from both\nmethodological and applications standpoints. The aim of this tutorial is to\nprovide readers who are interested in optimal dynamic treatment regimes with a\nsystematic, detailed but accessible introduction, including the formal\ndefinition and formulation of this topic within the framework of causal\ninference, identification assumptions required to link the causal quantity of\ninterest to the observed data, existing statistical models and estimation\nmethods to learn the optimal regime from data, and application of these methods\nto both simulated and real data.",
      "generated_abstract": "In a series of papers, we show that the Bayesian information criterion (BIC)\nis not a proper likelihood-based estimator, and that it is in fact an\nasymptotic estimator of the Bayes optimal likelihood, as the number of\nparameters increases. In this paper, we show that the BIC is in fact the\nasymptotic limit of a sequence of estimators of the Bayes optimal likelihood.\nWe then give an explicit formula for this asymptotic limit, and we prove that\nit is a proper likelihood-based estimator. We also show that it is an\nasymptotic estimator of the Bayes optimal likelihood, and that it is the\nasymptotic limit of the BIC in the limit of a large number of parameters. We\nalso show that the BIC and the Bayes optimal likelihood are consistent, and we\ngive an explicit formula for the asympt",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12631578947368421,
          "p": 0.23076923076923078,
          "f": 0.16326530155028013
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.044444444444444446,
          "f": 0.036363631528926264
        },
        "rouge-l": {
          "r": 0.11578947368421053,
          "p": 0.21153846153846154,
          "f": 0.14965985937340937
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10304v2",
      "true_abstract": "A popular approach to perform inference on a target parameter in the presence\nof nuisance parameters is to construct estimating equations that are orthogonal\nto the nuisance parameters, in the sense that their expected first derivative\nis zero. Such first-order orthogonalization may, however, not suffice when the\nnuisance parameters are very imprecisely estimated. Leading examples where this\nis the case are models for panel and network data that feature fixed effects.\nIn this paper, we show how, in the conditional-likelihood setting, estimating\nequations can be constructed that are orthogonal to any chosen order. Combining\nthese equations with sample splitting yields higher-order bias-corrected\nestimators of target parameters. In an empirical application we apply our\nmethod to a fixed-effect model of team production and obtain estimates of\ncomplementarity in production and impacts of counterfactual re-allocations.",
      "generated_abstract": "This paper examines the determinants of firm performance in the US manufacturing\nindustry over the period 1980-2023. Using a panel dataset comprising 1,300\nfirms, we employ panel unit root tests and Granger causality tests to assess the\nstationarity and causal influence of key macroeconomic variables, including\nGDP, interest rates, inflation, and unemployment on firm performance. We\ndiscovered that a significant portion of the variance in firm performance is\nexplained by the economy's macroeconomic variables. Furthermore, we found that\nmacroeconomic variables were positively correlated with firm performance,\nresulting in an overall positive causal effect. Our findings suggest that\neconomic conditions, particularly interest rates, inflation, and unemployment,\nhave a significant impact on firm performance, with these variables acting as\npositive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15053763440860216,
          "p": 0.175,
          "f": 0.1618497060108926
        },
        "rouge-2": {
          "r": 0.008130081300813009,
          "p": 0.009259259259259259,
          "f": 0.008658003679094332
        },
        "rouge-l": {
          "r": 0.11827956989247312,
          "p": 0.1375,
          "f": 0.12716762508603716
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.02178v1",
      "true_abstract": "This paper proposes an online inference method of the stochastic gradient\ndescent (SGD) with a constant learning rate for quantile loss functions with\ntheoretical guarantees. Since the quantile loss function is neither smooth nor\nstrongly convex, we view such SGD iterates as an irreducible and positive\nrecurrent Markov chain. By leveraging this interpretation, we show the\nexistence of a unique asymptotic stationary distribution, regardless of the\narbitrarily fixed initialization. To characterize the exact form of this\nlimiting distribution, we derive bounds for its moment generating function and\ntail probabilities, controlling over the first and second moments of SGD\niterates. By these techniques, we prove that the stationary distribution\nconverges to a Gaussian distribution as the constant learning rate\n$\\eta\\rightarrow0$. Our findings provide the first central limit theorem\n(CLT)-type theoretical guarantees for the last iterate of constant\nlearning-rate SGD in non-smooth and non-strongly convex settings. We further\npropose a recursive algorithm to construct confidence intervals of SGD iterates\nin an online manner. Numerical studies demonstrate strong finite-sample\nperformance of our proposed quantile estimator and inference method. The\ntheoretical tools in this study are of independent interest to investigate\ngeneral transition kernels in Markov chains.",
      "generated_abstract": "We consider a multi-stage classification problem where a model is trained\non the first $k$ stages and tested on the remaining stages. We consider a\nheterogeneous dataset where each sample has a single label, and we study the\nfollowing question: Can we find a sequence of classifiers that can perform well\non the entire dataset, but only on the first stage, and then only on the\nremaining stages? To answer this question, we first present a new general\nframework for analyzing sequential classification, and then we present an\nalgorithm that uses the framework to find a sequence of classifiers that can\nperform well on the entire dataset, but only on the first stage. We further\nderive an expression for the expected classification error for a specific classifier\nin the sequence, and show that the expected classification error is a\npolynomial function of the number of stages $k$ and the number of samples $n$.\nOur algorithm runs in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1796875,
          "p": 0.2911392405063291,
          "f": 0.22222221750239224
        },
        "rouge-2": {
          "r": 0.027624309392265192,
          "p": 0.04032258064516129,
          "f": 0.03278688042053282
        },
        "rouge-l": {
          "r": 0.1640625,
          "p": 0.26582278481012656,
          "f": 0.2028985460048077
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/CP/2501.15106v1",
      "true_abstract": "We study operator learning in the context of linear propagator models for\noptimal order execution problems with transient price impact \\`a la Bouchaud et\nal. (2004) and Gatheral (2010). Transient price impact persists and decays over\ntime according to some propagator kernel. Specifically, we propose to use\nIn-Context Operator Networks (ICON), a novel transformer-based neural network\narchitecture introduced by Yang et al. (2023), which facilitates data-driven\nlearning of operators by merging offline pre-training with an online few-shot\nprompting inference. First, we train ICON to learn the operator from various\npropagator models that maps the trading rate to the induced transient price\nimpact. The inference step is then based on in-context prediction, where ICON\nis presented only with a few examples. We illustrate that ICON is capable of\naccurately inferring the underlying price impact model from the data prompts,\neven with propagator kernels not seen in the training data. In a second step,\nwe employ the pre-trained ICON model provided with context as a surrogate\noperator in solving an optimal order execution problem via a neural network\ncontrol policy, and demonstrate that the exact optimal execution strategies\nfrom Abi Jaber and Neuman (2022) for the models generating the context are\ncorrectly retrieved. Our introduced methodology is very general, offering a new\napproach to solving optimal stochastic control problems with unknown state\ndynamics, inferred data-efficiently from a limited number of examples by\nleveraging the few-shot and transfer learning capabilities of transformer\nnetworks.",
      "generated_abstract": "The increasing use of machine learning (ML) in the financial sector has\nmotivated researchers to develop ML-based methods for asset pricing. While\nexisting studies have explored the use of ML models for asset pricing, they\noften rely on pre-trained models and are not designed to address real-world\nproblems. This paper introduces the DRL-ASSET, a novel framework that combines\ndeep reinforcement learning (DRL) with asset pricing. The framework integrates\nDRL with a novel approach for pricing risky assets, leveraging a\nmulti-agent-reinforcement learning framework. By incorporating DRL, this\nframework enables the pricing of risky assets using a unified framework,\nwithout the need for extensive data pre-processing. The proposed framework\nrepresents a promising approach for pricing risky assets and has the potential\nto advance the financial sector by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15436241610738255,
          "p": 0.2839506172839506,
          "f": 0.19999999543705113
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.03508771929824561,
          "f": 0.02352940730726728
        },
        "rouge-l": {
          "r": 0.1476510067114094,
          "p": 0.2716049382716049,
          "f": 0.1913043432631381
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2408.15310v1",
      "true_abstract": "Recent studies suggest that drug-drug interaction (DDI) prediction via\ncomputational approaches has significant importance for understanding the\nfunctions and co-prescriptions of multiple drugs. However, the existing silico\nDDI prediction methods either ignore the potential interactions among drug-drug\npairs (DDPs), or fail to explicitly model and fuse the multi-scale drug feature\nrepresentations for better prediction. In this study, we propose RGDA-DDI, a\nresidual graph attention network (residual-GAT) and dual-attention based\nframework for drug-drug interaction prediction. A residual-GAT module is\nintroduced to simultaneously learn multi-scale feature representations from\ndrugs and DDPs. In addition, a dual-attention based feature fusion block is\nconstructed to learn local joint interaction representations. A series of\nevaluation metrics demonstrate that the RGDA-DDI significantly improved DDI\nprediction performance on two public benchmark datasets, which provides a new\ninsight into drug development.",
      "generated_abstract": "In this study, we present a novel approach for the detection of cancer cells\nin an autologous cancer tissue microarray (TCMA) using a deep learning\n(DL) model trained on the TCMA images. This novel approach integrates the\ndeep learning model with an integrated microscopy methodology, which\nenables the detection of cancer cells in an autologous cancer tissue microarray\n(TCMA). The integrated microscopy methodology is a deep learning model,\ntrained on the TCMA images, that provides a high-resolution image of the\ncancerous tissue. This model is able to differentiate cancerous from noncancerous\ntissue. Additionally, the integrated microscopy methodology provides a\nhigh-resolution image of the cancerous tissue, which is then used to train the\ndeep learning model. This integrated microscopy methodology allows the\ndetermination of the cancer",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16842105263157894,
          "p": 0.2711864406779661,
          "f": 0.20779220306544116
        },
        "rouge-2": {
          "r": 0.031496062992125984,
          "p": 0.04395604395604396,
          "f": 0.03669724284277482
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.2542372881355932,
          "f": 0.1948051900784282
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.21311v1",
      "true_abstract": "Comb Sign is an important imaging biomarker to detect multiple\ngastrointestinal diseases. It shows up as increased blood flow along the\nintestinal wall indicating potential abnormality, which helps doctors diagnose\ninflammatory conditions. Despite its clinical significance, current detection\nmethods are manual, time-intensive, and prone to subjective interpretation due\nto the need for multi-planar image-orientation. To the best of our knowledge,\nwe are the first to propose a fully automated technique for the detection of\nComb Sign from CTE scans. Our novel approach is based on developing a\nprobabilistic map that shows areas of pathological hypervascularity by\nidentifying fine vascular bifurcations and wall enhancement via processing\nthrough stepwise algorithmic modules. These modules include utilising deep\nlearning segmentation model, a Gaussian Mixture Model (GMM), vessel extraction\nusing vesselness filter, iterative probabilistic enhancement of vesselness via\nneighborhood maximization and a distance-based weighting scheme over the\nvessels. Experimental results demonstrate that our pipeline effectively\nidentifies Comb Sign, offering an objective, accurate, and reliable tool to\nenhance diagnostic accuracy in Crohn's disease and related hypervascular\nconditions where Comb Sign is considered as one of the important biomarkers.",
      "generated_abstract": "The rapid growth of 3D ultrasound (3DUS) data, combined with advances in\nprocessors and computational power, has enabled the development of large\nscale 3DUS datasets. However, the inherent challenges of 3DUS data, including\nhigh variability and sparse acquisition, have prevented the widespread\nadoption of large scale datasets. In this paper, we propose a novel framework\nto address the challenges of 3DUS data, and present a comprehensive analysis of\nthe key issues in 3DUS data. To address the variability of 3DUS data, we\nintroduce a new dataset called 3DUS-Variability. We further propose a novel\nmethod called Cut-and-Add for 3DUS-Variability. This method is designed to\naddress the sparse acquisition problem, by leveraging the existing 3DUS\ndatasets",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09420289855072464,
          "p": 0.18309859154929578,
          "f": 0.12440190938943722
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.02,
          "f": 0.014388484602247734
        },
        "rouge-l": {
          "r": 0.08695652173913043,
          "p": 0.16901408450704225,
          "f": 0.11483253139900661
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08747v1",
      "true_abstract": "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
      "generated_abstract": "The advent of next-generation sequencing technologies has revolutionized\nthe study of complex biological systems. In this context, the development of\nstatistical and machine learning methods has allowed the improvement of data\nanalysis strategies for genomics, proteomics, and metabolomics studies. In\nparticular, a powerful approach based on kernel methods has emerged, which\nprovides an efficient way to integrate genomic, proteomic, and metabolomic data.\nIn this work, we present a comprehensive survey of the most commonly used\nkernel methods in genomics and proteomics, with a focus on their applications\nin metabolomics. We provide an introduction to the concept of kernel methods,\nincluding kernel functions, kernel matrices, and kernel matrices in\nhigh-dimensional settings. We also review the main kernel methods used in\nmetabolomics, including support vector machines, nearest-neighbor,\ndecision-tree",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.2261904761904762,
          "f": 0.18536584882141593
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10743801652892562,
          "p": 0.15476190476190477,
          "f": 0.12682926345556236
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.09744v1",
      "true_abstract": "The Sun's open-closed flux boundary (OCB) separates closed and open magnetic\nfield lines, and is the site for interchange magnetic reconnection processes\nthought to be linked to the origin of the slow solar wind (SSW). We analyse the\nglobal magnetic field structure and OCB from December 2010 to December 2019\nusing three coronal magnetic field models: a potential field source surface\n(PFSS) model, a static equilibrium magnetofrictional model, and a\ntime-dependent magnetofrictional model. We analyse the model and cycle\ndependence of the OCB length on the photosphere, as well as the magnetic flux\nin the vicinity of the OCB. Near solar maximum, the coronal magnetic field for\neach model consists predominantly of long, narrow coronal holes, and nearly all\nthe open flux lies within one supergranule-diameter (25 Mm) of the OCB. By\ncomparing to interplanetary scintillation measurements of SSW speeds, we argue\nthat the fraction of open flux within this 25 Mm band is a good predictor of\nthe amount of SSW in the heliosphere. Importantly, despite its simplicity, we\nshow that the PFSS model estimates this fraction as well as the time-dependent\nmodel. We discuss the implications of our results for understanding SSW origins\nand interchange reconnection at the OCB.",
      "generated_abstract": "The Sun's magnetic field is a fundamental component of its dynamo action,\neffectively storing the energy of solar flares and coronal mass ejections.\nHowever, the strength and location of the Sun's magnetic field is still poorly\nunderstood, especially in the solar corona. Here, we present an optical\nobservation of the active coronal region on 2024 March 11. Using the Hinode\nSolar Optical Telescope (SOT), we observed a region located above the\nchromosphere. The observations revealed a brightening of the coronal flux and\na brightening of the coronal plasma density. The brightening was accompanied by\na rapid increase in the magnetic field strength. The field strengths were\nestimated to be 1.25-2.27 MG, with a mean value of 1.72 MG. The field strength\nincreased",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.22784810126582278,
          "f": 0.1884816705419261
        },
        "rouge-2": {
          "r": 0.05027932960893855,
          "p": 0.08108108108108109,
          "f": 0.062068960792152565
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.22784810126582278,
          "f": 0.1884816705419261
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.08920v1",
      "true_abstract": "A distributed integrated sensing and communication (D-ISAC) system offers\nsignificant cooperative gains for both sensing and communication performance.\nThese gains, however, can only be fully realized when the distributed nodes are\nperfectly synchronized, which is a challenge that remains largely unaddressed\nin current ISAC research. In this paper, we propose an over-the-air\ntime-frequency synchronization framework for the D-ISAC system, leveraging the\nreciprocity of bistatic sensing channels. This approach overcomes the\nimpractical dependency of traditional methods on a direct line-of-sight (LoS)\nlink, enabling the estimation of time offset (TO) and carrier frequency offset\n(CFO) between two ISAC nodes even in non-LoS (NLOS) scenarios. To achieve this,\nwe introduce a bistatic signal matching (BSM) technique with delay-Doppler\ndecoupling, which exploits offset reciprocity (OR) in bistatic observations.\nThis method compresses multiple sensing links into a single offset for\nestimation. We further present off-grid super-resolution estimators for TO and\nCFO, including the maximum likelihood estimator (MLE) and the matrix pencil\n(MP) method, combined with BSM processing. These estimators provide accurate\noffset estimation compared to spectral cross-correlation techniques. Also, we\nextend the pairwise synchronization leveraging OR between two nodes to the\nsynchronization of $N$ multiple distributed nodes, referred to as centralized\npairwise synchronization. We analyze the Cramer-Rao bounds (CRBs) for TO and\nCFO estimates and evaluate the impact of D-ISAC synchronization on the\nbottom-line target localization performance. Simulation results validate the\neffectiveness of the proposed algorithm, confirm the theoretical analysis, and\ndemonstrate that the proposed synchronization approach can recover up to 96% of\nthe bottom-line target localization performance of the fully-synchronous\nD-ISAC.",
      "generated_abstract": "This paper proposes a novel approach for learning the optimal controller for\na class of linear multi-agent systems with complex dynamics. The approach is\nbased on a linear-quadratic-gaussian (LQG) controller and a deep learning\narchitecture, enabling fast and accurate control synthesis in high-dimensional\nand nonlinear settings. The proposed architecture is based on a fully\nend-to-end learning paradigm, where the controller is trained using a\nmulti-task learning (MTL) framework. The architecture employs a\nrepresentation-learning module that learns the LQG controller parameters\nthrough minimizing a loss function, while a feature-learning module\noptimizes the network structure, ensuring the model is capable of capturing\nfeatures relevant to the underlying dynamics. The proposed architecture\noutperforms the state-of-the-art in terms of robustness and accuracy, offering\nsignificant advantages over traditional control",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12121212121212122,
          "p": 0.24390243902439024,
          "f": 0.16194331540264564
        },
        "rouge-2": {
          "r": 0.00823045267489712,
          "p": 0.017094017094017096,
          "f": 0.011111106723612842
        },
        "rouge-l": {
          "r": 0.10909090909090909,
          "p": 0.21951219512195122,
          "f": 0.14574898341883996
        }
      }
    },
    {
      "paper_id": "cs.LO.cs/LO/2503.08530v1",
      "true_abstract": "We present a choreographic framework for modelling and\n  analysing concurrent probabilistic systems based on the PRISM\n  model-checker. This is achieved through the development of a\n  choreography language, which is a specification language that allows\n  to describe the desired interactions within a concurrent system from\n  a global viewpoint. Using choreographies gives a clear and complete\n  view of system interactions, making it easier to understand the\n  process flow and identify potential errors, which helps ensure\n  correct execution and improves system reliability. We equip our\n  language with a probabilistic semantics and then define a formal\n  encoding into the PRISM language and discuss its\n  correctness. Properties of programs written in our choreographic\n  language can be model-checked by the PRISM model-checker via their\n  translation into the PRISM language. Finally, we implement a\n  compiler for our language and demonstrate its practical\n  applicability via examples drawn from the use cases featured in the\n  PRISM website.",
      "generated_abstract": "This paper investigates the problems of computing and enumerating the\nnumber of all substrings of a given string that are contained in a given\nsubset of the string. We prove that computing the number of all substrings of a\ngiven string that are contained in a given subset is equivalent to computing the\nnumber of all substrings of the given string. We then present an efficient\nalgorithm for computing the number of all substrings of a given string that are\ncontained in a given subset, as well as an efficient algorithm for enumerating\nthe number of all substrings of a given string that are contained in a given\nsubset. We provide experimental results demonstrating the effectiveness of our\nalgorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14432989690721648,
          "p": 0.34146341463414637,
          "f": 0.2028985465479942
        },
        "rouge-2": {
          "r": 0.007246376811594203,
          "p": 0.01694915254237288,
          "f": 0.010152280068027191
        },
        "rouge-l": {
          "r": 0.13402061855670103,
          "p": 0.3170731707317073,
          "f": 0.18840579292480578
        }
      }
    },
    {
      "paper_id": "math.RA.math/KT/2502.16257v1",
      "true_abstract": "The aim of this paper is twofold. In the first part, we define the cohomology\nof a Nijenhuis Lie algebra with coefficients in a suitable representation. Our\ncohomology of a Nijenhuis Lie algebra governs the simultaneous deformations of\nthe underlying Lie algebra and the Nijenhuis operator. Subsequently, we define\nhomotopy Nijenhuis operators on $2$-term $L_\\infty$-algebras and show that in\nsome cases they are related to third cocycles of Nijenhuis Lie algebras. In\nanother part of this paper, we extend our study to (generic) Nijenhuis Lie\nbialgebras where the Nijenhuis operators on the underlying Lie algebras and Lie\ncoalgebras need not be the same. In due course, we introduce matched pairs and\nManin triples of Nijenhuis Lie algebras and show that they are equivalent to\nNijenhuis Lie bialgebras. Finally, we consider the admissible classical\nYang-Baxter equation whose antisymmetric solutions yield Nijenhuis Lie\nbialgebras.",
      "generated_abstract": "We provide a novel construction of the homotopy groups of the universal cover\nof a loop space in terms of the universal cover of a universal space. This\nconstruction is motivated by the homotopy theory of the universal cover of a\nspace and the universal cover of the loop space. It is based on a comparison\nbetween the homotopy groups of the universal cover of a space and the\nhomotopy groups of the universal cover of a loop space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10126582278481013,
          "p": 0.3076923076923077,
          "f": 0.1523809486548754
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.075,
          "f": 0.03896103511553419
        },
        "rouge-l": {
          "r": 0.10126582278481013,
          "p": 0.3076923076923077,
          "f": 0.1523809486548754
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/optics/2503.10553v1",
      "true_abstract": "Between the absorption and the emission spectral lineshapes of dense atomic\nand molecular media, such as dye solutions and alkali-noble buffer gas mixtures\nat high pressure, in many cases there exists a universal scaling, the\nKennard-Stepanov relation, which is a manifestation of detailed balance. This\nrelation plays a crucial role in recent Bose-Einstein condensation experiments\nof visible-spectral-photons in e.g. dye-solution-filled optical microcavities.\nIt has recently been proposed to use high-pressure xenon-noble gas mixtures as\na thermalization medium for vacuum-ultraviolet regime photons, so as to extend\nthe achievable wavelength range of such Bose-Einstein-condensed optical sources\nfrom the visible to the vacuum-ultraviolet regime. In this work, we report\ntwo-photon excitation spectroscopy measurements of ground state ($5p^6$) xenon\natoms subject to up to 80bar of helium or krypton buffer gas pressure,\nrespectively, in the 220nm - 260nm wavelength range. The study of such\ntwo-photon spectra is of interest e.g. for the exploration of possible pumping\nschemes of a future vacuum-ultraviolet photon Bose-Einstein condensate. We have\nalso recorded absorption and emission spectra of the $5p^6 \\leftrightarrow\n5p^56s$ single-photon transition near 147nm wavelength of xenon atoms subject\nto 80bar of krypton buffer gas pressure. We find that the ratio of absorption\nand emission follows a Kennard-Stepanov scaling, which suggests that such gas\nmixtures are promising candidates as a thermalization medium for a\nBose-Einstein condensate of vacuum-ultraviolet photons.",
      "generated_abstract": "The observation of time-dependent electric field fluctuations has become a\nstaple of experimental optics in the study of nonlinear phenomena in\nelectro-optic materials. However, the intrinsic limitations of current\nexperimental setups have hindered the observation of large-amplitude electric\nfield fluctuations. Here, we present a novel setup that enables observation of\nlarge-amplitude fluctuations in the time domain with high temporal resolution\nand high-sensitivity. We show that our setup can observe the temporal\nvariation of the electric field fluctuations in an electric field-modulated\nsupercontinuum, which is a promising platform for nonlinear studies. The\nobserved fluctuations are characterized by a power spectral density that\nfollows a logarithmic law in time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15037593984962405,
          "p": 0.29411764705882354,
          "f": 0.1990049706472613
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.061855670103092786,
          "f": 0.04026845198482098
        },
        "rouge-l": {
          "r": 0.09774436090225563,
          "p": 0.19117647058823528,
          "f": 0.12935322935372903
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.gr-qc/2503.10423v1",
      "true_abstract": "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
      "generated_abstract": "We present a systematic study of the gravitational wave (GW) signal from a\nsingle black hole (BH) merger in the framework of the general relativistic\ncosmological hydrodynamic (GRCH) model. Our analysis is based on the\nnumerical simulation of a hierarchical structure formation scenario that leads\nto the formation of a large-scale structure of the universe. The BH merger\noccurs in a dark matter-dominated universe with a flat spatial geometry. We\nfind that the GW signal from the merger of a BH with a mass of $M_\\bullet =\n10^8$ M$_\\odot$ and a BH mass of $M_{\\rm BH}=10^5$ M$_\\odot$ can be detected\nwith the Advanced Virgo Collaboration (AV) GW detector network within the next\nten years, with the Advanced LIGO (AL)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.168141592920354,
          "p": 0.25333333333333335,
          "f": 0.20212765477874617
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.05555555555555555,
          "f": 0.044444439644444965
        },
        "rouge-l": {
          "r": 0.1504424778761062,
          "p": 0.22666666666666666,
          "f": 0.18085105903406531
        }
      }
    },
    {
      "paper_id": "physics.ins-det.hep-ex/2503.09303v1",
      "true_abstract": "This contribution introduces a novel test system developed to evaluate the\nsignal transmission quality in high-speed data links for the 2026 Inner Tracker\n(ITk) upgrade of the ATLAS experiment. Using an FPGA-based data acquisition\n(DAQ) framework, the setup can run simultaneous Bit Error Rate (BER) tests for\nup to 64 channels and generate virtual eye diagrams, for qualifying the\n$\\sim$26k electrical links at the ATLAS ITk data rate of 1.28Gb/s. The paper\nincludes results from system calibration, yielding its contribution to the\nmeasured losses, and preliminary results from tests of prototype and\npre-production assemblies of on-detector links of the three ATLAS ITk Pixel\nsubsystems.",
      "generated_abstract": "A novel approach for the analysis of large-area high-resolution electron\nnuclear physics data is presented. This approach uses the non-linear regression\nof the energy distribution in the detector's response. This method enables\nin-depth analyses of the energy distribution of electrons in different\ngeometries, as well as the energy distribution of all charged particles in the\ndetector. The method is applied to data taken at the Large Hadron Collider (LHC)\nat CERN, analyzing the energy distribution of electrons and positrons produced\nin a wide range of collision energies. The method is shown to be reliable and\nsuitable for analysis of large data samples, providing a robust framework for\nthe analysis of experimental data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.18181818181818182,
          "f": 0.16901407953183908
        },
        "rouge-2": {
          "r": 0.03,
          "p": 0.031914893617021274,
          "f": 0.03092783005632985
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.16666666666666666,
          "f": 0.15492957248958558
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10004v1",
      "true_abstract": "In this paper, we present a hierarchical framework that integrates\nupper-level routing with low-level optimal trajectory planning for connected\nand automated vehicles (CAVs) traveling in an urban network. The upper-level\ncontroller efficiently distributes traffic flows by utilizing a dynamic\nre-routing algorithm that leverages real-time density information and the\nfundamental diagrams of each network edge. This re-routing approach predicts\nwhen each edge will reach critical density and proactively adjusts the routing\nalgorithm's weights to prevent congestion before it occurs. The low-level\ncontroller coordinates CAVs as they cross signal-free intersections, generating\noptimal, fuel-efficient trajectories while ensuring safe passage by satisfying\nall relevant constraints. We formulate the problem as an optimal control\nproblem and derive an analytical solution. Using the SUMO micro-simulation\nplatform, we conduct simulation experiments on a realistic network. The results\nshow that our hierarchical framework significantly enhances network performance\ncompared to a baseline static routing approach. By dynamically re-routing\nvehicles, our approach successfully reduces total travel time and mitigates\ncongestion before it develops.",
      "generated_abstract": "In this paper, we propose a novel distributed learning algorithm for\nstress-driven, nonlinear system identification, called the\n\\emph{Stress-Induced Learning} algorithm (SIL). The proposed algorithm\nleverages the information from the system's response to nonlinear inputs to\nimprove identification accuracy. Specifically, we derive a new expression for\nthe system's response to nonlinear inputs, which is a function of the system's\nstate. Using this, we propose a novel learning step for nonlinear identification\nby incorporating the information from the system's response to nonlinear\ninputs. The performance of the proposed algorithm is examined using a\nbenchmark known matrix system and a dynamical system simulator. Results from\nsimulation show that the proposed algorithm significantly improves identification\naccuracy compared to the conventional linear identification method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19166666666666668,
          "p": 0.3484848484848485,
          "f": 0.24731182337842536
        },
        "rouge-2": {
          "r": 0.03164556962025317,
          "p": 0.05154639175257732,
          "f": 0.0392156815606311
        },
        "rouge-l": {
          "r": 0.19166666666666668,
          "p": 0.3484848484848485,
          "f": 0.24731182337842536
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.12479v2",
      "true_abstract": "The motif-scaffolding problem is a central task in computational protein\ndesign: Given the coordinates of atoms in a geometry chosen to confer a desired\nbiochemical function (a motif), the task is to identify diverse protein\nstructures (scaffolds) that include the motif and maintain its geometry.\nSignificant recent progress on motif-scaffolding has been made due to\ncomputational evaluation with reliable protein structure prediction and\nfixed-backbone sequence design methods. However, significant variability in\nevaluation strategies across publications has hindered comparability of\nresults, challenged reproducibility, and impeded robust progress. In response\nwe introduce MotifBench, comprising (1) a precisely specified pipeline and\nevaluation metrics, (2) a collection of 30 benchmark problems, and (3) an\nimplementation of this benchmark and leaderboard at\ngithub.com/blt2114/MotifBench. The MotifBench test cases are more difficult\ncompared to earlier benchmarks, and include protein design problems for which\nsolutions are known but on which, to the best of our knowledge,\nstate-of-the-art methods fail to identify any solution.",
      "generated_abstract": "Recent advances in neural networks have demonstrated remarkable success in\neither directly generating or understanding natural language text, or in\nrepresenting complex data structures. However, the inherent limitations of\nneural networks in representing structured data, particularly text, remain\nunaddressed. In this paper, we propose a novel method that extends the\napproximate nearest neighbor (ANN) search paradigm by leveraging the power of\nneural networks to generate structured data. We demonstrate that this approach\ncan significantly improve the performance of existing text generation\nmodels, such as GPT-4o and GPT-4m, while preserving the expressive power of\nneural networks. In particular, our method generates richer and more\ninterpretable text by enabling neural networks to better understand the\nstructure of the data they are tasked with generating. Additionally, our method\nis particularly well suited for data that is difficult for neural networks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17857142857142858,
          "p": 0.21978021978021978,
          "f": 0.1970443300288773
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13392857142857142,
          "p": 0.16483516483516483,
          "f": 0.14778324628503497
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CV/2503.10633v1",
      "true_abstract": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
      "generated_abstract": "Recent advances in large language models (LLMs) have inspired extensive\nresearch in natural language processing (NLP). However, there is still no\npractical and effective method for enhancing LLMs in medical fields, such as\ndiagnosis, treatment, and prognosis. In this paper, we present a novel\nframework, called AIDA-L, which combines the advantages of LLMs and\nartificial intelligence (AI). Our framework integrates a pre-trained language\nmodel with an AI-based knowledge graph (KG) to provide a comprehensive\ndiagnosis and treatment of patients. First, a pre-trained LLM is used to\ngenerate medical knowledge. Then, the knowledge is integrated into the KG to\nprovide a more comprehensive understanding of patients. Finally, the KG is used\nto generate treatment plans based on the patient's medical history and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.23809523809523808,
          "f": 0.2105263108565098
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.017857142857142856,
          "f": 0.0158730109347458
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.21428571428571427,
          "f": 0.18947367927756245
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/SY/2503.09904v1",
      "true_abstract": "In studies on complex network systems using graph theory, eigen-analysis is\ntypically performed on an undirected graph model of the network. However, when\nanalyzing cascading failures in a power system, the interactions among failures\nsuggest the need for a directed graph beyond the topology of the power system\nto model directions of failure propagation. To accurately quantify failure\ninteractions for effective mitigation strategies, this paper proposes a\nstochastic interaction graph model and associated eigen-analysis. Different\ntypes of modes on failure propagations are defined and characterized by the\neigenvalues of a stochastic interaction matrix, whose absolute values are\nunity, zero, or in between. Finding and interpreting these modes helps identify\nthe probable patterns of failure propagation, either local or widespread, and\nthe participating components based on eigenvectors. Then, by lowering the\nfailure probabilities of critical components highly participating in a mode of\nwidespread failures, cascading can be mitigated. The validity of the proposed\nstochastic interaction graph model, eigen-analysis and the resulting mitigation\nstrategies is demonstrated using simulated cascading failure data on an NPCC\n140-bus system.",
      "generated_abstract": "The increasing use of deep neural networks (DNNs) in the control of robot\nmachines has led to significant advancements in the field. However, these\nadvances have often been achieved at the cost of safety. While a number of\napproaches have been proposed to address safety concerns in DNN-based control\nsystems, these approaches are typically restricted to specific scenarios or\nspecific types of DNNs. In this paper, we present a general approach to\naddress safety concerns in DNN-based control systems, including the ability to\ndeal with different types of DNNs, including deep neural networks, recurrent\nneural networks, and convolutional neural networks. We show that this approach\ncan be used to develop a safety-by-design approach for DNN-based control\nsystems. Our approach involves a safety checkpoint mechanism, which is\nincorporated in the control system to ensure that the system is within safe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.205607476635514,
          "p": 0.2619047619047619,
          "f": 0.23036648721910047
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.024390243902439025,
          "f": 0.021052626672577318
        },
        "rouge-l": {
          "r": 0.205607476635514,
          "p": 0.2619047619047619,
          "f": 0.23036648721910047
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/atom-ph/2503.09946v1",
      "true_abstract": "The radiative properties of atoms are inherently linked to their surrounding\nenvironment. Placing an electromagnetic resonator around atoms can enhance\nspontaneous emission, as shown by Purcell in the 1940s. This approach is now\nroutinely used in quantum computing and communication to channel photons\nemitted by atoms into well-defined modes and control atom-photon interactions.\nFor solid-state artificial atoms, such as color-centers, the host lattice\nintroduces an acoustic environment, allowing excited atoms to relax by emitting\nphonons. Here we observe the acoustic Purcell effect by constructing a\nspecially engineered, microwave-frequency nanomechanical resonator around a\ncolor-center spin qubit in diamond. Using a co-localized optical mode of the\nstructure that strongly couples to the color-center's excited state, we perform\nsingle-photon-level laser spectroscopy at milliKelvin temperatures and observe\nten-fold faster spin relaxation when the spin qubit is tuned into resonance\nwith a 12 GHz acoustic mode. Additionally, we use the color-center as an\natomic-scale probe to measure the broadband phonon spectrum of the\nnanostructure up to a frequency of 28 GHz. Our work establishes a new regime of\ncontrol for quantum defects in solids and paves the way for interconnects\nbetween atomic-scale quantum memories and qubits encoded in acoustic and\nsuperconducting devices.",
      "generated_abstract": "We study the dynamics of a two-level system interacting with a two-level\nsystem of a non-equilibrium thermal bath. The system is driven by a laser field\nand is initially in an entangled state. We find that the two-level system can\nreach a thermal state after a long time if the interaction is sufficiently\nstrong. When the interaction is weak, the system remains in a thermal state for\na long time. We also show that if the interaction is sufficiently strong, the\ntwo-level system can also be in a thermal state after a short time. The\ndynamics of the two-level system are also studied in the context of\nstochastic thermodynamics. We find that if the interaction is sufficiently\nstrong, the two-level system can reach a thermal state after a short time,\nwhile if the interaction is weak, the system remains in a thermal state for a\nlong time. In both cases",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11363636363636363,
          "p": 0.2727272727272727,
          "f": 0.16042780333438197
        },
        "rouge-2": {
          "r": 0.015544041450777202,
          "p": 0.03333333333333333,
          "f": 0.021201409089888386
        },
        "rouge-l": {
          "r": 0.09848484848484848,
          "p": 0.23636363636363636,
          "f": 0.1390374290028312
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.18710v1",
      "true_abstract": "Understanding convergent learning -- the extent to which artificial and\nbiological neural networks develop similar representations -- is crucial for\nneuroscience and AI, as it reveals shared learning principles and guides\nbrain-like model design. While several studies have noted convergence in early\nand late layers of vision networks, key gaps remain. First, much existing work\nrelies on a limited set of metrics, overlooking transformation invariances\nrequired for proper alignment. We compare three metrics that ignore specific\nirrelevant transformations: linear regression (ignoring affine\ntransformations), Procrustes (ignoring rotations and reflections), and\npermutation/soft-matching (ignoring unit order). Notably, orthogonal\ntransformations align representations nearly as effectively as more flexible\nlinear ones, and although permutation scores are lower, they significantly\nexceed chance, indicating a robust representational basis. A second critical\ngap lies in understanding when alignment emerges during training. Contrary to\nexpectations that convergence builds gradually with task-specific learning, our\nfindings reveal that nearly all convergence occurs within the first epoch --\nlong before networks achieve optimal performance. This suggests that shared\ninput statistics, architectural biases, or early training dynamics drive\nconvergence rather than the final task solution. Finally, prior studies have\nnot systematically examined how changes in input statistics affect alignment.\nOur work shows that out-of-distribution (OOD) inputs consistently amplify\ndifferences in later layers, while early layers remain aligned for both\nin-distribution and OOD inputs, suggesting that this alignment is driven by\ngeneralizable features stable across distribution shifts. These findings fill\ncritical gaps in our understanding of representational convergence, with\nimplications for neuroscience and AI.",
      "generated_abstract": "We present a novel framework for designing and analyzing molecular-level\nmodels of complex cellular systems, using the concept of \\emph{stochastic\nsystems of molecular models}. This approach provides a flexible and\ninterpretable framework for modeling biological processes, combining the\npredictive power of molecular dynamics simulations with the analytical\nsimplicity of stochastic models. We illustrate the potential of this framework\nby applying it to the study of the dynamics of RNA polymerization, a\nfundamental process in biology that underlies many important biological\nprocesses, from gene expression to cellular morphogenesis. By combining\nmolecular dynamics simulations with Markov chain Monte Carlo methods, we\nsimultaneously obtain the full distribution of molecular trajectories and\nestablish the statistical laws of RNA polymerization. We demonstrate the\nimportance of our approach by comparing its predictions with experimental\ndata from cellular",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10106382978723404,
          "p": 0.2235294117647059,
          "f": 0.1391941349058757
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09042553191489362,
          "p": 0.2,
          "f": 0.12454212025386108
        }
      }
    },
    {
      "paper_id": "cs.DB.stat/OT/2403.08127v2",
      "true_abstract": "Globally, there is an increased need for guidelines to produce high-quality\ndata outputs for analysis. No framework currently exists that provides\nguidelines for a comprehensive approach to producing analysis ready data (ARD).\nThrough critically reviewing and summarising current literature, this paper\nproposes such guidelines for the creation of ARD. The guidelines proposed in\nthis paper inform ten steps in the generation of ARD: ethics, project\ndocumentation, data governance, data management, data storage, data discovery\nand collection, data cleaning, quality assurance, metadata, and data\ndictionary. These steps are illustrated through a substantive case study that\naimed to create ARD for a digital spatial platform: the Australian Child and\nYouth Wellbeing Atlas (ACYWA).",
      "generated_abstract": "We consider the problem of estimating the maximum likelihood estimator (MLE)\nof a density function. Our results cover a wide range of models, including\nPoisson, exponential, normal, logistic, binomial, and Weibull distributions.\nIn each case, we provide bounds on the error between the MLE and the true\ndensity function. These bounds are both in terms of the dimension of the\nparameter space and the variance of the MLE estimator. We also provide a\nsimple heuristic to determine the dimension of the parameter space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.1509433962264151,
          "f": 0.1203007470857597
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.1509433962264151,
          "f": 0.1203007470857597
        }
      }
    },
    {
      "paper_id": "gr-qc.math/MP/2503.09222v1",
      "true_abstract": "In this paper, we study the energy conditions of charged traversable\nwormholes in the framework of $f(R, \\mathscr{L}_m)$\n  modified gravity. In the first case, we derive the shape functions (SFs) for\ntwo different choices of the charge function $\\mathcal{E}^2$ by considering the\nExponential Spheroid (ES) model and analyze the null energy condition (NEC). In\nthe second case, we consider a particular shape function and study its\nimplications for the energy conditions. In both cases, we obtain expressions\nfor energy density and pressure in radial and tangential directions. Our\nfindings show that the radial NEC remains satisfied across a wide range of\ncharge parameters $\\mathcal{E}$ consistent with established physical laws.\nHowever, the tangential NEC is only sustained in the range $0.1 \\leq\n\\mathcal{E} \\leq 0.6$; for higher charge values, violations occur, indicating\nthe formation of a throat-like structure necessary for wormhole stability.\nAdditionally, we compare the pressure-density profiles of these charged\nwormholes with those of compact objects such as neutron stars, revealing\ndistinct variations in matter distribution. This analysis highlights the\ncrucial role of charge and modified gravity in determining the stability and\nphysical characteristics of wormhole structures.",
      "generated_abstract": "We discuss the construction of a non-perturbative field theory on the\nplanar product space, $\\mathbb{P}^1 \\times \\mathbb{P}^1$, from a perturbative\nfield theory on $\\mathbb{P}^1 \\times \\mathbb{C}$. We show that the theory has\nthree families of gauge groups, with different properties: the $U(1)$ group is\na subgroup of the $U(2)$ group, the $U(1) \\times U(1)$ group is a subgroup of\nthe $U(2) \\times U(2)$, and the $SU(2)$ group is a direct product of the\n$SU(2)$ groups of the $U(1)$ and $U(1) \\times U(1)$ groups. We also discuss\nthe relation between the non-perturbative theory and the perturbative theory,\nand we present a construction of the non-perturbative theory",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08333333333333333,
          "p": 0.20408163265306123,
          "f": 0.11834319114876944
        },
        "rouge-2": {
          "r": 0.022099447513812154,
          "p": 0.04878048780487805,
          "f": 0.030418246659053072
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.20408163265306123,
          "f": 0.11834319114876944
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.16378v1",
      "true_abstract": "Machine learning (ML) has been playing important roles in drug discovery in\nthe past years by providing (pre-)screening tools for prioritising chemical\ncompounds to pass through wet lab experiments. One of the main ML tasks in drug\ndiscovery is to build quantitative structure-activity relationship (QSAR)\nmodels, associating the molecular structure of chemical compounds with an\nactivity or property. These properties -- including absorption, distribution,\nmetabolism, excretion and toxicity (ADMET) -- are essential to model compound\nbehaviour, activity and interactions in the organism. Although several methods\nexist, the majority of them do not provide an appropriate model's\npersonalisation, yielding to bias and lack of generalisation to new data since\nthe chemical space usually shifts from application to application. This fact\nleads to low predictive performance when completely new data is being tested by\nthe model. The area of Automated Machine Learning (AutoML) emerged aiming to\nsolve this issue, outputting tailored ML algorithms to the data at hand.\nAlthough an important task, AutoML has not been practically used to assist\ncheminformatics and computational chemistry researchers often, with just a few\nworks related to the field. To address these challenges, this work introduces\nAuto-ADMET, an interpretable evolutionary-based AutoML method for chemical\nADMET property prediction. Auto-ADMET employs a Grammar-based Genetic\nProgramming (GGP) method with a Bayesian Network Model to achieve comparable or\nbetter predictive performance against three alternative methods -- standard GGP\nmethod, pkCSM and XGBOOST model -- on 12 benchmark chemical ADMET property\nprediction datasets. The use of a Bayesian Network model on Auto-ADMET's\nevolutionary process assisted in both shaping the search procedure and\ninterpreting the causes of its AutoML performance.",
      "generated_abstract": "In recent years, the study of protein conformational dynamics has received\nlarge attention due to the importance of proteins in biological processes.\nHowever, the study of protein conformational dynamics in a molecular dynamics\n(MD) simulation is not straightforward, due to the high computational cost and\nthe complex interplay between protein and MD dynamics. To address these\nchallenges, we propose a new approach, named PROTO-MD, which consists of two\nkey components: (i) a protein-specific force field, and (ii) a\nmolecular-dynamics-specific force field. The protein-specific force field\nallows us to predict the MD trajectory of a protein with respect to its\nconformational state. The molecular-dynamics-specific force field is a\ngeneralization of the widely used Lennard-Jones force field, and it is able to\ncapture the non-local interaction between atoms in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11731843575418995,
          "p": 0.2727272727272727,
          "f": 0.16406249579376234
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.05309734513274336,
          "f": 0.032608691396651075
        },
        "rouge-l": {
          "r": 0.11173184357541899,
          "p": 0.2597402597402597,
          "f": 0.1562499957937623
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10397v1",
      "true_abstract": "We give a pedagogical introduction to hadron spectroscopy and structure\nstudies using functional methods. We explain the basic features of\nDyson-Schwinger, Bethe-Salpeter and Faddeev equations, which are employed to\ncalculate the spectra of mesons, baryons and four-quark states. We discuss\ndynamical mass generation as a consequence of the spontaneous breaking of\nchiral symmetry, which is intertwined with the emergence of the light pions as\nGoldstone bosons of QCD. We highlight the importance of diquark correlations in\nthe baryon sector, while for four-quark states such as the light scalar mesons\nand heavy exotics the dominant two-body clusters are typically mesons. We\nconclude with a brief discussion of hadron matrix elements like electromagnetic\nform factors and how vector-meson dominance is an automatic outcome of\nfunctional equations.",
      "generated_abstract": "We propose a new, more realistic, and non-perturbative theory of hadron\ndensity matrix, which is constructed based on the density functional theory.\nThis density functional theory is constructed in the same spirit as the\nrelativistic density functional theory. It has the same functional form as the\nrelativistic density functional theory, but its functional parameters are\ndetermined by the hadron scattering data and the strong interaction data. This\nnew theory has the same basic form as the hadron density matrix theory. It\nincludes hadron scattering data and the strong interaction data. The\nhadron-hadron scattering data are calculated with the light-cone sum rule\nmethod. The hadron-hadron scattering data are calculated with the\nlight-cone sum rule method, and the strong interaction data are calculated with\nthe chiral perturbation theory. The new theory can be used to study the\nproperties of hadrons and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21176470588235294,
          "p": 0.2903225806451613,
          "f": 0.24489795430607628
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.05319148936170213,
          "f": 0.04672896703642291
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.27419354838709675,
          "f": 0.23129251212920554
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.08262v1",
      "true_abstract": "The search for the optimal pair of active and protection paths in a network\nwith Shared Risk Link Groups (SRLG) is a challenging but high-value problem in\nthe industry that is inevitable in ensuring reliable connections on the modern\nInternet. We propose a new approach to solving this problem, with a novel use\nof statistical analysis of the distribution of paths with respect to their\ncost, which is an integral part of our innovation. The key idea in our\nalgorithm is to employ iterative updates of cost bounds, allowing efficient\npruning of suboptimal paths. This idea drives an efficacious exploration of the\nsearch space. We benchmark our algorithms against the state-of-the-art\nalgorithms that exploit the alternative strategy of conflicting links\nexclusion, showing that our approach has the advantage of finding more feasible\nconnections within a set time limit.",
      "generated_abstract": "This paper studies the dynamic programming-based algorithm of minimizing the\nminimum sum of the absolute values of the products of two given vectors.\nSpecifically, the problem is to find a minimum value of $k$ such that for all\n$i=1,...,k$, $x_i$ is a minimal sum of the absolute values of the products of\nits $i$th and $(i+1)$st coordinates. The paper proposes a dynamic programming\nalgorithm for solving this problem with a complexity of $O(n^3)$, where $n$ is\nthe number of coordinates of the vectors.\n  A dynamic programming-based algorithm for solving this problem with a\ncomplexity of $O(n^2)$ is presented in [12",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.2962962962962963,
          "f": 0.21768707018186875
        },
        "rouge-2": {
          "r": 0.03676470588235294,
          "p": 0.0625,
          "f": 0.04629629163237359
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.25925925925925924,
          "f": 0.19047618582812728
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.07357v1",
      "true_abstract": "In this work, we investigate the generalization of a multi-channel\nlearning-based replay speech detector, which employs adaptive beamforming and\ndetection, across different microphone arrays. In general, deep neural\nnetwork-based microphone array processing techniques generalize poorly to\nunseen array types, i.e., showing a significant training-test mismatch of\nperformance. We employ the ReMASC dataset to analyze performance degradation\ndue to inter- and intra-device mismatches, assessing both single- and\nmulti-channel configurations. Furthermore, we explore fine-tuning to mitigate\nthe performance loss when transitioning to unseen microphone arrays. Our\nfindings reveal that array mismatches significantly decrease detection\naccuracy, with intra-device generalization being more robust than inter-device.\nHowever, fine-tuning with as little as ten minutes of target data can\neffectively recover performance, providing insights for practical deployment of\nreplay detection systems in heterogeneous automatic speaker verification\nenvironments.",
      "generated_abstract": "This paper proposes a novel adaptive multi-objective multi-agent reinforcement\nlearning framework that integrates a multi-agent reinforcement learning (MARL)\nframework with a multi-objective optimization problem. The framework leverages\nthe multi-agent learning to identify the optimal action space for each agent\nbased on their individual goals, while optimizing the overall objective\nfunction. The proposed framework integrates the multi-objective optimization\nwith the multi-agent learning to achieve the desired objective. The\nmulti-objective optimization problem is formulated as a non-convex optimization\nproblem, and the MARL framework is used to achieve an efficient and robust\nsolution. Experiments are conducted to verify the effectiveness of the\nproposed framework. The results demonstrate that the proposed framework\neffectively combines the advantages of multi-agent learning and multi-objective\noptimization, achieving improved performance compared to the traditional\nframeworks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1262135922330097,
          "p": 0.18309859154929578,
          "f": 0.14942528252543283
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1262135922330097,
          "p": 0.18309859154929578,
          "f": 0.14942528252543283
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.02342v1",
      "true_abstract": "This paper proposes an original methodology based on Named Entity Recognition\n(NER) to identify companies involved in downstream space activities, i.e.,\ncompanies that provide services or products exploiting data and technology from\nspace. Using a rule-based approach, the method leverages a corpus of texts from\ndigitized French press articles to extract company names related to the\ndownstream space segment. This approach allowed the detection of 88 new\ndownstream space companies, enriching the existing database of the sector by\n33\\%. The paper details the identification process and provides guidelines for\nfuture replications, applying the method to other geographic areas, or adapting\nit to other industries where new entrants are challenging to identify using\ntraditional activity classifications.",
      "generated_abstract": "The development of artificial intelligence (AI) has been a topic of\nresearch and debate for decades. Despite the fact that AI has been around for\nover a century, the field continues to expand. The purpose of this paper is to\nprovide a brief overview of AI and its impact on the economy. We discuss the\nhistory of AI, its current state, and the future of AI. We also explore the\nimpact of AI on the economy, including job opportunities, technological\nprogress, and the creation of new industries. This paper aims to provide a\ncomprehensive and up-to-date overview of AI and its impact on the economy.\n  The use of AI in the economy has been growing rapidly over the past few\ndecades, with its influence spreading beyond traditional fields such as\nfinance and banking. The growth of AI has been driven by advancements",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20454545454545456,
          "p": 0.2222222222222222,
          "f": 0.21301774648786817
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.008333333333333333,
          "f": 0.00865800366560135
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.19753086419753085,
          "f": 0.18934910743461375
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2503.02697v1",
      "true_abstract": "This paper investigates an infinite horizon, discounted,\nconsumption-portfolio problem in a market with one bond, one liquid risky\nasset, and one illiquid risky asset with proportional transaction costs. We\nconsider an agent with liquidity preference, modeled by a Cobb-Douglas utility\nfunction that includes the liquid wealth. We analyze the properties of the\nvalue function and divide the solvency region into three regions: the buying\nregion, the no-trading region, and the selling region, and prove that all three\nregions are non-empty. We mathematically characterize and numerically solve the\noptimal policy and prove its optimality. Our numerical analysis sheds light on\nthe impact of various parameters on the optimal policy, and some intuition and\neconomic insights behind it are also analyzed. We find that liquidity\npreference encourages agents to retain more liquid wealth and inhibits\nconsumption, and may even result in a negative allocation to the illiquid\nasset. The liquid risky asset not only affects the location of the three\nregions but also has an impact on consumption. However, whether this impact on\nconsumption is promoted or inhibited depends on the degree of risk aversion of\nagents.",
      "generated_abstract": "We propose a novel framework for evaluating the risk and return of a\ncryptocurrency portfolio based on the analysis of the portfolio's returns\ntrajectory and its correlation with the general market return. Our framework\nutilizes a risk-neutral model to construct a virtual portfolio with the\nspecifications of the cryptocurrency portfolio, and then applies the Sharpe\nratio to evaluate the risk-return tradeoff of the virtual portfolio. The\nSharpe ratio is used to quantify the return-risk tradeoff of the virtual portfolio\nunder the risk-neutral model. We then apply the Sharpe ratio to evaluate the\nrisk-return tradeoff of the cryptocurrency portfolio. The risk-return tradeoff\nof the cryptocurrency portfolio is evaluated based on the Sharpe ratio. The\nproposed framework is tested on two cryptocurrency portfolios with a total of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12931034482758622,
          "p": 0.2777777777777778,
          "f": 0.17647058390034615
        },
        "rouge-2": {
          "r": 0.011695906432748537,
          "p": 0.021505376344086023,
          "f": 0.01515151058798347
        },
        "rouge-l": {
          "r": 0.11206896551724138,
          "p": 0.24074074074074073,
          "f": 0.15294117213564026
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/CP/2501.05278v1",
      "true_abstract": "Counterfactual estimators are critical for learning and refining policies\nusing logged data, a process known as Off-Policy Evaluation (OPE). OPE allows\nresearchers to assess new policies without costly experiments, speeding up the\nevaluation process. Online experimental methods, such as A/B tests, are\neffective but often slow, thus delaying the policy selection and optimization\nprocess.\n  In this work, we explore the application of OPE methods in the context of\nresource allocation in dynamic auction environments. Given the competitive\nnature of environments where rapid decision-making is crucial for gaining a\ncompetitive edge, the ability to quickly and accurately assess algorithmic\nperformance is essential. By utilizing counterfactual estimators as a\npreliminary step before conducting A/B tests, we aim to streamline the\nevaluation process, reduce the time and resources required for experimentation,\nand enhance confidence in the chosen policies. Our investigation focuses on the\nfeasibility and effectiveness of using these estimators to predict the outcomes\nof potential resource allocation strategies, evaluate their performance, and\nfacilitate more informed decision-making in policy selection. Motivated by the\noutcomes of our initial study, we envision an advanced analytics system\ndesigned to seamlessly and dynamically assess new resource allocation\nstrategies and policies.",
      "generated_abstract": "We consider a market in which buyers can purchase a stock at a price $x$ and\n sell it at a price $y$. The seller then receives the difference between $x$\n and $y$ as a payment. In this setting, the buyer is rational and is only\n interested in maximizing her expected utility from the transaction, and the\n seller is rational and is only interested in maximizing her expected\n utility from the transaction. We provide conditions under which the seller\n can make an efficient transaction if the buyer is rational. Our result is\n significantly stronger than previous work in the literature, and it gives a\n full characterization of the conditions under which the seller can make an\n efficient transaction in this market.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08661417322834646,
          "p": 0.18333333333333332,
          "f": 0.1176470544653838
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.03296703296703297,
          "f": 0.021897805782674456
        },
        "rouge-l": {
          "r": 0.08661417322834646,
          "p": 0.18333333333333332,
          "f": 0.1176470544653838
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2412.11084v1",
      "true_abstract": "DNA barcodes are crucial in biodiversity analysis for building automatic\nidentification systems that recognize known species and discover unseen\nspecies. Unlike human genome modeling, barcode-based invertebrate\nidentification poses challenges in the vast diversity of species and taxonomic\ncomplexity. Among Transformer-based foundation models, BarcodeBERT excelled in\nspecies-level identification of invertebrates, highlighting the effectiveness\nof self-supervised pretraining on barcode-specific datasets. Recently,\nstructured state space models (SSMs) have emerged, with a time complexity that\nscales sub-quadratically with the context length. SSMs provide an efficient\nparameterization of sequence modeling relative to attention-based\narchitectures. Given the success of Mamba and Mamba-2 in natural language, we\ndesigned BarcodeMamba, a performant and efficient foundation model for DNA\nbarcodes in biodiversity analysis. We conducted a comprehensive ablation study\non the impacts of self-supervised training and tokenization methods, and\ncompared both versions of Mamba layers in terms of expressiveness and their\ncapacity to identify \"unseen\" species held back from training. Our study shows\nthat BarcodeMamba has better performance than BarcodeBERT even when using only\n8.3% as many parameters, and improves accuracy to 99.2% on species-level\naccuracy in linear probing without fine-tuning for \"seen\" species. In our\nscaling study, BarcodeMamba with 63.6% of BarcodeBERT's parameters achieved\n70.2% genus-level accuracy in 1-nearest neighbor (1-NN) probing for unseen\nspecies. The code repository to reproduce our experiments is available at\nhttps://github.com/bioscan-ml/BarcodeMamba.",
      "generated_abstract": "Graph-based models have emerged as a powerful approach for modeling complex\nseveral biological systems. However, the integration of multiple biological\nnetworks into a unified framework remains a challenging task. This study\nintroduces the Graph-based Network Refinement (GNNRefiner) method to\nenhance the performance of graph neural networks (GNNs) in the context of\nintegrating multiple biological networks. The proposed GNNRefiner utilizes\nGraph Convolutional Networks (GCNs) to capture the structural and functional\nsimilarities among networks. This is achieved through a novel method for\nsynthesizing graph-based node embeddings through a graph-based similarity\nmechanism. The proposed method is then applied to refine the network representation\nof multiple biological networks, effectively enhancing the GNN performance.\nExperiments conducted on the MAGIC dataset demonstrate that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1346153846153846,
          "p": 0.25609756097560976,
          "f": 0.17647058371866406
        },
        "rouge-2": {
          "r": 0.014018691588785047,
          "p": 0.02727272727272727,
          "f": 0.018518514033684975
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.25609756097560976,
          "f": 0.17647058371866406
        }
      }
    },
    {
      "paper_id": "cs.LG.physics/ao-ph/2503.03038v1",
      "true_abstract": "Machine learning models have shown great success in predicting weather up to\ntwo weeks ahead, outperforming process-based benchmarks. However, existing\napproaches mostly focus on the prediction task, and do not incorporate the\nnecessary data assimilation. Moreover, these models suffer from error\naccumulation in long roll-outs, limiting their applicability to seasonal\npredictions or climate projections. Here, we introduce Generative Assimilation\nand Prediction (GAP), a unified deep generative framework for assimilation and\nprediction of both weather and climate. By learning to quantify the\nprobabilistic distribution of atmospheric states under observational,\npredictive, and external forcing constraints, GAP excels in a broad range of\nweather-climate related tasks, including data assimilation, seamless\nprediction, and climate simulation. In particular, GAP is competitive with\nstate-of-the-art ensemble assimilation, probabilistic weather forecast and\nseasonal prediction, yields stable millennial simulations, and reproduces\nclimate variability from daily to decadal time scales.",
      "generated_abstract": "The recent progress in deep generative models has been driven by the\nexplosion of large language models (LLMs) in various applications, but the\nexploitation of LLMs for image generation is still in its infancy. However,\ngenerative image models, such as Inception-V4 and Vision Transformer (ViT),\nhave shown remarkable performance in various applications, such as\nsynthetic data generation and image editing. In this work, we propose a novel\ngenerative image model, ViT-Generative, which leverages the ViT architecture\nand transformer layers to generate high-quality images. We first propose the\nViT-Generative model, which employs a novel feature transformer block to\nintegrate the image and text features into a single feature representation. The\ntransformer block is then used as the first layer of the ViT, enabling the\ntraining of the model in a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1588785046728972,
          "p": 0.20481927710843373,
          "f": 0.17894736350083112
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.024793388429752067,
          "f": 0.023166018187565356
        },
        "rouge-l": {
          "r": 0.14953271028037382,
          "p": 0.1927710843373494,
          "f": 0.1684210477113575
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2411.05425v1",
      "true_abstract": "This article presents a generic hybrid numerical method to price a wide range\nof options on one or several assets, as well as assets with stochastic drift or\nvolatility. In particular for equity and interest rate hybrid with local\nvolatility.",
      "generated_abstract": "We introduce the first quantitative market-neutral hedging strategy for\nthe multi-asset futures market. Our method is based on a new approach for\napproximating the Black-Litterman-style market neutral hedging strategy, which\nis known to be computationally intractable. To overcome this challenge, we\npropose a novel approach, which uses the log-normal approximation to the\nBlack-Litterman-style strategy. The proposed method is shown to be both\nefficient and accurate, while being simple to implement and easily extendable\nto other multi-asset futures markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.10714285714285714,
          "f": 0.13333332863209893
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.10714285714285714,
          "f": 0.13333332863209893
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10343v1",
      "true_abstract": "We present a comprehensive reappraisal of the in-medium properties of the rho\nmeson using the inverse QCD sum rules (QCDSR) formalism, offering a novel,\nmodel-independent approach to studying hadronic modifications in nuclear\nmatter. Unlike conventional QCDSR, which rely on a predefined pole+continuum\nstructure, the inverse method reconstructs the spectral function directly from\nthe operator product expansion (OPE), eliminating assumptions about the\nspectral ansatz. To the best of our knowledge, this is the first application of\nthe inverse QCDSR method to the rho meson in nuclear matter. Our analysis\nreveals a significant reduction in the rho meson mass, consistent with previous\ntheoretical predictions, and highlights the crucial role of medium-induced\nmodifications, including condensate suppression and factorization-breaking\neffects. Furthermore, we assess the sensitivity of our results to the\nfactorization assumption and higher-dimensional condensates, demonstrating the\nnecessity of refining nonperturbative contributions for an accurate description\nof in-medium hadron properties. Our findings establish the inverse QCDSR method\nas a robust alternative to conventional spectral analysis techniques, providing\na systematically controlled framework for exploring strongly interacting matter\nunder extreme conditions. These results offer important theoretical benchmarks\nfor lattice QCD simulations and heavy-ion collision experiments, shedding light\non the restoration of chiral symmetry and the evolution of hadronic matter in\ndense environments.",
      "generated_abstract": "We present a new approach to the problem of generating $K\\to\\pi\\pi$\ntransitions, based on a realistic model for hadronic interactions. The method\ninvolves a detailed study of the $K\\to\\pi\\pi$ transition amplitudes, and their\ninteraction with hadronic resonances. We find that the main contribution to\n$K\\to\\pi\\pi$ transitions arises from resonant amplitudes, and that a simple\nmodel for these amplitudes can be constructed. The model is valid to next-to-leading\norder in QCD, and is based on a quark-gluon model for the resonances. We also\npresent a simple procedure for estimating the $K\\to\\pi\\pi$ transition rates,\nwhich is based on a resummation of the leading logarithms of the QCD coupling\nconstant. We demonstrate that our approach can be applied to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.2878787878787879,
          "f": 0.1890547219573774
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.06060606060606061,
          "f": 0.0416666621549484
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.2727272727272727,
          "f": 0.17910447320115852
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.16352v1",
      "true_abstract": "We present a design-based model of a randomized experiment in which the\nobserved outcomes are informative about the joint distribution of potential\noutcomes within the experimental sample. We derive a likelihood function that\nmaintains curvature with respect to the joint distribution of potential\noutcomes, even when holding the marginal distributions of potential outcomes\nconstant -- curvature that is not maintained in a sampling-based likelihood\nthat imposes a large sample assumption. Our proposed decision rule guesses the\njoint distribution of potential outcomes in the sample as the distribution that\nmaximizes the likelihood. We show that this decision rule is Bayes optimal\nunder a uniform prior. Our optimal decision rule differs from and significantly\noutperforms a ``monotonicity'' decision rule that assumes no defiers or no\ncompliers. In sample sizes ranging from 2 to 40, we show that the Bayes\nexpected utility of the optimal rule increases relative to the monotonicity\nrule as the sample size increases. In two experiments in health care, we show\nthat the joint distribution of potential outcomes that maximizes the likelihood\nneed not include compliers even when the average outcome in the intervention\ngroup exceeds the average outcome in the control group, and that the maximizer\nof the likelihood may include both compliers and defiers, even when the average\nintervention effect is large and statistically significant.",
      "generated_abstract": "We introduce a novel estimator for heterogeneous treatment effects in\ndemand-driven markets that incorporates both average treatment effects and\ntreatment effect heterogeneity. Our estimator uses a novel estimand for average\ntreatment effects and employs a new treatment effect estimator to capture\ntreatment effect heterogeneity. We show that our estimator is consistent and\nefficient. Our estimator is particularly useful when average treatment effects\nare small or non-existent. We show that our estimator performs better than\nstandard demand-driven estimators in terms of efficiency and consistency, and\nalso provides a better approximation of the true average treatment effect. We\nalso show that our estimator can be used to test whether average treatment\neffects are zero. We apply our estimator to a real-world application,\ndemonstrating its practical utility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17475728155339806,
          "p": 0.27692307692307694,
          "f": 0.21428570954152504
        },
        "rouge-2": {
          "r": 0.017045454545454544,
          "p": 0.030927835051546393,
          "f": 0.021978017396719007
        },
        "rouge-l": {
          "r": 0.17475728155339806,
          "p": 0.27692307692307694,
          "f": 0.21428570954152504
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2402.10638v2",
      "true_abstract": "During cell division, the mitotic spindle moves dynamically through the cell\nto position the chromosomes and determine the ultimate spatial position of the\ntwo daughter cells. These movements have been attributed to the action of\ncortical force generators which pull on the astral microtubules to position the\nspindle, as well as pushing events by these same microtubules against the cell\ncortex and plasma membrane. Attachment and detachment of cortical force\ngenerators working antagonistically against centring forces of microtubules\nhave been modelled previously (Grill et al. 2005, Phys. Rev. Lett. 94:108104)\nvia stochastic simulations and mean-field Fokker-Planck equations (describing\nrandom motion of force generators) to predict oscillations of a spindle pole in\none spatial dimension. Using systematic asymptotic methods, we reduce the\nFokker-Planck system to a set of ordinary differential equations (ODEs),\nconsistent with a set proposed by Grill et al., which can provide accurate\npredictions of the conditions for the Fokker-Planck system to exhibit\noscillations. In the limit of small restoring forces, we derive an algebraic\nprediction of the amplitude of spindle-pole oscillations and demonstrate the\nrelaxation structure of nonlinear oscillations. We also show how noise-induced\noscillations can arise in stochastic simulations for conditions in which the\nmean-field Fokker-Planck system predicts stability, but for which the period\ncan be estimated directly by the ODE model and the amplitude by a related\nstochastic differential equation that incorporates random binding kinetics.",
      "generated_abstract": "Protein-protein interactions (PPIs) are crucial for cellular processes and\nprotein function. However, the number of PPIs is vast, making it challenging to\ndiscover the key ones. PPI detection methods are often limited by\ncomputational costs and need to consider the intrinsic properties of the\nproteins. Recent advancements in graph neural networks (GNNs) have demonstrated\npromise in PPI discovery, but their efficacy is often limited by the\ncomputational cost of training and inference. To address this, we introduce\nGraph-Augmented Gated Recurrent Units (GAGRUs), a novel framework for\nPPI-aware graph neural networks. By integrating graph augmentation and GNNs,\nGAGRUs enhance the ability of GNNs to capture intrinsic graph structures\nnecessary for accurate PPI prediction. Furthermore, we propose a novel\nattention",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09420289855072464,
          "p": 0.15294117647058825,
          "f": 0.11659192353355205
        },
        "rouge-2": {
          "r": 0.009523809523809525,
          "p": 0.017857142857142856,
          "f": 0.012422355711586858
        },
        "rouge-l": {
          "r": 0.07971014492753623,
          "p": 0.12941176470588237,
          "f": 0.09865470380261038
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/EM/2502.13461v1",
      "true_abstract": "Style investing creates asset classes (or the so-called \"styles\") with low\ncorrelations, aligning well with the principle of \"Holy Grail of investing\" in\nterms of portfolio selection. The returns of styles naturally form a\ntensor-valued time series, which requires new tools for studying the dynamics\nof the conditional correlation matrix to facilitate the aforementioned\nprinciple. Towards this goal, we introduce a new tensor dynamic conditional\ncorrelation (TDCC) model, which is based on two novel treatments:\ntrace-normalization and dimension-normalization. These two normalizations adapt\nto the tensor nature of the data, and they are necessary except when the tensor\ndata reduce to vector data. Moreover, we provide an easy-to-implement\nestimation procedure for the TDCC model, and examine its finite sample\nperformance by simulations. Finally, we assess the usefulness of the TDCC model\nin international portfolio selection across ten global markets and in large\nportfolio selection for 1800 stocks from the Chinese stock market.",
      "generated_abstract": "This paper examines the impact of portfolio diversification on portfolio\neffectiveness using a panel dataset comprising 3,305 portfolios of the\nHong Kong Stock Exchange (HKEX) from 2010 to 2022. The results indicate that\ndiversification improves portfolio performance, with the effect being larger\nfor smaller portfolios. However, the magnitude of the impact is limited by\nportfolio liquidity and the size of the stocks in the portfolio. Diversification\nalso improves portfolio performance in the long run, but this effect is\nreduced by the expiration effect. This finding highlights the importance of\ndiversifying portfolios and suggests that investors should manage their\nportfolios in a manner that takes into account the potential of the stocks in\nthe portfolio to improve performance over time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.24324324324324326,
          "f": 0.19999999515802477
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.027777777777777776,
          "f": 0.023904377567341206
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.20270270270270271,
          "f": 0.1666666618246915
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/plasm-ph/2503.06176v1",
      "true_abstract": "In strictly axisymmetric configurations of tokamaks, field-line tracing\nreduces from a three-dimensional ODE system to a two-dimensional one, where\nPoincar\\'e-Bendixson theorem applies and guarantees the nonexistence of chaos.\nThe formulae of functional perturbation theory (FPT) mostly simplify to compact\nclosed-form expressions to allow the computation to finish instantly, which\ncould improve and accelerate the existing plasma control systems by detangling\nthe plasma dynamics from the magnetic topology change. FPT can conveniently\ncalculate how the key geometric objects of magnetic topology:\n  1. the divertor X-point(s) and the magnetic axis,\n  2. the last closed flux surface (LCFS)\n  3. flux surfaces\n  change under perturbation. For example, when the divertor X-point shifts\noutwards, the LCFS there must expand accordingly, but not necessarily for other\nplaces of the LCFS, which could also contract, depending on the perturbation.\nFPT can not only facilitate adaptive control of plasma, but also enable\nutilizing as much as possible space in the vacuum vessel by weakening the\nplasma-wall interaction (PWI) via tuning the eigenvalues of $\\mathcal{DP}^m$ of\nthe divertor X-point(s), such that the field line connection lengths in the\nscrape-off layer (SOL) are long enough to achieve detachment. Increasing flux\nexpansion $f_x$ is another option for detachment and can also be facilitated by\nFPT.\n  Apart from the edge, FPT can also benefit the understanding of the plasma\ncore. Since the magnetic axis O-point would also shift under perturbation and\nthe shift is known by FPT, the O-point can be controlled without full knowledge\nof the plasma response, which shall not significantly change the tendency.",
      "generated_abstract": "The development of large-scale plasma machines and the corresponding\nexperimental facilities has increased the interest in the study of the\nmagnetohydrodynamic (MHD) instabilities in plasmas. However, the complexity\nof the problem and the difficulty in interpreting the experimental results\nlimit the applicability of current approaches. This study aims to provide an\nalternative approach to the study of the MHD instabilities in plasmas based on\nthe concept of dynamical systems. We introduce a new approach to analyze the\nMHD instabilities in plasmas based on the analysis of the dynamical systems.\nWe use the concept of dynamical systems and the concepts of instability and\nattractor to analyze the MHD instabilities in plasmas. The results of this\nstudy show that the MHD instabilities in plasmas are complex and difficult to\nunderstand. The existence of multiple stable solutions, chaotic behavior, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0736196319018405,
          "p": 0.18181818181818182,
          "f": 0.10480348934688523
        },
        "rouge-2": {
          "r": 0.0211864406779661,
          "p": 0.04854368932038835,
          "f": 0.029498520843362568
        },
        "rouge-l": {
          "r": 0.06748466257668712,
          "p": 0.16666666666666666,
          "f": 0.09606986489273676
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16694v1",
      "true_abstract": "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
      "generated_abstract": "The study of the evolution of the HIV-1 genome, which has persisted for\ncumulatively 35 years, has revealed a striking and puzzling feature: the\npreservation of a highly diverse set of mutations. This set of mutations\nrepresents a stochastic process, with the mutations being selected for by the\nvirus. The evolution of this set of mutations is akin to the evolution of a\nstochastic process, and the evolution of this process is also akin to the\nevolution of a stochastic process. The study of these processes, which we call\nthe stochastic process of mutations, the stochastic process of selection, and\nthe stochastic process of evolution, is the focus of the present paper.\n  We show that the HIV-1 genome is a random walk, with the mutations being\nselected for by the virus. The HIV-1 genome is a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09734513274336283,
          "p": 0.19642857142857142,
          "f": 0.13017751036168215
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.03296703296703297,
          "f": 0.02575106820092556
        },
        "rouge-l": {
          "r": 0.09734513274336283,
          "p": 0.19642857142857142,
          "f": 0.13017751036168215
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.00455v1",
      "true_abstract": "Existing Existing automatic audio generation methods struggle to generate\npodcast-like audio programs effectively. The key challenges lie in in-depth\ncontent generation, appropriate and expressive voice production. This paper\nproposed PodAgent, a comprehensive framework for creating audio programs.\nPodAgent 1) generates informative topic-discussion content by designing a\nHost-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for\nsuitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis\nmethod to generate expressive conversational speech. Given the absence of\nstandardized evaluation criteria for podcast-like audio generation, we\ndeveloped comprehensive assessment guidelines to effectively evaluate the\nmodel's performance. Experimental results demonstrate PodAgent's effectiveness,\nsignificantly surpassing direct GPT-4 generation in topic-discussion dialogue\ncontent, achieving an 87.4% voice-matching accuracy, and producing more\nexpressive speech through LLM-guided synthesis. Demo page:\nhttps://podcast-agent.github.io/demo/. Source code:\nhttps://github.com/yujxx/PodAgent.",
      "generated_abstract": "Recent advancements in speech recognition have enabled the integration of\nacoustic models into neural speech synthesis systems, enabling the generation\nof high-quality speech with complex, multi-modal text. However, these\nmulti-modal systems often fail to capture the nuanced differences between\ndifferent languages and accents. To address this, we introduce Vocaloid-T, a\nnew multilingual and multimodal Vocaloid model that learns to generate speech\naccurately from both text and speech. Our model integrates multi-modal\nlanguage models, including Tacotron 2, which can generate speech from text,\nand the ViT-based speech model, which can generate speech from audio. To\nenable real-time multilingual speech synthesis, we design a multilingual\nmulti-head attention module, which allows the model to process text and speech\nfrom multiple languages at the same time. Additionally,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1188118811881188,
          "p": 0.14634146341463414,
          "f": 0.131147536037505
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.017543859649122806,
          "f": 0.01666666167916816
        },
        "rouge-l": {
          "r": 0.1188118811881188,
          "p": 0.14634146341463414,
          "f": 0.131147536037505
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10316v1",
      "true_abstract": "In this paper, we consider a tunable liquid convex lens-assisted imaging\nreceiver for indoor multiple-input multiple-output (MIMO) visible light\ncommunication (VLC) systems. In contrast to existing MIMO VLC receivers that\nrely on fixed optical lenses, the proposed receiver leverages the additional\ndegrees of freedom offered by liquid lenses via adjusting both focal length and\norientation angles of the lens. This capability facilitates the mitigation of\nspatial correlation between the channel gains, thereby enhancing the overall\nsignal quality and leading to improved bit-error rate (BER) performance. We\npresent an accurate channel model for the liquid lens-assisted VLC system by\nusing three-dimensional geometry and geometric optics. To achieve optimal\nperformance under practical conditions such as random receiver orientation and\nuser mobility, optimization of both focal length and orientation angles of the\nlens are required. To this end, driven by the fact that channel models are\nmathematically complex, we present two optimization schemes including a\nblockwise machine learning (ML) architecture that includes convolution layers\nto extract spatial features from the received signal, long-short term memory\nlayers to predict the user position and orientation, and fully connected layers\nto estimate the optimal lens parameters. Numerical results are presented to\ncompare the performance of each scheme with conventional receivers. Results\nshow that a significant BER improvement is achieved when liquid lenses and\npresented ML-based optimization approaches are used. Specifically, the BER can\nbe improved from $6\\times 10^{-2}$ to $1.4\\times 10^{-3}$ at an average\nsignal-to-noise ratio of $30$ dB.",
      "generated_abstract": "This paper proposes a novel method for controlling the pitch of a musical\nsystem based on a continuous-time discrete-time (DT) system, which can be\napplied to various musical instruments. The system consists of a tuning\nprocessor, a pitch-shaping filter, and a pitch-detecting circuit. The tuning\nprocessor generates a series of tuning tones, each of which is processed by\nthe pitch-shaping filter to obtain a new tone. The pitch-shaping filter then\napplies a threshold to the new tone to obtain a new pitch. The pitch-detecting\ncircuit measures the new pitch, and the tuning processor provides the new\ntone with the appropriate pitch. This process is repeated until the pitch is\ncorrected. The tuning processor is controlled by a tuning knob to set the\npitch. The pitch of the musical system is controlled using this method. The\nresults show",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1165644171779141,
          "p": 0.2878787878787879,
          "f": 0.16593886052592446
        },
        "rouge-2": {
          "r": 0.01293103448275862,
          "p": 0.02586206896551724,
          "f": 0.017241374865901528
        },
        "rouge-l": {
          "r": 0.11042944785276074,
          "p": 0.2727272727272727,
          "f": 0.157205236071776
        }
      }
    },
    {
      "paper_id": "math.ST.stat/CO/2502.03849v1",
      "true_abstract": "This paper presents a new algorithm (and an additional trick) that allows to\ncompute fastly an entire curve of post hoc bounds for the False Discovery\nProportion when the underlying bound $V^*_{\\mathfrak{R}}$ construction is based\non a reference family $\\mathfrak{R}$ with a forest structure {\\`a} la Durand et\nal. (2020). By an entire curve, we mean the values\n$V^*_{\\mathfrak{R}}(S_1),\\dotsc,V^*_{\\mathfrak{R}}(S_m)$ computed on a path of\nincreasing selection sets $S_1\\subsetneq\\dotsb\\subsetneq S_m$, $|S_t|=t$. The\nnew algorithm leverages the fact that going from $S_t$ to $S_{t+1}$ is done by\nadding only one hypothesis.",
      "generated_abstract": "This paper presents a novel approach to estimating the parameter of a\ngeneralized additive model (GAM) by leveraging the generalized additive\nlog-likelihood ratio test. Specifically, we propose a novel criterion that\ncaptures the behavior of the log-likelihood ratio in the vicinity of a GAM\nparameter. The proposed criterion is shown to be a convex function of the\nGAM parameter, and is valid for any value of the parameter. We prove that the\ncriterion is an alternative to the traditional Wald test in the vicinity of the\nGAM parameter, and establish that the GAM parameter is asymptotically\noptimal in this setting. We also present a simulation study to demonstrate the\nperformance of the proposed criterion. We conclude by discussing the potential\napplications of the proposed criterion.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1917808219178082,
          "p": 0.21875,
          "f": 0.2043795570653739
        },
        "rouge-2": {
          "r": 0.03529411764705882,
          "p": 0.0297029702970297,
          "f": 0.03225805955312829
        },
        "rouge-l": {
          "r": 0.1780821917808219,
          "p": 0.203125,
          "f": 0.18978101691938848
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09848v1",
      "true_abstract": "In this work, we present a second-order numerical scheme to address the\nsolution of optimal control problems constrained by the evolution of nonlinear\nFokker-Planck equations arising from socio-economic dynamics. In order to\ndesign an appropriate numerical scheme for control realization, a coupled\nforward-backward system is derived based on the associated optimality\nconditions. The forward equation, corresponding to the Fokker-Planck dynamics,\nis discretized using a structure preserving scheme able to capture steady\nstates. On the other hand, the backward equation, modeled as a\nHamilton-Jacobi-Bellman problem, is solved via a semi-Lagrangian scheme that\nsupports large time steps while preserving stability. Coupling between the\nforward and backward problems is achieved through a gradient descent\noptimization strategy, ensuring convergence to the optimal control. Numerical\nexperiments demonstrate second-order accuracy, computational efficiency, and\neffectiveness in controlling different examples across various scenarios in\nsocial dynamics. This approach provides a reliable computational tool for the\nstudy of opinion manipulation and consensus formation in socially structured\nsystems.",
      "generated_abstract": "In this paper, we study the optimal control problem of a two-dimensional\ndynamical system subject to both a linear and a nonlinear stochastic\ndissipation. We first investigate the optimal control problem with a linear\nstochastic dissipation. We derive a necessary and sufficient condition for\nstability of the optimal control problem with linear stochastic dissipation,\nand propose an efficient algorithm to obtain the optimal control. Then we\npropose a nonlinear optimization algorithm to obtain the optimal control with\nnonlinear stochastic dissipation. We show that the nonlinear optimization\nalgorithm is able to obtain the optimal control with nonlinear stochastic\ndissipation, and that the obtained optimal control is stable. Finally, we\nconsider the optimal control problem of a two-dimensional nonlinear system. We\npropose a nonlinear optimization algorithm to obtain the optimal control, and\nshow that the obtained optimal control is stable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17699115044247787,
          "p": 0.40816326530612246,
          "f": 0.24691357602728245
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.04938271604938271,
          "f": 0.034042548673970725
        },
        "rouge-l": {
          "r": 0.168141592920354,
          "p": 0.3877551020408163,
          "f": 0.2345678970149368
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12309v1",
      "true_abstract": "Square matrices often arise in microeconomics, particularly in network models\naddressing applications from opinion dynamics to platform regulation. Spectral\ntheory provides powerful tools for analyzing their properties. We present an\naccessible overview of several fundamental applications of spectral methods in\nmicroeconomics, focusing especially on the Perron-Frobenius Theorem's role and\nits connection to centrality measures. Applications include social learning,\nnetwork games, public goods provision, and market intervention under\nuncertainty. The exposition assumes minimal social science background, using\nspectral theory as a unifying mathematical thread to introduce interested\nreaders to some exciting current topics in microeconomic theory.",
      "generated_abstract": "This paper studies the Nash equilibrium in the repeated game of auction\nestimation, where the buyer is willing to pay the highest price possible and\nthe seller is willing to accept the lowest price possible. We show that the\nattractor is a point where the buyer and the seller have identical valuations\nof the prize. We also show that the Nash equilibrium is unique for any\nprobability distribution on the prize that is non-negative and has finite\nvariance. We show that the equilibrium can be characterized in terms of a\nconstrained stochastic dominance inequality, and we establish that the\nequilibrium is an equilibrium of a game where the buyer and the seller are\ninterested in the same random variable. We further show that the Nash\nequilibrium is a global attractor for the game and we derive its analytical\nform.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.17647058823529413,
          "f": 0.16216215719503302
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.009433962264150943,
          "f": 0.010050246277621716
        },
        "rouge-l": {
          "r": 0.1375,
          "p": 0.16176470588235295,
          "f": 0.14864864368151953
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2503.07406v1",
      "true_abstract": "This paper presents a distinctive prime detection approach. This method use\nGM-(n+1) sequences to effectively eliminate complex numbers. The sequences,\nwhich consist of odd a number of (n+1), exclude all components except for the\ninitial prime integer. Only the first prime number is presented. This research\nproposes an approach using this model to identify exceptional candidates and\nexamine their distribution. This study examines the interconnections among the\nlaws of division, basic gaps, and their applications in analytical procedures.\nComputer studies may provide a novel perspective on the theory of prime\nnumbers, demonstrating the effectiveness of this approach in refining the\nsearch space for primes.",
      "generated_abstract": "Let $X$ be a smooth projective variety over an algebraically closed field of\nkinds. We prove that if the singular locus of $X$ is a singularity of type\n$\\operatorname{sing}(X)=\\alpha$, then there is a rational map $\\varphi\\colon X\\to\n\\mathbb{P}^1$ with the following properties:\n\n1. $\\varphi$ is biholomorphic to a $\\alpha$-fibration on $\\mathbb{P}^1$,\n\n2. the image of $\\varphi$ is contained in a smooth projective variety $Y$ with\nsingular locus of type $\\operatorname{sing}(Y)=\\beta$,\n\n3. $Y$ is the product of two smooth projective varieties $Y_1$ and $Y_2$\ncontaining $X$,\n\n4. the fibers of $\\varphi$ are smooth projective varieties, and\n\n5. the morphism $\\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11688311688311688,
          "p": 0.140625,
          "f": 0.1276595695105882
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11688311688311688,
          "p": 0.140625,
          "f": 0.1276595695105882
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.04206v1",
      "true_abstract": "The refraction of light by dispersion-free dielectric media can be modeled\nusing well-localized macroscopic wave packets, enabling a description in terms\nof pseudo-particles. This approach is often used in thought experiments to\nillustrate aspects of the Abraham-Minkowski debate. This work uses the particle\npicture to show at an elementary level how different types of momenta come into\nplay, and how light refraction can be explained at the level of particles. A\nspecial exactly solvable microscopic model is used to illustrate the interplay\nand tension between microscopic physics and the conventional effective-medium\nMaxwell equations.",
      "generated_abstract": "In this study, we propose a novel method to generate a multi-frequency\naccelerating wavefield with arbitrary wave directions by using the Helmholtz\nequation. This method is based on the idea of using a single-frequency wavefield\nto generate a multi-frequency wavefield by a certain number of time-shifts. In\nthe proposed method, we first formulate the Helmholtz equation as a system of\nordinary differential equations, which is solved by a time-shifting method.\nThis method is also referred to as a time-shifting-based solution for the\nHelmholtz equation. We demonstrate that the proposed method is effective in\ngenerating a multi-frequency wavefield with arbitrary wave directions. We also\nshow that the proposed method can be applied to other fields of science and\nengineering.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20588235294117646,
          "p": 0.21875,
          "f": 0.21212120712580362
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.010526315789473684,
          "f": 0.01081080581446542
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.203125,
          "f": 0.19696969197428849
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10629v1",
      "true_abstract": "Adversarial attacks pose significant challenges for vision models in critical\nfields like healthcare, where reliability is essential. Although adversarial\ntraining has been well studied in natural images, its application to biomedical\nand microscopy data remains limited. Existing self-supervised adversarial\ntraining methods overlook the hierarchical structure of histopathology images,\nwhere patient-slide-patch relationships provide valuable discriminative\nsignals. To address this, we propose Hierarchical Self-Supervised Adversarial\nTraining (HSAT), which exploits these properties to craft adversarial examples\nusing multi-level contrastive learning and integrate it into adversarial\ntraining for enhanced robustness. We evaluate HSAT on multiclass histopathology\ndataset OpenSRH and the results show that HSAT outperforms existing methods\nfrom both biomedical and natural image domains. HSAT enhances robustness,\nachieving an average gain of 54.31% in the white-box setting and reducing\nperformance drops to 3-4% in the black-box setting, compared to 25-30% for the\nbaseline. These results set a new benchmark for adversarial training in this\ndomain, paving the way for more robust models. Our Code for training and\nevaluation is available at https://github.com/HashmatShadab/HSAT.",
      "generated_abstract": "We introduce the concept of a {\\em multi-resolution coarse-to-fine network} (MRCFN)\nfor image restoration, which we generalize from the coarse-to-fine (C2F)\nframework to a multi-resolution (MR) framework. By using a C2F-like pipeline\nwith three coarse stages and two fine stages, the C2F model is designed to\nrestore a high-resolution image from a low-resolution one. However, this\napproach does not consider the high-resolution information in the intermediate\nstages and thus may lead to artifacts in the final image. In contrast, the\nMRCFN architecture, by adding a coarse-to-fine structure between the two\nfine stages, effectively captures both the high-resolution and low-resolution\ninformation. To further enhance the restoration performance, we further add a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14173228346456693,
          "p": 0.2465753424657534,
          "f": 0.17999999536450012
        },
        "rouge-2": {
          "r": 0.006097560975609756,
          "p": 0.009615384615384616,
          "f": 0.0074626818177798105
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.2328767123287671,
          "f": 0.1699999953645001
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.10323v1",
      "true_abstract": "Einstein equations can be written in the so-called Fully Constrained\nFormulation (FCF). This formulation has two different sectors: the elliptic\nsector, formed by the Hamiltonian and Momentum constraints together with the\nequations derived from the gauge choice; and the hyperbolic sector, formed by\nthe evolution of the rest of the spacetime metric variables, which encodes the\ngravitational radiation. In this work, we present a modification of both\nsectors that keeps local uniqueness properties of the elliptic system of\nequations and includes a hierarchical post-Newtonian structure of all the\nelliptic and hyperbolic equations. This reformulation can have potential\napplications in cosmology and relativistic astrophysics. Moreover, we show how\ninitial stationary data can be computed numerically using this formulation\nwithout assuming a conformally flat spatial metric, with the illustrative\nexample of a rotating neutron star.",
      "generated_abstract": "We consider the problem of constructing a localized perturbation of a scalar\nfield in a curved spacetime that preserves the topology of the domain\nmanifold. We first show that if the perturbation is sufficiently smooth,\nthe resulting domain can be continuously deformed into a domain with the same\ntopology. We then discuss the properties of the resulting domain under local\nperturbations of the scalar field. Our results reveal that, in general, the\nperturbation must be sufficiently smooth in order for the resulting domain to\nbe a deformation of the domain under local perturbations of the scalar field.\nFurthermore, we demonstrate that in certain cases the localized perturbation\ncan be made sufficiently smooth in order to construct a deformation that is\ntopologically equivalent to the domain under local perturbations of the scalar\nfield.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14444444444444443,
          "p": 0.22033898305084745,
          "f": 0.1744966395117338
        },
        "rouge-2": {
          "r": 0.04065040650406504,
          "p": 0.05,
          "f": 0.044843044380543096
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.2033898305084746,
          "f": 0.16107382071978757
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/NE/2503.05573v1",
      "true_abstract": "Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm\nfor autonomous driving, where data efficiency and robustness are critical. Yet,\nexisting solutions often rely on carefully crafted, task specific extrinsic\nrewards, limiting generalization to new tasks or environments. In this paper,\nwe propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle\nExploration), a method that leverages purely intrinsic, disagreement based\nrewards within a Dreamer based MBRL framework. By training an ensemble of world\nmodels, the agent actively explores high uncertainty regions of environments\nwithout any task specific feedback. This approach yields a task agnostic latent\nrepresentation, allowing for rapid zero shot or few shot fine tuning on\ndownstream driving tasks such as lane following and collision avoidance.\nExperimental results in both seen and unseen environments demonstrate that\nInDRiVE achieves higher success rates and fewer infractions compared to\nDreamerV2 and DreamerV3 baselines despite using significantly fewer training\nsteps. Our findings highlight the effectiveness of purely intrinsic exploration\nfor learning robust vehicle control behaviors, paving the way for more scalable\nand adaptable autonomous driving systems.",
      "generated_abstract": "Recent advancements in 3D object detection and reconstruction have\ndemonstrated the potential of generative models in unsupervised object\nreconstruction. However, existing methods often rely on high-quality\npre-trained models for reconstruction, which are often trained on\nsupervised datasets. This approach limits their generalization capabilities,\nespecially in novel environments, where they face challenges such as\nunseen object classes and unseen viewpoints. To address this limitation, we\npropose Pixrecon, a novel method that leverages a generative model to learn\nreconstruction-free representations of 3D objects. Specifically, we train a\ngenerative model that maps 3D objects into a low-dimensional latent space,\nwhere each object is represented by a unique point in this space. During\nreconstruction, we utilize the learned latent space to predict the object\nreconstruction. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19852941176470587,
          "p": 0.3176470588235294,
          "f": 0.2443438866689872
        },
        "rouge-2": {
          "r": 0.04678362573099415,
          "p": 0.06837606837606838,
          "f": 0.05555555073133722
        },
        "rouge-l": {
          "r": 0.19852941176470587,
          "p": 0.3176470588235294,
          "f": 0.2443438866689872
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ML/2503.03356v1",
      "true_abstract": "We use tools from random matrix theory to study the multi-spiked tensor\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\nparticular, thanks to the nature of local optimization methods used to find the\nmaximum likelihood estimator of this model, we propose to study the phase\ntransition phenomenon for finding critical points of the corresponding\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\nestimated signals corresponding to a critical point of the likelihood and the\nground truth signals. With the help of these results, we propose a new\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\nequations, which is asymptotically unbiased contrary the maximum likelihood\nestimator.",
      "generated_abstract": "In the statistical learning literature, the theory of Gaussian process\n(GP) regression is a powerful framework that enables the prediction of\nfunctional values, or moments, from a finite number of samples drawn from a\nGaussian distribution. In the context of image processing, this framework\nprovides a powerful foundation for estimating the moments of image\nfeatures. In this work, we extend the GP framework to the setting of\nunivariate functions by introducing a Gaussian process (GP) for univariate\nfunctions. We demonstrate that the GP for univariate functions offers\nsignificant advantages over the conventional GP for multivariate functions,\nparticularly in terms of prediction accuracy and robustness to outliers. We\nprovide theoretical guarantees for the convergence of the GP for univariate\nfunctions and demonstrate its superiority over the conventional GP for\nmultivariate functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.21428571428571427,
          "f": 0.20270269771731203
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.018518518518518517,
          "f": 0.0178571378635218
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.18571428571428572,
          "f": 0.17567567069028506
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.10099v1",
      "true_abstract": "While the trend of decentralized governance is obvious (cryptocurrencies and\nblockchains are widely adopted by multiple sovereign countries), initiating\ngovernance proposals within Decentralized Autonomous Organizations (DAOs) is\nstill challenging, i.e., it requires providing a low-level transaction payload,\ntherefore posing significant barriers to broad community participation. To\naddress these challenges, we propose a multi-agent system powered by Large\nLanguage Models with a novel Label-Centric Retrieval algorithm to automate the\ntranslation from natural language inputs into executable proposal transactions.\nThe system incorporates DAOLang, a Domain-Specific Language to simplify the\nspecification of various governance proposals. The key optimization achieved by\nDAOLang is a semantic-aware abstraction of user input that reliably secures\nproposal generation with a low level of token demand. A preliminary evaluation\non real-world applications reflects the potential of DAOLang in terms of\ngenerating complicated types of proposals with existing foundation models, e.g.\nGPT-4o.",
      "generated_abstract": "In 2019, Google announced that it would ban websites that hosted third\nparties' code in their search results. This is a significant change for\ndevelopers, who rely on third-party code for their products. In this paper, we\npresent a study of the effect of the new policy on developers' usage of Google\nSearch. We analyzed data from over 2 million websites, using Google's search\nengine to identify pages with third-party code. Our results show that, even\nwithin the Google-hosted search results, developers are not using third-party\ncode to the same extent as they previously did. We discuss how our findings\nrelate to broader trends in software development and the challenges of\nmanaging third-party code in large software systems. We also discuss the\npotential impact of our findings on developer behavior, and we present a\nframework for evaluating and improving the policy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1504424778761062,
          "p": 0.17894736842105263,
          "f": 0.16346153349898315
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.015151515151515152,
          "f": 0.014598535152647026
        },
        "rouge-l": {
          "r": 0.1415929203539823,
          "p": 0.16842105263157894,
          "f": 0.15384614888359854
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.10578v1",
      "true_abstract": "In this paper we investigate the generalization error of gradient descent\n(GD) applied to an $\\ell_2$-regularized OLS objective function in the linear\nmodel. Based on our analysis we develop new methodology for computationally\ntractable and statistically efficient linear prediction in a high-dimensional\nand massive data scenario (large-$n$, large-$p$). Our results are based on the\nsurprising observation that the generalization error of optimally tuned\nregularized gradient descent approaches that of an optimal benchmark procedure\n$monotonically$ in the iteration number $m$. On the other hand standard GD for\nOLS (without explicit regularization) can achieve the benchmark only in\ndegenerate cases. This shows that (optimal) explicit regularization can be\nnearly statistically efficient (for large $m$) whereas implicit regularization\nby (optimal) early stopping can not.\n  To complete our methodology, we provide a fully data driven and\ncomputationally tractable choice of $\\ell_2$ regularization parameter $\\lambda$\nthat is computationally cheaper than cross-validation. On this way, we follow\nand extend ideas of Dicker (2014) to the non-gaussian case, which requires new\nresults on high-dimensional sample covariance matrices that might be of\nindependent interest.",
      "generated_abstract": "In this paper, we propose a novel framework for estimating the optimal\nnonparametric mean estimator in the presence of a misspecified parametric\nmodel. In particular, we propose an efficient method for estimating the mean\nestimator under a misspecified parametric model when the parameter is unknown\nand the number of observations is fixed. Our method combines a quasi-likelihood\nestimator with the penalized least squares estimator, which is an extension of\nthe penalized spline regression estimator. The proposed method is based on the\nproposed quasi-likelihood estimator, which is a penalized least squares\nestimator that is based on the penalized likelihood estimator. Our proposed\nmethod is robust to misspecification of the parametric model, and the\ncomputational complexity of our proposed method is much lower than that of the\npenalized spline regression estimator. Numer",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20168067226890757,
          "p": 0.38095238095238093,
          "f": 0.26373625920963656
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.058823529411764705,
          "f": 0.04444443974321038
        },
        "rouge-l": {
          "r": 0.18487394957983194,
          "p": 0.3492063492063492,
          "f": 0.24175823723161458
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2502.11954v1",
      "true_abstract": "This paper presents a novel approach to stochastic volatility (SV) modeling\nby utilizing nonparametric techniques that enhance our ability to capture the\nvolatility of financial time series data, with a particular emphasis on the\nnon-Gaussian behavior of asset return distributions. Although traditional\nparametric SV models can be useful, they often suffer from restrictive\nassumptions regarding errors, which may inadequately represent extreme values\nand tail behavior in financial returns. To address these limitations, we\npropose two semiparametric SV models that use data to better approximate error\ndistributions. To facilitate the computation of model parameters, we developed\na Markov Chain Monte Carlo (MCMC) method for estimating model parameters and\nvolatility dynamics. Simulations and empirical tests on S&P 500 data indicate\nthat nonparametric models can minimize bias and variance in volatility\nestimation, providing a more accurate reflection of market expectations about\nvolatility. This methodology serves as a promising alternative to conventional\nparametric models, improving precision in financial risk assessment and\ndeepening our understanding of the volatility dynamics of financial returns.",
      "generated_abstract": "We study the Bayesian inference for the Markov Chain Monte Carlo\n(MCMC) algorithm of the Ornstein-Uhlenbeck process. In this process, the mean\nprocess evolves with a stochastic diffusion. We provide a sufficient\nconditionalization condition to the mean process, which allows us to derive a\nBayesian estimator of the mean process. Then, we derive a posteriori error\nestimates for the mean process. We also show that the posterior mean process\ncan be written as a Markov Chain Monte Carlo (MCMC) process. We also derive the\nposteriori error estimates for the Markov Chain Monte Carlo (MCMC) process.\nFinally, we apply our methods to the Bayesian inference for the Ornstein-Uhlenbeck\nprocess.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16806722689075632,
          "p": 0.39215686274509803,
          "f": 0.23529411344705892
        },
        "rouge-2": {
          "r": 0.05660377358490566,
          "p": 0.12,
          "f": 0.07692307256739013
        },
        "rouge-l": {
          "r": 0.15966386554621848,
          "p": 0.37254901960784315,
          "f": 0.22352940756470593
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2502.15867v1",
      "true_abstract": "Artificial intelligence (AI) is transforming scientific research, including\nproteomics. Advances in mass spectrometry (MS)-based proteomics data quality,\ndiversity, and scale, combined with groundbreaking AI techniques, are unlocking\nnew challenges and opportunities in biological discovery. Here, we highlight\nkey areas where AI is driving innovation, from data analysis to new biological\ninsights. These include developing an AI-friendly ecosystem for proteomics data\ngeneration, sharing, and analysis; improving peptide and protein identification\nand quantification; characterizing protein-protein interactions and protein\ncomplexes; advancing spatial and perturbation proteomics; integrating\nmulti-omics data; and ultimately enabling AI-empowered virtual cells.",
      "generated_abstract": "The emergence of modern biotechnology has resulted in the development of\nvarious tools for genetic analysis and manipulation. These tools, while\nimportant, have limited applicability due to the lack of a standardized,\ncomprehensive, and freely available database of genetic sequences. This\nrestriction has resulted in the development of alternative approaches for\nbiological research. These alternative approaches, such as DNA-based\nelectrochemical sensors, enable the detection of biological molecules at the\nnanoscale. This study presents a comprehensive and standardized database for\nDNA-based electrochemical sensors, including their properties, their\ninterpretation in terms of molecular interactions, and their application in\ndifferent fields of science and technology. The database is based on\nliterature and patents, and includes all the essential information about\ndifferent types of electrochemical sensors, their properties, and their\napp",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12162162162162163,
          "p": 0.11392405063291139,
          "f": 0.11764705382886945
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.11392405063291139,
          "f": 0.11764705382886945
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2502.00024v1",
      "true_abstract": "This project focuses on analyzing retail market trends using historical sales\ndata, search trends, and customer reviews. By identifying the patterns and\ntrending products, the analysis provides actionable insights for retailers to\noptimize inventory management and marketing strategies, ultimately enhancing\ncustomer satisfaction and maximizing revenue.",
      "generated_abstract": "This paper investigates the pricing of option-related derivatives with\nintroduced concepts and tools from the theory of stochastic partial differential\nequations (SPDEs). The pricing method is based on the use of the\nmulti-dimensional BSDEs with stochastic volatility, which are solved using the\nrecently developed multi-dimensional BSDE solver of the Q-Finance project. The\nsolution of these SPDEs is performed with the help of the BSDE solver for\nstochastic volatility (BSSV) of the Q-Finance project. The SPDEs are solved\nusing the multi-dimensional BSDE solver of the Q-Finance project, which\nintroduces an additional term in the BSDEs that ensures the continuity of the\nsolution. The effectiveness of the developed pricing method is demonstrated\nusing the option pricing example with the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.11666666666666667,
          "f": 0.13999999520000017
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.175,
          "p": 0.11666666666666667,
          "f": 0.13999999520000017
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02713v1",
      "true_abstract": "This paper presents new empirical evidence from four emerging economies on\nthe relationship between educational assortative mating and household income\ninequality. Using a methodological approach that allows for studying marital\nsorting patterns without imposing restrictive assumptions about search\nfrictions, the study finds that people in Brazil, Indonesia, Mexico, and South\nAfrica tend to sort into internally homogeneous marriages based on education\nlevel. While educational sorting has a noticeable impact on household income\ninequality in any given year, changes in the degree of sorting over time barely\nhave any impact on inequality. Further analysis reveals that this\ncounterintuitive result is due to different dynamics within educational groups.\nThe inequality-decreasing impact from reduced sorting among the highly educated\nis almost entirely offset by the inequality-increasing impact from increased\nsorting among the least educated. While it is certainly reassuring that\nconcerns about educational assortative mating having a potentially large effect\non income disparities between households appear to be unwarranted, these\nfindings suggest another concerning narrative. Marginalization processes are\noccurring at low levels of the educational distribution. The least educated are\nbeing left behind, facing limited labor market opportunities and diminished\nchances of achieving upward socioeconomic mobility through marriage to more\neducated partners.",
      "generated_abstract": "This paper explores the implications of an emerging technology, blockchain,\nfor the governance of public goods. By analyzing the current governance of\npublic goods in the United States, the paper proposes that blockchain technology\ncan enhance the efficiency and effectiveness of public goods by providing\nefficient mechanisms for the creation, storage, and distribution of public\ngoods. This paper explores the potential implications of blockchain technology\non public goods and presents an analytical framework for assessing the\nimplications of blockchain technology on the governance of public goods. The\npaper concludes by offering policy recommendations for policymakers and\ngovernments to consider when implementing blockchain technology for the\nmanagement of public goods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1056338028169014,
          "p": 0.2777777777777778,
          "f": 0.1530612204977094
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.037037037037037035,
          "f": 0.022222218022223014
        },
        "rouge-l": {
          "r": 0.1056338028169014,
          "p": 0.2777777777777778,
          "f": 0.1530612204977094
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/mes-hall/2503.09689v1",
      "true_abstract": "Using a self-consistent Hartree-Fock theory, we show that the recently\nobserved ferromagnetism in twisted bilayer WSe$_2$ [Nat. Commun. 16, 1959\n(2025)] can be understood as a Stoner-like instability of\ninteraction-renormalized moir\\'e bands. We quantitatively reproduce the\nobserved Lifshitz transition as function of hole filling and applied electric\nfield that marks the boundary between layer-hybridized and layer-polarized\nregimes. The former supports a ferromagnetic valley-polarized ground state\nbelow half-filling, developing a topological charge gap at half-filling for\nsmall twists. At larger twist angles there is a transition to a gapped\ntriangular N\\'eel antiferromagnet. The layer-polarized regime supports a stripe\nantiferromagnet below half-filling and a wing-shaped multiferroic ground state\nabove half-filling. We map the evolution of these states as a function of\nfilling factor, electric field, twist angle, and interaction strength. Beyond\nproviding an understanding of recent experiments, our methodology is applicable\nto a broad class of moir\\'e systems.",
      "generated_abstract": "We investigate the electronic properties of the high-symmetry point $M$ in\nthe superconducting phase of the superfluid $^3He$B. The superfluid is\nsuperconducting in the $M$ point, and the $^3He$B superconducting in the\n$M$+$K$ point. The $^3He$B superconducting state is in the superconducting\ngapless phase, and the superfluid $^3He$B in the $M$+$K$ point is in the\nsuperconducting gapless phase. We perform a quantum Monte Carlo study of the\nsuperconducting gap and the superfluid $^3He$B gap, using a two-orbital model for\nthe superconducting gap. We obtain a gapless superconducting gap of $2.3$\nmeV in the supercon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10576923076923077,
          "p": 0.28205128205128205,
          "f": 0.1538461498792118
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10576923076923077,
          "p": 0.28205128205128205,
          "f": 0.1538461498792118
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.hep-th/2503.10423v1",
      "true_abstract": "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
      "generated_abstract": "We present a novel approach to the quantum dynamics of quantum fields in a\nnon-linear scalar field theory. By coupling the action to the Hamiltonian of\nthe field, we can consider a non-linear version of the Kadanoff-Baym equation\nin a non-linear field theory. We solve this equation for the evolution of the\nHamiltonian, and find the equations of motion for the fields. We find that\nthe equations of motion are in fact exactly the same as the Kadanoff-Baym\nequation, and the field equations reduce to a linear set of ordinary\ndifferential equations. We then show that the solutions to the non-linear\nHamiltonian equations of motion are precisely the solutions to the linear\nHamiltonian equations. We then find the equations of motion for the quantum\nfields, and show that the solutions to the equations of motion are precisely\nthe solutions to the quantum equations. We find that the quantum equations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13274336283185842,
          "p": 0.2830188679245283,
          "f": 0.18072288721948043
        },
        "rouge-2": {
          "r": 0.030864197530864196,
          "p": 0.05154639175257732,
          "f": 0.03861003392495697
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.24528301886792453,
          "f": 0.15662650167731176
        }
      }
    },
    {
      "paper_id": "math.SG.math/GT/2503.10283v1",
      "true_abstract": "Given a closed connected symplectic manifold $(M,\\omega)$, we construct an\nalternating $\\mathbb{R}$-bilinear form\n$\\mathfrak{b}=\\mathfrak{b}_{\\mu_{\\mathrm{Sh}}}$ on the real first cohomology of\n$M$ from Shelukhin's quasimorphism $\\mu_{\\mathrm{Sh}}$. Here\n$\\mu_{\\mathrm{Sh}}$ is defined on the universal cover of the group of\nHamiltonian diffeomorphisms on $(M,\\omega)$. This bilinear form is invariant\nunder the symplectic mapping class group action, and $\\mathfrak{b}$ yields a\nconstraint on the fluxes of commuting two elements in the group of\nsymplectomorphisms on $(M,\\omega)$. These results might be seen as an analog of\nRousseau's result for an open connected symplectic manifold, where he recovered\nthe symplectic pairing from the Calabi homomorphism. Furthermore,\n$\\mathfrak{b}$ controls the extendability of Shelukhin's quasimorphisms, as\nwell as the triviality of a characteristic class of Reznikov. To construct\n$\\mathfrak{b}$, we build general machinery for a group $G$ of producing a\nreal-valued $\\mathbb{Z}$-bilinear form $\\mathfrak{b}_{\\mu}$ from a\n$G$-invariant quasimorphism $\\mu$ on the commutator subgroup of $G$.",
      "generated_abstract": "We study the problem of constructing a minimal cofibrant replacement of a\ncoalgebroid in the category of dg categories over a field of characteristic\nzero. We give a characterization of a minimal cofibrant replacement in terms of\nthe existence of a minimal cofibrant replacement for the trivial coalgebroid\nwith trivial fiber. We also prove that the existence of a minimal cofibrant\nreplacement for the trivial coalgebroid implies the existence of a minimal\ncofibrant replacement for the trivial dg category. We also give a characterization\nof a minimal cofibrant replacement of a dg category in terms of the existence\nof a minimal cofibrant replacement of its trivial dg category. We also prove\nthat the existence of a minimal cofibrant replacement of the trivial dg category\nimplies the existence of a minimal cofibrant replacement for the trivial dg\ncategory.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06593406593406594,
          "p": 0.1875,
          "f": 0.09756097176019582
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.05172413793103448,
          "f": 0.030612240730946004
        },
        "rouge-l": {
          "r": 0.054945054945054944,
          "p": 0.15625,
          "f": 0.08130080915856983
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.03356v1",
      "true_abstract": "We use tools from random matrix theory to study the multi-spiked tensor\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\nparticular, thanks to the nature of local optimization methods used to find the\nmaximum likelihood estimator of this model, we propose to study the phase\ntransition phenomenon for finding critical points of the corresponding\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\nestimated signals corresponding to a critical point of the likelihood and the\nground truth signals. With the help of these results, we propose a new\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\nequations, which is asymptotically unbiased contrary the maximum likelihood\nestimator.",
      "generated_abstract": "In this paper we introduce a new measure of uncertainty in the\nrandom variable $X$ that depends on the conditional distribution $P(X|Y=y)$\nwith respect to $Y$. In other words, we study the uncertainty of $X$ conditional\non $Y$ given $Y$. We show that the measure of uncertainty in $X$ conditional on\n$Y$ given $Y$ can be expressed as a function of the probability distribution\n$P(X|Y)$ and a constant. We also show that the measure of uncertainty in $X$\nconditional on $Y$ given $Y$ is a concave function of the probability\ndistribution $P(X|Y)$. Furthermore, we show that the measure of uncertainty in\n$X$ conditional on $Y$ given $Y$ is a convex function of the probability\ndistribution $P(X|Y)$. We also present a sufficient condition for the\nexistence of a measure of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.28,
          "f": 0.21874999523925792
        },
        "rouge-2": {
          "r": 0.034482758620689655,
          "p": 0.04878048780487805,
          "f": 0.040404035551474926
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.24,
          "f": 0.18749999523925795
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.05290v1",
      "true_abstract": "Transformers are central to advances in artificial intelligence (AI),\nexcelling in fields ranging from computer vision to natural language\nprocessing. Despite their success, their large parameter count and\ncomputational demands challenge efficient acceleration. To address these\nlimitations, this paper proposes MatrixFlow, a novel co-designed\nsystem-accelerator architecture based on a loosely coupled systolic array\nincluding a new software mapping approach for efficient transformer code\nexecution. MatrixFlow is co-optimized via a novel dataflow-based matrix\nmultiplication technique that reduces memory overhead. These innovations\nsignificantly improve data throughput, which is critical for handling the\nextensive computations required by transformers. We validate our approach\nthrough full system simulation using gem5 across various BERT and ViT\nTransformer models featuring different data types, demonstrating significant\napplication-wide speed-ups. Our method achieves up to a 22x improvement\ncompared to a many-core CPU system, and outperforms the closest\nstate-of-the-art loosely-coupled and tightly-coupled accelerators by over 5x\nand 8x, respectively.",
      "generated_abstract": "This paper introduces a novel approach to modeling the interplay between\naccuracy and latency in multi-agent systems, which we refer to as the\naccuracy-latency trade-off (ALT). A multi-agent system (MAS) is a network of\nagents that collectively perform a task, with each agent being task-interested\nand capable of executing a specific task. We define the accuracy-latency\ntrade-off as the trade-off between the accuracy of the task execution and the\nlatency of the execution. In this paper, we propose a framework for modeling\nthe accuracy-latency trade-off in MASs and analyze the performance of different\nMASs with different accuracy-latency trade-offs. Our analysis shows that a\nhigh-accuracy MAS with low-latency, or a low-accuracy MAS with high",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14173228346456693,
          "p": 0.26865671641791045,
          "f": 0.18556700578754398
        },
        "rouge-2": {
          "r": 0.00684931506849315,
          "p": 0.009615384615384616,
          "f": 0.00799999514112295
        },
        "rouge-l": {
          "r": 0.14173228346456693,
          "p": 0.26865671641791045,
          "f": 0.18556700578754398
        }
      }
    },
    {
      "paper_id": "math.MG.math/GN/2502.11615v1",
      "true_abstract": "The Gromov-Hausdorff distance is a dissimilarity metric capturing how far two\nspaces are from being isometric. The Gromov-Prokhorov distance is a similar\nnotion for metric measure spaces. In this paper, we study the topological\ndimension of the Gromov-Hausdorff and Gromov-Prokhorov spaces. We show that the\ndimension of the space of isometry classes of metric spaces with at most $n$\npoints endowed with the Gromov-Hausdorff distance is $\\frac{n(n-1)}{2}$, and\nthat of mm-isomorphism classes of metric measure spaces whose support consists\nof $n$ points is $\\frac{(n+2)(n-1)}{2}$. Hence, the spaces of all isometry\nclasses of finite metric spaces and of all mm-isomorphism classes of finite\nmetric measure spaces are strongly countable dimensional. If, instead, the\ncardinalities are not limited, the spaces are strongly infinite-dimensional.",
      "generated_abstract": "We introduce the theory of semi-local varieties, i.e. a variety $X$ over a\nfinite field $k$ is a semi-local variety if it is a locally closed subvariety\nof the affine space $\\prod_{i=1}^{n}k(t_{i})$, $n\\geq 1$, over $k$. The\nsemi-locality of a variety $X$ is defined as the intersection property\n$\\cap_{i=1}^{n}X_{t_{i}} \\subset X$ for the closed subvarieties $X_{t_{i}}$\ndefined by the $t_{i}$'s. We show that the semi-locality of $X$ can be\ncharacterized in terms of the semi-locality of its closure. This allows us to\nobtain a sufficient condition for semi-locality of a variety over a field $k$\nfrom the semi-locality",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19672131147540983,
          "p": 0.20689655172413793,
          "f": 0.20168066727208542
        },
        "rouge-2": {
          "r": 0.05319148936170213,
          "p": 0.058823529411764705,
          "f": 0.055865916800349996
        },
        "rouge-l": {
          "r": 0.19672131147540983,
          "p": 0.20689655172413793,
          "f": 0.20168066727208542
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.04883v1",
      "true_abstract": "Automatic Speech Recognition (ASR) performance for low-resource languages is\nstill far behind that of higher-resource languages such as English, due to a\nlack of sufficient labeled data. State-of-the-art methods deploy\nself-supervised transfer learning where a model pre-trained on large amounts of\ndata is fine-tuned using little labeled data in a target low-resource language.\nIn this paper, we present and examine a method for fine-tuning an SSL-based\nmodel in order to improve the performance for Frisian and its regional dialects\n(Clay Frisian, Wood Frisian, and South Frisian). We show that Frisian ASR\nperformance can be improved by using multilingual (Frisian, Dutch, English and\nGerman) fine-tuning data and an auxiliary language identification task. In\naddition, our findings show that performance on dialectal speech suffers\nsubstantially, and, importantly, that this effect is moderated by the\nelicitation approach used to collect the dialectal data. Our findings also\nparticularly suggest that relying solely on standard language data for ASR\nevaluation may underestimate real-world performance, particularly in languages\nwith substantial dialectal variation.",
      "generated_abstract": "This paper presents the first systematic and comprehensive study of the\ncombination of language and music. We introduce a novel methodology for\ncombining language and music in the task of music-text summarization. Our\napproach relies on a multimodal model that simultaneously encodes both\ntextual and musical information in a unified representation, enabling a more\ncomprehensive understanding of the music-text pair. We evaluate our model\nacross four datasets: Music-Text Summarization (MTS), Music-Text Generation\n(MTG), Music-Text Generation with Temporal Relations (MTG-TR), and Music-Text\nGeneration with Emotional Relations (MTG-ER). Our results demonstrate that our\nmultimodal model outperforms state-of-the-art models in all datasets. The\ncombination of music and language is an important topic in music and language",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.2535211267605634,
          "f": 0.19780219304371466
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.00980392156862745,
          "f": 0.007575752834025006
        },
        "rouge-l": {
          "r": 0.15315315315315314,
          "p": 0.23943661971830985,
          "f": 0.18681318205470362
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2412.13311v1",
      "true_abstract": "This paper investigates cash productivity as a signal for future stock\nperformance, building on the cash-return framework of Faulkender and Wang\n(2006). Using financial and market data from WRDS, we calculate cash returns as\na proxy for operational efficiency and evaluate a long-only strategy applied to\nNasdaq-listed non-financial firms. Results show limited predictive power across\nthe broader Nasdaq universe but strong performance in a handpicked portfolio,\nwhich achieves significant positive alpha after controlling for the Fama-French\nthree factors. These findings underscore the importance of refined universe\nselection. While promising, the strategy requires further validation, including\nthe incorporation of transaction costs and performance testing across economic\ncycles. Our results suggest that cash productivity, when combined with other\ncomplementary signals and careful universe selection, can be a valuable tool\nfor generating excess returns.",
      "generated_abstract": "We introduce a novel methodology for pricing options in the presence of\nprincipal-agent risk. Our approach is based on the idea of decomposing the\noption price into two parts: a term corresponding to the expected value of the\noption's payoff, and a term representing the principal-agent risk. By\neliminating the term representing principal-agent risk, our methodology\nintroduces a risk-neutral measure of the option's value, which we call the\n\"expected risk-neutral option price.\" Our framework allows us to obtain\nasymptotically normal and asymptotically normal-convergent estimators of the\nexpected risk-neutral option price. These estimators are based on the\nasymptotic normality of the pricing kernel and on asymptotic normality of the\nunderlying option's payoff. We also propose a method for computing the\nasymptotic normality of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.19402985074626866,
          "f": 0.15204677885982026
        },
        "rouge-2": {
          "r": 0.007751937984496124,
          "p": 0.009708737864077669,
          "f": 0.008620684717972504
        },
        "rouge-l": {
          "r": 0.10576923076923077,
          "p": 0.16417910447761194,
          "f": 0.12865496599432322
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.20838v1",
      "true_abstract": "Marine ecosystem monitoring via Passive Acoustic Monitoring (PAM) generates\nvast data, but deep learning often requires precise annotations and short\nsegments. We introduce DSMIL-LocNet, a Multiple Instance Learning framework for\nwhale call detection and localization using only bag-level labels. Our\ndual-stream model processes 2-30 minute audio segments, leveraging spectral and\ntemporal features with attention-based instance selection. Tests on Antarctic\nwhale data show longer contexts improve classification (F1: 0.8-0.9) while\nmedium instances ensure localization precision (0.65-0.70). This suggests MIL\ncan enhance scalable marine monitoring. Code:\nhttps://github.com/Ragib-Amin-Nihal/DSMIL-Loc",
      "generated_abstract": "This paper presents the development of a new method to predict the quality of\ndiscrete audio signals, known as Audio Quality Prediction (AQP). The AQP\nmodel uses deep learning techniques to predict audio quality. This work aims to\naddress the limitations of traditional approaches, such as neural networks and\nmachine learning models, by implementing a novel deep learning model based on\nattention mechanisms. The proposed AQP model is based on a transformer-based\nencoder-decoder architecture, with the ability to capture long-range dependencies\nand effectively process complex audio signals. This model is trained on a\nlarge-scale dataset, which includes both audio quality and audio features. The\nmodel was evaluated using various metrics, including Mean Opinion Score (MOS)\nscores and Jensen-Shannon divergence (JSD) scores, demonstrating its\nhigh-performance capabilities. The proposed AQP model is currently",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12941176470588237,
          "p": 0.125,
          "f": 0.12716762505930723
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.008403361344537815,
          "f": 0.009615379719399682
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.125,
          "f": 0.12716762505930723
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CY/2503.08588v1",
      "true_abstract": "Previous studies have established that language models manifest stereotyped\nbiases. Existing debiasing strategies, such as retraining a model with\ncounterfactual data, representation projection, and prompting often fail to\nefficiently eliminate bias or directly alter the models' biased internal\nrepresentations. To address these issues, we propose BiasEdit, an efficient\nmodel editing method to remove stereotypical bias from language models through\nlightweight networks that act as editors to generate parameter updates.\nBiasEdit employs a debiasing loss guiding editor networks to conduct local\nedits on partial parameters of a language model for debiasing while preserving\nthe language modeling abilities during editing through a retention loss.\nExperiments on StereoSet and Crows-Pairs demonstrate the effectiveness,\nefficiency, and robustness of BiasEdit in eliminating bias compared to\ntangental debiasing baselines and little to no impact on the language models'\ngeneral capabilities. In addition, we conduct bias tracing to probe bias in\nvarious modules and explore bias editing impacts on different components of\nlanguage models.",
      "generated_abstract": "This paper presents a novel framework for generating knowledge-based\ngraphs for multi-modal learning. We introduce a novel framework for generating\nknowledge-based graphs for multi-modal learning. By leveraging a large-scale\nmulti-modal knowledge graph, we propose to train a graph-based model to\naccurately capture the relations between entities in the knowledge graph and\ntheir visual representations. The knowledge graph is utilized as the\nsupervision signal to enhance the model's understanding of visual entities and\ntheir relations, thus enhancing its ability to learn. We introduce a novel\nframework for generating knowledge-based graphs for multi-modal learning. By\nleveraging a large-scale multi-modal knowledge graph, we propose to train a\ngraph-based model to accurately capture the relations between entities in the\nknowledge graph and their visual representations. The knowledge graph is\nutilized as the supervision signal to enhance the model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11214953271028037,
          "p": 0.22641509433962265,
          "f": 0.14999999556953136
        },
        "rouge-2": {
          "r": 0.006535947712418301,
          "p": 0.014285714285714285,
          "f": 0.008968605558127106
        },
        "rouge-l": {
          "r": 0.11214953271028037,
          "p": 0.22641509433962265,
          "f": 0.14999999556953136
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10624v1",
      "true_abstract": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
      "generated_abstract": "Despite the advancement of large language models (LLMs), they struggle to\nadhere to the semantics of natural language. To address this issue, we\nintroduce a novel framework, SLIME-LLM, that enhances the LLM's understanding\nof visual concepts by utilizing visual language models (VLMs). Our approach\nintegrates visual and textual features, enabling the LLM to extract\nsemantic-relevant information from both modalities. Additionally, we leverage\na multi-task LLM architecture to enhance the LLM's ability to generate\nsemantic-relevant visual tokens and contextualize them in the visual domain.\nExperimental results demonstrate that SLIME-LLM achieves state-of-the-art\nperformance in several vision-language modeling benchmarks, such as the\nVisual Genome, VQA, and VLQA, and outperforms several",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1342281879194631,
          "p": 0.2597402597402597,
          "f": 0.17699114594995705
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.028846153846153848,
          "f": 0.020689650572177004
        },
        "rouge-l": {
          "r": 0.12080536912751678,
          "p": 0.23376623376623376,
          "f": 0.15929203090570926
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/TR/2412.10823v1",
      "true_abstract": "Financial sentiment analysis is crucial for understanding the influence of\nnews on stock prices. Recently, large language models (LLMs) have been widely\nadopted for this purpose due to their advanced text analysis capabilities.\nHowever, these models often only consider the news content itself, ignoring its\ndissemination, which hampers accurate prediction of short-term stock movements.\nAdditionally, current methods often lack sufficient contextual data and\nexplicit instructions in their prompts, limiting LLMs' ability to interpret\nnews. In this paper, we propose a data-driven approach that enhances\nLLM-powered sentiment-based stock movement predictions by incorporating news\ndissemination breadth, contextual data, and explicit instructions. We cluster\nrecent company-related news to assess its reach and influence, enriching\nprompts with more specific data and precise instructions. This data is used to\nconstruct an instruction tuning dataset to fine-tune an LLM for predicting\nshort-term stock price movements. Our experimental results show that our\napproach improves prediction accuracy by 8\\% compared to existing methods.",
      "generated_abstract": "We introduce a novel methodology for the automatic assessment of the\nfairness of machine learning (ML) systems. Our approach focuses on the\nrobustness of ML models in a wide range of fairness-related metrics, such as\nthe Equalized Odds metric (EO), the Equalized Disparity metric (ED), and the\nEqualized Adjusted Greedy Algorithm (EAGA) metric. The methodology is based on\na novel loss function that incorporates both the fairness and the\nrobustness metrics into a single objective, thereby enabling a fairer, more\nrobust decision-making process. We demonstrate the effectiveness of our\nmethodology through experiments on a variety of datasets. Our findings highlight\nthe importance of considering fairness and robustness metrics in ML\nassessment and suggest directions for future research.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.20270270270270271,
          "f": 0.15873015396545465
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.20270270270270271,
          "f": 0.15873015396545465
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.05514v1",
      "true_abstract": "Securing Internet of Things (IoT) devices presents increasing challenges due\nto their limited computational and energy resources. Radio Frequency\nFingerprint Identification (RFFI) emerges as a promising authentication\ntechnique to identify wireless devices through hardware impairments. RFFI\nperformance under low signal-to-noise ratio (SNR) scenarios is significantly\ndegraded because the minute hardware features can be easily swamped in noise.\nIn this paper, we leveraged the diffusion model to effectively restore the RFF\nunder low SNR scenarios. Specifically, we trained a powerful noise predictor\nand tailored a noise removal algorithm to effectively reduce the noise level in\nthe received signal and restore the device fingerprints. We used Wi-Fi as a\ncase study and created a testbed involving 6 commercial off-the-shelf Wi-Fi\ndongles and a USRP N210 software-defined radio (SDR) platform. We conducted\nexperimental evaluations on various SNR scenarios. The experimental results\nshow that the proposed algorithm can improve the classification accuracy by up\nto 34.9%.",
      "generated_abstract": "In this paper, we consider the problem of a multi-user wireless system with\nmultiple transmitters and multiple receivers, where each receiver is equipped\nwith a single antenna and has no physical information about the transmitters.\nWe first introduce a new formulation that incorporates the received signal\nstrength into the objective function and also introduces the concept of a\nmulti-user information theoretic capacity region, which captures the amount of\ninformation that a receiver can extract from a given information set of\ntransmitters. Then, we present a novel approach to the problem that involves\nthe design of a distributed linear precoder and an equalization scheme that\nachieves the optimal sum rate in the limit of an infinite number of receivers.\nTo this end, we first show that the optimal precoder is the maximum weighted\nsum of the optimal precoders for each transmitter. Then, we introduce a\ndistributed equalization scheme that uses",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1592920353982301,
          "p": 0.20689655172413793,
          "f": 0.17999999508450013
        },
        "rouge-2": {
          "r": 0.05442176870748299,
          "p": 0.058394160583941604,
          "f": 0.0563380231752137
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.20689655172413793,
          "f": 0.17999999508450013
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.20164v1",
      "true_abstract": "This paper concerns various models of ``at-most-$n$-valued maps''. That is,\nmultivalued maps $f:X\\multimap Y$ for which $f(x)$ has cardinality at most $n$\nfor each $x$. We consider 4 classes of such maps which have appeared in the\nliterature: $\\mathcal U$, the set of exactly $n$-valued maps, or unions of\nsuch; $\\mathcal F$, the set of $n$-fold maps defined by Crabb; $\\mathcal S$,\nthe set of symmetric product maps; and $\\mathcal W$, the set of weighted maps\nwith weights in $\\mathbb N$. Our main result is roughly that these classes\nsatisfy the following containments: \\[ \\mathcal U \\subsetneq \\mathcal F\n\\subsetneq \\mathcal S = \\mathcal W \\]\n  Furthermore we define the general class $\\mathcal C$ of all\nat-most-$n$-valued maps, and show that there are maps in $\\mathcal C$ which are\noutside of any of the other classes above. We also describe a\nconfiguration-space point of view for the class $\\mathcal C$, defining a\nconfiguration space $C_n(Y)$ such that any at-most-$n$-valued map $f:X\\multimap\nY$ corresponds naturally to a single-valued map $f:X\\to C_n(Y)$. We give a full\ncalculation of the fundamental group and homology groups of $C_n(S^1)$.",
      "generated_abstract": "We study the existence of an invariant partition of unity on the quotient\nspace\n  $X=S^1\\backslash\\left(D^1\\cup D^2\\right)$ with respect to a certain\npositive\n  exponent $p$ in the sense of Zygmund. We also characterize the Sobolev spaces\n$H^{s,p}(X)$\n  where $s\\in\\mathbb{R}$ and $1\\leq p<\\infty$ when the partition of unity is\nconstructed\n  from the family of holomorphic functions. In particular, the Sobolev spaces\n$H^{s,1}(X)$\n  ($s\\in\\mathbb{R}$) are recovered. The main tool in the proof of our main\nresults\n  is the existence of a maximal function for the Dirichlet form associated to\nthe\n  partition of unity. The proof is based on the study of a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.22950819672131148,
          "f": 0.1555555510746915
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.046511627906976744,
          "f": 0.031007747493540686
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.22950819672131148,
          "f": 0.1555555510746915
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10341v1",
      "true_abstract": "The field of high-speed autonomous racing has seen significant advances in\nrecent years, with the rise of competitions such as RoboRace and the Indy\nAutonomous Challenge providing a platform for researchers to develop software\nstacks for autonomous race vehicles capable of reaching speeds in excess of 170\nmph. Ensuring the safety of these vehicles requires the software to\ncontinuously monitor for different faults and erroneous operating conditions\nduring high-speed operation, with the goal of mitigating any unreasonable risks\nposed by malfunctions in sub-systems and components. This paper presents a\ncomprehensive overview of the HALO safety architecture, which has been\nimplemented on a full-scale autonomous racing vehicle as part of the Indy\nAutonomous Challenge. The paper begins with a failure mode and criticality\nanalysis of the perception, planning, control, and communication modules of the\nsoftware stack. Specifically, we examine three different types of faults - node\nhealth, data health, and behavioral-safety faults. To mitigate these faults,\nthe paper then outlines HALO safety archetypes and runtime monitoring methods.\nFinally, the paper demonstrates the effectiveness of the HALO safety\narchitecture for each of the faults, through real-world data gathered from\nautonomous racing vehicle trials during multi-agent scenarios.",
      "generated_abstract": "This paper presents a new approach to the task of object detection and\nthe corresponding tracking of such objects in videos using a 3D model of a\nrobotic arm. The approach combines a reinforcement learning (RL) algorithm\nwith a graph neural network (GNN) to capture the dynamics of the object's\nmovement. The GNN serves as a representation layer that extracts relevant\nfeatures from the 3D point cloud of the object, enabling effective learning of\nthe object's trajectory. We demonstrate that our method outperforms the state\nof the art in object detection and tracking, achieving an Average Precision\n(AP) score of 74.88% and an Average Detection Latency (ADL) of 2.39 ms for the\ntask of object detection. Additionally, we show that our approach enables\neffective tracking of objects, achieving an ADL",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.19047619047619047,
          "f": 0.15686274025374872
        },
        "rouge-2": {
          "r": 0.03409090909090909,
          "p": 0.05128205128205128,
          "f": 0.04095562660205766
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.16666666666666666,
          "f": 0.13725489711649383
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.11189v1",
      "true_abstract": "This paper is a sequel of the 2019 paper [5]. It demonstrates the following:\na) the Poisson multi-Bernoulli mixture (PMBM) approach to detected vs.\nundetected (U/D) targets cannot be rigorously formulated using either the\ntwo-step or single-step multitarget recursive Bayes filter (MRBF); b) it can,\nhowever, be partially salvaged using a novel single-step MRBF; c) probability\nhypothesis density (PHD) filters can be derived for both the original \"S-U/D\"\napproach in [5] and the novel \"D-U/D\" approach; d) important U/D formulas in\n[5] can be verified using purely algebraic methods rather than the intricate\nstatistical analysis employed in that paper; and e) the claim, that PMBM\nfilters can propagate detected and undetected targets separately in parallel,\nis doubtful.",
      "generated_abstract": "The problem of predicting the future value of a stochastic process is\nknown as the Value-at-Risk (VaR) problem, which has been extensively studied\nin the literature. The traditional approach for estimating the VaR involves\nusing the Central Limit Theorem (CLT) to derive asymptotic confidence intervals\nfor the future value, which are then used to construct confidence bands for the\nprocess. However, these confidence bands can be computationally expensive and\noften lead to inadequate coverage. In this paper, we propose a novel approach\nthat uses the Empirical Processes (EP) framework to estimate the VaR. The EP\napproach enables us to directly derive confidence intervals for the future\nvalue without relying on the CLT. Moreover, the EP approach allows us to\nconstruct confidence bands that are both computationally efficient and\ncover the process with high probability. We provide theoretical guarantees for\nthe accuracy of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18823529411764706,
          "p": 0.17777777777777778,
          "f": 0.18285713786122462
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.016,
          "f": 0.016806717701788074
        },
        "rouge-l": {
          "r": 0.15294117647058825,
          "p": 0.14444444444444443,
          "f": 0.14857142357551037
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.07088v1",
      "true_abstract": "We construct a family of estimators for a regression function based on a\nsample following a qdistribution. Our approach is nonparametric, using kernel\nmethods built from operations that leverage the properties of q-calculus.\nFurthermore, under appropriate assumptions, we establish the weak convergence\nand strong consistency of this family of estimators.",
      "generated_abstract": "This paper studies the statistical inference problem in the context of\ndynamical systems on the torus. We consider the Gaussian model and\ncorresponding Fisher information matrix, and the Gaussian Markov chain\nmodel. We derive the asymptotic distribution of the empirical Fisher\ninformation matrix, and derive the asymptotic distribution of the\nautocorrelation function of the Gaussian Markov chain model. We propose a\nnonparametric inference method based on the bootstrap method. The asymptotic\ndistribution of the proposed method is derived, and the asymptotic\ndistribution of the proposed method is compared with the asymptotic distribution\nof the classical bootstrap method. The proposed method is applied to the\ndynamical systems on the torus.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21951219512195122,
          "p": 0.1956521739130435,
          "f": 0.2068965467406528
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.014492753623188406,
          "f": 0.017241374490191596
        },
        "rouge-l": {
          "r": 0.21951219512195122,
          "p": 0.1956521739130435,
          "f": 0.2068965467406528
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.10101v1",
      "true_abstract": "Financial bubbles and crashes have repeatedly caused economic turmoil notably\nbut not only during the 2008 financial crisis. However, both in the popular\npress as well as scientific publications, the meaning of bubble is sometimes\nunspecified. Due to the multitude of bubble definitions, we conduct a\nsystematic review with the following questions: What definitions of asset price\nbubbles exist in the literature? Which definitions are used in which\ndisciplines and how frequently? We develop a system of definition categories\nand categorize a total of 122 papers from eleven research areas. Our results\nshow that although one definition is indeed prevalent in the literature, the\noverall definition landscape is not uniform. Next to the mostly used definition\nas deviation from a present value of expected future cash flows, we identify\nseveral other definitions, which rely on price properties or other\nspecifications of a fundamental value. This research contributes by shedding\nlight on the possible variations in which bubbles are defined and\noperationalized.",
      "generated_abstract": "The financialization of the global economy has led to a significant\nchurning of wealth. This paper studies the impact of financialization on the\ngrowth of the value of financial assets (equity, debt, and bonds) by\nconsidering the impact of changes in the ratio of these assets to GDP. We\nfind that financialization has a positive and significant impact on the growth\nof the value of financial assets, but the impact is significantly smaller than\nthat of the growth of the value of non-financial assets. Our results suggest\nthat financialization may not have a positive impact on the growth of GDP,\nindicating that financialization may not be the primary driver of economic\ngrowth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.35714285714285715,
          "f": 0.2409638509507912
        },
        "rouge-2": {
          "r": 0.025806451612903226,
          "p": 0.047058823529411764,
          "f": 0.03333332875868119
        },
        "rouge-l": {
          "r": 0.12727272727272726,
          "p": 0.25,
          "f": 0.1686746943242852
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08001v1",
      "true_abstract": "Mobile edge computing (MEC) enables the provision of high-reliability and\nlow-latency applications by offering computation and storage resources in close\nproximity to end-users. Different from traditional computation task offloading\nin MEC systems, the large data volume and complex task computation of\nartificial intelligence involved intelligent computation task offloading have\nincreased greatly. To address this challenge, we propose a MEC system for\nmultiple base stations and multiple terminals, which exploits semantic\ntransmission and early exit of inference. Based on this, we investigate a joint\nsemantic transmission and resource allocation problem for maximizing system\nreward combined with analysis of semantic transmission and intelligent\ncomputation process. To solve the formulated problem, we decompose it into\ncommunication resource allocation subproblem, semantic transmission subproblem,\nand computation capacity allocation subproblem. Then, we use 3D matching and\nconvex optimization method to solve subproblems based on the block coordinate\ndescent (BCD) framework. The optimized feasible solutions are derived from an\nefficient BCD based joint semantic transmission and resource allocation\nalgorithm in MEC systems. Our simulation demonstrates that: 1) The proposed\nalgorithm significantly improves the delay performance for MEC systems compared\nwith benchmarks; 2) The design of transmission mode and early exit of inference\ngreatly increases system reward during offloading; and 3) Our proposed system\nachieves efficient utilization of resources from the perspective of system\nreward in the intelligent scenario.",
      "generated_abstract": "This paper introduces a novel adaptive adaptive control (AC) algorithm\nfor the optimal control of a class of nonlinear stochastic optimal power\ntrajectory (OPT) problems. The OPT problem is defined as an optimal control\nproblem for the stochastic optimal power flow (OPF) problem under a dynamic\nstochastic environment (DSE) with random parameters and uncertainties. The\nOPF problem is formulated as a stochastic optimal power flow problem with\nuncertainty. The OPT problem is formulated as a stochastic optimal power\ntrajectory problem with uncertainty. The OPT problem is characterized by a\nMarkovian stochastic environment with random parameters and uncertainties. The\ncontrol input is designed as a linear combination of the control input from\nprevious OPT problem iterations. This combination is then used to update the\ncontrol input for the next OPT problem iteration. The performance of the\nproposed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12121212121212122,
          "p": 0.2711864406779661,
          "f": 0.16753926274608713
        },
        "rouge-2": {
          "r": 0.005050505050505051,
          "p": 0.010309278350515464,
          "f": 0.006779656603047976
        },
        "rouge-l": {
          "r": 0.11363636363636363,
          "p": 0.2542372881355932,
          "f": 0.15706805855760542
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cs/CE/2503.07684v1",
      "true_abstract": "Lithium-sulfur (Li-S) batteries offer a promising alternative to current\nlithium-ion (Li-ion) batteries, with a high theoretical energy density,\nimproved safety and high abundance, low cost of materials. For Li-S to reach\ncommercial application, it is essential to understand how the behaviour scales\nbetween cell formats; new material development is predominately completed at\ncoin-cell level, whilst pouch-cells will be used for commercial applications.\nDifferences such as reduced electrolyte-to-sulfur (E/S) ratios and increased\ngeometric size at larger cell formats contribute to the behavioural\ndifferences, in terms of achievable capacity, cyclability and potential\ndegradation mechanisms.\n  This work focuses on the steps required to capture and test coin-cell\nbehaviour, building upon the existing models within the literature, which\npredominately focus on pouch-cells. The areas investigated throughout this\nstudy, to improve the capability of the model in terms of scaling ability and\ncausality of predictions, include the cathode surface area, precipitation\ndynamics and C-rate dependence.",
      "generated_abstract": "The non-equilibrium dynamics of nanoporous materials is an emerging field\nthat has been extensively studied for several decades. Here, we review the\ncurrent state of the art in the application of thermodynamic principles to the\nstudy of this complex and challenging class of materials. We emphasize the\nimportance of defining the thermodynamic potential that captures the key\nfeatures of the non-equilibrium dynamics, and discuss the challenges that this\napproach presents, including the need for accurate and robust models of the\nnanoporous material and the use of a multiscale approach that considers both\nthe local atomic structure and the thermodynamic potential. Finally, we\ndiscuss the implications of this approach for the development of new\nnanoporous materials and the design of novel materials for applications in\nenvironmental science and energy storage.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17094017094017094,
          "p": 0.2702702702702703,
          "f": 0.2094240790230532
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.025423728813559324,
          "f": 0.022641504493842304
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.20270270270270271,
          "f": 0.15706805808064483
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2410.22030v1",
      "true_abstract": "Metabolite biosynthesis is regulated via metabolic pathways, which can be\nactivated and deactivated within organisms. Understanding and identifying an\norganism's metabolic pathway network is a crucial aspect for various research\nfields, including crop and life stock breeding, pharmacology, and medicine. The\nproblem of identifying whether a pathway is part of a studied metabolic system\nis commonly framed as a hyperlink prediction problem. The most important\nchallenge in prediction of metabolic pathways is the sparsity of the labeled\ndata. This challenge can partially be mitigated using metabolite correlation\nnetworks which are affected by all active pathways including those that were\nnot confirmed yet in laboratory experiments. Unfortunately, extracting\nproperties that can confirm or refute existence of a metabolic pathway in a\nparticular organism is not a trivial task. In this research, we introduce the\nNetwork Auralization Hyperlink Prediction (NetAurHPD) which is a framework that\nrelies on (1) graph auralization to extract and aggregate representations of\nnodes in metabolite correlation networks and (2) data augmentation method that\ngenerates metabolite correlation networks given a subset of chemical reactions\ndefined as hyperlinks. Experiments with metabolites correlation-based networks\nof tomato pericarp demonstrate promising results for NetAurHPD, compared to\nalternative methods. Furthermore, the application of data augmentation improved\nNetAurHPD's learning capabilities and overall performance. Additionally,\nNetAurHPD outperformed state-of-the-art method in experiments under challenging\nconditions, and has the potential to be a valuable tool for exploring organisms\nwith limited existing knowledge.",
      "generated_abstract": "The study of the effect of microbes on the evolution of host species is of\ninterest to both microbiologists and evolutionary biologists. While the\nevolutionary history of a species is well understood, the evolutionary history\nof a single microbe is less understood. The study of the evolutionary history of\na species is often aided by the identification of new species, and the study of\nthe evolutionary history of a single microbe is often aided by the identification\nof new species. The identification of new species is often facilitated by\nmodern sequencing techniques, which are described in the Introduction. The\nidentification of new species is also often facilitated by the identification\nof new microbes. The identification of new species is facilitated by the\nidentification of new microbes. The identification of new species is also\nfacilitated by the identification of new species. The identification of new\nspecies is also facilitated by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07792207792207792,
          "p": 0.27906976744186046,
          "f": 0.12182740775490232
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.043478260869565216,
          "f": 0.020408159673053525
        },
        "rouge-l": {
          "r": 0.07792207792207792,
          "p": 0.27906976744186046,
          "f": 0.12182740775490232
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/MF/2502.00740v1",
      "true_abstract": "This paper examines a semi-analytical approach for pricing American options\nin time-inhomogeneous models characterized by negative interest rates (for\nequity, FX) or negative convenience yields (for commodities, cryptocurrencies).\nUnder such conditions, exercise boundaries may exhibit a \"floating\" structure -\ndynamically appearing and disappearing. For example, a second exercise boundary\ncould emerge within the computational domain and subsequently both could\ncollapse, demanding specialized pricing methodologies.",
      "generated_abstract": "We study a model of portfolio choice under time-inconsistent expectations\nin which the expected return is a time-varying function of the current value of\nthe underlying asset. We consider two types of expectations:\nexpectations that are non-decreasing in the time variable and expectations that\nare non-increasing in the current value. We show that the choice of expectations\nhas a significant impact on the optimal portfolio selection and that the\neffects of the two types of expectations are distinct. We also show that the\nchoice of expectations can be characterized in terms of the first-order\nderivative of the expected return. Finally, we provide a simple example to\nillustrate our results.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08928571428571429,
          "p": 0.08620689655172414,
          "f": 0.08771929324715326
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08928571428571429,
          "p": 0.08620689655172414,
          "f": 0.08771929324715326
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2501.05232v1",
      "true_abstract": "Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.",
      "generated_abstract": "We introduce a novel framework for predicting short-term returns, focusing\non returns from stocks and exchange-traded funds (ETFs) traded on the NYSE and\nNASDAQ exchanges. This framework is based on a neural network that processes\nhigh-dimensional financial data, including returns, volatility, and momentum.\nThe model is trained using a data set comprising historical stock returns\ncalculated for a period of five years, beginning in 2011 and ending in 2015.\nOur results indicate that this framework is able to accurately predict the\nshort-term return of the NYSE and NASDAQ stocks, as well as the ETFs. This\nwork establishes a foundation for developing predictive models for the future\nreturns of financial assets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12162162162162163,
          "p": 0.11842105263157894,
          "f": 0.11999999500088909
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.11842105263157894,
          "f": 0.11999999500088909
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.00648v1",
      "true_abstract": "T-cells play a key role in adaptive immunity by mounting specific responses\nagainst diverse pathogens. An effective binding between T-cell receptors (TCRs)\nand pathogen-derived peptides presented on Major Histocompatibility Complexes\n(MHCs) mediate an immune response. However, predicting these interactions\nremains challenging due to limited functional data on T-cell reactivities.\nHere, we introduce a computational approach to predict TCR interactions with\npeptides presented on MHC class I alleles, and to design novel immunogenic\npeptides for specified TCR-MHC complexes. Our method leverages HERMES, a\nstructure-based, physics-guided machine learning model trained on the protein\nuniverse to predict amino acid preferences based on local structural\nenvironments. Despite no direct training on TCR-pMHC data, the implicit\nphysical reasoning in HERMES enables us to make accurate predictions of both\nTCR-pMHC binding affinities and T-cell activities across diverse viral epitopes\nand cancer neoantigens, achieving up to 72% correlation with experimental data.\nLeveraging our TCR recognition model, we develop a computational protocol for\nde novo design of immunogenic peptides. Through experimental validation in\nthree TCR-MHC systems targeting viral and cancer peptides, we demonstrate that\nour designs--with up to five substitutions from the native sequence--activate\nT-cells at success rates of up to 50%. Lastly, we use our generative framework\nto quantify the diversity of the peptide recognition landscape for various\nTCR-MHC complexes, offering key insights into T-cell specificity in both humans\nand mice. Our approach provides a platform for immunogenic peptide and\nneoantigen design, opening new computational paths for T-cell vaccine\ndevelopment against viruses and cancer.",
      "generated_abstract": "The genetic and environmental factors that influence the onset and course of\nsyndromic diseases are not fully understood. While genetic risk factors are\nimportant, they are only one factor in determining disease susceptibility.\nEnvironmental factors such as air pollution, climate change, and exposure to\ncertain foods can have a significant impact on the risk of developing\nsyndromic diseases. This study explores the relationship between air pollution\nand the onset of syndromic diseases, including diabetes, asthma, and eczema. We\nutilized data from the 2010 to 2019 Air Quality Index (AQI) and the 2010 to 2019\nEnvironmental Protection Agency (EPA) air quality standard for PM2.5 to\ncalculate the average AQI for each city in China. We then",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0718562874251497,
          "p": 0.14634146341463414,
          "f": 0.0963855377513268
        },
        "rouge-2": {
          "r": 0.008438818565400843,
          "p": 0.018518518518518517,
          "f": 0.011594198597607137
        },
        "rouge-l": {
          "r": 0.0718562874251497,
          "p": 0.14634146341463414,
          "f": 0.0963855377513268
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.09411v1",
      "true_abstract": "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1/(2p+1)}/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios.",
      "generated_abstract": "We study a statistical learning problem in which the goal is to classify a\ndata sample from a population of sources into one of two classes. In the\nunsupervised setting, the goal is to classify a data sample from a population of\nsources into one of two classes, without access to the labels of the samples. In\nthe supervised setting, the goal is to classify a data sample from a population\nof sources into one of two classes, with access to the labels of the samples.\nIn this work, we focus on the classification problem under the binary classification\nmodel and the multi-class classification model. We develop two methods to\nsolve the statistical learning problem: the mean-field (MF) method and the\nsub-Gaussian-optimal (SGO) method. We prove that both methods achieve the\noptimal rate of convergence in the supervised setting and the optimal rate of\nconvergence in the unsupervised",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14018691588785046,
          "p": 0.2459016393442623,
          "f": 0.17857142394628697
        },
        "rouge-2": {
          "r": 0.017964071856287425,
          "p": 0.03225806451612903,
          "f": 0.023076918481953576
        },
        "rouge-l": {
          "r": 0.14018691588785046,
          "p": 0.2459016393442623,
          "f": 0.17857142394628697
        }
      }
    },
    {
      "paper_id": "math.OC.econ/GN/2412.05234v1",
      "true_abstract": "Risk measures, which typically evaluate the impact of extreme losses, are\nhighly sensitive to misspecification in the tails. This paper studies a robust\noptimization approach to combat tail uncertainty by proposing a unifying\nframework to construct uncertainty sets for a broad class of risk measures,\ngiven a specified nominal model. Our framework is based on a parametrization of\nrobust risk measures using two (or multiple) $\\phi$-divergence functions, which\nenables us to provide uncertainty sets that are tailored to both the\nsensitivity of each risk measure to tail losses and the tail behavior of the\nnominal distribution. In addition, our formulation allows for a tractable\ncomputation of robust risk measures, and elicitation of $\\phi$-divergences that\ndescribe a decision maker's risk and ambiguity preferences.",
      "generated_abstract": "In this paper, we investigate the impact of the social network structure on\nthe evolution of infectious disease outbreaks in a population. We focus on\nvirus-host networks with a directed link structure and a non-negative weight\nfunction, with the goal of studying how the network structure affects the\nspreading dynamics of the virus. We introduce a new model where a host\ncommunicates with other hosts in the network via a directed link that is\nnon-negative, but only when the host is infected. We show that, under a\nsuitable condition on the network structure, the model is equivalent to a\nclass of Markov processes with a single state, and we use this equivalence to\nprove that the network structure has a significant impact on the dynamics of\nthe infection process. We illustrate the results of our analysis on two\nexamples: the SIR model with a social network and the SIS model with a\nsocial network.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.18518518518518517,
          "f": 0.1863353987269011
        },
        "rouge-2": {
          "r": 0.05172413793103448,
          "p": 0.04477611940298507,
          "f": 0.04799999502592052
        },
        "rouge-l": {
          "r": 0.175,
          "p": 0.1728395061728395,
          "f": 0.1739130384784539
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08902v1",
      "true_abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures.",
      "generated_abstract": "We study the problem of finding a good approximation of a set of samples\nby a set of points. We consider the case of a set of samples and a set of\npoints. We show that for any good approximation of a set of samples by a set\nof points, the set of points must be chosen in such a way that the set of\nsamples is almost surely in the convex hull of the set of points. We provide an\nalgorithm for this task.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0783132530120482,
          "p": 0.35135135135135137,
          "f": 0.128078814753088
        },
        "rouge-2": {
          "r": 0.012396694214876033,
          "p": 0.05454545454545454,
          "f": 0.020202017184188004
        },
        "rouge-l": {
          "r": 0.06626506024096386,
          "p": 0.2972972972972973,
          "f": 0.10837438125555107
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.09318v1",
      "true_abstract": "Modern data analytics requires a huge amount of computing power and processes\na massive amount of data. At the same time, the underlying computing platform\nis becoming much more heterogeneous on both hardware and software. Even though\nspecialized hardware, e.g., FPGA- or GPU- or TPU-based systems, often achieves\nbetter performance than a CPU-only system due to the slowing of Moore's law,\nsuch systems are limited in what they can do. For example, GPU-only approaches\nsuffer from severe IO limitations. To truly exploit the potential of hardware\nheterogeneity, we present FpgaHub, an FPGA-centric hyper-heterogeneous\ncomputing platform for big data analytics. The key idea of FpgaHub is to use\nreconfigurable computing to implement a versatile hub complementing other\nprocessors (CPUs, GPUs, DPUs, programmable switches, computational storage,\netc.). Using an FPGA as the basis, we can take advantage of its highly\nreconfigurable nature and rich IO interfaces such as PCIe, networking, and\non-board memory, to place it at the center of the architecture and use it as a\ndata and control plane for data movement, scheduling, pre-processing, etc.\nFpgaHub enables architectural flexibility to allow exploring the rich design\nspace of heterogeneous computing platforms.",
      "generated_abstract": "In this paper, we present the first large-scale study of multi-agent\nsystems (MASs) with hierarchical structure, focusing on the design of an\nadaptive distributed control (ADC) framework for the autonomous aerial vehicle\n(AAV) to cooperate with the ground vehicles (GVs). The AAV is equipped with\ntwo types of sensors: a visual sensor and a radar sensor. The radar sensor is\nresponsible for the perception of the GVs, while the visual sensor provides\ninformation on the current AAV's surroundings. In this work, we propose an\nadaptive ADC framework that is based on a hybrid control strategy consisting of\ntwo parts: a perception-based control module that is adapted to the\nsurrounding environment and a planning-based control module that is\nadapted to the AAV's specific characteristics. This",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10948905109489052,
          "p": 0.20270270270270271,
          "f": 0.14218009023247472
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.027522935779816515,
          "f": 0.02013422354826466
        },
        "rouge-l": {
          "r": 0.10948905109489052,
          "p": 0.20270270270270271,
          "f": 0.14218009023247472
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10164v1",
      "true_abstract": "This paper addresses the safety challenges in impulsive systems, where abrupt\nstate jumps introduce significant complexities into system dynamics. A unified\nframework is proposed by integrating Quadratic Programming (QP), Control\nBarrier Functions (CBFs), and adaptive gain mechanisms to ensure system safety\nduring impulsive events. The CBFs are constructed to enforce safety constraints\nby capturing the system's continuous dynamics and the effects of impulsive\nstate transitions. An adaptive gain mechanism dynamically adjusts control\ninputs based on the magnitudes of the impulses and the system's proximity to\nsafety boundaries, maintaining safety during instantaneous state jumps. A\ntailored QP formulation incorporates CBFs constraints and adaptive gain\nadjustments, optimizing control inputs while ensuring compliance with\nsafety-critical requirements. Theoretical analysis establishes the boundedness,\ncontinuity, and feasibility of the adaptive gain and the overall framework. The\neffectiveness of the method is demonstrated through simulations on a robotic\nmanipulator, showcasing its practical applicability to impulsive systems with\nstate jumps.",
      "generated_abstract": "We prove a characterization of the non-degeneracy of the solution of a\nnon-linear differential system. We first define the notion of non-degeneracy\nof the solution of a non-linear differential system and then prove that the\nnon-degeneracy of the solution of a non-linear differential system depends on\nthe non-degeneracy of the solution of the differential system in the space of\nthe initial condition. We then provide a sufficient condition for the\nnon-degeneracy of the solution of the differential system and show that this\ncondition is equivalent to the non-degeneracy of the solution of the\ndifferential system in the space of the initial condition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.2903225806451613,
          "f": 0.1384615348295859
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.0392156862745098,
          "f": 0.02116401722348272
        },
        "rouge-l": {
          "r": 0.0707070707070707,
          "p": 0.22580645161290322,
          "f": 0.10769230406035514
        }
      }
    },
    {
      "paper_id": "math.AG.math/DG/2503.09195v1",
      "true_abstract": "Refined algebraic domains are regions in the plane surrounded by finitely\nmany non-singular real algebraic curves which may intersect with normal\ncrossing. We are interested in shapes of such regions with surrounding real\nalgebraic curves. Poincar'e-Reeb Graphs of them are graphs the regions\nnaturally collapse to respecting the projection to a straight line. Such graphs\nwere first formulated by Sorea, for example, around 2020, and regions\nsurrounded by mutually disjoint non-singular real algebraic curves were mainly\nconsidered. The author has generalized the studies to several general\nsituations.\n  We find classes of such objects defined inductively by adding curves. We\nrespect characteristic finite sets in the curves. We consider regions\nsurrounded by the curves and of a new type. We investigate geometric properties\nand combinatorial ones of them and discuss important examples. We also\npreviously studied explicit classes defined inductively in this way and review\nthem.",
      "generated_abstract": "In this paper, we provide a complete description of the group of automorphisms\nof the product of two varieties. We construct the group of automorphisms of the\nproduct of two varieties as a subgroup of the group of automorphisms of the\nproduct of two algebraic varieties and study its relationship with the group of\nautomorphisms of the product of two algebraic varieties. We show that the group\nof automorphisms of the product of two varieties is isomorphic to the group of\nautomorphisms of the product of two algebraic varieties.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10112359550561797,
          "p": 0.3,
          "f": 0.15126050043076064
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10112359550561797,
          "p": 0.3,
          "f": 0.15126050043076064
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10402v1",
      "true_abstract": "We explore a family of numerical methods, based on the Steffensen divided\ndifference iterative algorithm, that do not evaluate the derivative of the\nobjective functions. The family of methods achieves second-order convergence\nwith two function evaluations per iteration with marginal additional\ncomputational cost. An important side benefit of the method is the improvement\nin stability for different initial conditions compared to the vanilla\nSteffensen method. We present numerical results for scalar functions, fields,\nand scalar fields. This family of methods outperforms the Steffensen method\nwith respect to standard quantitative metrics in most cases.",
      "generated_abstract": "The non-smooth, non-convex (NSNC) programming problem is the most challenging\nproblem in inverse problems. It has been addressed with various methods, such\nas the Neural Network (NN) and the Deep Neural Network (DNN), but it is still\nstill a challenging task due to the non-smooth and non-convex nature of the\nproblem. In this paper, we propose a novel framework for solving the NSNC\nproblem. The main innovation is that we firstly propose a new method for\nrepresenting the data as a sequence of images. Then, we employ a novel\nconvolutional neural network (CNN) to model the data sequence as a signal. The\nkey idea is that the NSNC problem can be viewed as a supervised learning\nproblem, which can be solved by the CNN. We further propose a regularization\nterm that incorporates the structural information of the problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22058823529411764,
          "p": 0.17647058823529413,
          "f": 0.19607842643427753
        },
        "rouge-2": {
          "r": 0.03488372093023256,
          "p": 0.024193548387096774,
          "f": 0.028571423735148208
        },
        "rouge-l": {
          "r": 0.22058823529411764,
          "p": 0.17647058823529413,
          "f": 0.19607842643427753
        }
      }
    },
    {
      "paper_id": "math.OA.math/FA/2503.09548v1",
      "true_abstract": "Let $\\Gamma$ be a countable discrete group. We say that $\\Gamma$ has\n$C^*$-invariant subalgebra rigidity (ISR) property if every $\\Gamma$-invariant\n$C^*$-subalgebra $\\mathcal{A}\\le C_r^*(\\Gamma)$ is of the form $C_r^*(N)$ for\nsome normal subgroup $N\\triangleleft\\Gamma$. We show that all torsion-free,\nnon-amenable (cylindrically) hyperbolic groups with property-AP and a finite\ndirect product of such groups have this property. We also prove that an\ninfinite group $\\Gamma$ has the C$^*$-ISR property only if $\\Gamma$ is simple\namenable or $C^*$-simple.",
      "generated_abstract": "We consider the problem of finding an arbitrary point $y\\in \\mathbb{R}^n$\nsuch that $f(x)=|x-y|^p$ has a positive maximum on $\\mathbb{R}^n$ for some\n$p\\in(1,2)$. In this paper, we prove that the problem has a unique solution\nwith the maximum located at $x=y$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22413793103448276,
          "p": 0.38235294117647056,
          "f": 0.2826086909924386
        },
        "rouge-2": {
          "r": 0.0273972602739726,
          "p": 0.05263157894736842,
          "f": 0.03603603153315534
        },
        "rouge-l": {
          "r": 0.1896551724137931,
          "p": 0.3235294117647059,
          "f": 0.2391304301228734
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2501.15828v4",
      "true_abstract": "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.",
      "generated_abstract": "In this paper, we study the estimation of the Black-Scholes model with\nexogenous market impact (EMI) using a stochastic volatility (SV) model. The SV\nmodel is a generalization of the standard SV model, where the volatility\nstructure is defined by a functional of the logarithm of the log-price and\nthe E-W condition. We propose a novel algorithm to estimate the SV model using\na particle filter. Our simulation results show that the proposed algorithm\noutperforms the particle filter in terms of the mean-squared prediction error\nand the coverage rate in the finite sample. The proposed algorithm is also\nvalidated using the ETH Zurich stock data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.21212121212121213,
          "f": 0.1599999953018777
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.021052631578947368,
          "f": 0.018181813274794713
        },
        "rouge-l": {
          "r": 0.11009174311926606,
          "p": 0.18181818181818182,
          "f": 0.13714285244473487
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.00732v1",
      "true_abstract": "In conventional approaches for multiobject tracking (MOT), raw sensor data\nundergoes several preprocessing stages to reduce data rate and computational\ncomplexity. This typically includes coherent processing that aims at maximizing\nthe signal-to-noise ratio (SNR), followed by a detector that extracts \"point\"\nmeasurements, e.g., the range and bearing of objects, which serve as inputs for\nsequential Bayesian MOT. While using point measurements significantly\nsimplifies the statistical model, the reduced data rate can lead to a loss of\ncritical, object-related information and, thus, potentially to reduced tracking\nperformance. In this paper, we propose a direct tracking approach that avoids a\ndetector and most preprocessing stages. For direct tracking, we introduce a\nmeasurement model for the data-generating process of the sensor data, along\nwith state-transition and birth models for the dynamics and the appearance and\ndisappearance of objects. Based on the new statistical model, we develop a\nfactor graph and particle-based belief propagation (BP) method for efficient\nsequential Bayesian estimation. Contrary to the track-before-detect (TBD)\nparadigm which also avoids a detector, direct tracking integrates coherent\nprocessing within the Bayesian MOT framework. Numerical experiments based on a\npassive acoustic dataset demonstrate that the proposed direct approach\noutperforms state-of-the-art conventional methods that rely on multiple\npreprocessing stages.",
      "generated_abstract": "Accurate estimation of the position, velocity, and acceleration of an\ntarget object is essential for the safe and successful operation of many\nsensor-based robots. However, traditional methods for position estimation\nrequire the input of precise sensory measurements that are difficult to\nobtain. In this paper, we propose a novel method that can estimate the\nposition, velocity, and acceleration of an unknown target object in a\nnon-contact manner. This method is based on the principle of\ninverse-time-delay-based-detection (ITDD), which uses the delayed detection\neffect of the propagation delay of the target object's signal to estimate its\nposition, velocity, and acceleration. To ensure the robustness of the method,\nwe propose a robust calibration method, which consists of three key steps:\ncalibration of the delay model, calibration of the object's delay model, and\ncalibration of the object's delay",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17293233082706766,
          "p": 0.30666666666666664,
          "f": 0.22115384154262213
        },
        "rouge-2": {
          "r": 0.046875,
          "p": 0.08108108108108109,
          "f": 0.059405935951377685
        },
        "rouge-l": {
          "r": 0.15037593984962405,
          "p": 0.26666666666666666,
          "f": 0.1923076876964683
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17600v1",
      "true_abstract": "Providing wellbeing for all while safeguarding planetary boundaries may\nrequire governments to pursue post-growth policies. Previous empirical studies\nof sustainable wellbeing initiatives investigating enablers of and barriers to\npost-growth policymaking are either based on a small number of empirical cases\nor lack an explicit analytical framework. To better understand how post-growth\npolicymaking could be fostered, we investigate 29 initiatives across governance\nscales in Europe, New Zealand, and Canada. We apply a framework that\ndistinguishes polity, politics, and policy to analyze the data. We find that\nthe main enablers and barriers relate to the economic growth paradigm, the\norganization of government, attitudes towards policymaking, political\nstrategies, and policy tools and outcomes. Engaging in positive framings of\npost-growth visions to change narratives and building broad-based alliances\ncould act as drivers. However, initiatives face a tension between the need to\nconnect to broad audiences and a risk of co-optation by depolitization.",
      "generated_abstract": "We study the implications of the COVID-19 pandemic on welfare by considering\nthe impact of a lockdown on the consumption function. We use a vector-autoregressive\ndynamic framework to investigate how the lockdown affects consumption. We find\nthat the lockdown reduces consumption and raises unemployment, with the\nimpact being larger for the poor and women. In contrast, the lockdown has no\nsignificant impact on consumption for the rich and men. These findings\nhighlight the need to consider the intersectional impacts of the lockdown on\nwomen and the poor, and to develop targeted policy interventions to mitigate\nthe negative economic consequences of the pandemic. We propose a novel\nmechanism to capture the impact of the lockdown on consumption. Our results\nconfirm that the lockdown reduces consumption, especially for the poor and\nwomen, and that the impact on consumption varies depending",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1588785046728972,
          "p": 0.23943661971830985,
          "f": 0.1910112311595759
        },
        "rouge-2": {
          "r": 0.034722222222222224,
          "p": 0.04424778761061947,
          "f": 0.038910500909325525
        },
        "rouge-l": {
          "r": 0.14018691588785046,
          "p": 0.2112676056338028,
          "f": 0.16853932104721642
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.18714v1",
      "true_abstract": "The relationship of policy choice by majority voting and by maximization of\nutilitarian welfare has long been discussed. I consider choice between a status\nquo and a proposed policy when persons have interpersonally comparable cardinal\nutilities taking values in a bounded interval, voting is compulsory, and each\nperson votes for a policy that maximizes utility. I show that knowledge of the\nattained status quo welfare and the voting outcome yields an informative bound\non welfare with the proposed policy. The bound contains the value of status quo\nwelfare, so the better utilitarian policy is not known. The minimax-regret\ndecision and certain Bayes decisions choose the proposed policy if its vote\nshare exceeds the known value of status quo welfare. This procedure differs\nfrom majority rule, which chooses the proposed policy if its vote share exceeds\n1/2.",
      "generated_abstract": "We study a sequential game between two agents where one player has access\nto a publicly observable information set. The player observes the state of\nthis information set at the beginning of the game and is able to update it with\nthe help of a single information source. We analyze the optimal allocation of\nthe two agents to this information source, i.e., whether it is in the\ninterests of the player to choose which information source to use. We also\ninvestigate whether it is in the player's best interest to use an information\nsource with a low expected value of information gain.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1728395061728395,
          "p": 0.2413793103448276,
          "f": 0.2014388440577611
        },
        "rouge-2": {
          "r": 0.025210084033613446,
          "p": 0.03296703296703297,
          "f": 0.028571423660318306
        },
        "rouge-l": {
          "r": 0.12345679012345678,
          "p": 0.1724137931034483,
          "f": 0.14388488722322879
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08337v1",
      "true_abstract": "This paper provides a discretization-free solution to the synthesis of\napprox-imation-free closed-form controllers for unknown nonlinear systems to\nenforce complex properties expressed by $\\omega$-regular languages, as\nrecognized by Non-deterministic B\\\"uchi Automata (NBA). In order to solve this\nproblem, we first decompose NBA into a sequence of reach-avoid problems, which\nare solved using the Spatiotemporal Tubes (STT) approach. Controllers for each\nreach-avoid task are then integrated into a hybrid policy that ensures the\nfulfillment of the desired $\\omega$-regular properties. We validate our method\nthrough omnidirectional robot navigation and manipulator control case studies.",
      "generated_abstract": "We study the optimal control problem for the unconstrained three-dimensional\n(3D) and four-dimensional (4D) multi-agent system (MAS) with the nonlinear\nHamilton-Jacobi-Bellman (HJB) equation. The objective is to design the\ncontrol policies that achieve global asymptotic stability (GAS) of the\nstochastic optimal control problem. We establish the existence and uniqueness\nof the GAS solution and derive its analytical expression. Then we propose a\nnumerical scheme based on the discretization of the stochastic HJB equation\nand prove its convergence. Numerical results demonstrate that the optimal\ncontrol policy of the MAS with nonlinear dynamics is the GAS solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17333333333333334,
          "p": 0.20967741935483872,
          "f": 0.1897810169428314
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.011904761904761904,
          "f": 0.011560688645797205
        },
        "rouge-l": {
          "r": 0.14666666666666667,
          "p": 0.1774193548387097,
          "f": 0.16058393665086063
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2502.13325v1",
      "true_abstract": "In this paper, we consider catastrophe stop-loss reinsurance valuation for a\nreinsurance company with dynamic contagion claims. To deal with conventional\nand emerging catastrophic events, we propose the use of a compound dynamic\ncontagion process for the catastrophic component of the liability. Under the\npremise that there is an absence of arbitrage opportunity in the market, we\nobtain arbitrage-free premiums for these contacts. To this end, the Esscher\ntransform is adopted to specify an equivalent martingale probability measure.\nWe show that reinsurers have various ways of levying the security loading on\nthe net premiums to quantify the catastrophic liability in light of the growing\nchallenges posed by emerging risks arising from climate change, cyberattacks,\nand pandemics. We numerically compare arbitrage-free catastrophe stop-loss\nreinsurance premiums via the Monte Carlo simulation method. Sensitivity\nanalyzes are performed by changing the Esscher parameters and the retention\nlevel.",
      "generated_abstract": "We develop a novel and efficient framework for the estimation of\nestimators of the Sharpe ratio of portfolios of European call options. The\nframework is based on the asymptotic expansion of the Sharpe ratio of\nportfolios of European call options, and provides a closed-form expression for\nthe Sharpe ratio of a portfolio of call options. Our approach significantly\nreduces the computational cost of the estimation of the Sharpe ratio of\nportfolios of European call options, and provides a more accurate estimation\nof the Sharpe ratio of portfolios of European call options. The proposed\nframework can be applied to the estimation of the Sharpe ratio of portfolios\nof American options, and it is readily extendable to the estimation of the\nSharpe ratios of portfolios of American options and European put options. We\ndemonstrate the effectiveness of the proposed framework by applying it to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10416666666666667,
          "p": 0.19230769230769232,
          "f": 0.13513513057706372
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.04819277108433735,
          "f": 0.03652967565813952
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.19230769230769232,
          "f": 0.13513513057706372
        }
      }
    },
    {
      "paper_id": "math.AC.math/HO/2502.13273v1",
      "true_abstract": "We give a new proof of the fundamental theorem of algebra. It is entirely\nelementary, focused on using long division to its fullest extent. Further, the\nmethod quickly recovers a more general version of the theorem recently obtained\nby Joseph Shipman.",
      "generated_abstract": "We study the structure of $K$-theory of finite dimensional $C^*$-algebras\nwith a non-negative integer parameter. More specifically, we show that the\n$K$-theory of a finite dimensional $C^*$-algebra with a non-negative integer\nparameter is determined by the homotopy type of the underlying simplicial set\n(with an additional choice of a finite generating set). This result is\ngeneralized to the case of general finite dimensional $C^*$-algebras, and we\nalso obtain a description of $K$-theory of a finite dimensional $C^*$-algebra\nwith a non-negative integer parameter using the homotopy type of a certain\nquasi-isometric cover.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2571428571428571,
          "p": 0.1836734693877551,
          "f": 0.2142857094246033
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.015151515151515152,
          "f": 0.019047614378232436
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.14285714285714285,
          "f": 0.1666666618055557
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08755v1",
      "true_abstract": "Combining the technique of employing coset codes for communicating over a\nquantum broadcast channel and the recent discovery of \\textit{tilting,\nsmoothing and augmentation} by Sen to perform simultaneous decoding over\nnetwork quantum channels, we derive new inner bounds to the capacity region of\na $3-$user classical quantum broadcast channel that subsumes all known.",
      "generated_abstract": "In the era of distributed and decentralized computing, traditional network\narchitecture design has become increasingly challenging. This paper proposes a\ndistributed and decentralized network architecture design methodology. The\nmethodology focuses on building a network with multiple tiers, where each tier\nis built by using a local computing component. The local computing component\nis connected to the network and performs computation in the local network\ntopology. The local computing component is connected to the network through a\nnetwork fabric, which connects the local computing components and enables\ncommunication between them. The local computing components are connected to a\ncentralized cloud component that performs computation on the local computing\ncomponents. This centralized cloud component is connected to the local\ncomputing components and enables communication between them. The centralized\ncloud component performs computation on the local computing components and\ntransmits the results to the local computing components, which are then used to\noptimize the local computing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.12698412698412698,
          "f": 0.1538461490698966
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.01020408163265306,
          "f": 0.013513509039446328
        },
        "rouge-l": {
          "r": 0.17073170731707318,
          "p": 0.1111111111111111,
          "f": 0.13461537983912739
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/PF/2503.03274v1",
      "true_abstract": "Ensuring Service Level Objectives (SLOs) in large-scale architectures, such\nas Distributed Computing Continuum Systems (DCCS), is challenging due to their\nheterogeneous nature and varying service requirements across different devices\nand applications. Additionally, unpredictable workloads and resource\nlimitations lead to fluctuating performance and violated SLOs. To improve SLO\ncompliance in DCCS, one possibility is to apply machine learning; however, the\ndesign choices are often left to the developer. To that extent, we provide a\nbenchmark of Active Inference -- an emerging method from neuroscience --\nagainst three established reinforcement learning algorithms (Deep Q-Network,\nAdvantage Actor-Critic, and Proximal Policy Optimization). We consider a\nrealistic DCCS use case: an edge device running a video conferencing\napplication alongside a WebSocket server streaming videos. Using one of the\nrespective algorithms, we continuously monitor key performance metrics, such as\nlatency and bandwidth usage, to dynamically adjust parameters -- including the\nnumber of streams, frame rate, and resolution -- to optimize service quality\nand user experience. To test algorithms' adaptability to constant system\nchanges, we simulate dynamically changing SLOs and both instant and gradual\ndata-shift scenarios, such as network bandwidth limitations and fluctuating\ndevice thermal states. Although the evaluated algorithms all showed advantages\nand limitations, our findings demonstrate that Active Inference is a promising\napproach for ensuring SLO compliance in DCCS, offering lower memory usage,\nstable CPU utilization, and fast convergence.",
      "generated_abstract": "We present a new approach to designing and implementing large-scale\ndifferential privacy (DP) systems. Our approach combines the advantages of\nreliable and efficient DP with a distributed framework that enables flexible\napplication-specific configuration. Our approach provides a\ndifferentially-private-by-design (DPBD) system that achieves a\ncomputation-per-data-point trade-off, enabling scalability while maintaining\naccuracy. We demonstrate our approach through a series of case studies,\nshowcasing its ability to adapt to changing privacy requirements and\napplication-specific constraints. These case studies include: (1) a privacy\nsystem for a large-scale image search engine that uses DP to protect user\nprivacy while supporting real-time queries, (2) a DP system that ensures\naccurate detection of fraudulent transactions while maintaining user privacy,\nand (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10429447852760736,
          "p": 0.22077922077922077,
          "f": 0.1416666623086807
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09815950920245399,
          "p": 0.2077922077922078,
          "f": 0.1333333289753474
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2503.08272v1",
      "true_abstract": "Monotone mean-variance (MMV) utility is the minimal modification of the\nclassical Markowitz utility that respects rational ordering of investment\nopportunities. This paper provides, for the first time, a complete\ncharacterization of optimal dynamic portfolio choice for the MMV utility in\nasset price models with independent returns. The task is performed under\nminimal assumptions, weaker than the existence of an equivalent martingale\nmeasure and with no restrictions on the moments of asset returns. We interpret\nthe maximal MMV utility in terms of the monotone Sharpe ratio (MSR) and show\nthat the global squared MSR arises as the nominal yield from continuously\ncompounding at the rate equal to the maximal local squared MSR. The paper gives\nsimple necessary and sufficient conditions for mean-variance (MV) efficient\nportfolios to be MMV efficient. Several illustrative examples contrasting the\nMV and MMV criteria are provided.",
      "generated_abstract": "This study examines the evolution of the market for exchange-traded notes (ETNs)\nand the impact of the COVID-19 pandemic on the market. ETNs are a type of\nexchange-traded product that allows investors to participate in the performance\nof certain financial assets, such as stocks or bonds. They have grown in\npopularity in recent years, and this research examines their development and\nmarket performance during the COVID-19 pandemic. The study uses historical\ndata from 2014 to 2024 to analyze the evolution of the ETN market and its\nrelationship with various economic indicators, including stock market indices,\ncrude oil prices, and the S&P 500 Index. The findings show that ETNs have\nexperienced significant growth, with the market value increasing from $15\nbillion",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2,
          "f": 0.18181817685950424
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.02727272727272727,
          "f": 0.024793383471075372
        },
        "rouge-l": {
          "r": 0.14583333333333334,
          "p": 0.175,
          "f": 0.15909090413223156
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/SI/2503.07961v1",
      "true_abstract": "Although hypergraph neural networks (HGNNs) have emerged as a powerful\nframework for analyzing complex datasets, their practical performance often\nremains limited. On one hand, existing networks typically employ a single type\nof attention mechanism, focusing on either structural or feature similarities\nduring message passing. On the other hand, assuming that all nodes in current\nhypergraph models have the same level of overlap may lead to suboptimal\ngeneralization. To overcome these limitations, we propose a novel framework,\noverlap-aware meta-learning attention for hypergraph neural networks\n(OMA-HGNN). First, we introduce a hypergraph attention mechanism that\nintegrates both structural and feature similarities. Specifically, we linearly\ncombine their respective losses with weighted factors for the HGNN model.\nSecond, we partition nodes into different tasks based on their diverse overlap\nlevels and develop a multi-task Meta-Weight-Net (MWN) to determine the\ncorresponding weighted factors. Third, we jointly train the internal MWN model\nwith the losses from the external HGNN model and train the external model with\nthe weighted factors from the internal model. To evaluate the effectiveness of\nOMA-HGNN, we conducted experiments on six real-world datasets and benchmarked\nits perfor-mance against nine state-of-the-art methods for node classification.\nThe results demonstrate that OMA-HGNN excels in learning superior node\nrepresentations and outperforms these baselines.",
      "generated_abstract": "In this paper, we propose a novel approach to improve the quality of\ne-commerce product images, leveraging multi-modal data and advanced\ndeep-learning models. This approach extends the standard product image\nquality assessment process by enhancing the quality of the product image\nthrough the integration of the image's color, texture, and depth information.\nThis is achieved through the combination of color-based features, texture\nfeatures, and depth features, respectively. The proposed approach is\ncomprehensively evaluated on a dataset comprising 15,000 images from 20\nwell-known e-commerce websites, including Amazon, eBay, Alibaba, and TJMaxx.\nResults show that our approach significantly improves the quality of the\nproduct images, achieving an average improvement of 20.44% and 22.79% in the\ncolor and texture features, respectively. Additionally,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1044776119402985,
          "p": 0.17721518987341772,
          "f": 0.13145539439441042
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.028037383177570093,
          "f": 0.020066885036186328
        },
        "rouge-l": {
          "r": 0.09701492537313433,
          "p": 0.16455696202531644,
          "f": 0.12206572303290811
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2405.01342v1",
      "true_abstract": "Economic policy sciences are constantly investigating the quality of\nwell-being of broad sections of the population in order to describe the current\ninterdependence between unequal living conditions, low levels of education and\na lack of integration into society. Such studies are often carried out in the\nform of surveys, e.g. as part of the EU-SILC program. If the survey is designed\nat national or international level, the results of the study are often used as\na reference by a broad range of public institutions. However, the sampling\nstrategy per se may not capture enough information to provide an accurate\nrepresentation of all population strata. Problems might arise from rare, or\nhard-to-sample, populations and the conclusion of the study may be compromised\nor unrealistic. We propose here a two-phase methodology to identify rare,\npoorly sampled populations and then resample the hard-to-sample strata. We\nfocused our attention on the 2019 EU-SILC section concerning the Italian region\nof Liguria. Methods based on dispersion indices or deep learning were used to\ndetect rare populations. A multi-frame survey was proposed as the sampling\ndesign. The results showed that factors such as citizenship, material\ndeprivation and large families are still fundamental characteristics that are\ndifficult to capture.",
      "generated_abstract": "In the last decade, a wide range of machine learning models has been\ndeveloped to model and predict the performance of binary classification\ndatasets. However, these models often exhibit significant performance\ndegradation in their binary classification performance when applied to\nnon-binary datasets. This degradation is commonly attributed to overfitting and\nlimitations of the model architecture. This paper explores the hypothesis that\nthe overfitting phenomenon is caused by a mismatch between the data distribution\nand the model architecture, specifically the model's ability to model the\nnon-binary dataset. We propose a novel methodology to address this mismatch,\nnamely the use of a non-binary dataset to evaluate the performance of the\nmodel on the binary dataset. We validate the effectiveness of our proposed\nmethodology through the analysis of the classification performance of\nRandom Forest and Gradient Boosting Machine models applied to the Wine Quality\ndataset. The results demonstrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15942028985507245,
          "p": 0.26506024096385544,
          "f": 0.19909501793411286
        },
        "rouge-2": {
          "r": 0.041237113402061855,
          "p": 0.061068702290076333,
          "f": 0.04923076441865135
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.25301204819277107,
          "f": 0.19004524417845675
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.02073v1",
      "true_abstract": "Analyzing time-series cross-sectional (also known as longitudinal or panel)\ndata is an important process across a number of fields, including the social\nsciences, economics, finance, and medicine. PanelMatch is an R package that\nimplements a set of tools enabling researchers to apply matching methods for\ncausal inference with time-series cross-sectional data. Relative to other\ncommonly used methods for longitudinal analyses, like regression with fixed\neffects, the matching-based approach implemented in PanelMatch makes fewer\nparametric assumptions and offers more diagnostics. In this paper, we discuss\nthe PanelMatch package, showing users a recommended pipeline for doing causal\ninference analysis with it and highlighting useful diagnostic and visualization\ntools.",
      "generated_abstract": "We consider the problem of estimating the mean of a non-linear Gaussian\n$\\mathcal{N}(\\mu,\\Sigma)$ where $\\Sigma$ is a known positive definite\n(non-singular) matrix. This problem is known to be NP-hard and is known to be\nNP-hard to approximate within a factor of $\\sqrt{2}$ with the state-of-the-art\nmethods. In this paper, we propose a novel approach for estimating the mean of\nthe non-linear Gaussian $\\mathcal{N}(\\mu,\\Sigma)$ that avoids the need for\napproximating the inverse matrix $\\Sigma^{-1}$ and achieves an error bound of\n$\\sqrt{2} + \\epsilon$ with $\\epsilon > 0$ independent of the dimension of the\nobservations. We provide a new characterization of the optimal estimator for\nthis problem that is based on a novel technique for estimating the covariance\nmatrix of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20987654320987653,
          "p": 0.2537313432835821,
          "f": 0.22972972477447054
        },
        "rouge-2": {
          "r": 0.0297029702970297,
          "p": 0.029411764705882353,
          "f": 0.029556645246427593
        },
        "rouge-l": {
          "r": 0.16049382716049382,
          "p": 0.19402985074626866,
          "f": 0.1756756707204165
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.eess/IV/2503.00156v1",
      "true_abstract": "Neural posterior estimation (NPE), a type of amortized variational inference,\nis a computationally efficient means of constructing probabilistic catalogs of\nlight sources from astronomical images. To date, NPE has not been used to\nperform inference in models with spatially varying covariates. However,\nground-based astronomical images have spatially varying sky backgrounds and\npoint spread functions (PSFs), and accounting for this variation is essential\nfor constructing accurate catalogs of imaged light sources. In this work, we\nintroduce a method of performing NPE with spatially varying backgrounds and\nPSFs. In this method, we generate synthetic catalogs and semi-synthetic images\nfor these catalogs using randomly sampled PSF and background estimates from\nexisting surveys. Using this data, we train a neural network, which takes an\nastronomical image and representations of its background and PSF as input, to\noutput a probabilistic catalog. Our experiments with Sloan Digital Sky Survey\ndata demonstrate the effectiveness of NPE in the presence of spatially varying\nbackgrounds and PSFs for light source detection, star/galaxy separation, and\nflux measurement.",
      "generated_abstract": "The detection of gravitational waves (GWs) has revealed the existence of a\nsignificant population of binary black holes (BBHs) in the Galactic Center (GC)\nregion, but the nature of the systems remains unclear. Here, we present the\nfirst detection of a BBH in the GC using the LIGO-Virgo-KAGRA (LVK)\nobservational data set, which spans a duration of more than 10 years. The\nsystem is characterized by a mass ratio $q=0.17^{+0.03}_{-0.03}$, a binary\nsemi-major axis of $a=5.1^{+0.3}_{-0.2}$ pc, and a mass ratio of $0.16^{+0.04}_{-0.04}$\nwith the companion, which is of the order of $10^{-4}$ of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11214953271028037,
          "p": 0.16666666666666666,
          "f": 0.1340782074816643
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.010416666666666666,
          "f": 0.007999995269122796
        },
        "rouge-l": {
          "r": 0.102803738317757,
          "p": 0.1527777777777778,
          "f": 0.12290502312412241
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/ST/2411.04788v1",
      "true_abstract": "In recent years, the application of generative artificial intelligence\n(GenAI) in financial analysis and investment decision-making has gained\nsignificant attention. However, most existing approaches rely on single-agent\nsystems, which fail to fully utilize the collaborative potential of multiple AI\nagents. In this paper, we propose a novel multi-agent collaboration system\ndesigned to enhance decision-making in financial investment research. The\nsystem incorporates agent groups with both configurable group sizes and\ncollaboration structures to leverage the strengths of each agent group type. By\nutilizing a sub-optimal combination strategy, the system dynamically adapts to\nvarying market conditions and investment scenarios, optimizing performance\nacross different tasks. We focus on three sub-tasks: fundamentals, market\nsentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30\ncompanies listed on the Dow Jones Index. Our findings reveal significant\nperformance variations based on the configurations of AI agents for different\ntasks. The results demonstrate that our multi-agent collaboration system\noutperforms traditional single-agent models, offering improved accuracy,\nefficiency, and adaptability in complex financial environments. This study\nhighlights the potential of multi-agent systems in transforming financial\nanalysis and investment decision-making by integrating diverse analytical\nperspectives.",
      "generated_abstract": "Recent advances in large language models (LLMs) have enabled the development\nof large language models for financial risk prediction. Despite the\nprevalence of these models in the financial sector, there remains a need to\nunderstand their limitations and identify potential gaps in their capabilities.\nTo address this gap, we conducted an evaluation of LLMs for financial risk\nprediction, focusing on stock prediction models. Our study, conducted with 2,643\nstocks, evaluated the performance of 14 LLMs using various metrics, including\naccuracy, sensitivity, specificity, and F1 score. Our analysis highlighted\nkey challenges and limitations in the LLMs' capabilities, including\noverfitting, biased predictions, and poor generalization. Additionally, we\nexamined how these models' capabilities vary across different stocks,\nhighlighting the importance of tailored models and ensembling techniques for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.23255813953488372,
          "f": 0.1851851803926613
        },
        "rouge-2": {
          "r": 0.005747126436781609,
          "p": 0.00847457627118644,
          "f": 0.006849310252395955
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.20930232558139536,
          "f": 0.16666666187414284
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/OS/2502.02349v1",
      "true_abstract": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
      "generated_abstract": "This paper presents a novel approach to optimize the scheduling of\ntasks in cloud computing environments. In particular, we focus on the\nimproving the performance of the Kubernetes platform by improving its\nscheduling. We propose a method that integrates dynamic programming with the\nframework of the Constrained Integer Programming (CIP) to solve the task\nscheduling problem in Kubernetes. We developed an implementation of this\nmethodology in the CIP library and validated it through benchmarks in Kubernetes.\nThis paper contributes to the field of cloud computing by proposing a\nnovel approach to optimizing the scheduling of tasks in Kubernetes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1978021978021978,
          "p": 0.3050847457627119,
          "f": 0.2399999952275556
        },
        "rouge-2": {
          "r": 0.047244094488188976,
          "p": 0.07058823529411765,
          "f": 0.05660376878115029
        },
        "rouge-l": {
          "r": 0.18681318681318682,
          "p": 0.288135593220339,
          "f": 0.22666666189422233
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.07022v1",
      "true_abstract": "For some discretely observed path of oscillating Brownian motion with level\nof self-organized criticality $\\rho_0$, we prove in the infill asymptotics that\nthe MLE is $n$-consistent, where $n$ denotes the sample size, and derive its\nlimit distribution with respect to stable convergence. As the transition\ndensity of this homogeneous Markov process is not even continuous in $\\rho_0$,\nthe analysis is highly non-standard. Therefore, interesting and somewhat\nunexpected phenomena occur: The likelihood function splits into several\ncomponents, each of them contributing very differently depending on how close\nthe argument $\\rho$ is to $\\rho_0$. Correspondingly, the MLE is successively\nexcluded to lay outside a compact set, a $1/\\sqrt{n}$-neighborhood and finally\na $1/n$-neigborhood of $\\rho_0$ asymptotically. The crucial argument to derive\nthe stable convergence is to exploit the semimartingale structure of the\nsequential suitably rescaled local log-likelihood function (as a process in\ntime). Both sequentially and as a process in $\\rho$, it exhibits a bivariate\nPoissonian behavior in the stable limit with its intensity being a multiple of\nthe local time at $\\rho_0$.",
      "generated_abstract": "We prove the correctness of a proof system for noncommutative linear\nalgebra and related structures. Our proof system is based on a\ngeneralization of the proof system of the commutative case, and we give an\nimmediate characterization of the noncommutative linear structures which are\nsuitable for our proof system.\n  We also present a proof of the well-known result that a noncommutative\nHilbert space is a Hilbert space.\n  In addition, we discuss some examples of noncommutative linear structures,\nincluding the algebras of all polynomials on an infinite set, and the\nnoncommutative vector spaces of all polynomials on an infinite set.\n  In particular, we show that the commutative algebra of all polynomials on an\ninfinite set is noncommutative, and the noncommutative vector space of all\npolynomials on an infinite set is commutative.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09734513274336283,
          "p": 0.19642857142857142,
          "f": 0.13017751036168215
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.020833333333333332,
          "f": 0.015624995312501406
        },
        "rouge-l": {
          "r": 0.09734513274336283,
          "p": 0.19642857142857142,
          "f": 0.13017751036168215
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/FL/2502.19603v1",
      "true_abstract": "This work studies the planning problem for robotic systems under both\nquantifiable and unquantifiable uncertainty. The objective is to enable the\nrobotic systems to optimally fulfill high-level tasks specified by Linear\nTemporal Logic (LTL) formulas. To capture both types of uncertainty in a\nunified modelling framework, we utilise Markov Decision Processes with\nSet-valued Transitions (MDPSTs). We introduce a novel solution technique for\nthe optimal robust strategy synthesis of MDPSTs with LTL specifications. To\nimprove efficiency, our work leverages limit-deterministic B\\\"uchi automata\n(LDBAs) as the automaton representation for LTL to take advantage of their\nefficient constructions. To tackle the inherent nondeterminism in MDPSTs, which\npresents a significant challenge for reducing the LTL planning problem to a\nreachability problem, we introduce the concept of a Winning Region (WR) for\nMDPSTs. Additionally, we propose an algorithm for computing the WR over the\nproduct of the MDPST and the LDBA. Finally, a robust value iteration algorithm\nis invoked to solve the reachability problem. We validate the effectiveness of\nour approach through a case study involving a mobile robot operating in the\nhexagonal world, demonstrating promising efficiency gains.",
      "generated_abstract": "This study explores the use of a 3D-printed autonomous vehicle (AV) as a\nnon-intrusive tool for monitoring and controlling the temperature of a\nhigh-rise building. The AV, a modified Honda Insight, was equipped with\ntemperature sensors and a motion detector, and autonomously navigated to the\ntop of the building. The temperature sensors were mounted on the roof, while\nthe motion detector was mounted on the ground floor, monitoring the temperature\nand activity of the building's occupants. The AV was able to navigate around\nthe building and collect temperature readings from various locations,\nincluding the ground floor, third floor, and the top floor. The collected data\nwas analyzed using machine learning techniques, with the goal of predicting\noccupant activity and detecting potential leaks in the building's HVAC\nsystem. The AV was able to successfully detect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0975609756097561,
          "p": 0.15584415584415584,
          "f": 0.11999999526450017
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.03418803418803419,
          "f": 0.026936022161004824
        },
        "rouge-l": {
          "r": 0.08130081300813008,
          "p": 0.12987012987012986,
          "f": 0.09999999526450022
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.05548v1",
      "true_abstract": "We study integer linear programs (ILP) of the form $\\min\\{c^\\top x\\ \\vert\\\nAx=b,l\\le x\\le u,x\\in\\mathbb Z^n\\}$ and analyze their parameterized complexity\nwith respect to their distance to the generalized matching problem--following\nthe well-established approach of capturing the hardness of a problem by the\ndistance to triviality. The generalized matching problem is an ILP where each\ncolumn of the constraint matrix has $1$-norm of at most $2$. It captures\nseveral well-known polynomial time solvable problems such as matching and flow\nproblems. We parameterize by the size of variable and constraint backdoors,\nwhich measure the least number of columns or rows that must be deleted to\nobtain a generalized matching ILP.\n  We present the following results: (i) a fixed-parameter tractable (FPT)\nalgorithm for ILPs parameterized by the size $p$ of a minimum variable backdoor\nto generalized matching; (ii) a randomized slice-wise polynomial (XP) time\nalgorithm for ILPs parameterized by the size $h$ of a minimum constraint\nbackdoor to generalized matching as long as $c$ and $A$ are encoded in unary;\n(iii) we complement (ii) by proving that solving an ILP is W[1]-hard when\nparameterized by $h$ even when $c,A,l,u$ have coefficients of constant size. To\nobtain (i), we prove a variant of lattice-convexity of the degree sequences of\nweighted $b$-matchings, which we study in the light of SBO jump M-convex\nfunctions. This allows us to model the matching part as a polyhedral constraint\non the integer backdoor variables. The resulting ILP is solved in FPT time\nusing an integer programming algorithm. For (ii), the randomized XP time\nalgorithm is obtained by pseudo-polynomially reducing the problem to the exact\nmatching problem. To prevent an exponential blowup in terms of the encoding\nlength of $b$, we bound the Graver complexity of the constraint matrix and\nemploy a Graver augmentation local search framework.",
      "generated_abstract": "This study investigates the impact of cloud computing on cybersecurity by\nexploring the relationship between cloud computing and cybersecurity. The\nstudy utilizes a mixed methods approach that combines quantitative and\nqualitative research. The quantitative research included a survey of 40\nuniversity students, while the qualitative research included interviews with\nsenior faculty members and experts in the field. The findings show that the\nuse of cloud computing can increase cybersecurity risks. Specifically, the\nuse of cloud computing increases the risk of security breaches, increased\ncosts, and decreased efficiency. Furthermore, the study found that cybersecurity\npractices in cloud computing can be improved through better awareness,\ncommunication, and collaboration. The study makes recommendations for\nuniversities, governments, and industry to improve cybersecurity in the cloud\ncomputing era.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09815950920245399,
          "p": 0.21621621621621623,
          "f": 0.13502109275151788
        },
        "rouge-2": {
          "r": 0.003745318352059925,
          "p": 0.009259259259259259,
          "f": 0.005333329232216486
        },
        "rouge-l": {
          "r": 0.09202453987730061,
          "p": 0.20270270270270271,
          "f": 0.12658227418611706
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.01180v1",
      "true_abstract": "It is well known that the graph isomorphism problem is polynomial-time\nreducible to the graph automorphism problem (in fact these two problems are\npolynomial-time equivalent). We show that, analogously, the group isomorphism\nproblem is polynomial-time reducible to the group automorphism problem.\nReductions to other relevant problems like automorphism counting are also\ngiven.",
      "generated_abstract": "Recent advances in deep learning and attention mechanisms have revolutionized\nresearch on long-form text generation, particularly in the medical domain.\nHowever, existing approaches often struggle with data efficiency and\ninterpretability, making it difficult to accurately evaluate their\nperformance. To address these challenges, we introduce \\textsc{Scoring-GPT}, a\nnovel framework that employs attention-based scoring functions to analyze\nlong-form text generation. Our approach employs a novel scoring function that\nenables accurate scoring of text generation tasks. We then integrate this\nscoring function into a multi-task attention-based model, GPT-450, which\nachieves state-of-the-art performance on the WSJ0-1000, METEOR, and\nROUGE-L metrics, with the ability to efficiently evaluate large text\ngeneration models. Our code is available at",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.07228915662650602,
          "f": 0.1043478220703215
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.060240963855421686,
          "f": 0.08695651772249545
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2503.06634v1",
      "true_abstract": "In our recent papers, we studied semiclassical spectral problems for the\nBochner-Schr\\\"odinger operator on a manifold of bounded geometry. We survey\nsome results of these papers in the setting of the magnetic Schr\\\"odinger\noperator in the Euclidean space and describe some ideas of the proofs.",
      "generated_abstract": "We give a new proof of the theorem of Kadomtsev and Pitaevskii (KP) for the\nnonlinear Schr\\\"odinger equation with a local nonlinearity. The proof is based\non a nonlocal counterpart of the Kadomtsev-Pitaevskii transformation, which\nallows one to write the equation in a form that is more amenable to the\nnonlinear analysis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2571428571428571,
          "p": 0.23684210526315788,
          "f": 0.24657533747419788
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.061224489795918366,
          "f": 0.0659340609636521
        },
        "rouge-l": {
          "r": 0.22857142857142856,
          "p": 0.21052631578947367,
          "f": 0.2191780772002253
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.03781v1",
      "true_abstract": "This technical report presents a novel DMD-based characterization method for\nvision sensors, particularly neuromorphic sensors such as event-based vision\nsensors (EVS) and Tianmouc, a complementary vision sensor. Traditional image\nsensor characterization standards, such as EMVA1288, are unsuitable for BVS due\nto their dynamic response characteristics. To address this, we propose a\nhigh-speed, high-precision testing system using a Digital Micromirror Device\n(DMD) to modulate spatial and temporal light intensity. This approach enables\nquantitative analysis of key parameters such as event latency, signal-to-noise\nratio (SNR), and dynamic range (DR) under controlled conditions. Our method\nprovides a standardized and reproducible testing framework, overcoming the\nlimitations of existing evaluation techniques for neuromorphic sensors.\nFurthermore, we discuss the potential of this method for large-scale BVS\ndataset generation and conversion, paving the way for more consistent\nbenchmarking of bio-inspired vision technologies.",
      "generated_abstract": "As the development of image quality assessment (QA) techniques advances,\ntechnical limitations in conventional QA systems have become a significant\nchallenge. To address this, we propose a novel QA framework for 2D MRI using\na two-stage framework. In the first stage, we develop a new loss function,\nGAN-based Quality-Enhanced Loss (GQEL), to enhance the image quality of\ncontrast-enhanced MRI (CE-MRI) by leveraging the generative modeling capability\nof the GAN framework. In the second stage, we introduce a novel Diffusion\nMixing Model (DMM) to generate high-quality images by mixing in-pixel\nperturbations with realistic noise and employing a diffusion-based\ntraining-augmentation framework. We validate our proposed framework on the\nMIT-BIAN CE-MRI dataset, demonstr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17346938775510204,
          "p": 0.2073170731707317,
          "f": 0.1888888839283952
        },
        "rouge-2": {
          "r": 0.05384615384615385,
          "p": 0.0673076923076923,
          "f": 0.05982905489078864
        },
        "rouge-l": {
          "r": 0.16326530612244897,
          "p": 0.1951219512195122,
          "f": 0.17777777281728407
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2502.20425v1",
      "true_abstract": "The measurements of cluster abundances, gravitational lensings, redshift\nspace distortions and peculiar velocities at lower redshifts point out to much\nsmaller sigma_8 than its value deduced from the measurements of the CMB\nfluctuations assuming the standard LCDM cosmology. We examine and compare the\nsigma_8 redshift dependence calculated within the gauge invariant formalism in\nthe LCDM and the Einstein-Cartan cosmology. It appears that the Einstein-Cartan\ncosmology provides systematically larger sigma_8(z) for higher redshifts\ncompared to the LCDM. Because the CMB fluctuations comprise a cosmological data\nfrom the recombination era to the present, the S8 problem of the LCDM cosmology\nis not a surprise from the standpoint of the Einstein-Cartan cosmology.",
      "generated_abstract": "This work presents a comprehensive study of the quantum dynamics of\nresonators, using a two-level system as a model system. We explore a variety\nof quantum systems, focusing on the behavior of the quantum state as a\nresonator undergoes a continuous transition between the ground and excited\nstates. By analyzing the quantum dynamics of the system, we explore the\ninfluence of the resonator's size, the transition frequency, and the coupling\nstrength on the resonator's behavior. Additionally, we investigate the\ndynamics of the system using a non-equilibrium Green's function method. Our\nfindings provide insights into the quantum dynamics of the resonator,\nhighlighting the importance of considering the resonator's quantum state when\nstudying its dynamics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08695652173913043,
          "p": 0.09836065573770492,
          "f": 0.0923076873266275
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.021505376344086023,
          "f": 0.021505371344087185
        },
        "rouge-l": {
          "r": 0.07246376811594203,
          "p": 0.08196721311475409,
          "f": 0.07692307194201216
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.00863v1",
      "true_abstract": "This paper studies the relationship between soft and hard paternalism by\nexamining two kinds of restriction: a waiting period and a hard limit (cap) on\nrisk-seeking behavior. Mandatory waiting periods have been instituted for\nmedical procedures, gun purchases and other high-stakes decisions. Are these\npolicies substitutes for hard restrictions, and are delayed decisions more\nrespected? In an experiment, decision-makers are informed about an impending\nhigh-stakes decision. Treatments define when the decision is made: on the spot\nor after one day, and whether the initial decision can be revised. In a general\npopulation survey experiment, another class of subjects (Choice Architects) is\ngranted the opportunity to make rules for decision-makers. Given a decision's\ntemporal structure, Choice Architects can decide on a cap to the\ndecision-maker's risk taking. In another treatment, Choice Architects can\nimplement a mandatory waiting period in addition to the cap. This allows us to\nstudy the substitutional relationship between waiting periods and paternalistic\naction and the effect of deliberation on the autonomy afforded to the\ndecision-maker. Our highly powered experiment reveals that exogenous\ndeliberation has no effect on the cap. Moreover, endogenously prescribed\nwaiting periods represent add-on restrictions that do not substitute for the\ncap. Choice Architects believe that, with time, the average decision-maker will\ntake less risk and -- because of the distribution of Choice Architects' bliss\npoints -- come closer to Choice Architects' subjective ideal choice. These\nfindings highlight the complementarity of policy tools in targeting various\nparts of a distribution of decision-makers.",
      "generated_abstract": "We introduce a novel methodology to study the impact of international\ntrade on labor markets using microdata from the Wave 4 of the European\nEconomic and Social Research Survey (EES). Our approach combines a two-stage\nmethodology with a panel approach that allows us to capture the impact of\ninternational trade on labor markets over the period 2014-2022. Our results\nshow that a 1% increase in the share of imports in total imports has a\nsignificant effect on wages. The impact of imports on wages is strongest for\nhigh-skilled workers and women. The results suggest that the impact of\ninternational trade on labor markets is a key factor in shaping labor market\noutcomes, particularly for high-skilled workers and women.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11612903225806452,
          "p": 0.2727272727272727,
          "f": 0.16289592341270664
        },
        "rouge-2": {
          "r": 0.025974025974025976,
          "p": 0.0625,
          "f": 0.03669724355862348
        },
        "rouge-l": {
          "r": 0.10967741935483871,
          "p": 0.25757575757575757,
          "f": 0.15384614965705054
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/NE/2503.06484v1",
      "true_abstract": "Accurate sign language understanding serves as a crucial communication\nchannel for individuals with disabilities. Current sign language translation\nalgorithms predominantly rely on RGB frames, which may be limited by fixed\nframe rates, variable lighting conditions, and motion blur caused by rapid hand\nmovements. Inspired by the recent successful application of event cameras in\nother fields, we propose to leverage event streams to assist RGB cameras in\ncapturing gesture data, addressing the various challenges mentioned above.\nSpecifically, we first collect a large-scale RGB-Event sign language\ntranslation dataset using the DVS346 camera, termed VECSL, which contains\n15,676 RGB-Event samples, 15,191 glosses, and covers 2,568 Chinese characters.\nThese samples were gathered across a diverse range of indoor and outdoor\nenvironments, capturing multiple viewing angles, varying light intensities, and\ndifferent camera motions. Due to the absence of benchmark algorithms for\ncomparison in this new task, we retrained and evaluated multiple\nstate-of-the-art SLT algorithms, and believe that this benchmark can\neffectively support subsequent related research. Additionally, we propose a\nnovel RGB-Event sign language translation framework (i.e., M$^2$-SLT) that\nincorporates fine-grained micro-sign and coarse-grained macro-sign retrieval,\nachieving state-of-the-art results on the proposed dataset. Both the source\ncode and dataset will be released on https://github.com/Event-AHU/OpenESL.",
      "generated_abstract": "We introduce the first unified framework for evaluating the performance of\nunsupervised and supervised face recognition models across multiple\ndatasets. Our approach, called FACETS (Face-to-Face Evaluation of\nCross-Dataset Transfers), is built on the concept of a cross-dataset\ntransfer-domain transfer, which we define as a transfer from one dataset to\nanother where the source and target domains are from different domains, but\nshare similar face attributes. This approach not only accounts for the\ndifferences between source and target datasets but also considers the\ninteractions between the source and target datasets. We further introduce\nFACETS-X, a novel extension of FACETS that extends the cross-dataset transfer\nanalysis to unsupervised face recognition models. We demonstrate the effectiveness\nof FACETS-X by evaluating 11 state-of-the-art unsupervised face",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.26582278481012656,
          "f": 0.18584070341726067
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.036036036036036036,
          "f": 0.02640263562134513
        },
        "rouge-l": {
          "r": 0.1360544217687075,
          "p": 0.25316455696202533,
          "f": 0.1769911458951368
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.11448v1",
      "true_abstract": "Gaussian processes (GPs) are flexible, probabilistic, non-parametric models\nwidely employed in various fields such as spatial statistics, time series\nanalysis, and machine learning. A drawback of Gaussian processes is their\ncomputational cost having $\\mathcal{O}(N^3)$ time and $\\mathcal{O}(N^2)$ memory\ncomplexity which makes them prohibitive for large datasets. Numerous\napproximation techniques have been proposed to address this limitation. In this\nwork, we systematically compare the accuracy of different Gaussian process\napproximations concerning marginal likelihood evaluation, parameter estimation,\nand prediction taking into account the time required to achieve a certain\naccuracy. We analyze this trade-off between accuracy and runtime on multiple\nsimulated and large-scale real-world datasets and find that Vecchia\napproximations consistently emerge as the most accurate in almost all\nexperiments. However, for certain real-world data sets, low-rank inducing\npoint-based methods, i.e., full-scale and modified predictive process\napproximations, can provide more accurate predictive distributions for\nextrapolation.",
      "generated_abstract": "We consider a framework for the design of experiments that allows for the\nrepresentation of a wide range of designs, including a large number of\ncombinatorial options, by a compact set of variables. We present a general\nmethodology for this design problem that is based on a combination of\nnon-parametric and parametric approaches. The non-parametric approach consists\nof a set of random variables, whose values are conditionally independent given\nthe design. We show that, in the case of a finite set of design options,\nrepresented by a compact set of variables, the non-parametric approach is a\nconvenient and computationally efficient alternative to the parametric\napproach. We illustrate the approach through a case study, with the aim of\nsimplifying the design of experiments in the context of large-scale clinical\ntrials.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14782608695652175,
          "p": 0.2463768115942029,
          "f": 0.1847826040081523
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12173913043478261,
          "p": 0.2028985507246377,
          "f": 0.1521739083559784
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.21306v1",
      "true_abstract": "Solar prosumers, residential electricity consumers equipped with photovoltaic\n(PV) systems and battery storage, are transforming electricity markets. Their\ninteractions with the transmission grid under varying tariff designs are not\nyet fully understood. We explore the influence of different pricing regimes on\nprosumer investment and dispatch decisions and their subsequent impact on the\ntransmission grid. Using an integrated modeling approach that combines two\nopen-source dispatch, investment and grid models, we simulate prosumage\nbehavior in Germany's electricity market under real-time pricing or\ntime-invariant pricing, as well as under zonal or nodal pricing. Our findings\nshow that zonal pricing favors prosumer investments, while time-invariant\npricing rather hinders it. In comparison, regional solar availability emerges\nas a larger driver for rooftop PV investments. The impact of prosumer\nstrategies on grid congestion remains limited within the scope of our\nmodel-setup, in which home batteries cannot be used for energy arbitrage.",
      "generated_abstract": "This paper explores how the introduction of artificial intelligence (AI)\ninto decision-making in the financial sector might affect the efficiency and\nliquidity of capital markets. Using the S&P 500 as a case study, we employ\nsemi-parametric and non-parametric regression models to analyze the impact of\nAI on trading volume and turnover, and the behavior of liquidity providers. Our\nfindings indicate that the introduction of AI can lead to a significant\nreduction in trading volume, particularly for small-cap stocks, while\nincreasing turnover. However, this effect is mitigated by the implementation\nof AI-based trading strategies. These findings highlight the potential\nimpact of AI on capital markets and highlight the need for greater transparency\nand oversight of these technologies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1651376146788991,
          "p": 0.22784810126582278,
          "f": 0.19148935682944784
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.028037383177570093,
          "f": 0.024193543481075393
        },
        "rouge-l": {
          "r": 0.1559633027522936,
          "p": 0.21518987341772153,
          "f": 0.18085105895710746
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2412.03606v1",
      "true_abstract": "This paper aims to study the prediction of the bank stability index based on\nthe Time Series Transformer model. The bank stability index is an important\nindicator to measure the health status and risk resistance of financial\ninstitutions. Traditional prediction methods are difficult to adapt to complex\nmarket changes because they rely on single-dimensional macroeconomic data. This\npaper proposes a prediction framework based on the Time Series Transformer,\nwhich uses the self-attention mechanism of the model to capture the complex\ntemporal dependencies and nonlinear relationships in financial data. Through\nexperiments, we compare the model with LSTM, GRU, CNN, TCN and RNN-Transformer\nmodels. The experimental results show that the Time Series Transformer model\noutperforms other models in both mean square error (MSE) and mean absolute\nerror (MAE) evaluation indicators, showing strong prediction ability. This\nshows that the Time Series Transformer model can better handle multidimensional\ntime series data in bank stability prediction, providing new technical\napproaches and solutions for financial risk management.",
      "generated_abstract": "This paper introduces a novel framework for the analysis of portfolio\ndiversification based on a multivariate conditional dependence model, where\nboth the conditional dependence and the conditional variance are modeled in a\nunified framework. We develop a new method for evaluating diversification\nratios based on the Sharpe ratio and the volatility skew. Furthermore, we\nintroduce a novel measure for the diversification impact of a portfolio,\nderived from the Sharpe ratio and the volatility skew. This measure is\ndesigned to capture the effect of the portfolio's conditional dependence on its\ncontribution to the overall conditional variance, rather than solely on the\nportfolio's conditional dependence on the asset's mean. We also introduce a\nnew measure of diversification impact based on the Sharpe ratio and the\nconditional variance. This measure is designed to capture the effect of the\nportfolio",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19607843137254902,
          "p": 0.3333333333333333,
          "f": 0.24691357558299043
        },
        "rouge-2": {
          "r": 0.0425531914893617,
          "p": 0.0625,
          "f": 0.050632906572665094
        },
        "rouge-l": {
          "r": 0.18627450980392157,
          "p": 0.31666666666666665,
          "f": 0.23456789657064478
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06572v1",
      "true_abstract": "Recent research has paid little attention to complex driving behaviors,\nnamely merging car-following and lane-changing behavior, and how lane-changing\naffects algorithms designed to model and control a car-following vehicle.\nDuring the merging behavior, the Follower Vehicle (FV) might significantly\ndiverge from typical car-following models. Thus, this paper aims to control the\nFV witnessing lane-changing behavior based on anticipation, perception,\npreparation, and relaxation states defined by a novel measurable human\nperception index. Data from human drivers are utilized to create a\nperception-based fuzzy controller for the behavior vehicle's route guidance,\ntaking into account the opacity of human driving judgments. We illustrate the\nefficacy of the established technique using simulated trials and data from\nactual drivers, focusing on the benefits of the increased comfort, safety, and\nuniformity of traffic flow and the decreased of wait time and motion sickness\nthis brings about.",
      "generated_abstract": "We present an optimization framework for the design of adaptive,\ndynamic, and multi-agent systems. The framework uses the extended Kalman filter\n(EKF) to compute a set of state estimates for the dynamics of the adaptive\nsystem. It is shown that the EKF is a gradient descent optimizer, and that the\noptimization problem can be formulated as an infinite-dimensional constrained\noptimization problem. This allows the EKF to be used as a cost function for\noptimization. To overcome the difficulty of computing the gradient of the\ncost function with respect to the state estimates, we propose a gradient\nestimation procedure that employs Kalman filtering and Bayesian estimation. The\nproposed framework is used to design an adaptive system with dynamic and\nmulti-agent components. The framework is validated through simulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07,
          "p": 0.10144927536231885,
          "f": 0.08284023185462723
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.017543859649122806,
          "f": 0.01587301091836889
        },
        "rouge-l": {
          "r": 0.07,
          "p": 0.10144927536231885,
          "f": 0.08284023185462723
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.07806v1",
      "true_abstract": "The integration of Quantum Deep Learning (QDL) techniques into the landscape\nof financial risk analysis presents a promising avenue for innovation. This\nstudy introduces a framework for credit risk assessment in the banking sector,\ncombining quantum deep learning techniques with adaptive modeling for Row-Type\nDependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed\napproach tailors predictive models to different loan categories, aiming to\nenhance the accuracy and efficiency of credit risk evaluation. While this work\nexplores the potential of integrating quantum methods with classical deep\nlearning for risk assessment, it focuses on the feasibility and performance of\nthis hybrid framework rather than claiming transformative industry-wide\nimpacts. The findings offer insights into how quantum techniques can complement\ntraditional financial analysis, paving the way for further advancements in\npredictive modeling for credit risk.",
      "generated_abstract": "The application of deep learning to the pricing of derivatives is a\ncurrently active research field, with a focus on the development of efficient\nand scalable models. However, despite significant advances, the application of\ndeep learning in this area remains challenging, with significant room for\nimprovement. This paper focuses on the design of a deep learning framework that\nachieves high accuracy and scalability. Our proposed framework is based on a\ntwo-stage deep learning pipeline, which first employs a convolutional neural\nnetwork (CNN) to extract features, and then uses a recurrent neural network (RNN)\nto predict the forward cash flow. The first stage features are used to train\nthe RNN, which is then used to predict the cash flow of the derivative.\nExperiments on two different datasets demonstrate the effectiveness of the\nproposed framework, with a significant improvement in accuracy compared to\ntraditional deep learning approaches",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22826086956521738,
          "p": 0.25,
          "f": 0.2386363586466943
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.038461538461538464,
          "f": 0.03921568127643278
        },
        "rouge-l": {
          "r": 0.20652173913043478,
          "p": 0.2261904761904762,
          "f": 0.2159090859194216
        }
      }
    },
    {
      "paper_id": "cs.MA.cs/GT/2503.10186v1",
      "true_abstract": "Beyond specific settings, many multi-agent learning algorithms fail to\nconverge to an equilibrium solution, and instead display complex,\nnon-stationary behaviours such as recurrent or chaotic orbits. In fact, recent\nliterature suggests that such complex behaviours are likely to occur when the\nnumber of agents increases. In this paper, we study Q-learning dynamics in\nnetwork polymatrix games where the network structure is drawn from classical\nrandom graph models. In particular, we focus on the Erdos-Renyi model, a\nwell-studied model for social networks, and the Stochastic Block model, which\ngeneralizes the above by accounting for community structures within the\nnetwork. In each setting, we establish sufficient conditions under which the\nagents' joint strategies converge to a unique equilibrium. We investigate how\nthis condition depends on the exploration rates, payoff matrices and,\ncrucially, the sparsity of the network. Finally, we validate our theoretical\nfindings through numerical simulations and demonstrate that convergence can be\nreliably achieved in many-agent systems, provided network sparsity is\ncontrolled.",
      "generated_abstract": "In the age of deep learning, the use of large language models (LLMs) in\nresearch and industry has gained significant momentum. However, the\ninterpretability of LLMs remains a significant challenge, especially in\nhigh-stakes applications such as medical diagnosis. To address this issue, we\npropose a novel framework, iLLMaViD, that integrates the LLM with a visual\nrepresentation model, ViD, to enable the visualization of internal\ninterpretations of LLMs. We first train LLMs with various visual prompts, and\nthen use ViD to generate semantic-level visual interpretations for each LLM\ninput. These visual interpretations are then used as inputs to a\nsupervised-learning-based LLM for further predictions. Experimental results on\nthe MIMIC-III and ClinicalTrials datasets demonstrate that our approach\noutperforms existing methods in terms of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17355371900826447,
          "p": 0.2441860465116279,
          "f": 0.20289854586758163
        },
        "rouge-2": {
          "r": 0.025806451612903226,
          "p": 0.03389830508474576,
          "f": 0.029304024395873568
        },
        "rouge-l": {
          "r": 0.15702479338842976,
          "p": 0.22093023255813954,
          "f": 0.18357487436999712
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.physics/optics/2503.10359v1",
      "true_abstract": "In this study, we systematically explore the non-Hermitian skin effect (NHSE)\nand its associated complex-frequency detection in the context of a\nfrequency-dependent non-Hermitian Hamiltonian. This Hamiltonian arises from the\nself-energy correction of the subsystem and can be calculated exactly within\nour theoretical model, without the need for non-Hermitian approximations.\nAdditionally, complex frequency detection, which encompasses complex frequency\nexcitation, synthesis, and fingerprint, enables us to detect the physical\nresponses driven by complex frequency excitations. Our calculations reveal that\nboth complex frequency excitation and synthesis are sensitive to the\nnon-Hermitian approximation and are unable to characterize the presence or\nabscence of the NHSE. In contrast, the complex-frequency fingerprint\nsuccessfully detects the novel responses induced by the NHSE through the\nintroduction of a double-frequency Green's function. Our work paves the way for\na rigorous understanding of non-Hermitian physics in quantum systems and their\nexperimental verification through complex frequency-domain techniques.",
      "generated_abstract": "The current state of the art in microcavity photonics relies on a combination\nof microresonators with large resonant frequency and broadband light-matter\ninteractions. However, the latter are often limited by optical losses in\nresonators. To address this, we propose to use a hybrid cavity composed of\nresonators with different frequency and mode confinement. We demonstrate that\nthe hybrid cavity allows to enhance the coupling between the two resonators,\nresulting in a significant enhancement of the broadband light-matter\ninteraction. Furthermore, we demonstrate that the hybrid cavity can be used to\ntune the resonance frequency of the two resonators, enabling the control of\nthe optical properties of the hybrid cavity. We also show that the hybrid\ncavity can be used to control the coupling between the resonators with\nrespect to the light-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1326530612244898,
          "p": 0.18571428571428572,
          "f": 0.1547618999007938
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.02912621359223301,
          "f": 0.02489626066562311
        },
        "rouge-l": {
          "r": 0.1326530612244898,
          "p": 0.18571428571428572,
          "f": 0.1547618999007938
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.08728v1",
      "true_abstract": "We use a methodology based on a machine learning algorithm to quantify firms'\ncyber risks based on their disclosures and a dedicated cyber corpus. The model\ncan identify paragraphs related to determined cyber-threat types and\naccordingly attribute several related cyber scores to the firm. The cyber\nscores are unrelated to other firms' characteristics. Stocks with high cyber\nscores significantly outperform other stocks. The long-short cyber risk factors\nhave positive risk premia, are robust to all factors' benchmarks, and help\nprice returns. Furthermore, we suggest the market does not distinguish between\ndifferent types of cyber risks but instead views them as a single, aggregate\ncyber risk.",
      "generated_abstract": "This paper provides a unified framework for modeling the evolution of\nfinancial markets, combining the general theory of stochastic processes with\nthe theory of stochastic valuation. Our model, known as the SVM (Stochastic\nValue Model), provides a unified approach to the analysis of a wide range of\nfinancial markets, including stock markets, futures markets, and derivatives\nmarkets. The SVM is an extension of the standard stochastic volatility model\n(SVM) to the stochastic value framework, providing a unified framework for\nanalyzing a wide range of financial markets. The SVM is a stochastic\nvolatility model with value functions that are continuous and strictly\ndifferentiable. In this paper, we present a unified analysis of the SVM,\nexplaining how it can be used to model both stochastic volatility and value\nfunctions in a single",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.16666666666666666,
          "f": 0.16438355664477405
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.009523809523809525,
          "f": 0.009803916572955255
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.1527777777777778,
          "f": 0.15068492650778773
        }
      }
    },
    {
      "paper_id": "math.DG.math/SG/2503.06570v1",
      "true_abstract": "In this paper, we propose a condition on the coefficients of a\ncohomology-valued power series, which we call ``asymptotically\nMittag-Leffler''. We show that if the $J$-function of a Fano manifold is\nasymptotically Mittag-Leffler, then it has the exponential growth as $t\\to\n+\\infty$. This provides an alternative method to compute the principal\nasymptotic class of a Fano manifold using the coefficients of $J$-function. We\nalso verify that the $J$-function of the projective space is asymptotically\nMittag-Leffler, and the property of having an asymptotically Mittag-Leffler\n$J$-function is preserved when taking product and hypersurface.",
      "generated_abstract": "We study the $A_\\infty$ structure on the derived category of coherent sheaves\non a smooth projective variety $X$ with an action of a group $G$\nco-adjoint to a complex reflection. We show that $G$ is in general a non-trivial\n$A_\\infty$ algebra. Furthermore, we describe a $\\mathbb{Z}$-graded\n$A_\\infty$ algebra structure on the derived category of coherent sheaves,\nwhich is compatible with the $G$-action and with the canonical $G$-equivariant\npermutation action. We also show that the resulting $A_\\infty$ algebra is\nisomorphic to the $A_\\infty$ algebra of a suitable $G$-equivariant permsion\nbundle on $X$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3,
          "f": 0.27272726776859507
        },
        "rouge-2": {
          "r": 0.075,
          "p": 0.07792207792207792,
          "f": 0.07643311602093424
        },
        "rouge-l": {
          "r": 0.23333333333333334,
          "p": 0.28,
          "f": 0.25454544958677694
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09972v1",
      "true_abstract": "It is known that, when $n$ is even, the number of permutations of\n$\\{1,2,\\dots,n\\}$ all of whose cycles have odd length equals the number of\nthose all of whose cycles have even length. Adin, Heged\\H{u}s and Roichman\nrecently found a surprising refinement of this identity. They showed that, for\nany fixed set $J$, the equality still holds when restricting to permutations\nwith descent set $J$ on one side, and permutations with ascent set $J$ on the\nother. Their proof uses generating functions for higher Lie characters. They\nalso deduce a version for odd $n$.\n  Here we give a bijective proof of their result. We first use known bijections\nto restate the identity in terms of multisets of necklaces, and then describe a\nnew weight-preserving bijection between words all of whose Lyndon factors have\nodd length and are distinct, and words all of whose Lyndon factors have even\nlength. We also show that the corresponding equality about Lyndon\nfactorizations has a short proof using generating functions.",
      "generated_abstract": "In this paper, we study the existence and uniqueness of solutions to a class of\nnonlinear elliptic equations with Dirichlet or Neumann boundary conditions in\none-dimensional domains, where the coefficients are smooth and bounded. In\naddition, we assume that the boundary conditions on the boundary of the domain\nare Neumann type. Our main results are the existence of unique solutions for\nboth Dirichlet and Neumann boundary conditions. The estimates for the\nexistence of solutions are given in terms of the Lipschitz constant of the\nboundary coefficients. In addition, we show that the boundary conditions can be\nsatisfied by suitable boundary conditions for the coefficients.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15463917525773196,
          "p": 0.2727272727272727,
          "f": 0.19736841643438377
        },
        "rouge-2": {
          "r": 0.035211267605633804,
          "p": 0.05952380952380952,
          "f": 0.04424778293993315
        },
        "rouge-l": {
          "r": 0.13402061855670103,
          "p": 0.23636363636363636,
          "f": 0.17105262696069956
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.00877v1",
      "true_abstract": "This study investigates the inherently random structures of dry bulk shipping\nnetworks, often likened to a taxi service, and identifies the underlying trade\ndynamics that contribute to this randomness within individual cargo\nsub-networks. By analysing micro-level trade flow data from 2015 to 2023, we\nexplore the evolution of dry commodity networks, including grain, coal, and\niron ore, and suggest that the Giant Strongly Connected Components exhibit\nsmall-world phenomena, indicative of efficient bilateral trade. The significant\nheterogeneity of in-degree and out-degree within these sub-networks, primarily\ndriven by importing ports, underscores the complexity of their dynamics. Our\ntemporal analysis shows that while the Covid-19 pandemic profoundly impacted\nthe coal network, the Ukraine conflict significantly altered the grain network,\nresulting in changes in community structures. Notably, grain sub-networks\ndisplay periodic changes, suggesting distinct life cycles absent in coal and\niron ore networks. These findings illustrate that the randomness in dry bulk\nshipping networks is a reflection of real-world trade dynamics, providing\nvaluable insights for stakeholders in navigating and predicting network\nbehaviours.",
      "generated_abstract": "We investigate the problem of portfolio selection in the presence of\nportfolio weight misalignments, where agents are not allowed to fully align\ntheir portfolio weights. In this model, agents' preferences are modeled by\nlog-normal distributions, which is a natural choice for portfolio\ndistributions. We first derive an exact expression for the portfolio\noptimal-weight distribution, which is a log-normal distribution with mean and\nvariance that are determined by the agent's preferences. We then derive a\ncontinuous-time optimal-portfolio policy that minimizes the portfolio\nmarginalization cost. As the cost function is non-convex, we propose a\nproximal gradient algorithm to solve this problem. We then apply our\nframework to the portfolio selection problem of a group of agents with\nindependent preferences. We show that the proposed algorithm achieves an\noptimal rate of convergence, which is $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10743801652892562,
          "p": 0.1625,
          "f": 0.12935322903888535
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.01652892561983471,
          "f": 0.014134270723322
        },
        "rouge-l": {
          "r": 0.09917355371900827,
          "p": 0.15,
          "f": 0.11940298028266647
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.04607v1",
      "true_abstract": "Heegaard splittings stratify 3-manifolds by complexity; only $S^3$ admits a\ngenus-zero splitting, and only $S^3$, $S^1 \\times S^2$, and lens spaces\n$L(p,q)$ admit genus-one splittings. In dimension four, the second author and\nJeffrey Meier proved that only a handful of simply-connected 4-manifolds have\ntrisection genus two or less, while Meier conjectured that if $X$ admits a\ngenus-three trisection, then $X$ is diffeomorphic to a spun lens space $S_p$ or\nits sibling $S_p'$, $S^4$, or a connected sum of copies of $\\pm \\mathbb{CP}^2$,\n$S^1 \\times S^3$, and $S^2 \\times S^2$. We prove Meier's conjecture in the case\nthat $X$ admits a weakly reducible genus-three trisection, where weak\nreducibility is a new idea adapted from Heegaard theory and is defined in terms\nof disjoint curves bounding compressing disks in various handlebodies. The\ntools and techniques used to prove the main theorem borrow heavily from\n3-manifold topology. Of independent interest, we give a trisection-diagrammatic\ndescription of 4-manifolds obtained by surgery on loops and spheres in other\n4-manifolds.",
      "generated_abstract": "We provide a new perspective on the theory of the complex-valued moduli of\nsubvarieties of complex projective spaces. We introduce a notion of moduli of\nsubvarieties which is a generalization of the moduli of divisors and of the\nmoduli of subvarieties of algebraic varieties. We show that our moduli can be\nviewed as the moduli of moduli of complex subvarieties of complex projective\nspaces. In this perspective, the theory of the complex-valued moduli of\nsubvarieties can be seen as a generalization of the theory of the complex-valued\nmoduli of divisors.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10434782608695652,
          "p": 0.34285714285714286,
          "f": 0.1599999964222223
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.03773584905660377,
          "f": 0.018957342209744463
        },
        "rouge-l": {
          "r": 0.10434782608695652,
          "p": 0.34285714285714286,
          "f": 0.1599999964222223
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SY/2503.07997v1",
      "true_abstract": "Autonomous stores leverage advanced sensing technologies to enable\ncashier-less shopping, real-time inventory tracking, and seamless customer\ninteractions. However, these systems face significant challenges, including\nocclusion in vision-based tracking, scalability of sensor deployment, theft\nprevention, and real-time data processing. To address these issues, researchers\nhave explored multi-modal sensing approaches, integrating computer vision,\nRFID, weight sensing, vibration-based detection, and LiDAR to enhance accuracy\nand efficiency. This survey provides a comprehensive review of sensing\ntechnologies used in autonomous retail environments, highlighting their\nstrengths, limitations, and integration strategies. We categorize existing\nsolutions across inventory tracking, environmental monitoring, people-tracking,\nand theft detection, discussing key challenges and emerging trends. Finally, we\noutline future directions for scalable, cost-efficient, and privacy-conscious\nautonomous store systems.",
      "generated_abstract": "The real-time control of multi-agent systems with a multi-robot network\nhas attracted significant research interest due to their practical applications\nin robotics, automation, and the military. However, the control of a multi-robot\nnetwork is more challenging than that of a single robot due to the lack of\ninformation exchange among the robots, the complex interplay between the\ndynamics of the robotic systems, and the coordination between the robots. In\nthis paper, we propose a distributed multi-agent control framework for\nmulti-robot systems, where robots communicate with each other through a\ncentral controller, and each robot adjusts its control strategy based on the\nlatest information received from the central controller. To address the\ncommunication and coordination challenges, we propose a distributed\ncoordinated multi-agent (DCMA) control scheme, which consists of a central\ncontroller, a distributed coordination mechanism,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1595744680851064,
          "p": 0.18518518518518517,
          "f": 0.17142856645616342
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.008403361344537815,
          "f": 0.008620684658519543
        },
        "rouge-l": {
          "r": 0.1595744680851064,
          "p": 0.18518518518518517,
          "f": 0.17142856645616342
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.10327v1",
      "true_abstract": "Solutions to the quiver-theoretic quantum Yang-Baxter equation are associated\nwith structure categories and structure groupoids. We prove that the structure\ngroupoids of involutive non-degenerate solutions are Garside. This generalises\na well-known result about the structure groups of set-theoretic solutions, due\nto Chouraqui. We also construct involutive non-degenerate solutions from\nsuitable presented categories. We then investigate the case of solutions of\nprincipal homogeneous type. Finally, we present some examples of this new class\nof Garside groupoids.",
      "generated_abstract": "In this paper, we construct a noncommutative analogue of the $q$-deformed\nnoncommutative Schur function. In the noncommutative version, we utilize the\nnoncommutative version of the $q$-Kummer sequence. For the $q$-deformed\nnoncommutative Schur function, we give an explicit expression in terms of the\nnoncommutative Schur function. In addition, we give an explicit expression in\nterms of the classical Schur function. Furthermore, we give an explicit\nexpression in terms of the noncommutative Kadane sum.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.1935483870967742,
          "f": 0.1445783085730877
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.1935483870967742,
          "f": 0.1445783085730877
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.05022v2",
      "true_abstract": "Understanding firm conduct is crucial for industrial organization and\nantitrust policy. In this article, we develop a testing procedure based on the\nRivers and Vuong non-nested model selection framework. Unlike existing methods\nthat require estimating the demand and supply system, our approach compares the\nmodel fit of two first-stage price regressions. Through an extensive Monte\nCarlo study, we demonstrate that our test performs comparably to, or\noutperforms, existing methods in detecting collusion across various collusive\nscenarios. The results are robust to model misspecification, alternative\nfunctional forms for instruments, and data limitations. By simplifying the\ndiagnosis of firm behavior, our method provides an efficient tool for\nresearchers and regulators to assess industry conduct. Additionally, our\napproach offers a practical guideline for enhancing the strength of BLP-style\ninstruments in demand estimation: once collusion is detected, researchers are\nadvised to incorporate the product characteristics of colluding partners into\nown-firm instruments while excluding them from other-firm instruments.",
      "generated_abstract": "This paper presents a novel approach to the study of the interplay between\npolitical and economic factors in a multi-period setting. We develop a\ngeneral framework for modeling the dynamic interaction between political and\neconomic factors, with particular focus on the role of institutions and\ngovernance in shaping these interactions. Our framework is based on a\nmultiperspective approach that incorporates the perspectives of political\ninstitutions, economic institutions, and the economy. We develop a model that\ncaptures the interplay between political and economic factors through the\ninteraction between political and economic institutions, and economic\ninstitutions. Our model allows us to study the effects of changes in\ninstitutions on the interactions between political and economic factors, as well\nas the effects of economic factors on political factors. We illustrate the\nimpact of the interaction between political and economic factors on economic\noutcomes using a simple model and a real-world economic data",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14035087719298245,
          "p": 0.25396825396825395,
          "f": 0.18079095586708813
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.02631578947368421,
          "f": 0.02272726782024899
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.23809523809523808,
          "f": 0.16949152083883953
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2503.02611v1",
      "true_abstract": "Statistical integration of diverse data sources is an essential step in the\nbuilding of generalizable prediction tools, especially in precision health. The\ninvariant features model is a new paradigm for multi-source data integration\nwhich posits that a small number of covariates affect the outcome identically\nacross all possible environments. Existing methods for estimating invariant\neffects suffer from immense computational costs or only offer good statistical\nperformance under strict assumptions. In this work, we provide a general\nframework for estimation under the invariant features model that is\ncomputationally efficient and statistically flexible. We also provide a robust\nextension of our proposed method to protect against possibly corrupted or\nmisspecified data sources. We demonstrate the robust properties of our method\nvia simulations, and use it to build a transferable prediction model for end\nstage renal disease using electronic health records from the All of Us research\nprogram.",
      "generated_abstract": "This paper introduces a novel methodology to model the impact of\nstress on a stochastic, nonlinear system. The approach relies on the notion of\n\"stochastic dynamic programming\", which allows to obtain the optimal control\nproblems that maximize the expected value of a given stochastic process. The\nmodeling strategy relies on the introduction of a novel stochastic dynamic\nprogramming problem and uses the seminal results of Bouchaud et al. [2002",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08653846153846154,
          "p": 0.1836734693877551,
          "f": 0.11764705446964858
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08653846153846154,
          "p": 0.1836734693877551,
          "f": 0.11764705446964858
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2409.13957v1",
      "true_abstract": "One type of bond with the most implicit government guarantee is municipal\ninvestment bonds. In recent years, there have been an increasing number of\ndowngrades in the credit ratings of municipal bonds, which has led some people\nto question whether the implicit government guarantee may affect the\nobjectivity of the bond ratings? This paper uses text mining methods to mine\nrelevant policy documents related to municipal investment bond issuance, and\ncalculates the implicit guarantee strength of municipal investment bonds based\non the PMC index model. It further analyzes the impact of the implicit\nguarantee strength of municipal bonds on their credit evaluation. The study\nfound that the implicit government guarantee on municipal investment bonds does\nindeed help to raise the credit ratings assigned by credit rating agencies. The\nstudy found that, moreover, the government's implicit guarantee has a more\npronounced effect in boosting credit ratings in less developed western regions.",
      "generated_abstract": "This paper introduces a new approach to credit risk modelling based on a\ngeneralised Black-Scholes model. The proposed model captures both the short-\nand long-term dynamics of the credit risk, providing a more comprehensive view\nof the credit risk. The model is based on a two-asset Black-Scholes model with\nthe option to extend it to a generalised Black-Scholes model. The model is\nextended to include various risk factors, including market risk, default risk\nand credit default swap. The model is evaluated using a simulated dataset of\ninvestment grade corporate bonds, covering a range of credit ratings. The\nresults demonstrate that the proposed model outperforms the benchmark Black-Scholes\nmodel, offering a more comprehensive approach to credit risk modelling.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2111111111111111,
          "p": 0.3064516129032258,
          "f": 0.24999999516966767
        },
        "rouge-2": {
          "r": 0.064,
          "p": 0.08421052631578947,
          "f": 0.07272726782024827
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.2903225806451613,
          "f": 0.2368421004328256
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2501.16391v1",
      "true_abstract": "Significant differences in protein structures hinder the generalization of\nexisting drug-target interaction (DTI) models, which often rely heavily on\npre-learned binding principles or detailed annotations. In contrast, BioBridge\ndesigns an Inductive-Associative pipeline inspired by the workflow of\nscientists who base their accumulated expertise on drawing insights into novel\ndrug-target pairs from weakly related references. BioBridge predicts novel\ndrug-target interactions using limited sequence data, incorporating multi-level\nencoders with adversarial training to accumulate transferable binding\nprinciples. On these principles basis, BioBridge employs a dynamic prototype\nmeta-learning framework to associate insights from weakly related annotations,\nenabling robust predictions for previously unseen drug-target pairs. Extensive\nexperiments demonstrate that BioBridge surpasses existing models, especially\nfor unseen proteins. Notably, when only homologous protein binding data is\navailable, BioBridge proves effective for virtual screening of the epidermal\ngrowth factor receptor and adenosine receptor, underscoring its potential in\ndrug discovery.",
      "generated_abstract": "We consider a scenario in which a group of individuals is exposed to a\nrisk of infection. The individuals can make decisions on whether to be\nvaccinated and how to be vaccinated. The decision to be vaccinated depends on\ntheir current state of health, their age, and the risk of infection. The\ndecision to be vaccinated also depends on the vaccination policy of the\ngovernment. We show that a group of vaccinated individuals, but not an\nunvaccinated group, can prevent the spread of the disease if the vaccination\npolicy is high enough. Furthermore, the infection spreads only if the\ngovernment policy is sufficiently low. We characterize the optimal vaccination\npolicy, and we show that it depends on the current state of health and the\nrisk of infection. Moreover, we show that the optimal vaccination",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11607142857142858,
          "p": 0.20967741935483872,
          "f": 0.14942528276919026
        },
        "rouge-2": {
          "r": 0.007352941176470588,
          "p": 0.01,
          "f": 0.008474571387535133
        },
        "rouge-l": {
          "r": 0.11607142857142858,
          "p": 0.20967741935483872,
          "f": 0.14942528276919026
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/AI/2503.10529v1",
      "true_abstract": "3D Multimodal Large Language Models (MLLMs) have recently made substantial\nadvancements. However, their potential remains untapped, primarily due to the\nlimited quantity and suboptimal quality of 3D datasets. Current approaches\nattempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but\nstill face modality and domain gaps. To this end, we introduce PiSA-Engine\n(Point-Self-Augmented-Engine), a new framework for generating instruction\npoint-language datasets enriched with 3D spatial semantics. We observe that\nexisting 3D MLLMs offer a comprehensive understanding of point clouds for\nannotation, while 2D MLLMs excel at cross-validation by providing complementary\ninformation. By integrating holistic 2D and 3D insights from off-the-shelf\nMLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation.\nWe select PointLLM as the baseline and adopt this co-evolution training\nframework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally,\nwe identify limitations in previous 3D benchmarks, which often feature coarse\nlanguage captions and insufficient category diversity, resulting in inaccurate\nevaluations. To address this gap, we further introduce PiSA-Bench, a\ncomprehensive 3D benchmark covering six key aspects with detailed and diverse\nlabels. Experimental results demonstrate PointLLM-PiSA's state-of-the-art\nperformance in zero-shot 3D object captioning and generative classification on\nour PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and\n63.75% (+16.25%), respectively. We will release the code, datasets, and\nbenchmark.",
      "generated_abstract": "Large language models (LLMs) have achieved impressive performance on natural\nlanguage understanding (NLU) tasks. However, the LLMs' ability to generalize to\nunseen tasks is limited, which hinders their practical use in real-world\napplications. To address this issue, we propose a novel framework called\nMulti-Task Generalization (MTG), which integrates a multi-task learning (MTL)\nstrategy with a generalization-aware loss function. Our framework integrates\ntask-specific objective functions into the MTL process, ensuring that LLMs can\ngeneralize to unseen tasks. Additionally, MTG integrates a generalized loss\nfunction to balance model performance and generalization. Experiments on two\nNLU benchmarks show that MTG significantly improves LLMs' ability to generalize\nto unseen tasks, achieving superior performance compared to previous\nstate-of-the-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.147239263803681,
          "p": 0.2891566265060241,
          "f": 0.1951219467482981
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.0196078431372549,
          "f": 0.012738849116801381
        },
        "rouge-l": {
          "r": 0.1411042944785276,
          "p": 0.27710843373493976,
          "f": 0.18699186544748508
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.02846v1",
      "true_abstract": "We introduce a novel machine learning model for credit risk by combining\ntree-boosting with a latent spatio-temporal Gaussian process model accounting\nfor frailty correlation. This allows for modeling non-linearities and\ninteractions among predictor variables in a flexible data-driven manner and for\naccounting for spatio-temporal variation that is not explained by observable\npredictor variables. We also show how estimation and prediction can be done in\na computationally efficient manner. In an application to a large U.S. mortgage\ncredit risk data set, we find that both predictive default probabilities for\nindividual loans and predictive loan portfolio loss distributions obtained with\nour novel approach are more accurate compared to conventional independent\nlinear hazard models and also linear spatio-temporal models. Using\ninterpretability tools for machine learning models, we find that the likely\nreasons for this outperformance are strong interaction and non-linear effects\nin the predictor variables and the presence of large spatio-temporal frailty\neffects.",
      "generated_abstract": "We propose a model to simulate the price dynamics of options on a\nportfolio of assets. The model is based on the Delta-Gamma model with a\nmultiplicative noise, but we show that it can also be generalized to other\nmodels, such as the Langevin model, the L\\'evy model, and the Cox-Ingersoll-Ross\nmodel. We derive analytical expressions for the price and the variance of the\noption price. We also provide numerical results that show the good agreement\nbetween the simulation results and the analytical solutions. The model is\napplicable to both short-term and long-term options and can be used for\nsimulating the price of options in a wide variety of scenarios.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20202020202020202,
          "p": 0.31746031746031744,
          "f": 0.24691357549382725
        },
        "rouge-2": {
          "r": 0.035211267605633804,
          "p": 0.049019607843137254,
          "f": 0.04098360169175011
        },
        "rouge-l": {
          "r": 0.1717171717171717,
          "p": 0.2698412698412698,
          "f": 0.20987653845679022
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/AP/2502.16520v2",
      "true_abstract": "The increasing complexity of supply chains and the rising costs associated\nwith defective or substandard goods (bad goods) highlight the urgent need for\nadvanced predictive methodologies to mitigate risks and enhance operational\nefficiency. This research presents a novel framework that integrates Time\nSeries ARIMA (AutoRegressive Integrated Moving Average) models with a\nproprietary formula specifically designed to calculate bad goods after time\nseries forecasting. By leveraging historical data patterns, including sales,\nreturns, and capacity, the model forecasts potential quality failures, enabling\nproactive decision-making. ARIMA is employed to capture temporal trends in time\nseries data, while the newly developed formula quantifies the likelihood and\nimpact of defects with greater precision. Experimental results, validated on a\ndataset spanning 2022-2024 for Organic Beer-G 1 Liter, demonstrate that the\nproposed method outperforms traditional statistical models, such as Exponential\nSmoothing and Holt-Winters, in both prediction accuracy and risk evaluation.\nThis study advances the field of predictive analytics by bridging time series\nforecasting, ARIMA, and risk management in supply chain quality control,\noffering a scalable and practical solution for minimizing losses due to bad\ngoods.",
      "generated_abstract": "In recent years, reinforcement learning (RL) has been widely applied to\noptimize the design of data centers, including power allocation, server\nscheduling, and cooling strategies. However, the existing RL algorithms face\nseveral challenges, including the lack of explicit modeling of the system\nbehavior, the lack of modeling of system interactions, and the difficulty in\nselecting appropriate actions. To address these issues, we propose a novel\nframework for optimizing data center power allocation and cooling. Our\nframework leverages a modeling framework that combines a generative model for\ndata center power allocation with a cooperative game-theoretic model for\ncooling. The generative model captures the dynamics of the data center and\nincludes the server behavior, cooling demand, and the power consumption of\nthe cooling system. The cooperative game-theoretic model models the interactions\nbetween",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.21621621621621623,
          "f": 0.1523809478167802
        },
        "rouge-2": {
          "r": 0.028901734104046242,
          "p": 0.043859649122807015,
          "f": 0.034843200786218775
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.21621621621621623,
          "f": 0.1523809478167802
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10405v1",
      "true_abstract": "In this paper we aim to construct piecewise-linear (PWL) approximations for\nfunctions of multiple variables and to build compact mixed-integer linear\nprogramming (MILP) formulations to represent the resulting PWL function. On the\none hand, we describe a simple heuristic to iteratively construct a\ntriangulation with a small number of triangles, while decreasing the error of\nthe piecewise-linear approximation. On the other hand, we extend known\ntechniques for modeling PWLs in MILPs more efficiently than state-of-the-art\nmethods permit. The crux of our method is that the MILP model is a result of\nsolving some hard combinatorial optimization problems, for which we present\nheuristic algorithms. The effectiveness of our techniques is demonstrated by a\nseries of computational experiments including a short-term hydropower\nscheduling problem",
      "generated_abstract": "This paper proposes a novel method for the estimation of the second\ncomputational derivative of the objective function based on the so-called\n\"inverse\" method. This method is based on the use of the inverse of the\nsecond-order derivatives of the objective function and the computation of the\nsecond derivative of the objective function. This approach is particularly\napplicable to problems with large dimensions and/or non-convex objective\nfunctions. The method is demonstrated by solving two optimization problems: one\ninvolving a non-convex objective function with a large number of variables and\nthe other a convex one. The results obtained in these two problems are\ncomparable with the results obtained using the so-called \"direct\" method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.26136363636363635,
          "p": 0.39655172413793105,
          "f": 0.315068488361794
        },
        "rouge-2": {
          "r": 0.06779661016949153,
          "p": 0.08791208791208792,
          "f": 0.0765550190068912
        },
        "rouge-l": {
          "r": 0.26136363636363635,
          "p": 0.39655172413793105,
          "f": 0.315068488361794
        }
      }
    },
    {
      "paper_id": "cs.CC.stat/TH/2502.15024v1",
      "true_abstract": "We investigate implications of the (extended) low-degree conjecture (recently\nformalized in [MW23]) in the context of the symmetric stochastic block model.\nAssuming the conjecture holds, we establish that no polynomial-time algorithm\ncan weakly recover community labels below the Kesten-Stigum (KS) threshold. In\nparticular, we rule out polynomial-time estimators that, with constant\nprobability, achieve correlation with the true communities that is\nsignificantly better than random. Whereas, above the KS threshold,\npolynomial-time algorithms are known to achieve constant correlation with the\ntrue communities with high probability[Mas14,AS15].\n  To our knowledge, we provide the first rigorous evidence for the sharp\ntransition in recovery rate for polynomial-time algorithms at the KS threshold.\nNotably, under a stronger version of the low-degree conjecture, our lower bound\nremains valid even when the number of blocks diverges. Furthermore, our results\nprovide evidence of a computational-to-statistical gap in learning the\nparameters of stochastic block models.\n  In contrast to prior work, which either (i) rules out polynomial-time\nalgorithms for hypothesis testing with 1-o(1) success probability [Hopkins18,\nBBK+21a] under the low-degree conjecture, or (ii) rules out low-degree\npolynomials for learning the edge connection probability matrix [LG23], our\napproach provides stronger lower bounds on the recovery and learning problem.\n  Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with\ngraph splitting and cross-validation techniques. In order to rule out general\nrecovery algorithms, we employ the correlation preserving projection method\ndeveloped in [HS17].",
      "generated_abstract": "We study the problem of learning a latent space that generates images from a\nunseen distribution in a two-player Bayesian optimization (BO) setting.\nUnlike in the classical BO setting where the objective function is known, the\nBO problem in the unseen distribution setting is challenging due to the\ninherent uncertainty in the distribution. In this work, we consider a\ntwo-player setting where the agent (A) has access to a reference distribution\n(B), and the agent (A) attempts to learn a latent space that generates images\nfrom the reference distribution. In particular, we study the case where the\nreference distribution is a Gaussian distribution. In this case, we prove that\nthe agent is not able to learn the latent space in a finite number of BO\niterations. We show that the agent learns the latent space only if the\nreference distribution is sufficiently close to the target distribution. This\nfinding provides new ins",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.2571428571428571,
          "f": 0.1706161093102133
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.03418803418803419,
          "f": 0.024464827208709447
        },
        "rouge-l": {
          "r": 0.12056737588652482,
          "p": 0.24285714285714285,
          "f": 0.16113743632443128
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.08506v1",
      "true_abstract": "We introduce a two-parameter modification of the cofinality invariant of\nideals. This allows us to include the interaction of a pair of ideals in the\nstudy of base-like structures. We find the values (cardinal numbers or\nwell-known cardinal invariants) of the invariant for pairs of some critical\nideals on $\\omega$. We also dichotomously divide pairs of known ideals on the\nreal line based on whether their relative cofinality is trivial or uncountable.\nFinally, we also study the relative cofinality of maximal ideals.",
      "generated_abstract": "We study the local existence and global existence of the classical\nsolution of the 1D compressible Euler equation in the presence of a\nnon-uniform wall. In this model, the wall is described by the non-local\nboundary condition $w(0)=0$ and $w(L)=0$. We prove that the classical\nsolution exists globally in time, if the initial data belongs to a\ncertain $L^p$-space, where $p\\geq 1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.17777777777777778,
          "f": 0.16666666168619804
        },
        "rouge-2": {
          "r": 0.05263157894736842,
          "p": 0.06896551724137931,
          "f": 0.059701487627534376
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.13333333333333333,
          "f": 0.12499999501953145
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.03084v1",
      "true_abstract": "We present simple to implement Wald-type statistics that deliver a general\nnonparametric inference theory for linear restrictions on varying coefficients\nin a range of spatial autoregressive models. Our theory covers error dependence\nof a general form, allows for a degree of misspecification robustness via\nnonparametric spatial weights and permits inference on both varying regression\nand spatial coefficients. One application of our method finds evidence for\nconstant returns to scale in the production function of the Chinese nonmetal\nmineral industry, while another finds a nonlinear impact of the distance to the\nemployment center on housing prices in Boston. A simulation study confirms that\nour tests perform well in finite-samples.",
      "generated_abstract": "We introduce a new nonparametric estimator of the spatial correlation\nin spatially-structured random-effects models that does not require\nassumptions on the spatial autocorrelation. We derive its asymptotic\nproperties and illustrate its performance with simulations and empirical\napplications. Our estimator is designed to capture the spatial correlation in\nthe data, not the spatial dependence in the model. The spatial dependence in\nmodel is assumed to be random and uncorrelated with the spatial dependence in\nthe data. However, we allow the spatial dependence in the model to be\ncorrelated with the spatial dependence in the data. We illustrate the\napplicability of our estimator in simulated data and in empirical applications\nto the spatial cointegration relationship between the U.S. and 100 European\ncountries. The empirical application shows that our estimator captures the\nspatial correlation in the data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21333333333333335,
          "p": 0.25,
          "f": 0.23021582236944269
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.049019607843137254,
          "f": 0.04830917374501208
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.234375,
          "f": 0.2158273331608096
        }
      }
    },
    {
      "paper_id": "quant-ph.math/IT/2503.09012v1",
      "true_abstract": "The thought experiment of Maxwell's demon highlights the effect of side\ninformation in thermodynamics. In this paper, we present an axiomatic treatment\nof a quantum Maxwell's demon, by introducing a resource-theoretic framework of\nquantum thermodynamics in the presence of quantum side information. Under\nminimal operational assumptions that capture the demon's behaviour, we derive\nthe one-shot work costs of preparing, as well as erasing, a thermodynamic\nsystem whose coupling with the demon's mind is described by a bipartite quantum\nstate. With trivial Hamiltonians, these work costs are precisely captured by\nthe smoothed conditional min- and max-entropies, respectively, thus providing\noperational interpretations for these one-shot information-theoretic quantities\nin microscopic thermodynamics. An immediate, information-theoretic implication\nof our results is an affirmative proof of the conjectured maximality of the\nconditional max-entropy among all axiomatically plausible conditional\nentropies, complementing the recently established minimality of the conditional\nmin-entropy. We then generalize our main results to the setting with nontrivial\nHamiltonians, wherein the work costs of preparation and erasure are captured by\na generalized type of mutual information. Finally, we present a macroscopic\nsecond law of thermodynamics in the presence of quantum side information, in\nterms of a conditional version of the Helmholtz free energy. Our results extend\nthe conceptual connection between thermodynamics and quantum information theory\nby refining the axiomatic common ground between the two theories and revealing\nfundamental insights of each theory in light of the other.",
      "generated_abstract": "We propose a new way to construct quantum algorithms based on quantum\nquantum Monte Carlo (QQMC) simulations. We use the QQMC method to simulate the\neffects of interactions on quantum systems. We use QQMC to calculate the\nexpectation value of operators on a quantum state, then we use the\nexpectation value to calculate the expectation value of a Hamiltonian. We can\nuse QQMC to calculate the expectation value of an operator in a complex quantum\nstate. We can use QQMC to calculate the expectation value of a complex\noperator. We can use QQMC to calculate the expectation value of a complex\nHamiltonian. We can use QQMC to calculate the expectation value of a real\nHamiltonian. We can use QQMC to calculate the expectation value of an\noperator that is a product of a real operator and a complex operator. We can use\nQQMC to calculate the expectation value of a complex operator",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1037037037037037,
          "p": 0.32558139534883723,
          "f": 0.15730336712220688
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.041666666666666664,
          "f": 0.021276591942055906
        },
        "rouge-l": {
          "r": 0.08148148148148149,
          "p": 0.2558139534883721,
          "f": 0.12359550195366759
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.03333v1",
      "true_abstract": "Dynamic networks models describe temporal interactions between social actors,\nand as such have been used to describe financial fraudulent transactions,\ndispersion of destructive invasive species across the globe, and the spread of\nfake news. An important question in all of these examples is what are the\ncausal drivers underlying these processes. Current network models are\nexclusively descriptive and based on correlative structures.\n  In this paper we propose a causal extension of dynamic network modelling. In\nparticular, we prove that the causal model satisfies a set of population\nconditions that uniquely identifies the causal drivers. The empirical analogue\nof these conditions provide a consistent causal discovery algorithm, which\ndistinguishes it from other inferential approaches. Crucially, data from a\nsingle environment is sufficient. We apply the method in an analysis of bike\nsharing data in Washington D.C. in July 2023.",
      "generated_abstract": "This paper proposes a new method for identifying a non-linear relationship\nbetween a single-valued continuous response and a set of binary response\nvariables. The method is based on the use of a Gaussian mixture model to\nrepresent the response of each binary variable as a weighted linear combination\nof a set of basis functions, each representing a distinct non-linear\ninterpretation of the response. The Gaussian mixture model is constructed using\na kernel-based approach, and the basis functions are determined using\ninformation-theoretic criteria. The proposed method is illustrated by\napplications to the estimation of the relationship between the length of the\nfetal heart tube and the onset of fetal heart rate. The method is also\napplied to the estimation of the relationship between the fetal heart rate and\nthe onset of fetal heart tube length. The method is compared to a kernel\nridge regression approach and a kernel support vector machine approach in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2361111111111111,
          "f": 0.1954022939992074
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.03418803418803419,
          "f": 0.03187250498309628
        },
        "rouge-l": {
          "r": 0.14705882352941177,
          "p": 0.20833333333333334,
          "f": 0.172413788252081
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.09075v1",
      "true_abstract": "The pinching-antenna system (PASS) introduces new degrees of freedom (DoFs)\nfor physical layer security (PLS) through pinching beamforming. In this paper,\na couple of scenarios for secure beamforming for PASS are studied. 1) For the\ncase with a single legitimate user (Bob) and a single eavesdropper (Eve), a\nclosed-form expression for the optimal baseband beamformer is derived. On this\nbasis, a gradient-based method is proposed to optimize the activated positions\nof pinching antennas (PAs). 2) For the case with multiple Bobs and multiple\nEves, a fractional programming (FP)-based block coordinate descent (BCD)\nalgorithm, termed FP-BCD, is proposed for optimizing the weighted secrecy\nsum-rate (WSSR). Specifically, a closed-form baseband beamformer is obtained\nvia Lagrange multiplier method. Furthermore, owing to the non-convex objective\nfunction exhibiting numerous stationary points, a low-complexity\none-dimensional search is used to find a high-quality solution of the PAs'\nactivated locations. Numerical results are provided to demonstrate that: i) All\nproposed algorithms achieve stable convergence within a few iterations, ii)\nacross all considered power ranges, the FP-BCD algorithm outperforms baseline\nmethods using zero-forcing (ZF) and maximal-ratio transmission (MRT)\nbeamforming in terms of the WSSR, and iii) PASS achieves a significantly higher\nsecrecy rate than traditional fixed-antenna systems.",
      "generated_abstract": "This paper presents a novel deep learning (DL) framework for the automatic\ndiscovery of spectral characteristics of large-scale wireless systems. The\nframework, which is based on a deep neural network, is designed to extract\nspectral features from time-domain (TD) and frequency-domain (FD) channel\nestimates. The proposed method is based on the use of a DL-based feature\nextraction network, which is trained and tested on the Wireless Sensor Network\n(WSN) channel dataset. The DL-based feature extraction network is employed to\nidentify spectral characteristics of the WSN channel. The DL-based feature\nextraction network is trained and tested on a WSN channel dataset. The\nproposed framework is validated against the state-of-the-art spectral\ncharacteristics extraction methods and is compared to a conventional\nspectral-characteristics-extraction method. Results show that the proposed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08391608391608392,
          "p": 0.18181818181818182,
          "f": 0.114832531563838
        },
        "rouge-2": {
          "r": 0.015957446808510637,
          "p": 0.0297029702970297,
          "f": 0.020761241127861055
        },
        "rouge-l": {
          "r": 0.08391608391608392,
          "p": 0.18181818181818182,
          "f": 0.114832531563838
        }
      }
    },
    {
      "paper_id": "eess.IV.cs/HC/2503.09885v1",
      "true_abstract": "Analyzing CT scans, MRIs and X-rays is pivotal in diagnosing and treating\ndiseases. However, detecting and identifying abnormalities from such medical\nimages is a time-intensive process that requires expert analysis and is prone\nto interobserver variability. To mitigate such issues, machine learning-based\nmodels have been introduced to automate and significantly reduce the cost of\nimage segmentation. Despite significant advances in medical image analysis in\nrecent years, many of the latest models are never applied in clinical settings\nbecause state-of-the-art models do not easily interface with existing medical\nimage viewers. To address these limitations, we propose QuickDraw, an\nopen-source framework for medical image visualization and analysis that allows\nusers to upload DICOM images and run off-the-shelf models to generate 3D\nsegmentation masks. In addition, our tool allows users to edit, export, and\nevaluate segmentation masks to iteratively improve state-of-the-art models\nthrough active learning. In this paper, we detail the design of our tool and\npresent survey results that highlight the usability of our software. Notably,\nwe find that QuickDraw reduces the time to manually segment a CT scan from four\nhours to six minutes and reduces machine learning-assisted segmentation time by\n10\\% compared to prior work. Our code and documentation are available at\nhttps://github.com/qd-seg/quickdraw",
      "generated_abstract": "In this work, we propose a novel approach for detecting and segmenting\nnon-melanoma skin cancer (NMSC) lesions in skin biopsy images. Our approach\nintegrates a multi-scale convolutional neural network (CNN) with a\nmultiscale feature extractor and an attention mechanism, to enhance the\nsegmentation of NMSC lesions. Our method uses a pre-trained CNN to extract\nmultiscale features, which are then used for the segmentation task. We also\nintroduce a multiscale feature extractor that extracts features at multiple\nscales, from low to high resolution. This approach enables the detection of\nlesions at various scales and enhances the segmentation of these lesions. We\nalso propose an attention mechanism, which allows us to focus on the\nimportant features for the segmentation task. The results of our experiments\nshow that our method achieves superior performance compared",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19117647058823528,
          "p": 0.3058823529411765,
          "f": 0.2352941129133311
        },
        "rouge-2": {
          "r": 0.020512820512820513,
          "p": 0.03389830508474576,
          "f": 0.025559100733906485
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.2823529411764706,
          "f": 0.2171945654020189
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.08144v2",
      "true_abstract": "In recent years, the dominance of machine learning in stock market\nforecasting has been evident. While these models have shown decreasing\nprediction errors, their robustness across different datasets has been a\nconcern. A successful stock market prediction model minimizes prediction errors\nand showcases robustness across various data sets, indicating superior\nforecasting performance. This study introduces a novel multiple lag order\nprobabilistic model based on trend encoding (TeMoP) that enhances stock market\npredictions through a probabilistic approach. Results across different stock\nindexes from nine countries demonstrate that the TeMoP outperforms the\nstate-of-the-art machine learning models in predicting accuracy and\nstabilization.",
      "generated_abstract": "We develop a novel method to estimate the expected short-run and long-run\nintertemporal choice utility functions under bounded rationality. Our approach\nincorporates the short-run and long-run expected utility functions estimated\nfrom two different data sources, which can be obtained from the self-reported\nutility of individual respondents. We use the estimated utility functions to\nestimate the expected short-run and long-run utilities of individuals. The\nresults indicate that the expected short-run and long-run utilities of\nindividuals are relatively stable over the long run, which is consistent with\nprevious findings that individuals have limited rationality. The stability of\nthe expected short-run and long-run utilities of individuals indicates that\nindividuals can make rational decisions under limited rationality, which may\nbe useful for policy making in various scenarios.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1780821917808219,
          "p": 0.18571428571428572,
          "f": 0.18181817682038257
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.020618556701030927,
          "f": 0.021164016167521685
        },
        "rouge-l": {
          "r": 0.1780821917808219,
          "p": 0.18571428571428572,
          "f": 0.18181817682038257
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/CB/2408.07551v1",
      "true_abstract": "In the recently proposed Graph Vertex Model (GVM), cellular rearrangements\nare implemented as local graph transformations of the cell aggregate,\nrepresented by a knowledge graph [1]. This study extends GVM to incorporate\ncell division, a critical biological process involved in morphogenesis,\nhomeostasis, and disease progression. Like cellular rearrangements, cell\ndivision in GVM begins by identifying a subgraph of nodes and links, involved\nin the division, by matching suitable graph patterns or templates within the\nfull knowledge graph. The matched subgraph is then transformed to incorporate\ntopological changes within the knowledge graph, caused by the division event.\nImportantly, when this transformation is applied to a polygon in a 2D tiling,\nit performs the transformation, required to divide a polygon, indicating that\nthe 3D graph transformation is general and applicable also to 2D vertex models.\nOur extension of GVM enables the study of the dynamics of growing cell\naggregates in 3D to offer new insights into developmental processes and cancer\ngrowth.",
      "generated_abstract": "The recent advances in genome sequencing and computational methods have\nopened a new era of biological studies, enabling a deeper understanding of\nbiomolecular structures and their dynamics. This progress has driven the\ndevelopment of novel tools, such as single-molecule microscopy and microfluidic\ndevices, to study molecular interactions and dynamics in the native environment\nof biomolecules. In this review, we summarize the recent advances in\nsingle-molecule microscopy and microfluidics for studying biomolecular\ndynamics. We focus on two distinct areas: 1) single-molecule localization\nmeasurements, including fluorescence microscopy, optical tweezers, and\nforce-sensitive probes; and 2) single-molecule tracking, including single-molecule\nmagnetic resonance, optical twe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14705882352941177,
          "p": 0.20270270270270271,
          "f": 0.1704545405810952
        },
        "rouge-2": {
          "r": 0.006578947368421052,
          "p": 0.010526315789473684,
          "f": 0.00809716125817779
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.16216216216216217,
          "f": 0.13636363149018613
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.06837v1",
      "true_abstract": "This paper investigates a robust empirical Bayes correction for Bayesian\nmodeling. We show the application of the model on income distribution. Income\nshock includes temporal and permanent shocks. We aim to eliminate temporal\nshock and permanent shock using two-step local empirical correction method. Our\nresults show that only 6.7% of the observed income shocks were permanent shock,\nand the posterior (permanent) mean weekly income was reduced from the observed\nincome 415 pounds to 202 pounds for the United Kingdom using the Living Costs\nand Food Survey in 2021-2022. Keywords: Empirical Bayes correction; Outliers;\nBayesian modeling",
      "generated_abstract": "The increasing prevalence of electronic cigarettes (e-cigarettes) has\nprompted researchers to develop methods to analyze e-cigarette data. However,\nthe complexity of the data poses challenges for traditional statistical\napproaches. In this study, we develop a novel method for analyzing e-cigarette\ndata using the lasso. The lasso is a regularization method that penalizes\nestimators that are dependent on the data. This method is used to estimate the\neffects of the e-cigarette on the number of pack-years smoked. We demonstrate\nthat the lasso method offers a robust alternative to traditional methods in\nthis context. Our results demonstrate that the lasso method performs well in\nidentifying the effect of e-cigarette use on the number of pack-years smoked,\nespecially in models with high dimensionality.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.21428571428571427,
          "f": 0.22222221722908106
        },
        "rouge-2": {
          "r": 0.044444444444444446,
          "p": 0.03773584905660377,
          "f": 0.0408163215639323
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.21428571428571427,
          "f": 0.22222221722908106
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10445v1",
      "true_abstract": "Reducing the spread of misinformation is challenging. AI-based fact\nverification systems offer a promising solution by addressing the high costs\nand slow pace of traditional fact-checking. However, the problem of how to\neffectively communicate the results to users remains unsolved. Warning labels\nmay seem an easy solution, but they fail to account for fuzzy misinformation\nthat is not entirely fake. Additionally, users' limited attention spans and\nsocial media information should be taken into account while designing the\npresentation. The online experiment (n = 537) investigates the impact of\nsources and granularity on users' perception of information veracity and the\nsystem's usefulness and trustworthiness. Findings show that fine-grained\nindicators enhance nuanced opinions, information awareness, and the intention\nto use fact-checking systems. Source differences had minimal impact on opinions\nand perceptions, except for informativeness. Qualitative findings suggest the\nproposed indicators promote critical thinking. We discuss implications for\ndesigning concise, user-friendly AI fact-checking feedback.",
      "generated_abstract": "This paper presents a novel framework for automated assessment of the\ntechnical quality of distributed systems. The framework is based on a novel\napproach to assessing the technical quality of distributed systems, namely\nverification by automated synthesis (VAS). VAS is a model-driven approach to\ndeveloping verification techniques for distributed systems. In this paper, we\nfirst introduce VAS and discuss the theoretical basis for its approach. We\nthen propose an experimental evaluation of VAS, which uses the Rust programming\nlanguage and a Rust-based distributed system. Our evaluation examines the\neffectiveness of VAS in the assessment of the technical quality of Rust\ndistributed systems. The results of this evaluation indicate that VAS is\neffective in the assessment of the technical quality of Rust distributed\nsystems. We conclude by discussing the potential of VAS for automated\nassessment of the technical quality of distributed",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14912280701754385,
          "p": 0.25,
          "f": 0.1868131821325928
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.23529411764705882,
          "f": 0.17582417114358181
        }
      }
    },
    {
      "paper_id": "eess.SY.stat/TH/2503.03328v1",
      "true_abstract": "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
      "generated_abstract": "We study the problem of estimating the conditional expectation of a random\nprocess given a finite set of observed samples. This problem arises in\nstatistical inference and is known as the conditional expectation estimation\nproblem. We first establish a new result for a class of conditional\nexpectation estimators that is applicable when the conditional expectation is a\nfunction of the underlying random process. We then generalize this result to\nthe case when the conditional expectation is a function of the sample mean and\nsample variance of the underlying random process. We show that our generalization\nleads to an estimator that is consistent and asymptotically normal, provided that\nthe number of samples is large enough. We also establish the convergence rate\nof the estimator under suitable assumptions, and provide a finite sample\nanalysis of the estimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.2537313432835821,
          "f": 0.17525772743702847
        },
        "rouge-2": {
          "r": 0.02358490566037736,
          "p": 0.045454545454545456,
          "f": 0.03105589612283542
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.2537313432835821,
          "f": 0.17525772743702847
        }
      }
    },
    {
      "paper_id": "cs.DM.cs/DM/2503.07208v1",
      "true_abstract": "In the Subset Feedback Arc Set in Tournaments, Subset-FAST problem we are\ngiven as input a tournament $T$ with a vertex set $V(T)$ and an arc set $A(T)$,\nalong with a terminal set $S \\subseteq V(T)$, and an integer $ k$. The\nobjective is to determine whether there exists a set $ F \\subseteq A(T) $ with\n$|F| \\leq k$ such that the resulting graph $T-F $ contains no cycle that\nincludes any vertex of $S$. When $S=V(T)$ this is the classic Feedback Arc Set\nin Tournaments (FAST) problem. We obtain the first polynomial kernel for this\nproblem parameterized by the solution size. More precisely, we obtain an\nalgorithm that, given an input instance $(T, S, k)$, produces an equivalent\ninstance $(T',S',k')$ with $k'\\leq k$ and $V(T')=O(k^2)$.\n  It was known that FAST admits a simple quadratic vertex kernel and a\nnon-trivial linear vertex kernel. However, no such kernel was previously known\nfor Subset-FAST. Our kernel employs variants of the most well-known reduction\nrules for FAST and introduces two new reduction rules to identify irrelevant\nvertices. As a result of our kernelization, we also obtain the first\nsub-exponential time FPT algorithm for Subset-FAST.",
      "generated_abstract": "We introduce the first formalism for defining the dynamic properties of\nmechanisms, focusing on their properties over time. We introduce the notion of\ntime-varying mechanisms, which we define as algorithms that have access to a\ntime-dependent oracle. We show that time-varying mechanisms are in some sense\nequivalent to time-varying data-processing networks, and we further show that\nthe existence of a time-varying mechanism is equivalent to the existence of a\ntime-varying data-processing network. We then define the notion of time-varying\nproperties, and we show that the existence of a time-varying mechanism is\nequivalent to the existence of a time-varying data-processing network with\nspecific properties. We then show that it is NP-hard to decide if a given\nmechanism is time-varying, and we show that it is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15126050420168066,
          "p": 0.32727272727272727,
          "f": 0.2068965474005814
        },
        "rouge-2": {
          "r": 0.011049723756906077,
          "p": 0.023529411764705882,
          "f": 0.015037589636215853
        },
        "rouge-l": {
          "r": 0.13445378151260504,
          "p": 0.2909090909090909,
          "f": 0.183908041653455
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04091v1",
      "true_abstract": "Federated Learning (FL) is a widely adopted privacy-preserving distributed\nlearning framework, yet its generalization performance remains less explored\ncompared to centralized learning. In FL, the generalization error consists of\ntwo components: the out-of-sample gap, which measures the gap between the\nempirical and true risk for participating clients, and the participation gap,\nwhich quantifies the risk difference between participating and\nnon-participating clients. In this work, we apply an information-theoretic\nanalysis via the conditional mutual information (CMI) framework to study FL's\ntwo-level generalization. Beyond the traditional supersample-based CMI\nframework, we introduce a superclient construction to accommodate the two-level\ngeneralization setting in FL. We derive multiple CMI-based bounds, including\nhypothesis-based CMI bounds, illustrating how privacy constraints in FL can\nimply generalization guarantees. Furthermore, we propose fast-rate evaluated\nCMI bounds that recover the best-known convergence rate for two-level FL\ngeneralization in the small empirical risk regime. For specific FL model\naggregation strategies and structured loss functions, we refine our bounds to\nachieve improved convergence rates with respect to the number of participating\nclients. Empirical evaluations confirm that our evaluated CMI bounds are\nnon-vacuous and accurately capture the generalization behavior of FL\nalgorithms.",
      "generated_abstract": "We consider the problem of estimating the expectation and covariance of a\nrandom variable in a convex body. We propose two algorithms based on\nconvex relaxations and prove their convergence. Our method is a convex\nrelaxation of the Kullback-Leibler divergence between the empirical distribution\nof the random variable and the convex body. We show that the convex relaxation\nis convex and that the relaxed objective is equivalent to the original\nobjective under a mild condition. We show that the two proposed algorithms\nconverge to the same limit in the limiting case where the convex body is the\nunit ball. We show that our method is faster than the classical method based on\na convex relaxation of the KL divergence and that it can be used to estimate\nthe expectation and covariance of random variables in convex bodies that are\nnot necessarily bounded. We provide numerical simulations to illustrate the\neffectiveness of our algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.144,
          "p": 0.2465753424657534,
          "f": 0.18181817716304466
        },
        "rouge-2": {
          "r": 0.03867403314917127,
          "p": 0.056,
          "f": 0.045751629154385576
        },
        "rouge-l": {
          "r": 0.136,
          "p": 0.2328767123287671,
          "f": 0.1717171670620346
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03629v3",
      "true_abstract": "Traffic simulation is essential for autonomous vehicle (AV) development,\nenabling comprehensive safety evaluation across diverse driving conditions.\nHowever, traditional rule-based simulators struggle to capture complex human\ninteractions, while data-driven approaches often fail to maintain long-term\nbehavioral realism or generate diverse safety-critical events. To address these\nchallenges, we propose TeraSim, an open-source, high-fidelity traffic\nsimulation platform designed to uncover unknown unsafe events and efficiently\nestimate AV statistical performance metrics, such as crash rates. TeraSim is\ndesigned for seamless integration with third-party physics simulators and\nstandalone AV stacks, to construct a complete AV simulation system.\nExperimental results demonstrate its effectiveness in generating diverse\nsafety-critical events involving both static and dynamic agents, identifying\nhidden deficiencies in AV systems, and enabling statistical performance\nevaluation. These findings highlight TeraSim's potential as a practical tool\nfor AV safety assessment, benefiting researchers, developers, and policymakers.\nThe code is available at https://github.com/mcity/TeraSim.",
      "generated_abstract": "The integration of autonomous driving and robotics systems in the real world\ninteractions is a challenging task due to the complexity of interactions and\ndifferent goals. This paper proposes a framework for autonomous driving\ninteractions in real-world scenarios, which can be divided into three phases:\n(1) planning, (2) execution, and (3) monitoring. The planning phase aims to\ndetermine the interaction plan between the autonomous driving system and the\nhuman driver. The execution phase is designed to achieve the interaction plan.\nThe monitoring phase is used to continuously observe the interaction between\nthe autonomous driving system and the human driver. The proposed framework is\nbased on the A* path planning algorithm and the Deep Reinforcement Learning\n(DRL) technique. The DRL agent is designed to monitor the interaction between\nthe autonomous driving system and the human driver, and the agent is trained\nthrough reinforcement learning. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11504424778761062,
          "p": 0.16883116883116883,
          "f": 0.13684210044265946
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.02631578947368421,
          "f": 0.023529406820762284
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.16883116883116883,
          "f": 0.13684210044265946
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.10611v1",
      "true_abstract": "We provide a full characterization of geodesic completeness for spaces of\nconfigurations of landmarks with smooth Riemannian metrics that satisfy a\nrotational and translation invariance and which are induced from metrics on\nsubgroups of the diffeomorphism group for the shape domain. These spaces are\nwidely used for applications in shape analysis, for example, for measuring\nshape changes in medical imaging and morphometrics in biology. For statistics\nof such data to be well-defined, it is imperative to know if geodesics exist\nfor all times. We extend previously known sufficient conditions for geodesic\ncompleteness based on the regularity of the metric to give a full\ncharacterization for smooth Riemannian metrics with a rotational and\ntranslation invariance by means of an integrability criterion that involves\nonly the behavior of the cometric kernel as landmarks approach collision. We\nfurther use the integrability criterion for geodesic completeness and previous\nwork on stochastic completeness to construct a family of Riemannian landmark\nmanifolds that are geodesically complete but stochastically incomplete.",
      "generated_abstract": "The paper studies the problem of finding a boundary value for a 1-dimensional\ninfinite-dimensional non-autonomous Hamiltonian system. We prove that if the\nHamiltonian is non-constant, then the problem has a unique solution. In\nparticular, we prove that if the Hamiltonian is a polynomial of degree two or\nless, then the problem has a unique solution. In both cases, we establish a\nnon-autonomous version of the Harnack inequality. We also prove that if the\nHamiltonian is a polynomial of degree one or greater, then the problem has a\nunique solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08163265306122448,
          "p": 0.18604651162790697,
          "f": 0.11347517306574133
        },
        "rouge-2": {
          "r": 0.006756756756756757,
          "p": 0.016666666666666666,
          "f": 0.009615380510356781
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.16279069767441862,
          "f": 0.09929077590262077
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/CB/2502.05459v1",
      "true_abstract": "White blood cells (WBC) are important parts of our immune system, and they\nprotect our body against infections by eliminating viruses, bacteria, parasites\nand fungi. The number of WBC types and the total number of WBCs provide\nimportant information about our health status. A traditional method,\nconvolutional neural networks (CNN), a deep learning architecture, can classify\nthe blood cell from a part of an object and perform object recognition. Various\nCNN models exhibit potential; however, their development often involves ad-hoc\nprocesses that neglect unnecessary layers, leading to issues with unbalanced\ndatasets and insufficient data augmentation. To address these challenges, we\npropose a novel ensemble approach that integrates three CNN architectures, each\nuniquely configured with different dropout and max-pooling layer settings to\nenhance feature learning. This ensemble model, named DCENWCNet, effectively\nbalances the bias-variance trade-off. When evaluated on the widely recognized\nRabbin-WBC dataset, our model outperforms existing state-of-the-art networks,\nachieving highest mean accuracy. Additionally, it demonstrates superior\nperformance in precision, recall, F1-score, and Area Under the ROC Curve (AUC)\nacross all categories. To delve deeper into the interpretability of\nclassifiers, we employ reliable post-hoc explanation techniques, including\nLocal Interpretable Model-Agnostic Explanations (LIME). These methods\napproximate the behavior of a black-box model by elucidating the relationships\nbetween feature values and predictions. Interpretable results enable users to\ncomprehend and validate the model's predictions, thereby increasing their\nconfidence in the automated diagnosis.",
      "generated_abstract": "We present a novel approach for training and evaluating multimodal\ndiscovery models, integrating and leveraging both multimodal and textual\ninformation. Our approach extends the popular Multimodal Discovery Models (MDMs)\nframework by incorporating a novel multi-task learning (MTL) module that\nenables joint training across modalities and textual features. This module\nenables the discovery of novel associations, including those between\nmultimodal features and textual features, as well as interactions between\nmodalities. Additionally, we introduce a novel methodology for evaluating\ndiscovery models. We show that our approach outperforms existing models in\nmulti-task learning and textual evaluation tasks, demonstrating its ability to\nleverage both modalities and textual information.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12154696132596685,
          "p": 0.3384615384615385,
          "f": 0.17886178472965836
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.02127659574468085,
          "f": 0.012499995850782626
        },
        "rouge-l": {
          "r": 0.12154696132596685,
          "p": 0.3384615384615385,
          "f": 0.17886178472965836
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2501.10482v1",
      "true_abstract": "Random fuzzy variables join the modeling of the impreciseness (due to their\n``fuzzy part'') and randomness. Statistical samples of such objects are widely\nused, and their direct, numerically effective generation is therefore\nnecessary. Usually, these samples consist of triangular or trapezoidal fuzzy\nnumbers. In this paper, we describe theoretical results and simulation\nalgorithms for another family of fuzzy numbers -- LR fuzzy numbers with\ninterval-valued cores. Starting from a simulation perspective on the piecewise\nlinear LR fuzzy numbers with the interval-valued cores, their limiting behavior\nis then considered. This leads us to the numerically efficient algorithm for\nsimulating a sample consisting of such fuzzy values.",
      "generated_abstract": "This paper introduces the Sparsity-Aware Kernel Regression (SAR) model for\nregression tasks, which incorporates the sparsity structure of the data to\nfacilitate efficient estimation. The SAR model is a hybrid of the Sparse\nRecurrent Kernel Machines (SRKM) model and the sparse kernel regression\n(SKR) model, with the former leveraging the recurrent structure of the data to\nmodel the dependency between observations, while the latter utilizes the\nsparsity structure to enhance the efficiency of estimation. To address the\nchallenges of sparse data in kernel regression, we propose a sparse kernel\nregularization method that is based on the Cholesky decomposition and a\nmodified regularization loss. Extensive experiments on real and simulated\ndatasets demonstrate the superior performance of the SAR model compared to the\nSRKM and SKR models. The results also show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17567567567567569,
          "p": 0.16455696202531644,
          "f": 0.1699346355282158
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.03361344537815126,
          "f": 0.036866354493831494
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.1518987341772152,
          "f": 0.1568627401033792
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2501.07508v1",
      "true_abstract": "This paper applies deep reinforcement learning (DRL) to optimize liquidity\nprovisioning in Uniswap v3, a decentralized finance (DeFi) protocol\nimplementing an automated market maker (AMM) model with concentrated liquidity.\nWe model the liquidity provision task as a Markov Decision Process (MDP) and\ntrain an active liquidity provider (LP) agent using the Proximal Policy\nOptimization (PPO) algorithm. The agent dynamically adjusts liquidity positions\nby using information about price dynamics to balance fee maximization and\nimpermanent loss mitigation. We use a rolling window approach for training and\ntesting, reflecting realistic market conditions and regime shifts. This study\ncompares the data-driven performance of the DRL-based strategy against common\nheuristics adopted by small retail LP actors that do not systematically modify\ntheir liquidity positions. By promoting more efficient liquidity management,\nthis work aims to make DeFi markets more accessible and inclusive for a broader\nrange of participants. Through a data-driven approach to liquidity management,\nthis work seeks to contribute to the ongoing development of more efficient and\nuser-friendly DeFi markets.",
      "generated_abstract": "We propose a novel approach to hedging risks arising from the spread between\nthe price of an asset and the implied volatility of its option. This spread\nentails the difference between the price of the underlying asset and the\nimplied volatility of its option, and is known as the \"spread\". We derive a\nclosed-form solution for hedging this spread in the limit of an infinite number\nof options. This solution is based on a simple recursive formula, and can be\nused to efficiently hedge the spread over multiple maturities. We show that the\nhedging strategies proposed in the literature are related to our solution. We\nalso show that the hedging strategies proposed in the literature are related to\nour solution. We numerically demonstrate that our approach is more efficient\nthan the existing methods for hedging the spread, and provides a more\nrobust approach for hedging the spread. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2328767123287671,
          "f": 0.17708332862033432
        },
        "rouge-2": {
          "r": 0.025157232704402517,
          "p": 0.03418803418803419,
          "f": 0.028985502362162137
        },
        "rouge-l": {
          "r": 0.12605042016806722,
          "p": 0.2054794520547945,
          "f": 0.156249995287001
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.06046v1",
      "true_abstract": "Televised debates between presidential candidates are often regarded as the\nexemplar of persuasive communication. Yet, recent evidence from Le Pennec and\nPons (2023) indicates that they may not sway voters as strongly as popular\nbelief suggests. We revisit their findings through the lens of the persuasion\nrate and introduce a robust framework that does not require exogenous\ntreatment, parallel trends, or credible instruments. Instead, we leverage\nplausible monotonicity assumptions to partially identify the persuasion rate\nand related parameters. Our results reaffirm that the sharp upper bounds on the\npersuasive effects of TV debates remain modest.",
      "generated_abstract": "This paper introduces a novel methodology for estimating and testing\nintertemporal substitution models. Specifically, we propose a two-stage\nmaximum likelihood estimation procedure for the generalized model\n\\begin{equation*}\n    Y_{t+1} = \\theta + \\beta_{t} X_{t} + \\varepsilon_{t+1}\n\\end{equation*}\nwhere $X_t$ and $\\theta$ are the exogenous variables, and $\\varepsilon_{t+1}$ is\nan unobserved random variable. We then employ a parametric bootstrap\nprocedure to test the model for misspecification, and a nonparametric bootstrap\nprocedure to test the model's parameter estimates for consistency. This\nprocedure extends the bootstrap framework in the literature by incorporating\nthe nonparametric bootstrap, and we provide a novel derivation of the bootstrap\ndistribution for the estimated model parameters. We demonstrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.1527777777777778,
          "f": 0.14765100171703996
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.01,
          "f": 0.010471199199585722
        },
        "rouge-l": {
          "r": 0.12987012987012986,
          "p": 0.1388888888888889,
          "f": 0.13422818292509364
        }
      }
    },
    {
      "paper_id": "math.PR.q-bio/SC/2406.12493v1",
      "true_abstract": "We prove a Large Deviation Principle for Piecewise Deterministic Markov\nProcesses (PDMPs). This is an asymptotic estimate for the probability of a\ntrajectory in the large size limit. Explicit Euler-Lagrange equations are\ndetermined for computing optimal first-hitting-time trajectories. The results\nare applied to a model of stochastic calcium dynamics. It is widely conjectured\nthat the mechanism of calcium puff generation is a multiscale process: with\nmicroscopic stochastic fluctuations in the opening and closing of individual\nchannels generating cell-wide waves via the diffusion of calcium and other\nsignaling molecules. We model this system as a PDMP, with $N \\gg 1$ stochastic\ncalcium channels that are coupled via the ambient calcium concentration. We\nemploy the Large Deviations theory to estimate the probability of cell-wide\ncalcium waves being produced through microscopic stochasticity.",
      "generated_abstract": "We study the non-commutative geometry of the Heisenberg-Weyl algebra\n$\\mathbb{H}_2$ and the associated Heisenberg algebra $\\mathbb{H}_1$.\nSpecifically, we provide a basis for the Heisenberg algebra, including\nnon-commutative versions of the Heisenberg-Weyl algebra and Heisenberg-Weyl\nalgebra-type algebras, and we construct the non-commutative Heisenberg\nalgebra-type algebras. We then show that the Heisenberg algebra is a non-commutative\ncounterpart of the Heisenberg-Weyl algebra. Furthermore, we propose a\nnon-commutative counterpart of the Heisenberg-Weyl algebra. We show that the\nHeisenberg-Weyl algebra-type algebras form a Lie subalgebra of the Heisenberg\nalgebra, and the non-commutative",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09523809523809523,
          "p": 0.2222222222222222,
          "f": 0.13333332913333346
        },
        "rouge-2": {
          "r": 0.02459016393442623,
          "p": 0.04918032786885246,
          "f": 0.03278688080145779
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.2222222222222222,
          "f": 0.13333332913333346
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.01179v1",
      "true_abstract": "Chemical reaction network theory provides powerful tools for rigorously\nunderstanding chemical reactions and the dynamical systems and differential\nequations that represent them. A frequent issue with mathematical analyses of\nthese networks is the reliance on explicit parameter values which in many cases\ncannot be determined experimentally. This can make analyzing a dynamical system\ninfeasible, particularly when the size of the system is large. One approach is\nto analyze subnetworks of the full network and use the results for a full\nanalysis.\n  Our focus is on the equilibria of reaction networks. Gr\\\"obner basis\ncomputation is a useful approach for solving the polynomial equations which\ncorrespond to equilibria of a dynamical system. We identify a class of networks\nfor which Gr\\\"obner basis computations of subnetworks can be used to\nreconstruct the more expensive Gr\\\"obner basis computation of the whole\nnetwork. We compliment this result with tools to determine if a steady state\ncan exist, and if so, how many.",
      "generated_abstract": "The development of novel treatments for Alzheimer's disease (AD) is a\nhigh priority for both industry and academia. This study uses a Bayesian\nnetwork approach to analyze the efficacy of a novel drug candidate, JZP-110,\nin treating AD. The Bayesian network model is constructed using the data from\n15 AD patients, 15 healthy controls, and 15 AD patients and controls treated\nwith a placebo. The model predicts the likelihood of a patient experiencing\nadverse events, such as cognitive decline and/or death, after taking JZP-110.\nThe model also provides an estimate of the efficacy of JZP-110 in reducing\nAD-related complications. This study provides a framework for the development\nand evaluation of novel treatments for AD. The findings provide valuable\ninsights into the efficacy of JZ",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15053763440860216,
          "p": 0.18181818181818182,
          "f": 0.16470587739723197
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.03636363636363636,
          "f": 0.03100774704645231
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.18181818181818182,
          "f": 0.16470587739723197
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10005v1",
      "true_abstract": "Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.",
      "generated_abstract": "We introduce the notion of a \"weakly-intersecting family\" and prove that\ntheir intersection is the unique maximizer of a certain functional in\nhigh-dimensional Hilbert spaces. This result generalizes a result by\nSchramm and Venkatesham in the context of the Euclidean setting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08035714285714286,
          "p": 0.2903225806451613,
          "f": 0.1258741224783609
        },
        "rouge-2": {
          "r": 0.006493506493506494,
          "p": 0.02564102564102564,
          "f": 0.010362691075734579
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.25806451612903225,
          "f": 0.11188810849234693
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/ST/2502.15726v1",
      "true_abstract": "The marketing departments of financial institutions strive to craft products\nand services that cater to the diverse needs of businesses of all sizes.\nHowever, it is evident upon analysis that larger corporations often receive a\nmore substantial portion of available funds. This disparity arises from the\nrelative ease of assessing the risk of default and bankruptcy in these more\nprominent companies. Historically, risk analysis studies have focused on data\nfrom publicly traded or stock exchange-listed companies, leaving a gap in\nknowledge about small and medium-sized enterprises (SMEs). Addressing this gap,\nthis study introduces a method for evaluating SMEs by generating images for\nprocessing via a convolutional neural network (CNN). To this end, more than\n10,000 images, one for each company in the sample, were created to identify\nscenarios in which the CNN can operate with higher assertiveness and reduced\ntraining error probability. The findings demonstrate a significant predictive\ncapacity, achieving 97.8% accuracy, when a substantial number of images are\nutilized. Moreover, the image creation method paves the way for potential\napplications of this technique in various sectors and for different analytical\npurposes.",
      "generated_abstract": "This paper presents a comprehensive methodology for the implementation of\nfinancial models on the R-project platform. The core component of this approach\nis the use of the Financial Modeling and Valuation Toolkit (FMVT) for\nconstructing and solving portfolio optimization problems. In addition to\nsolving portfolio optimization problems, FMVT is used to develop trading models\nand generate trading strategies. This paper discusses the fundamentals of\nportfolio optimization and trading model development using FMVT, as well as\nexplains how to implement the trading model using FMVT. The paper concludes with\na discussion of the challenges associated with implementing financial models\non the R-project platform. It also provides a brief overview of the R-project\nplatform, including its key features and functionality. The paper concludes by\nproviding recommendations for future research in this area.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10869565217391304,
          "p": 0.19230769230769232,
          "f": 0.1388888842746915
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.017543859649122806,
          "f": 0.013513508777393915
        },
        "rouge-l": {
          "r": 0.10144927536231885,
          "p": 0.1794871794871795,
          "f": 0.12962962501543226
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atom-ph/2503.07161v1",
      "true_abstract": "We show that atomic antimatter spectroscopy can be used to search for new\nbosons that carry spin-dependent exotic forces between antifermions. A\ncomparison of a recent precise measurement of the hyperfine splitting of the\n$1$S and $2$S electronic levels of antihydrogen and bound-state quantum\nelectrodynamics theory yields the first tests of positron-antiproton exotic\ninteractions, constraining the dimensionless coupling strengths $g_pg_p$,\n$g_Vg_V$ and $g_Ag_A$, corresponding to the exchange of a pseudoscalar\n(axionlike), vector, or axial-vector boson, respectively. We also discuss new\ntests of CPT invariance with exotic spin-dependent and spin-independent\ninteractions involving antimatter.",
      "generated_abstract": "We report the first observation of a strong, nonlinear optical response in\na monolayer of 2H-12C molecules. Using a time-resolved spectroscopy approach,\nwe determine the nonlinear susceptibility of the system as a function of\npolarization. We find that the nonlinear susceptibility of the 2H-12C molecule\nis significantly different from that of the 2H molecule, demonstrating a\nfundamental difference in the nature of the molecular polarizabilities. Our\nresults suggest that the molecular polarizabilities of 2H-12C are highly\nanisotropic, with a significant dependence on both the orientation and\norientation-dependent polarization of light. This anisotropic molecular\npolarizability is in stark contrast to the isotropic nature of the polarizabilities\nof 2H molecules, which remains una",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14492753623188406,
          "p": 0.14925373134328357,
          "f": 0.14705881853049327
        },
        "rouge-2": {
          "r": 0.045454545454545456,
          "p": 0.041237113402061855,
          "f": 0.043243238255077275
        },
        "rouge-l": {
          "r": 0.14492753623188406,
          "p": 0.14925373134328357,
          "f": 0.14705881853049327
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.08953v1",
      "true_abstract": "Digital twin (DT) has emerged as a powerful tool to facilitate monitoring,\ncontrol, and other decision-making tasks in real-world engineering systems.\nOnline update methods have been proposed to update DT models. Considering the\ndegradation behavior in the system lifecycle, these methods fail to enable DT\nmodels to predict the system responses affected by the system degradation over\ntime. To alleviate this problem, degradation models of measurable parameters\nhave been integrated into DT construction. However, identifying the degradation\nparameters relies on prior knowledge of the system and expensive experiments.\nTo mitigate those limitations, this paper proposes a lifelong update method for\nDT models to capture the effects of system degradation on system responses\nwithout any prior knowledge and expensive offline experiments on the system.\nThe core idea in the work is to represent the system degradation during the\nlifecycle as the dynamic changes of DT configurations (i.e., model parameters\nwith a fixed model structure) at all degradation stages. During the lifelong\nupdate process, an Autoencoder is adopted to reconstruct the model parameters\nof all hidden layers simultaneously, so that the latent features taking into\naccount the dependencies among hidden layers are obtained for each degradation\nstage. The dynamic behavior of latent features among successive degradation\nstages is then captured by a long short-term memory model, which enables\nprediction of the latent feature at any unseen stage. Based on the predicted\nlatent features, the model configuration at future degradation stage is\nreconstructed to determine the new DT model, which predicts the system\nresponses affected by the degradation at the same stage. The test results on\ntwo engineering datasets demonstrate that the proposed update method could\ncapture effects of system degradation on system responses during the lifecycle.",
      "generated_abstract": "We introduce a novel approach to the problem of learning a causal graph\nin the presence of incomplete information. This approach, which we call\n\"causal graph completion\", utilizes causal network embeddings, a representation\nof a causal graph that encodes information about the causal structure of a\ngiven node. These embeddings are learned using a variational autoencoder\nframework, with the goal of minimizing the Kullback-Leibler divergence between\nthe observed graph and its latent counterpart. We demonstrate that this\napproach outperforms state-of-the-art methods in terms of both model\nperformance and causal inference accuracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10344827586206896,
          "p": 0.23809523809523808,
          "f": 0.14423076500785886
        },
        "rouge-2": {
          "r": 0.008298755186721992,
          "p": 0.022988505747126436,
          "f": 0.012195118053429264
        },
        "rouge-l": {
          "r": 0.0896551724137931,
          "p": 0.20634920634920634,
          "f": 0.12499999577708965
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00741v2",
      "true_abstract": "Fully-supervised lesion recognition methods in medical imaging face\nchallenges due to the reliance on large annotated datasets, which are expensive\nand difficult to collect. To address this, synthetic lesion generation has\nbecome a promising approach. However, existing models struggle with\nscalability, fine-grained control over lesion attributes, and the generation of\ncomplex structures. We propose LesionDiffusion, a text-controllable lesion\nsynthesis framework for 3D CT imaging that generates both lesions and\ncorresponding masks. By utilizing a structured lesion report template, our\nmodel provides greater control over lesion attributes and supports a wider\nvariety of lesion types. We introduce a dataset of 1,505 annotated CT scans\nwith paired lesion masks and structured reports, covering 14 lesion types\nacross 8 organs. LesionDiffusion consists of two components: a lesion mask\nsynthesis network (LMNet) and a lesion inpainting network (LINet), both guided\nby lesion attributes and image features. Extensive experiments demonstrate that\nLesionDiffusion significantly improves segmentation performance, with strong\ngeneralization to unseen lesion types and organs, outperforming current\nstate-of-the-art models. Code will be available at\nhttps://github.com/HengruiTianSJTU/LesionDiffusion.",
      "generated_abstract": "In this paper, we propose a novel multimodal contrastive learning framework\nfor medical image segmentation, named as Multimodal Contrastive Learning\n(MCL). The key idea is to use multimodal contrastive learning (MCL) to\ncapture the semantic similarity between different modalities, thereby enhancing\nthe learning ability of the segmentation model. Specifically, we design a\nself-attention-based MCL module for medical images and a multi-scale\ncontrastive learning module for segmentation masks, which are combined to\nfine-tune the model. Extensive experiments on the medical image segmentation\nbenchmarks demonstrate that the proposed MCL significantly outperforms the\nstate-of-the-art methods in terms of AUC and segmentation accuracy, especially\nin challenging multi-organ segmentation tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19166666666666668,
          "p": 0.3194444444444444,
          "f": 0.23958332864583343
        },
        "rouge-2": {
          "r": 0.03680981595092025,
          "p": 0.061224489795918366,
          "f": 0.04597700680436331
        },
        "rouge-l": {
          "r": 0.15833333333333333,
          "p": 0.2638888888888889,
          "f": 0.19791666197916677
        }
      }
    },
    {
      "paper_id": "cs.MM.cs/MM/2503.07911v1",
      "true_abstract": "Pixel-level segmentation is essential in remote sensing, where foundational\nvision models like CLIP and Segment Anything Model(SAM) have demonstrated\nsignificant capabilities in zero-shot segmentation tasks. Despite their\nadvances, challenges specific to remote sensing remain substantial. Firstly,\nThe SAM without clear prompt constraints, often generates redundant masks, and\nmaking post-processing more complex. Secondly, the CLIP model, mainly designed\nfor global feature alignment in foundational models, often overlooks local\nobjects crucial to remote sensing. This oversight leads to inaccurate\nrecognition or misplaced focus in multi-target remote sensing imagery. Thirdly,\nboth models have not been pre-trained on multi-scale aerial views, increasing\nthe likelihood of detection failures. To tackle these challenges, we introduce\nthe innovative VTPSeg pipeline, utilizing the strengths of Grounding DINO,\nCLIP, and SAM for enhanced open-vocabulary image segmentation. The Grounding\nDINO+(GD+) module generates initial candidate bounding boxes, while the CLIP\nFilter++(CLIP++) module uses a combination of visual and textual prompts to\nrefine and filter out irrelevant object bounding boxes, ensuring that only\npertinent objects are considered. Subsequently, these refined bounding boxes\nserve as specific prompts for the FastSAM model, which executes precise\nsegmentation. Our VTPSeg is validated by experimental and ablation study\nresults on five popular remote sensing image segmentation datasets.",
      "generated_abstract": "Video editing is an essential tool for editing videos, allowing users to\nchange the order of frames, add or remove frames, or change the content of\nframes. Recently, deep learning-based video editing models have achieved\nsignificant progress, achieving remarkable editing capabilities. However,\nexisting methods are generally based on a single video, and the editing process\nis not fully realized, resulting in a loss of editing ability. This paper\nproposes a video editing model with multi-level editing ability, which is\nbased on a video of a certain length, and supports the editing of multiple\nframes simultaneously. To achieve this, we first introduce a novel model with\nmultiple video-level editing ability, and then we propose a novel video-level\nediting ability, which can be applied to each frame separately. The proposed\nmodel can adaptively adjust the editing ability of the video, enabling the\nvideo to be edited in a more flexible manner. In addition",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1643835616438356,
          "p": 0.2608695652173913,
          "f": 0.20168066752630476
        },
        "rouge-2": {
          "r": 0.005208333333333333,
          "p": 0.007407407407407408,
          "f": 0.0061162031029974185
        },
        "rouge-l": {
          "r": 0.1506849315068493,
          "p": 0.2391304347826087,
          "f": 0.18487394483722913
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.08355v1",
      "true_abstract": "This work addresses the problem of estimating a vector field from a noisy\nOrdinary Differential Equation (ODE) in a non-parametric regression setting\nwith a random design for initial values. More specifically, given a vector\nfield $ f:\\mathbb{R}^{D}\\rightarrow \\mathbb{R}^{D}$ governing a dynamical\nsystem defined by the autonomous ODE: $y' = f(y)$, we assume that the\nobservations are $\\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) +\n\\varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time\n$t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a\nprobability distribution $\\mu$, and $\\varepsilon_{i,j}$ some noise. In this\ncontext, we investigate, from a minimax perspective, the pointwise\nreconstruction of $f$ within the envelope of trajectories originating from the\nsupport of $\\mu$. We propose an estimation strategy based on preliminary flow\nreconstruction and techniques from derivative estimation in non-parametric\nregression. Under mild assumptions on $f$, we establish convergence rates that\ndepend on the temporal resolution, the number of sampled initial values and the\nmass concentration of $\\mu$. Importantly, we show that these rates are minimax\noptimal. Furthermore, we discuss the implications of our results in a manifold\nlearning setting, providing insights into how our approach can mitigate the\ncurse of dimensionality.",
      "generated_abstract": "We prove that the set of isomorphism classes of quantum loop algebras\nequipped with the canonical quantum loop structure is a subgroup of the\nsymmetric group $\\mathfrak{S}_n$ for any even $n\\geq 2$. We also show that the\nclassification of quantum loop algebras of type $A_n$ is equivalent to the\nclassification of irreducible representations of the symmetric group. Our\nresults extend a result of Nahm-Saito-Wakimoto [NCJM24.10.11201409031051001001",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07633587786259542,
          "p": 0.23255813953488372,
          "f": 0.11494252501453309
        },
        "rouge-2": {
          "r": 0.016216216216216217,
          "p": 0.05357142857142857,
          "f": 0.024896261992734797
        },
        "rouge-l": {
          "r": 0.07633587786259542,
          "p": 0.23255813953488372,
          "f": 0.11494252501453309
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2503.06707v1",
      "true_abstract": "We extend the scope of differential machine learning and introduce a new\nbreed of supervised principal component analysis to reduce dimensionality of\nDerivatives problems. Applications include the specification and calibration of\npricing models, the identification of regression features in least-square\nMonte-Carlo, and the pre-processing of simulated datasets for (differential)\nmachine learning.",
      "generated_abstract": "We study a class of stochastic volatility models with time-varying drift and\nvolatility coefficients. These models are characterized by a time-varying\nparameter $\\alpha$ in the drift, and a time-varying coefficient $\\beta$ in the\nvolatility. We establish the existence of the unique solution to the\nStochastic Volatility Model with Time-Varying Coefficients under mild regularity\nassumptions. We also provide a sufficient condition for the uniqueness of the\nsolution in terms of the drift and volatility coefficients. Furthermore, we\nderive the closed-form solutions to the solutions of the Stochastic Volatility\nModel with Time-Varying Coefficients. Finally, we characterize the existence of\nthe solutions for the Stochastic Volatility Model with Time-Varying Coefficients\nunder additional regularity assumptions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20512820512820512,
          "p": 0.1509433962264151,
          "f": 0.1739130385940455
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.20512820512820512,
          "p": 0.1509433962264151,
          "f": 0.1739130385940455
        }
      }
    },
    {
      "paper_id": "cs.CV.stat/OT/2501.12596v1",
      "true_abstract": "This expository paper introduces a simplified approach to image-based quality\ninspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image\nPretraining) model adapted for few-shot learning. While CLIP has demonstrated\nimpressive capabilities in general computer vision tasks, its direct\napplication to manufacturing inspection presents challenges due to the domain\ngap between its training data and industrial applications. We evaluate CLIP's\neffectiveness through five case studies: metallic pan surface inspection, 3D\nprinting extrusion profile analysis, stochastic textured surface evaluation,\nautomotive assembly inspection, and microstructure image classification. Our\nresults show that CLIP can achieve high classification accuracy with relatively\nsmall learning sets (50-100 examples per class) for single-component and\ntexture-based applications. However, the performance degrades with complex\nmulti-component scenes. We provide a practical implementation framework that\nenables quality engineers to quickly assess CLIP's suitability for their\nspecific applications before pursuing more complex solutions. This work\nestablishes CLIP-based few-shot learning as an effective baseline approach that\nbalances implementation simplicity with robust performance, demonstrated in\nseveral manufacturing quality control applications.",
      "generated_abstract": "This paper presents a novel framework for training and evaluating video\nclustering models that utilizes the rich temporal information within videos.\nSpecifically, we propose a temporal clustering network that leverages\nmulti-scale and multi-channel features to capture long-term and short-term\nrelationships within videos. To address the challenges of long-range\ninterdependencies and short-term sequential dependencies, we propose a\ntemporal-aware loss function that integrates the temporal information within\nthe network, as well as utilizes a novel spatial-temporal attention mechanism\nto capture the long-term dependencies within videos. To further improve the\nmodel's capability in capturing temporal information within videos, we\nintroduce a Temporal Feature Convolution (TFC) module, which incorporates a\ntemporal convolution layer to enhance the learning ability of the network.\nExperimental results on several benchmark datasets demonstrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.136,
          "p": 0.2125,
          "f": 0.16585365377751352
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.128,
          "p": 0.2,
          "f": 0.15609755621653792
        }
      }
    },
    {
      "paper_id": "math.SG.math/AT/2503.09783v1",
      "true_abstract": "For a Weinstein manifold, we compare and contrast the properties of admitting\nan arboreal skeleton and admitting Maslov data. Both properties are implied by\nthe existence of a polarization, for which a basic obstruction is that the odd\nChern classes are 2-torsion. In a similar spirit we establish cohomological\nobstructions to the existence of arboreal skeleta and to the existence of\nMaslov data, exhibiting concrete examples to illustrate their failure. For\ninstance, we show that complements of smooth anti-canonical divisors in complex\nprojective space may fail to admit either arboreal skeleta or Maslov data. We\nalso exhibit an example of a Weinstein manifold which admits Maslov data but\ndoes not admit an arboreal skeleton.",
      "generated_abstract": "In this paper, we study the asymptotic behavior of the solutions of the\nHamilton-Jacobi equation with the drift of the form $f(y)y'$, where $f$ is a\nnon-negative function, for the case of the $m$-dimensional Lorentzian space\n$M^m(\\mathbb{R}^n,\\mathbb{R}_y)$, where $m\\geq 2$ and $n\\geq 1$. We prove that\nthe solutions are non-degenerate and have a unique positive asymptotic\nexponential growth, which is exponentially close to $e^{f(y)y'}$. We also\nshow that the solutions are exponentially close to the solutions of the\nHamilton-Jacobi equation with the drift of the form $f(y)\\nabla^2 y$ for\n$f(y)\\in C^1(\\mathbb{R}^n)$ and $n\\geq",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.2962962962962963,
          "f": 0.25396824907029486
        },
        "rouge-2": {
          "r": 0.039603960396039604,
          "p": 0.056338028169014086,
          "f": 0.0465116230590865
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.2962962962962963,
          "f": 0.25396824907029486
        }
      }
    },
    {
      "paper_id": "cs.CY.physics/ed-ph/2502.08705v2",
      "true_abstract": "Engaging the public with science is critical for a well-informed population.\nA popular method of scientific communication is documentaries. Once released,\nit can be difficult to assess the impact of such works on a large scale, due to\nthe overhead required for in-depth audience feedback studies. In what follows,\nwe overview our complementary approach to qualitative studies through\nquantitative impact and sentiment analysis of Amazon reviews for several\nscientific documentaries. In addition to developing a novel impact category\ntaxonomy for this analysis, we release a dataset containing 1296\nhuman-annotated sentences from 1043 Amazon reviews for six movies created in\nwhole or part by the Advanced Visualization Lab (AVL). This interdisciplinary\nteam is housed at the National Center for Supercomputing Applications and\nconsists of visualization designers who focus on cinematic presentations of\nscientific data. Using this data, we train and evaluate several machine\nlearning and large language models, discussing their effectiveness and possible\ngeneralizability for documentaries beyond those focused on for this work.\nThemes are also extracted from our annotated dataset which, along with our\nlarge language model analysis, demonstrate a measure of the ability of\nscientific documentaries to engage with the public.",
      "generated_abstract": "This paper explores how the Internet of Things (IoT) can be used to\nimprove the educational experience of students with dyslexia and other\nlearning disabilities. We present a novel concept for a smart learning\nenvironment using an IoT-based platform, enabling students to access a\nuniversal digital library of educational materials, supported by a personalized\nlearning management system (LMS) and a cloud-based data storage and\nmanagement system. We also introduce a novel learning management system\narchitecture and a data management system architecture, which allow for the\nautomatic classification of learning problems and the implementation of\nadaptive learning strategies. This architecture allows the system to adapt to\nthe specific needs of each student, providing them with personalized learning\npaths. The proposed system can be used to create personalized learning\nenvironments, providing students with more individualized learning experiences.\nWe also demonstrate how the system can be integrated into existing learning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12403100775193798,
          "p": 0.1951219512195122,
          "f": 0.15165876302059716
        },
        "rouge-2": {
          "r": 0.02185792349726776,
          "p": 0.031007751937984496,
          "f": 0.025641020790804665
        },
        "rouge-l": {
          "r": 0.12403100775193798,
          "p": 0.1951219512195122,
          "f": 0.15165876302059716
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2501.12233v1",
      "true_abstract": "We address the challenge of identifying all real positive steady states in\nchemical reaction networks (CRNs) governed by mass-action kinetics. Traditional\nnumerical methods often require specific initial guesses and may fail to find\nall the solutions in systems exhibiting multistability. Gr\\\"obner bases offer\nan algebraic framework that systematically transforms polynomial equations into\nsimpler forms, facilitating comprehensive solution enumeration. In this work,\nwe propose a conjecture that CRNs with at most pairwise interactions yield\nGr\\\"obner bases possessing a near-\"triangular\" structure, under appropriate\nassumptions. We illustrate this phenomenon using examples from a gene\nregulatory network and the Wnt signaling pathway, where the Gr\\\"obner basis\napproach reliably captures all real positive solutions. Our computational\nexperiments reveal the potential of Gr\\\"obner bases to overcome limitations of\nlocal numerical methods for finding the steady states of complex biological\nsystems, making them a powerful tool for understanding dynamical processes\nacross diverse biochemical models.",
      "generated_abstract": "We propose a novel framework for simulating multimodal molecular dynamics\n(MD) trajectories using a multi-modal Markov chain Monte Carlo (MCMC) method.\nThis method combines a pair of MCMC algorithms for simulating individual\ntrajectories of the molecule and the dynamics of the ensemble of trajectories\nsimulated using a Markov chain Monte Carlo (MCMC) method. The first algorithm\nuses the ensemble of trajectories as an initial point for the second\nalgorithm, which uses the initial point to simulate the individual trajectories.\nWe show that the combined algorithm can achieve higher accuracy in molecular\nsimulations than the single MCMC algorithm. We use this method to simulate\nmolecular trajectories using the Gromacs MD simulation package. We compare the\nresults of our method to those obtained using other MCMC methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1391304347826087,
          "p": 0.2318840579710145,
          "f": 0.173913038790761
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.018691588785046728,
          "f": 0.01619432707305628
        },
        "rouge-l": {
          "r": 0.1391304347826087,
          "p": 0.2318840579710145,
          "f": 0.173913038790761
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.12669v1",
      "true_abstract": "This paper examines the optimal design of information sharing in\norganizations. Organizational performance depends on agents adapting to\nuncertain external environments while coordinating their actions, where\ncoordination incentives and synergies are modeled as graphs (networks). The\nequilibrium strategies and the principal's objective function are summarized\nusing Laplacian matrices of these graphs. I formulate a Bayesian persuasion\nproblem to determine the optimal public signal and show that it comprises a set\nof statistics on local states, necessarily including their average, which\nserves as the organizational goal. When the principal benefits equally from the\ncoordination of any two agents, the choice of disclosed statistics is based on\nthe Laplacian eigenvectors and eigenvalues of the incentive graph. The\nalgebraic connectivity (the second smallest Laplacian eigenvalue) determines\nthe condition for full revelation, while the Laplacian spectral radius (the\nlargest Laplacian eigenvalue) establishes the condition for minimum\ntransparency, where only the average state is disclosed.",
      "generated_abstract": "In this paper we develop a general theory of the dynamic game of\n(cooperative) auctions, focusing on the case of multiple sellers, and the\npotential for ex-ante price fixing. In the standard model, each seller is\nincentivized to engage in a dynamic game of bidding with the goal of maximizing\nits expected payoff. We show that this game is intractable, and introduce a\nnovel model where the seller is allowed to engage in a dynamic game of bidding\nwith the goal of maximizing the payoff of a single buyer. This is the first\nsuch model that is computationally tractable. We prove that the optimal\nbehavior of this buyer is to set a price floor. This model also allows for\nex-ante price fixing by the seller. We show that this ex-ante price fixing is\ndeterministic and leads to a Nash",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16037735849056603,
          "p": 0.25,
          "f": 0.19540229408904755
        },
        "rouge-2": {
          "r": 0.034722222222222224,
          "p": 0.043859649122807015,
          "f": 0.03875968499008536
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.23529411764705882,
          "f": 0.18390804121548432
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2503.03780v1",
      "true_abstract": "Camera-based vital signs monitoring in recent years has attracted more and\nmore researchers and the results are promising. However, a few research works\nfocus on heart rate extraction under extremely low illumination environments.\nIn this paper, we propose a novel framework for remote heart rate estimation\nunder low-light conditions. This method uses singular spectrum analysis (SSA)\nto decompose the filtered signal into several reconstructed components. A\nspectral masking algorithm is utilized to refine the preliminary candidate\ncomponents on the basis of a reference heart rate. The contributive components\nare fused into the final pulse signal. To evaluate the performance of our\nframework in low-light conditions, the proposed approach is tested on a\nlarge-scale multi-illumination HR dataset (named MIHR). The test results verify\nthat the proposed method has stronger robustness to low illumination than\nstate-of-the-art methods, effectively improving the signal-to-noise ratio and\nheart rate estimation precision. We further perform experiments on the PUlse\nRatE detection (PURE) dataset which is recorded under normal light conditions\nto demonstrate the generalization of our method. The experiment results show\nthat our method can stably detect pulse rate and achieve comparative results.\nThe proposed method pioneers a new solution to the remote heart rate estimation\nin low-light conditions.",
      "generated_abstract": "The field of medical imaging is facing a major challenge: the lack of\naccurate, scalable and reproducible methods for image analysis. Traditional\nmethods based on the use of deep learning, however, are complex, time-consuming\nand require the availability of a large volume of labeled data.\n  In this paper, we propose a new method for automatic brain tumor segmentation\nusing a novel approach based on the diffusion modeling technique. We also\nintroduce a novel method for automatic tumor segmentation in chest X-rays using\na diffusion modeling technique. We show that our method is able to identify\nneuro- and non-neuro-tumors with comparable performance compared to other\nstate-of-the-art methods, even when limited by the availability of a small\nnumber of labeled data. Additionally, our method is able to segment tumors in\nchest X-ray",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3333333333333333,
          "f": 0.2666666618666667
        },
        "rouge-2": {
          "r": 0.06989247311827956,
          "p": 0.11926605504587157,
          "f": 0.08813558856098846
        },
        "rouge-l": {
          "r": 0.20634920634920634,
          "p": 0.30952380952380953,
          "f": 0.2476190428190477
        }
      }
    },
    {
      "paper_id": "cs.MA.econ/GN/2502.13267v1",
      "true_abstract": "BeforeIT is an open-source software for building and simulating\nstate-of-the-art macroeconomic agent-based models (macro ABMs) based on the\nrecently introduced macro ABM developed in [1] and here referred to as the base\nmodel. Written in Julia, it combines extraordinary computational efficiency\nwith user-friendliness and extensibility. We present the main structure of the\nsoftware, demonstrate its ease of use with illustrative examples, and benchmark\nits performance. Our benchmarks show that the base model built with BeforeIT is\norders of magnitude faster than a Matlab version, and significantly faster than\nMatlab-generated C code. BeforeIT is designed to facilitate reproducibility,\nextensibility, and experimentation. As the first open-source, industry-grade\nsoftware to build macro ABMs of the type of the base model, BeforeIT can\nsignificantly foster collaboration and innovation in the field of agent-based\nmacroeconomic modelling. The package, along with its documentation, is freely\navailable at https://github.com/bancaditalia/BeforeIT.jl under the AGPL-3.0.",
      "generated_abstract": "The emergence of multi-agent systems, such as multi-agent reinforcement\nlearning (MARL) and multi-agent systems, has revolutionized the field of\ncomputer science. However, the study of these systems from a theoretical\nperspective is still in its infancy. In this work, we present a comprehensive\nsurvey of the theory of multi-agent systems, focusing on the mathematical\nfoundations of MARL and multi-agent systems, as well as their theoretical and\napplied applications in artificial intelligence, robotics, and the military.\nThe survey begins by providing a brief overview of the foundations of MARL,\nincluding reinforcement learning and multi-agent systems, followed by a\ncomprehensive review of the mathematical foundations of MARL and multi-agent\nsystems, including multi-agent learning and multi-agent systems. The survey\nthen proceeds to discussing the theoretical foundations of multi",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1262135922330097,
          "p": 0.2,
          "f": 0.1547619000177156
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.039603960396039604,
          "f": 0.033333328458681276
        },
        "rouge-l": {
          "r": 0.1262135922330097,
          "p": 0.2,
          "f": 0.1547619000177156
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.01578v1",
      "true_abstract": "We compute scalar products of off-shell Bethe vectors in models with\n$o_{2n+1}$ symmetry. The scalar products are expressed as a sum over partitions\nof the Bethe parameter sets, the building blocks being the so-called highest\ncoefficients. We prove some recurrence relations and a residue theorem for\nthese highest coefficients, and prove that they are consistent with the\nreduction to $gl_n$ invariant models. We also express the norm of on-shell\nBethe vectors as a Gaudin determinant.",
      "generated_abstract": "We introduce a new class of 2D self-similarity systems which exhibit\nexponential chaos in the thermodynamic limit. We provide a classification of\nthe critical exponents for the dynamical systems and their properties. We\npropose a new method to construct critical exponents from the numerical data.\nFinally, we give an example of a 2D system which exhibits exponential chaos in\nthe thermodynamic limit and also provides the critical exponents.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.21428571428571427,
          "f": 0.18947367927756245
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.017241379310344827,
          "f": 0.015503871019772023
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.19047619047619047,
          "f": 0.16842104769861513
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2503.03126v1",
      "true_abstract": "Groups of cells, including clusters of cancerous cells, multicellular\norganisms, and developing organs, may both grow and break apart. What physical\nfactors control these fractures? In these processes, what sets the eventual\nsize of clusters? We develop a framework for understanding cell clusters that\ncan fragment due to cell motility using an active particle model. We compute\nanalytically how the break rate of cell-cell junctions depends on cell speed,\ncell persistence, and cell-cell junction properties. Next, we find the cluster\nsize distributions, which differ depending on whether all cells can divide or\nonly the cells on the edge of the cluster divide. Cluster size distributions\ndepend solely on the ratio of the break rate to the growth rate - allowing us\nto predict how cluster size and variability depend on cell motility and\ncell-cell mechanics. Our results suggest that organisms can achieve better size\ncontrol when cell division is restricted to the cluster boundaries or when\nfracture can be localized to the cluster center. Our results link the general\nphysics problem of a collective active escape over a barrier to size control,\nproviding a quantitative measure of how motility can regulate organ or organism\nsize.",
      "generated_abstract": "The use of magnetic resonance imaging (MRI) in the diagnosis of\nmechanically-induced injuries (MIIs) in animals has been limited by the\ndifficulty in acquiring high-resolution images due to low contrast between the\nlesioned and healthy tissues. This study aims to develop an MRI-based tool to\nquantify the severity of MIIs in both young and old rats. To achieve this, we\nproposed a novel MRI-based method to identify the lesion-healthy boundary\n(LHB), which was then used to assess the MRI-derived lesion-healthy tissue\ndifferences (LHD). The LHD was quantified using the normalized root mean square\n(NRSRM) and the root mean square difference (RMSD) between the LHB and the\nlesion. Our findings demonstrated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11965811965811966,
          "p": 0.1728395061728395,
          "f": 0.14141413657943083
        },
        "rouge-2": {
          "r": 0.005555555555555556,
          "p": 0.009523809523809525,
          "f": 0.0070175392059125975
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.16049382716049382,
          "f": 0.13131312647842072
        }
      }
    },
    {
      "paper_id": "math.AP.math/SP/2503.01528v1",
      "true_abstract": "We examine semiclassical measures for Laplace eigenfunctions on compact\nhyperbolic $(n+1)$-manifolds. We prove their support must contain the cosphere\nbundle of a compact immersed totally geodesic submanifold. Our proof adapts the\nargument of Dyatlov and Jin to higher dimensions and classifies the closures of\nhorocyclic orbits using Ratner theory. An important step in the proof is a\ngeneralization of the higher-dimensional fractal uncertainty principle of Cohen\nto Fourier integral operators, which may be of independent interest.",
      "generated_abstract": "In this paper, we study a class of $p$-adic number fields $K$ over the\nalgebraic closure $\\bar{K}$ of $K$ which is a subfield of a certain extension\n$\\bar{K}/K$ of degree $p$. We prove that there exists a positive integer\n$\\ell$ such that every $\\ell$-torsion-free subgroup of $K^\\times$ has a\nnon-trivial torsion subfield $L$ of degree at most $\\ell$ over $\\bar{K}$. We\nalso give an explicit formula for the cardinality of the set of $\\ell$-torsion\nsubgroups of $K^\\times$ with trivial torsion subfield.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.14285714285714285,
          "f": 0.13675213176126838
        },
        "rouge-2": {
          "r": 0.05333333333333334,
          "p": 0.05194805194805195,
          "f": 0.052631573948234554
        },
        "rouge-l": {
          "r": 0.11475409836065574,
          "p": 0.125,
          "f": 0.11965811466725128
        }
      }
    },
    {
      "paper_id": "math-ph.math/MP/2503.09558v1",
      "true_abstract": "For a given graph $G$, Budzik, Gaiotto, Kulp, Wang, Williams, Wu, Yu, and the\nfirst author studied a ''topological'' differential form $\\alpha_G$, which\nexpresses violations of BRST-closedness of a quantum field theory along a\nsingle topological direction. In a seemingly unrelated context, Brown, Panzer,\nand the second author studied a ''Pfaffian'' differential form $\\phi_G$, which\nis used to construct cohomology classes of the odd commutative graph complex.\nWe give an explicit combinatorial proof that $\\alpha_G$ coincides with\n$\\phi_G$. We also discuss the equivalence of several properties of these forms,\nwhich had been established independently for both contexts in previous work.",
      "generated_abstract": "In this work, we develop a novel approach to the study of nonlinear\nparameter-dependent equations. We establish a number of fundamental properties\nof the solutions of the equation and provide a classification of the\nparameter-dependent solutions. Moreover, we investigate the stability of the\nsolutions under the parameter variation. We find that the parameter-dependent\nequations with a general nonlinearity can be classified into three types:\n(1) The equation has a parameter-independent solution. (2) The equation has a\nparameter-dependent solution which can be expressed as a linear combination of\nthe solution of the equation. (3) The equation has a parameter-dependent\nsolution which is a linear combination of the solutions of the equation. In\nparticular, the parameter-dependent equation with a linear term is called a\nlinear-nonlinear equation, and the parameter-dependent equation with a\nnonlinear term is called a nonlinear-nonlinear equation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1518987341772152,
          "p": 0.19672131147540983,
          "f": 0.17142856651122462
        },
        "rouge-2": {
          "r": 0.042105263157894736,
          "p": 0.04040404040404041,
          "f": 0.041237108404188094
        },
        "rouge-l": {
          "r": 0.1518987341772152,
          "p": 0.19672131147540983,
          "f": 0.17142856651122462
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.07830v1",
      "true_abstract": "Let (K,v) be a valued field. Take an extension of v to a fixed algebraic\nclosure L of K. In this paper we show that an element a in L admits a complete\ndistinguished chain over K if and only if the extension (K(a)|K,v) is\ndefectless and unibranched. This characterization generalizes the known result\nin the henselian case. In particular, our result shows that if a admits a\ncomplete distinguished chain over K, then it also admits one over the\nhenselization; however, the converse may not be true. The main tool employed in\nour analysis is the stability of the j-invariant associated to a valuation\ntranscendental extension under passage to the henselization.",
      "generated_abstract": "We prove a result on the asymptotic growth rate of the number of\nbranches of a certain infinite family of $n$-tuples of positive integers. The\nresult is obtained by using the classical inequality of the Riemann-Lebesgue\ntheorem. The proof is based on the application of the so-called\n$k$-$\\epsilon$ lemma, which is a version of the classical $k$-$\\epsilon$\ninequality. The $k$-$\\epsilon$ inequality is proved using a compactness\nargument and the use of the classical inequality of the Riemann-Lebesgue\ntheorem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09722222222222222,
          "p": 0.175,
          "f": 0.12499999540816344
        },
        "rouge-2": {
          "r": 0.009523809523809525,
          "p": 0.015625,
          "f": 0.011834314820911502
        },
        "rouge-l": {
          "r": 0.09722222222222222,
          "p": 0.175,
          "f": 0.12499999540816344
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07139v1",
      "true_abstract": "In this letter, we investigate a coordinated multiple point (CoMP)-aided\nintegrated sensing and communication (ISAC) system that supports multiple users\nand targets. Multiple base stations (BSs) employ a coordinated power allocation\nstrategy to serve their associated single-antenna communication users (CUs)\nwhile utilizing the echo signals for joint radar target (RT) detection. The\nprobability of detection (PoD) of the CoMP-ISAC system is then proposed for\nassessing the sensing performance. To maximize the sum rate while ensuring the\nPoD for each RT and adhering to the total transmit power budget across all BSs,\nwe introduce an efficient power allocation strategy. Finally, simulation\nresults are provided to validate the analytical findings, demonstrating that\nthe proposed power allocation scheme effectively enhances the sum rate while\nsatisfying the sensing requirements.",
      "generated_abstract": "This paper introduces a novel unified framework for speech enhancement\nand multi-speaker audio enhancement. The proposed approach integrates\nfrequency-domain speech enhancement with multi-speaker audio enhancement. To\nenable a seamless integration, we propose a unified model for speech and\nmulti-speaker audio, which leverages a common speech encoder and a\nmulti-speaker encoder to jointly enhance speech and multi-speaker audio\ndynamically. In addition, we propose a novel multi-speaker audio enhancement\nloss function, which considers both multi-speaker and speech components. The\nproposed framework is validated using several speech and multi-speaker audio\nbenchmarks, including the CLEVR-A speech benchmark and the ITA-11\nmulti-speaker benchmark, demonstrating superior performance compared to existing\nstate-of-the-art methods. The code is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14942528735632185,
          "p": 0.21311475409836064,
          "f": 0.17567567082998553
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.19672131147540983,
          "f": 0.16216215731647202
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.04765v2",
      "true_abstract": "Early-stage firms play a significant role in driving innovation and creating\nnew products and services, especially for cybersecurity. Therefore, evaluating\ntheir performance is crucial for investors and policymakers. This work presents\na financial evaluation of early-stage firms' performance in 19 cybersecurity\nsectors using a private-equity dataset from 2010 to 2022 retrieved from\nCrunchbase. We observe firms, their primary and secondary activities, funding\nrounds, and pre and post-money valuations. We compare cybersecurity sectors\nregarding the amount raised over funding rounds and post-money valuations while\ninferring missing observations. We observe significant investor interest\nvariations across categories, periods, and locations. In particular, we find\nthe average capital raised (valuations) to range from USD 7.24 mln (USD 32.39\nmln) for spam filtering to USD 45.46 mln (USD 447.22 mln) for the private cloud\nsector. Next, we assume a log process for returns computed from post-money\nvaluations and estimate the expected returns, systematic and specific risks,\nand risk-adjusted returns of investments in early-stage firms belonging to\ncybersecurity sectors. Again, we observe substantial performance variations\nwith annualized expected returns ranging from 9.72\\% for privacy to 177.27\\%\nfor the blockchain sector. Finally, we show that overall, the cybersecurity\nindustry performance is on par with previous results found in private equity.\nOur results shed light on the performance of cybersecurity investments and,\nthus, on investors' expectations about cybersecurity.",
      "generated_abstract": "In this work, we consider the problem of designing a set of trading rules that\nfacilitate the attainment of a given target return in the presence of\nnon-cooperative traders. The traders can either buy or sell a specific asset.\nWe consider a class of traders who are uninformed about the target return but\ncan estimate it based on their past transaction history. The target return is\nunknown to the traders, and the goal is to find a set of trading rules that\nmaximizes the expected return for each trading rule. We consider a class of\ntrading rules that includes a rule that buys the asset if the target return is\npositive and sells the asset if the target return is negative. We show that\nunder mild assumptions, the set of trading rules is optimal in the sense that\nit maximizes the expected return for each trading rule. We also provide a\nconstructive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13986013986013987,
          "p": 0.273972602739726,
          "f": 0.18518518071030532
        },
        "rouge-2": {
          "r": 0.009302325581395349,
          "p": 0.017241379310344827,
          "f": 0.012084587592301894
        },
        "rouge-l": {
          "r": 0.1258741258741259,
          "p": 0.2465753424657534,
          "f": 0.16666666219178683
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CG/2503.02336v1",
      "true_abstract": "We present a program for enumerating all pseudoline arrangements with a small\nnumber of pseudolines and abstract order types of small point sets. This\nprogram supports computer experiments with these structures, and it complements\nthe order-type database of Aichholzer, Aurenhammer, and Krasser. This system\nmakes it practical to explore the abstract order types for 12 points, and the\npseudoline arrangements of 11 pseudolines.",
      "generated_abstract": "We investigate the stability of the normalized eigenvalue $\\lambda_{\\text{N}}$\nof the Laplacian on a graph $\\mathcal{G}$ with a special class of\ninteractions. Namely, we consider the interaction potential $V$ which is\nnon-negative and convex, and we prove that the normalized eigenvalue $\\lambda_{\\text{N}}$\nis stable in the following sense: If $\\lambda_{\\text{N}}$ is stable, then\n$V$ is stable. In particular, we show that the Laplacian on $\\mathcal{G}$ is\nstable for any $V$ satisfying $\\nabla V \\cdot \\nabla V \\geq 0$ and\n$V(\\nabla V \\cdot \\nabla V) \\geq 0$. This is an important result in the\ncontext of stability of random networks. We also obtain a necessary condition\nfor the stability of $\\lambda_{\\text{N}}$ in the general case of\n$V",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.12307692307692308,
          "f": 0.14953270551139852
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.010309278350515464,
          "f": 0.01282050811719436
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.12307692307692308,
          "f": 0.14953270551139852
        }
      }
    },
    {
      "paper_id": "eess.SY.econ/TH/2502.14150v1",
      "true_abstract": "We propose a risk-sensitive security-constrained economic dispatch (R-SCED)\nformulation capturing the tradeoff between dispatch cost and resilience against\npotential line failures, where risk is modeled via the conditional value at\nrisk (CVaR). In the context of our formulation, we analyze revenue adequacy and\nside payments of two pricing models, one based on nominal generation costs, and\nanother based on total marginal cost including contingencies. In particular, we\nprove that the system operator's (SO) merchandising surplus (MS) and total\nrevenue are nonnegative under the latter, while under the former the same does\nnot hold in general. We demonstrate that the proposed R-SCED formulation is\namenable to decomposition and describe a Benders' decomposition algorithm to\nsolve it. In numerical examples, we illustrate the differences in MS and total\nrevenue under the considered pricing schemes, and the computational efficiency\nof our decomposition approach.",
      "generated_abstract": "This paper studies the problem of allocating a set of $N$ goods to $M$\nconsumers such that the total utility of each consumer is maximized. The\nindividual utility functions are assumed to be concave. The problem is\nreferred to as the concave maximization problem. This paper proposes a\nconstrained minimax algorithm to solve this problem. In this algorithm, the\noptimal solution is obtained by solving a nonconvex minimax problem, and\nsubsequently, a relaxed minimax problem is solved. The proposed algorithm\nprovides an efficient solution method for the concave maximization problem.\nFurthermore, the proposed algorithm is proved to converge to the optimal\nsolution. Numerical simulations show that the proposed algorithm outperforms\nthe existing algorithms in terms of computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.22857142857142856,
          "f": 0.19277107946000882
        },
        "rouge-2": {
          "r": 0.03787878787878788,
          "p": 0.04716981132075472,
          "f": 0.042016801782360585
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.22857142857142856,
          "f": 0.19277107946000882
        }
      }
    },
    {
      "paper_id": "math.OC.econ/EM/2502.05212v1",
      "true_abstract": "In this paper, we provide analytic expressions for the first-order loss\nfunction, the complementary loss function and the second-order loss function\nfor several probability distributions. These loss functions are important\nfunctions in inventory optimization and other quantitative fields. For several\nreasons, which will become apparent throughout this paper, the implementation\nof these loss functions prefers the use of an analytic expression, only using\nstandard probability functions. However, complete and consistent references of\nanalytic expressions for these loss functions are lacking in literature. This\npaper aims to close this gap and can serve as a reference for researchers,\nsoftware engineers and practitioners that are concerned with the optimization\nof a quantitative system. This should lead directly to easily using different\nprobability distributions in quantitive models which is at the core of\noptimization. Also, this paper serves as a broad introduction to loss functions\nand their use in inventory control.",
      "generated_abstract": "We consider a multivariate regression model with a structural parameter\nand a time-varying covariate, where the time-varying covariate is the\ncorrelation between the parameter and the past values of the covariate. This\nmodel is known as the \"cumulative correlation\" model. We consider a\nmulti-agent model where each agent has a random variable whose correlation with\nthe parameter is controlled by the model parameters. We derive a central limit\ntheorem for the expected value of the random variable. We use this result to\nestimate the parameter and covariate, and we derive the asymptotic properties\nof the estimators. We show that the estimators have the same asymptotic\ndistribution as the central limit theorem and propose a nonparametric\nestimator based on the central limit theorem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16091954022988506,
          "p": 0.22580645161290322,
          "f": 0.18791945822800785
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.02857142857142857,
          "f": 0.024999995078125968
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.1935483870967742,
          "f": 0.16107382064411527
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09599v1",
      "true_abstract": "Primordial Magnetic Fields (PMFs), long studied as potential relics of the\nearly Universe, accelerate the recombination process and have been proposed as\na possible way to relieve the Hubble tension. However, previous studies relied\non simplified toy models. In this study, for the first time, we use the recent\nhigh-precision evaluations of recombination with PMFs, incorporating full\nmagnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative\ntransfer, to test PMF-enhanced recombination ($b\\Lambda$CDM) against\nobservational data from the cosmic microwave background (CMB), baryon acoustic\noscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs\nwith a Batchelor spectrum, we find a preference for present-day total field\nstrengths of approximately 5-10 pico-Gauss. Depending on the dataset\ncombination, this preference ranges from mild ($\\sim 1.8\\sigma$ with Planck +\nDESI) to moderate ($\\sim 3\\sigma$ with Planck + DESI + SH0ES-calibrated SN)\nsignificance. The $b\\Lambda$CDM has Planck + DESI $\\chi^2$ values equal or\nbetter than those of the $\\Lambda$CDM model while predicting a higher Hubble\nconstant. The favored field strengths align closely with those required for\ncluster magnetic fields to originate entirely from primordial sources, without\nthe need for additional dynamo amplification or stellar magnetic field\ncontamination. Future high-resolution CMB temperature and polarization\nmeasurements will be crucial for confirming or further constraining the\npresence of PMFs at recombination.",
      "generated_abstract": "We present a new method to extract information about the thermal and\nevolution of hot Jupiters using the high-resolution spectra of the\nHAT-P-32c system. The method consists in determining the temperature and\nrotational velocity of the star using the spectral lines of the cool\natmosphere and then determining the temperature and velocity of the hot\natmosphere using the lines of the hotter component. We demonstrate the\nefficiency of our method by comparing it with the method proposed by\nHuang et al. (2023) to determine the temperature of the hot atmosphere. We\nalso discuss the implications of our results for the mass of the hot atmosphere\nand the thermal state of the star. Our results show that the temperature of\nthe hot atmosphere of HAT-P-32c is 1100 K, while the star's temperature is\n2500 K.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07741935483870968,
          "p": 0.18181818181818182,
          "f": 0.10859728087877005
        },
        "rouge-2": {
          "r": 0.014563106796116505,
          "p": 0.02857142857142857,
          "f": 0.019292600028950283
        },
        "rouge-l": {
          "r": 0.07096774193548387,
          "p": 0.16666666666666666,
          "f": 0.09954750712311396
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.08924v1",
      "true_abstract": "This article introduces efficient and user-friendly tools for analyzing the\nintersection curve between a ringed torus and an irreducible quadric surface.\nWithout loose of generality, it is assumed that the torus is centered at the\norigin, and its axis of revolution coincides with the $z$-axis. The paper\nprimarily focuses on examining the curve's projection onto the plane $z=0$,\nreferred to as the cutcurve, which is essential for ensuring accurate lifting\nprocedures. Additionally, we provide a detailed characterization of the\nsingularities in both the projection and the intersection curve, as well as the\nexistence of double tangents. A key tool for the analysis is the theory of\nresultant and subresultant polynomials.",
      "generated_abstract": "We prove the main conjecture of the author in [MZ17",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0379746835443038,
          "p": 0.3333333333333333,
          "f": 0.06818181634555791
        },
        "rouge-2": {
          "r": 0.009345794392523364,
          "p": 0.1111111111111111,
          "f": 0.0172413778790132
        },
        "rouge-l": {
          "r": 0.0379746835443038,
          "p": 0.3333333333333333,
          "f": 0.06818181634555791
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.02578v1",
      "true_abstract": "Bird's Eye View (BEV) perception technology is crucial for autonomous\ndriving, as it generates top-down 2D maps for environment perception,\nnavigation, and decision-making. Nevertheless, the majority of current BEV map\ngeneration studies focusing on visual map generation lack depth-aware reasoning\ncapabilities. They exhibit limited efficacy in managing occlusions and handling\ncomplex environments, with a notable decline in perceptual performance under\nadverse weather conditions or low-light scenarios. Therefore, this paper\nproposes TS-CGNet, which leverages Temporal-Spatial fusion with\nCenterline-Guided diffusion. This visual framework, grounded in prior\nknowledge, is designed for integration into any existing network for building\nBEV maps. Specifically, this framework is decoupled into three parts: Local\nmapping system involves the initial generation of semantic maps using purely\nvisual information; The Temporal-Spatial Aligner Module (TSAM) integrates\nhistorical information into mapping generation by applying transformation\nmatrices; The Centerline-Guided Diffusion Model (CGDM) is a prediction module\nbased on the diffusion model. CGDM incorporates centerline information through\nspatial-attention mechanisms to enhance semantic segmentation reconstruction.\nWe construct BEV semantic segmentation maps by our methods on the public\nnuScenes and the robustness benchmarks under various corruptions. Our method\nimproves 1.90%, 1.73%, and 2.87% for perceived ranges of 60x30m, 120x60m, and\n240x60m in the task of BEV HD mapping. TS-CGNet attains an improvement of 1.92%\nfor perceived ranges of 100x100m in the task of BEV semantic mapping. Moreover,\nTS-CGNet achieves an average improvement of 2.92% in detection accuracy under\nvarying weather conditions and sensor interferences in the perception range of\n240x60m. The source code will be publicly available at\nhttps://github.com/krabs-H/TS-CGNet.",
      "generated_abstract": "Recently, the application of Convolutional Neural Networks (CNNs) to\nhigh-resolution medical image segmentation has gained significant interest.\nHowever, despite significant progress, there are still significant limitations\nin terms of both efficiency and performance. One of the most significant\nlimitations in efficiency is the significant computational burden associated\nwith the high-dimensional, multi-channel, and multi-grid nature of medical\nimages. In this work, we propose a novel framework that addresses the computational\nlimitation associated with segmentation of medical images by leveraging the\nspectral properties of the Fourier Transform. Our approach, referred to as\nSpectral-Adaptive CNN (SACN), utilizes the Fourier Transform to perform\nsegmentation of medical images in a highly efficient manner. To this end, we\npropose a novel Spectral-Adaptive CNN architecture that incorporates a\nSpectral Adaptive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09142857142857143,
          "p": 0.2077922077922078,
          "f": 0.1269841227403
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09142857142857143,
          "p": 0.2077922077922078,
          "f": 0.1269841227403
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.02850v1",
      "true_abstract": "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
      "generated_abstract": "The use of machine learning (ML) in healthcare has evolved rapidly, but there\nis still a need for improved understanding of the fundamental challenges\nfaced by practitioners when designing and implementing ML models. In this\narticle, we aim to provide a more nuanced perspective on ML in healthcare,\nhighlighting the complexities and nuances that often go unnoticed by ML\npractitioners. We begin by providing a brief overview of ML and its application\nin healthcare. Next, we discuss the challenges that arise when applying ML in\nhealthcare, highlighting the need for more systematic approaches to mitigate\nthese risks. We then provide a detailed discussion of the potential risks and\nchallenges associated with ML in healthcare, including the potential for\nfalsely positive results, over-fitting, and over-reliance on training data. We\nalso provide a more detailed analysis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18253968253968253,
          "p": 0.27710843373493976,
          "f": 0.2200956889915525
        },
        "rouge-2": {
          "r": 0.005291005291005291,
          "p": 0.008333333333333333,
          "f": 0.006472487158705218
        },
        "rouge-l": {
          "r": 0.15079365079365079,
          "p": 0.2289156626506024,
          "f": 0.18181817702983
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.physics/pop-ph/2502.03191v2",
      "true_abstract": "This study intends to test the hypothesis that, contrary to traditional\ninterpretation, the social structure of the polity of Aksum - especially in its\nearly stages - was not characterized by a vertical hierarchy with highly\ncentralized administrative power, and that the leaders mentioned in the few\navailable inscriptions were predominantly ritual leaders with religious rather\nthan coercive political authority. This hypothesis, suggested by the available\narchaeological evidence, is grounded in Charles Stanish's model, which posits\nthat pre-state societies could achieve cooperative behavior without the\npresence of coercive authority. Using agent-based modeling applied to data\ninspired by the Aksum civilization, we examine the dynamics of cooperation in\nthe presence and absence of a Public Goods Game. Results show that while\ncooperative behavior can emerge in the short term without coercive power, it\nmay not be sustainable over the long term, suggesting a need for centralized\nauthority to foster stable, complex societies. These findings provide insights\ninto the evolutionary pathways that lead to state formation and complex social\nstructures.",
      "generated_abstract": "The growing use of digital tools in scientific research and data analysis\nprovides new opportunities for enhancing the reproducibility of results.\nHowever, the lack of standardized and open repositories makes it difficult for\nresearchers to share their codes and data. This paper introduces the\nREPROCODE-framework, a web-based tool that allows researchers to create and\nshare reproducible code and data. The framework integrates a data store, an\nautomated code checker, and a code review system, allowing researchers to\nautomatically validate their code and data against a set of reproducibility\nstandards. The REPROCODE-framework is open-source and is available at\nhttps://github.com/reprocode-project/reprocode.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11304347826086956,
          "p": 0.19696969696969696,
          "f": 0.14364640420622093
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11304347826086956,
          "p": 0.19696969696969696,
          "f": 0.14364640420622093
        }
      }
    },
    {
      "paper_id": "astro-ph.GA.astro-ph/GA/2503.10087v1",
      "true_abstract": "Galaxy formation models predict that local galaxies are surrounded by hot\nX-ray-emitting halos, which are technically difficult to detect due to their\nextended and low surface brightness nature. Previous X-ray studies have mostly\nfocused on disk galaxies more massive than the Milky Way, with essentially no\nconsensus on the halo X-ray properties at the lower mass end. We utilize the\nearly-released eROSITA and archival Chandra observations to analyze the diffuse\nX-ray emission of NGC7793, a nearby spiral galaxy with an estimated stellar\nmass of only $3.2\\times 10^9$ $M_{\\odot}$. We find evidence for extraplanar hot\ngas emission from both the radial and vertical soft X-ray intensity profiles,\nwhich spreads up to a galactocentric distance of $\\sim$ 6 kpc, nearly 30 $\\%$\nmore extended than its stellar disk. Analysis of the eROSITA spectra indicates\nthat the hot gas can be characterized by a temperature of\n$0.18^{+0.02}_{-0.03}$ keV, with 0.5--2 keV unabsorbed luminosity of $1.3\\times\n10^{38}$ erg $s^{-1}$. We compare our results with the IllustrisTNG simulations\nand find overall consistence on the disk scale, whereas excessive emission at\nlarge radii is predicted by TNG50. This work provides the latest detection of\nhot corona around a low-mass galaxy, putting new constrains on state-of-the-art\ncosmological simulations. We also verify the detectability of hot\ncircumgalactic medium around even low-mass spirals with future high-resolution\nX-ray spectrometer such as the Hot Universe Baryon Surveyor.",
      "generated_abstract": "We present the first detailed study of the evolution of the mass distribution\nin the protoplanetary discs of the $\\sim 10^4$ M$_{\\odot}$ star-forming galaxies\nin the GOODS-N and GOODS-S fields of the Two-Micron All-Sky Survey (2MASS)\nsurvey. We use the SED fitting technique to obtain the mass and age of the\ndiscs. The sample comprises 110, 109, and 123 galaxies at z$<$0.3, 0.3-0.5,\nand 0.5-1.0, respectively, with a total of 3077, 1530, and 1628 galaxies with\nHST imaging. The 2MASS-derived masses and ages are in good agreement with\nprevious estimates using the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07317073170731707,
          "p": 0.18181818181818182,
          "f": 0.10434782199470716
        },
        "rouge-2": {
          "r": 0.0043859649122807015,
          "p": 0.011111111111111112,
          "f": 0.006289304117719481
        },
        "rouge-l": {
          "r": 0.06707317073170732,
          "p": 0.16666666666666666,
          "f": 0.09565216982079414
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.07061v1",
      "true_abstract": "We consider a group of agents who can each take an irreversible costly action\nwhose payoff depends on an unknown state. Agents learn about the state from\nprivate signals, as well as from past actions of their social network\nneighbors, which creates an incentive to postpone taking the action. We show\nthat outcomes depend on network structure: on networks with a linear structure\npatient agents do not converge to the first-best action, while on regular\ndirected tree networks they do.",
      "generated_abstract": "We examine the use of randomized controlled trials to estimate the\nimpact of interventions. We focus on a novel method for estimating\n\\textit{per-unit} impact, which takes into account the number of units affected\nby the intervention. We provide a theoretical analysis of the per-unit\nimpact estimator, showing that it is consistent and asymptotically normal.\nFurther, we show that the per-unit impact estimator is a consistent estimator\nof the treatment effect when the treatment effect is monotone. We also\nestablish a connection between the per-unit impact estimator and the\nestimation of the average treatment effect using randomized controlled trials.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14754098360655737,
          "p": 0.15517241379310345,
          "f": 0.15126049920485857
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.011764705882352941,
          "f": 0.012195116957914
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.13793103448275862,
          "f": 0.134453776515783
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05674v3",
      "true_abstract": "Driven by advances in self-supervised learning for speech, state-of-the-art\nsynthetic speech detectors have achieved low error rates on popular benchmarks\nsuch as ASVspoof. However, prior benchmarks do not address the wide range of\nreal-world variability in speech. Are reported error rates realistic in\nreal-world conditions? To assess detector failure modes and robustness under\ncontrolled distribution shifts, we introduce ShiftySpeech, a benchmark with\nmore than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12\nvocoders, and 3 languages. We found that all distribution shifts degraded model\nperformance, and contrary to prior findings, training on more vocoders,\nspeakers, or with data augmentation did not guarantee better generalization. In\nfact, we found that training on less diverse data resulted in better\ngeneralization, and that a detector fit using samples from a single carefully\nselected vocoder and a small number of speakers, without data augmentations,\nachieved state-of-the-art results on the challenging In-the-Wild benchmark.",
      "generated_abstract": "In the past decades, deep learning has been applied in various domains,\nincluding healthcare. However, the application of deep learning in healthcare\nrequires significant efforts in data collection, preprocessing, and\nannotation. In this paper, we propose a novel deep learning framework for\nimproving the quality of medical image segmentation. Our framework integrates\nthe CNN model with a U-Net architecture. We propose a novel algorithm that\ncombines the U-Net architecture with a novel method for the enhancement of the\nsegmentation quality. This algorithm is called U-Net-Enhance. It enhances the\nsegmentation quality by improving the classification of the background. In\naddition, we propose a novel loss function for the enhancement of the segmentation\nquality. This loss function is called U-Net-Enhance-Loss. The performance of\nour proposed algorithm and loss function is evaluated on a large dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1651376146788991,
          "p": 0.25,
          "f": 0.19889502283324698
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.25,
          "f": 0.19889502283324698
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.06651v1",
      "true_abstract": "This paper explores the emerging research direction of electromagnetic\ninformation theory (EIT), which aims to integrate traditional Shannon-based\nmethodologies with physical consistency, particularly the electromagnetic\nproperties of communication channels. We propose an EIT-based multiple-input\nmultiple-output (MIMO) paradigm that enhances conventional spatially-discrete\nMIMO models by incorporating the concepts of electromagnetic (EM) precoding and\nEM combining. This approach aims to improve the modeling of next-generation\nsystems while remaining consistent with Shannon's theoretical foundations. We\nexplore typical EIT applications, such as densely spaced MIMO, near-field\ncommunications, and tri-polarized antennas, and analyze their channel\ncharacteristics through theoretical simulations and measured datasets. The\npaper also discusses critical research challenges and opportunities for EIT\napplications from an industrial perspective, emphasizing the field's potential\nfor practical applications.",
      "generated_abstract": "In the context of the Internet of Things, the emergence of wireless sensors\nincreases the complexity of the wireless networks. The interference between\nsensors and wireless access points (APs) is one of the main challenges in\nwireless networks, leading to poor performance and higher energy consumption.\nThe design of energy-efficient sensors and APs is therefore crucial for\nenhancing the performance of wireless networks. This study investigates the\neffect of the number of sensors and APs on the energy efficiency in the\nInternet of Things (IoT). The proposed approach is based on the analysis of\nthe energy efficiency of a wireless network using the Maximum Mean Discrepancy\n(MMD) metric. The MMD is a metric that measures the difference between the\nmaximum and minimum values of an input distribution and the maximum of a\ntarget distribution. This metric is used to evaluate the similarity between\ndistributions. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11578947368421053,
          "p": 0.14666666666666667,
          "f": 0.1294117597750867
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.13333333333333333,
          "f": 0.11764705389273376
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.12833v1",
      "true_abstract": "In areas infested with Aedes aegypti mosquitoes it may be possible to control\ndengue, and some other vector-borne diseases, by introducing Wolbachia-infected\nmosquitoes into the wildtype population. Thus far, empirical and theoretical\nstudies of Wolbachia release have tended to focus on the dynamics at the\ncommunity scale. However, Ae. aegypti mosquitoes typically dwell in and around\nthe same houses as the people they bite and it can be insightful to explore\nwhat happens at the household scale where small population sizes lead to\ninherently stochastic dynamics. Here we use a continuous-time Markov framework\nto develop a stochastic household model for small populations of wildtype and\nWolbachia-infected mosquitoes. We investigate the transient and long term\ndynamics of the system, in particular examining the impact of stochasticity on\nthe Wolbachia invasion threshold and bistability between the wildtype-only and\nWolbachia-only steady states previously observed in deterministic models. We\nfocus on the influence of key parameters which determine the fitness cost of\nWolbachia infection and the probability of Wolbachia vertical transmission.\nUsing Markov and matrix population theory, we derive salient characteristics of\nthe system including the probability of successful Wolbachia invasion, the\nexpected time until invasion and the probability that a Wolbachia-infected\npopulation reverts to a wildtype population. These attributes can inform\nstrategies for the release of Wolbachia-infected mosquitoes. In addition, we\nfind that releasing the minimum number of Wolbachia-infected mosquitoes\nrequired to displace a resident wildtype population according to the\ndeterministic model, only results in that outcome about 20% of the time in the\nstochastic model; a significantly larger release is required to reach a steady\nstate composed entirely of Wolbachia-infected mosquitoes 90% of the time.",
      "generated_abstract": "The use of single-cell RNA-sequencing (scRNA-seq) data in the biomedical\nscenario has led to a rapid evolution of the field. ScRNA-seq technologies\nhave increased from a few hundred cells to over a million and the number of\ngenes per cell has increased from hundreds to thousands. This paper presents a\nframework to develop a reliable and reproducible pipeline for scRNA-seq data\nanalysis. The framework consists of a high-throughput pipeline that uses\nintegrated computational methods and automated workflows to extract the most\nrelevant information from scRNA-seq data. The framework is flexible enough to\nbe applied to any experimental set-up and can be adapted to different\nscRNA-seq datasets. The paper describes how the framework is applied to\nexperimental data, highlighting its efficiency and reproducibility. The\nframework is applied to data from the Lung Cancer",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10457516339869281,
          "p": 0.2,
          "f": 0.13733905128479082
        },
        "rouge-2": {
          "r": 0.036290322580645164,
          "p": 0.07377049180327869,
          "f": 0.04864864422848836
        },
        "rouge-l": {
          "r": 0.09803921568627451,
          "p": 0.1875,
          "f": 0.12875536029766638
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08094v1",
      "true_abstract": "Medical image denoising is essential for improving the reliability of\nclinical diagnosis and guiding subsequent image-based tasks. In this paper, we\npropose a multi-scale approach that integrates anisotropic Gaussian filtering\nwith progressive Bezier-path redrawing. Our method constructs a scale-space\npyramid to mitigate noise while preserving critical structural details.\nStarting at the coarsest scale, we segment partially denoised images into\ncoherent components and redraw each using a parametric Bezier path with\nrepresentative color. Through iterative refinements at finer scales, small and\nintricate structures are accurately reconstructed, while large homogeneous\nregions remain robustly smoothed. We employ both mean square error and\nself-intersection constraints to maintain shape coherence during path\noptimization. Empirical results on multiple MRI datasets demonstrate consistent\nimprovements in PSNR and SSIM over competing methods. This coarse-to-fine\nframework offers a robust, data-efficient solution for cross-domain denoising,\nreinforcing its potential clinical utility and versatility. Future work extends\nthis technique to three-dimensional data.",
      "generated_abstract": "While the development of deep learning has significantly improved the\nprocess of image segmentation, the limitations of the deep learning-based\nmethods still remain. These limitations are mainly caused by the large\noverfitting and the lack of generalization ability of the deep learning-based\nmodels. In this paper, we propose a novel framework for image segmentation,\nwhich is based on the fusion of the two existing methods: the Mask R-CNN and\nthe Point-based Convolutional Neural Networks (PCNN). By fusing the two\npretrained models, the proposed method significantly improves the performance\nof the segmentation task. Furthermore, the proposed method can be applied to\nother domain-specific tasks without the need to train from scratch. Extensive\nexperiments demonstrate that our method can achieve superior performance on\nboth the object detection and segmentation tasks, and outperforms the\nstate-of-the-art methods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.183206106870229,
          "p": 0.2696629213483146,
          "f": 0.2181818133640497
        },
        "rouge-2": {
          "r": 0.03355704697986577,
          "p": 0.04132231404958678,
          "f": 0.03703703209080999
        },
        "rouge-l": {
          "r": 0.17557251908396945,
          "p": 0.25842696629213485,
          "f": 0.20909090427314062
        }
      }
    },
    {
      "paper_id": "physics.data-an.physics/data-an/2503.09415v1",
      "true_abstract": "SPARKX is an open-source Python package developed to analyze simulation data\nfrom heavy-ion collision experiments. By offering a comprehensive suite of\ntools, SPARKX simplifies data analysis workflows, supports multiple formats\nsuch as OSCAR2013, and integrates seamlessly with SMASH and JETSCAPE/X-SCAPE.\nThis paper describes SPARKX's architecture, features, and applications and\ndemonstrates its effectiveness through detailed examples and performance\nbenchmarks. SPARKX enhances productivity and precision in relativistic\nkinematics studies.",
      "generated_abstract": "We propose a novel approach to the problem of predicting the time-dependent\nsoliton solutions of the 2D Korteweg-de Vries (KdV) equation, which is a\nwell-studied model of nonlinear oscillations. We introduce an artificial\npotential function to represent the soliton solutions and apply the\nBernstein-Kaup transform to this potential to obtain a discrete spectrum of\nsoliton solutions. We then construct a discrete numerical model for the\nKdV equation based on the Bernstein-Kaup transform. This model can be used to\nsimulate the soliton solutions. We compare the numerical results with the\nsoliton solutions obtained from the discrete spectrum obtained by the\nBernstein-Kaup transform. Our numerical results indicate that the\ndiscrete-time soliton solutions have a better fit to the soliton solutions than\nthe continuous-time soliton solutions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15254237288135594,
          "p": 0.13636363636363635,
          "f": 0.14399999501568017
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13559322033898305,
          "p": 0.12121212121212122,
          "f": 0.1279999950156802
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.18923v1",
      "true_abstract": "Slutsky symmetry and negative semidefiniteness are necessary and sufficient\nconditions for the rationality of demand functions. While the empirical\nimplications of Slutsky negative semidefiniteness in repeated cross-sectional\ndemand data are well understood, the empirical content of Slutsky symmetry\nremains largely unexplored. This paper takes an important first step toward\naddressing this gap. We demonstrate that the average Slutsky matrix is not\nidentified and that its identified set always contains a symmetric matrix. A\nkey implication of our findings is that the symmetry of the average Slutsky\nmatrix is untestable, and consequently, individual Slutsky symmetry cannot be\ntested using the average Slutsky matrix.",
      "generated_abstract": "In this paper, we analyze the dynamic stochastic general equilibrium model\nunder the non-stationary condition. We provide a detailed analysis of the\nbehavior of the dynamic stochastic general equilibrium model with the\nnon-stationary condition in different situations. First, we establish the\nasymptotic behavior of the dynamic stochastic general equilibrium model with\nthe non-stationary condition. Second, we provide a detailed analysis of the\nbehavior of the dynamic stochastic general equilibrium model with the non-stationary\ncondition in different situations. In particular, we study the effect of\ncointegration on the dynamic stochastic general equilibrium model with the non-\nstationary condition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09230769230769231,
          "p": 0.16666666666666666,
          "f": 0.1188118766003335
        },
        "rouge-2": {
          "r": 0.011363636363636364,
          "p": 0.02,
          "f": 0.014492749002311912
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.1388888888888889,
          "f": 0.09900989640231371
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2502.11510v1",
      "true_abstract": "Longitudinal models with dynamics governed by differential equations may\nrequire numerical integration alongside parameter estimation. We have\nidentified a situation where the numerical integration introduces error in such\na way that it becomes a novel source of non-uniqueness in estimation. We obtain\ntwo very different sets of parameters, one of which is a good estimate of the\ntrue values and the other a very poor one. The two estimates have forward\nnumerical projections statistically indistinguishable from each other because\nof numerical error. In such cases, the posterior distribution for parameters is\nbimodal, with a dominant mode closer to the true parameter value, and a second\ncluster around the errant value. We demonstrate that bimodality exists both\ntheoretically and empirically for an affine first order differential equation,\nthat a simulation workflow can test for evidence of the issue more generally,\nand that Markov Chain Monte Carlo sampling with a suitable solution can avoid\nbimodality. The issue of bimodal posteriors arising from numerical error has\nconsequences for Bayesian inverse methods that rely on numerical integration\nmore broadly.",
      "generated_abstract": "This paper presents a novel methodology for estimating and comparing the\nstatistical properties of a set of random variables. The methodology is based\non the construction of a generalized version of the Fr\\'echet-Laplace\ndistribution, which is used to characterize the joint density of the random\nvariables. This distribution is then used to derive a finite-dimensional\napproximation to the extended Kullback-Leibler divergence between the joint\ndensity of the random variables and a fixed reference distribution. The\napproximation is validated through numerical experiments and demonstrates\neffective performance when compared to commonly used statistical tests for\nestimating and comparing the joint density of a set of random variables.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11403508771929824,
          "p": 0.24074074074074073,
          "f": 0.15476190039965998
        },
        "rouge-2": {
          "r": 0.029940119760479042,
          "p": 0.06097560975609756,
          "f": 0.040160638152933505
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.2222222222222222,
          "f": 0.1428571384948981
        }
      }
    },
    {
      "paper_id": "math.NA.stat/CO/2502.07918v2",
      "true_abstract": "Stochastic reaction networks (SRNs) model stochastic effects for various\napplications, including intracellular chemical or biological processes and\nepidemiology. A typical challenge in practical problems modeled by SRNs is that\nonly a few state variables can be dynamically observed. Given the measurement\ntrajectories, one can estimate the conditional probability distribution of\nunobserved (hidden) state variables by solving a stochastic filtering problem.\nIn this setting, the conditional distribution evolves over time according to an\nextensive or potentially infinite-dimensional system of coupled ordinary\ndifferential equations with jumps, known as the filtering equation. The current\nnumerical filtering techniques, such as the Filtered Finite State Projection\n(D'Ambrosio et al., 2022), are hindered by the curse of dimensionality,\nsignificantly affecting their computational performance. To address these\nlimitations, we propose to use a dimensionality reduction technique based on\nthe Markovian projection (MP), initially introduced for forward problems (Ben\nHammouda et al., 2024). In this work, we explore how to adapt the existing MP\napproach to the filtering problem and introduce a novel version of the MP, the\nFiltered MP, that guarantees the consistency of the resulting estimator. The\nnovel method employs a reduced-variance particle filter for estimating the jump\nintensities of the projected model and solves the filtering equations in a\nlow-dimensional space. The analysis and empirical results highlight the\nsuperior computational efficiency of projection methods compared to the\nexisting filtered finite state projection in the large dimensional setting.",
      "generated_abstract": "We study the problem of estimating the mean and variance of a random vector of\nfunctionals of a random variable. This problem arises in various fields,\nincluding statistics, probability theory, and machine learning. The\nestimation problem is typically formulated as a maximum likelihood\nestimation, which is well-posed if the random variable is conditionally\nindependent given the values of the functionals. We consider the case where\nthe random variable is conditionally dependent, that is, the conditional\ndistribution of the random variable depends on the values of the functionals. We\npropose a method for estimating the conditional mean and variance given the\nfunctionals, and establish conditions under which this estimator is\nwell-defined. We also provide a condition under which the estimator is\nconsistent, and discuss the asymptotic properties of the estimator under\nsuitable conditions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13836477987421383,
          "p": 0.3188405797101449,
          "f": 0.19298245191943686
        },
        "rouge-2": {
          "r": 0.0319634703196347,
          "p": 0.06542056074766354,
          "f": 0.04294478086623554
        },
        "rouge-l": {
          "r": 0.11949685534591195,
          "p": 0.2753623188405797,
          "f": 0.16666666244575265
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16525v1",
      "true_abstract": "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
      "generated_abstract": "The development of accurate computational models for the human brain has\nbeen one of the most challenging tasks in neuroscience. A major challenge is\nthe need for accurate modeling of the neurochemical network, which is a key\ncomponent of the brain. In this study, we introduce a new type of neural\nnetwork that we call \"Neurochemical Network-Based Neural Networks (NBCNNs).\"\nThis network-based framework enables the integration of chemical and neuronal\nnetworks into a unified framework, which is particularly valuable in the\nexploration of neurochemical pathways. We applied our model to study the\neffects of different pharmacological treatments on brain chemistry and\nfunctionality. We developed a novel framework for simulating neurochemical\nnetworks based on the principles of chemical reaction networks, and we\nintroduced a new class of neural networks called Neurochemical Network",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17482517482517482,
          "p": 0.2840909090909091,
          "f": 0.21645021173366324
        },
        "rouge-2": {
          "r": 0.03208556149732621,
          "p": 0.048,
          "f": 0.03846153365898318
        },
        "rouge-l": {
          "r": 0.16083916083916083,
          "p": 0.26136363636363635,
          "f": 0.19913419441764593
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.00920v2",
      "true_abstract": "This paper contributes to the literature on parametric demand estimation by\nusing deep learning to model consumer preferences. Traditional econometric\nmethods often struggle with limited within-product price variation, a challenge\naddressed by the proposed neural network approach. The proposed method\nestimates the functional form of the demand and demonstrates higher performance\nin both simulations and empirical applications. Notably, under low price\nvariation, the machine learning model outperforms econometric approaches,\nreducing the mean squared error of initial price parameter estimates by nearly\nthreefold. In empirical setting, the ML model consistently predicts a negative\nrelationship between demand and price in 100% of cases, whereas the econometric\napproach fails to do so in 20% of cases. The suggested model incorporates a\nwide range of product characteristics, as well as prices of other products and\ncompetitors.",
      "generated_abstract": "This paper explores the role of government policy in the development of\neconomic agents. We use the theory of economic agency to explore the\nperformance of individuals as economic agents. We first define a new measure of\nagent performance as the agent's contribution to the economy, which we show\nrepresents an alternative to the commonly used concept of \"growth.\" We then\npropose an interpretation of this measure as a measure of the economic\nagent's \"willingness to lend.\" This interpretation provides a natural\ninterpretation of the notion of \"willingness to lend\" in a monetary economy,\nallowing us to analyze how economic agents are willing to lend and how their\nlending decisions are affected by government policy. We show that these\nagent-level measures are in principle distinct from the traditional notion of\n\"willingness to lend,\" and we show that these alternative measures are\nnecess",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12087912087912088,
          "p": 0.14864864864864866,
          "f": 0.13333332838640977
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.024,
          "f": 0.023622042245335546
        },
        "rouge-l": {
          "r": 0.12087912087912088,
          "p": 0.14864864864864866,
          "f": 0.13333332838640977
        }
      }
    },
    {
      "paper_id": "math.AG.math/AC/2503.01752v1",
      "true_abstract": "Border basis schemes are open subschemes of the Hilbert scheme of $\\mu$\npoints in an affine space $\\mathbb{A}^n$. They have easily describable systems\nof generators of their vanishing ideals for a natural embedding into a large\naffine space $\\mathbb{A}^{\\mu\\nu}$. Here we bring together several techniques\nfor re-embedding affine schemes into lower dimensional spaces which we\ndeveloped in the last years. We study their efficacy for some special types of\nborder basis schemes such as MaxDeg border basis schemes, L-shape and\nsimplicial border basis schemes, as well as planar border basis schemes. A\nparticular care is taken to make these re-embeddings efficiently computable and\nto check when we actually get an isomorphism with $\\mathbb{A}^{n\\mu}$, i.e.,\nwhen the border basis scheme is an affine cell.",
      "generated_abstract": "We prove a result on the existence of a solution of a certain differential\nequations for the case when the parameter is a unitary matrix in a finite-dimensional\nHilbert space. The main result of this paper is a non-existence result for the\ncase when the parameter is a non-unitary matrix in a finite-dimensional\nHilbert space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.3448275862068966,
          "f": 0.17543859269775322
        },
        "rouge-2": {
          "r": 0.008695652173913044,
          "p": 0.025,
          "f": 0.012903221977108316
        },
        "rouge-l": {
          "r": 0.10588235294117647,
          "p": 0.3103448275862069,
          "f": 0.15789473304863044
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.nlin/CD/2503.08905v1",
      "true_abstract": "Sub-Neptunes have been found to be one of the most common types of\nexoplanets, yet their physical parameters and properties are poorly determined\nand in need of further investigation. In order to improve the mass measurement\nand parameter determination of two sub-Neptunes, K2-266 d and K2-266 e, we\npresent new transit observations obtained with CHaracterising ExOPlanets\nSatellite (CHEOPS) and Transiting Exoplanet Survey Satellite (TESS), increasing\nthe baseline of transit data from a few epochs to 165 epochs for K2-266 d, and\nto 121 epochs for K2-266 e. Through a two-stage fitting process, it is found\nthat the masses of K2-266 d and K2-266 e are 6.01$\\pm$0.43 $M_\\oplus$ and\n7.70$\\pm$0.58 $M_\\oplus$, respectively. With these updated values and one order\nof magnitude better precision, we confirm the planets to belong to the\npopulation of planets that has been determined to be volatile-rich. Finally, we\npresent the results of dynamical simulations, showing that the system is\nstable, the orbits are not chaotic, and that these two planets are close to but\nnot in 4:3 mean motion resonance.",
      "generated_abstract": "We present a novel method to determine the magnetic energy distribution in\nstellar dynamos. We propose a method to identify the magnetic energy distribution\nusing the Fourier spectrum of the time series of magnetic field data. Our\nmethodology is based on the idea that the Fourier spectrum is a measure of the\ntime-dependent magnetic energy distribution and that the Fourier spectrum of a\ntime series can be decomposed into two components: the magnetic energy\ndistribution at low frequencies and the cosmic-ray energy distribution at high\nfrequencies. We propose that the magnetic energy distribution can be\nestimated using the Fourier spectrum of the time series of the magnetic field.\nWe show that the magnetic energy distribution can be obtained from the Fourier\nspectrum of the time series of the magnetic field. We validate our method by\nusing the time series of the magnetic field of the Sun, where we can measure\nthe Fourier spectrum. We show that our method can accurately estimate the\nmagnet",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13513513513513514,
          "p": 0.25862068965517243,
          "f": 0.17751478839116291
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.03125,
          "f": 0.022727268099174493
        },
        "rouge-l": {
          "r": 0.11711711711711711,
          "p": 0.22413793103448276,
          "f": 0.15384614933790847
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.08457v1",
      "true_abstract": "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
      "generated_abstract": "We study the cohomology of the group of $2$-cocycles of a topological\ngroup $G$, with coefficients in the group of $2$-cocycles of an abelian group\n$A$. The main result of the paper is a new description of the cohomology of the\ngroup of $2$-cocycles of $G$ with coefficients in $A$ in terms of a\n$\\mathbb{Z}/2\\mathbb{Z}$-graded Lie algebra $L(G,A)$ with an action of $G$ and\nwith a representation of $A$ on $L(G,A)$. We show that the cohomology of the\ngroup of $2$-cocycles of $G$ with coefficients in $A$ is the kernel of the\nquasi-isomorphism between $L(G,A)$ and the universal cover of the Lie algebra\n$L(\\tilde{G},A)$ of the group",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.2926829268292683,
          "f": 0.20869564758563336
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.05555555555555555,
          "f": 0.04395603917401333
        },
        "rouge-l": {
          "r": 0.14864864864864866,
          "p": 0.2682926829268293,
          "f": 0.19130434323780732
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.08839v1",
      "true_abstract": "We introduce a new topological coproduct $\\Delta^{\\psi}_{u}$ for quantum\ntoroidal algebras $U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$ in untwisted types,\nleading to a well-defined tensor product on the category\n$\\widehat{\\mathcal{O}}_{\\mathrm{int}}$ of integrable representations. This is\ndefined by twisting the Drinfeld coproduct $\\Delta_{u}$ with an anti-involution\n$\\psi$ of $U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$ that swaps its horizontal and\nvertical quantum affine subalgebras. Other applications of $\\psi$ include\ngeneralising the celebrated Miki automorphism from type $A$, and an action of\nthe universal cover of $SL_{2}(\\mathbb{Z})$.\n  Next, we investigate the ensuing tensor representations of\n$U_{q}(\\mathfrak{g}_{\\mathrm{tor}})$, and prove quantum toroidal analogues for\na series of influential results by Chari-Pressley on the affine level. In\nparticular, there is a compatibility with Drinfeld polynomials, and the product\nof irreducibles is generically irreducible. Furthermore, we obtain $R$-matrices\nwith spectral parameter which provide solutions to the (trigonometric, quantum)\nYang-Baxter equation, and endow $\\widehat{\\mathcal{O}}_{\\mathrm{int}}$ with a\nmeromorphic braiding. These moreover give rise to a commuting family of\ntransfer matrices for each module.",
      "generated_abstract": "We prove that the limit of the distribution of the number of points in a\npoint process in a complete Riemannian manifold with a boundary is a\nprobability measure. This result generalizes the well-known fact that the\nlimit of the distribution of the number of points in the point process on the\nunit sphere in $\\mathbb R^n$ is the uniform measure on the sphere. In the case\nof the Poisson point process on the unit sphere, we show that the limit is\nconverging to a Poisson point process with intensity given by the metric\ntangential energy. We also show that the limit is the Poisson point process on\nthe unit ball with intensity given by the metric mean curvature. We prove that\nthe limit is the Poisson point process on the unit ball with intensity given by\nthe metric mean curvature and the boundary measure. We give a brief survey of\nthe history of these results and of the work of H. Po",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17142857142857143,
          "p": 0.3,
          "f": 0.2181818135537191
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.06060606060606061,
          "f": 0.048192766294092505
        },
        "rouge-l": {
          "r": 0.1619047619047619,
          "p": 0.2833333333333333,
          "f": 0.20606060143250698
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2502.14160v1",
      "true_abstract": "In this paper, we study inverse game theory (resp. inverse multiagent\nlearning) in which the goal is to find parameters of a game's payoff functions\nfor which the expected (resp. sampled) behavior is an equilibrium. We formulate\nthese problems as generative-adversarial (i.e., min-max) optimization problems,\nfor which we develop polynomial-time algorithms to solve, the former of which\nrelies on an exact first-order oracle, and the latter, a stochastic one. We\nextend our approach to solve inverse multiagent simulacral learning in\npolynomial time and number of samples. In these problems, we seek a simulacrum,\nmeaning parameters and an associated equilibrium that replicate the given\nobservations in expectation. We find that our approach outperforms the\nwidely-used ARIMA method in predicting prices in Spanish electricity markets\nbased on time-series data.",
      "generated_abstract": "We study the equilibrium allocation of a family of goods among agents who\nare indifferent between goods, but can purchase only one. The agents are\nindependent and have perfect information about the number of goods to purchase\nand the prices of each good. We find that the equilibrium is unique,\nidentically distributed, and non-monotonic. The non-monotonicity is due to\nagent preferences: if an agent prefers one good more than the others, then she\nprefers that good to the other goods, regardless of their prices.\n  We also show that the equilibrium is a fixed point of the Nash\nequilibrium-finding algorithm, and we provide a heuristic for finding the\nequilibrium quickly. We also find a class of families of goods that admit an\nequilibrium with non-monotonic allocation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19540229885057472,
          "p": 0.2361111111111111,
          "f": 0.2138364730319213
        },
        "rouge-2": {
          "r": 0.04838709677419355,
          "p": 0.05172413793103448,
          "f": 0.049999995005556054
        },
        "rouge-l": {
          "r": 0.19540229885057472,
          "p": 0.2361111111111111,
          "f": 0.2138364730319213
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2404.15226v3",
      "true_abstract": "We revisit granular models that represent the size of a firm as the sum of\nthe sizes of multiple constituents or sub-units. Originally developed to\naddress the unexpectedly slow reduction in volatility as firm size increases,\nthese models also explain the shape of the distribution of firm growth rates.\n  We introduce new theoretical insights regarding the relationship between firm\nsize and growth rate statistics within this framework, directly linking the\ngrowth statistics of a firm to how diversified it is. The non-intuitive nature\nof our results arises from the fat-tailed distributions of the size and the\nnumber of sub-units, which suggest the categorization of firms into three\ndistinct diversification types: well-diversified firms with sizes evenly\ndistributed across many sub-units, firms with many sub-units but concentrated\nsize in just a few, and poorly diversified firms consisting of only a small\nnumber of sub-units.\n  Inspired by our theoretical findings, we identify new empirical patterns in\nfirm growth. Our findings show that growth volatility, when adjusted by average\nsize-conditioned volatility, has a size-independent distribution, but with a\ntail that is much too thin to be in agreement with the predictions of granular\nmodels. Furthermore, the predicted Gaussian distribution of growth rates, even\nwhen rescaled for firm-specific volatility, remains fat-tailed across all\nsizes. Such discrepancies not only challenge the granularity hypothesis but\nalso underscore the need for deeper exploration into the mechanisms driving\nfirm growth.",
      "generated_abstract": "This paper introduces a novel methodology for the analysis of stock\ncommuting patterns. Using a data-driven approach, we identify the stocks that\nexhibit the highest level of inter-day commuting activity. The analysis is\nconducted at the company level, by aggregating stock returns of 12,426\ncompanies across the S&P 500 over the period 2009-2021. We then develop a\nstatistical framework to characterize inter-day commuting patterns. The\nmethodology is based on a novel time-varying correlation framework that\naccounts for the influence of both the cross-sectional and the longitudinal\neffects of stock returns on commuting patterns. The findings reveal a\nsignificant association between stock returns and commuting activity. We\ncharacterize this relationship in terms of a correlation coefficient and a\nstatistical",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.2857142857142857,
          "f": 0.19512194672218927
        },
        "rouge-2": {
          "r": 0.013824884792626729,
          "p": 0.02830188679245283,
          "f": 0.018575846983677684
        },
        "rouge-l": {
          "r": 0.14074074074074075,
          "p": 0.2714285714285714,
          "f": 0.18536584916121368
        }
      }
    },
    {
      "paper_id": "quant-ph.math/IT/2503.08736v1",
      "true_abstract": "We provide a streamlined elaboration on existing ideas that link Ising anyon\n(or equivalently, Majorana) stabilizer codes to certain classes of binary\nclassical codes. The groundwork for such Majorana-based quantum codes can be\nfound in earlier works (including, for example, Bravyi (arXiv:1004.3791) and\nVijay et al. (arXiv:1703.00459)), where it was observed that commuting families\nof fermionic (Clifford) operators can can often be systematically lifted from\nweakly self-dual or self-orthogonal binary codes. Here, we recast and unify\nthese ideas into a classification theorem that explicitly shows how explicitly\nshows how q-isotropic subspaces in $\\mathbb{F}_2^{2n}$ yield commuting Clifford\noperators relevant to Ising anyons, and how these subspaces naturally\ncorrespond to punctured self-orthogonal codes in $\\mathbb{F}_2^{2n+1}$.",
      "generated_abstract": "We study a non-local quantum channel with a fixed input and fixed output\nquantum systems, and a fixed output state. We consider a Gaussian channel,\nwhere the input state is assumed to be a product state of the form\n$\\rho_{in} = \\sum_i p_i \\ketbra{e_i}{e_i}$. The output state is given by\n$\\rho_o = \\sum_i q_i \\ketbra{e_i}{e_i}$ where $p_i = \\braket{e_i|e_i}$ and\n$q_i = \\braket{e_i|e_i}\\braket{e_i|e_i}$. The channel is non-local in the\nsense that the output state depends on the input state only through the\ncorrelation function $p_i q_i$. The channel is non-local in the sense that the\noutput",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1411764705882353,
          "p": 0.24489795918367346,
          "f": 0.1791044729728226
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.22448979591836735,
          "f": 0.16417909983849424
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2502.14497v1",
      "true_abstract": "Macroeconomic fluctuations and the narratives that shape them form a mutually\nreinforcing cycle: public discourse can spur behavioural changes leading to\neconomic shifts, which then result in changes in the stories that propagate. We\nshow that shifts in semantic embedding space can be causally linked to\nfinancial market shocks -- deviations from the expected market behaviour.\nFurthermore, we show how partisanship can influence the predictive power of\ntext for market fluctuations and shape reactions to those same shocks. We also\nprovide some evidence that text-based signals are particularly salient during\nunexpected events such as COVID-19, highlighting the value of language data as\nan exogenous variable in economic forecasting. Our findings underscore the\nbidirectional relationship between news outlets and market shocks, offering a\nnovel empirical approach to studying their effect on each other.",
      "generated_abstract": "We introduce a novel approach to modeling financial risk. We propose a\nmodel that combines a state-of-the-art risk-adjustment technique, the\nConditional Value-at-Risk (CVaR), with a novel, state-specific model of\nfinancial shocks. The model, which we call the Financial CVaR (FCV), captures\nthe financial shocks by modeling their conditional distributions. We show that\nthe FCV has a similar distribution as the CVA, and we prove that the FCV\npredicts the CVA within a factor of two. We evaluate the performance of the\nFCV using a large dataset of financial shocks, including those from the\n2008 financial crisis. Our results show that the FCV outperforms CVA in terms\nof predictive accuracy, with an error of less than 10 percent. The FCV\nprovides a more",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22772277227722773,
          "p": 0.2948717948717949,
          "f": 0.2569832353060142
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.04424778761061947,
          "f": 0.041152258398957406
        },
        "rouge-l": {
          "r": 0.2079207920792079,
          "p": 0.2692307692307692,
          "f": 0.23463686659093044
        }
      }
    },
    {
      "paper_id": "math-ph.math/SP/2503.08595v1",
      "true_abstract": "In this expository note, we study several families of periodic graphs which\nsatisfy a sufficient condition for the ergodicity of the associated\ncontinuous-time quantum walk. For these graphs, we compute the limiting\ndistribution of the walk explicitly. We uncover interesting behavior where in\nsome families, the walk is ergodic in both horizontal and sectional directions,\nwhile in others, ergodicity only holds in the horizontal (large N) direction.\nWe compare this to the limiting distribution of classical random walks on the\nsame graphs.",
      "generated_abstract": "We study the spectral properties of the 2D generalized Korteweg-de\n(V)reyt-Pauli operator, which is a generalization of the Korteweg-de (V)reyt\noperator. We study the spectral properties of the 2D generalized Korteweg-de\n(V)reyt-Pauli operator. We first prove the existence of the spectral radius of\nthe operator, and then study its spectrum. We show that for a general operator\n$A$, the spectrum of $A$ is a union of the sets of eigenvalues of all the\nsub-matrices of $A$. We also show that the operator $A$ is a polynomial\noperator if and only if its spectrum is contained in the real line. In this\ncase, we show that the spectrum of $A$ is a union of the eigenvalues of all\nthe sub-matrices of $A$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23728813559322035,
          "p": 0.2916666666666667,
          "f": 0.26168223804349733
        },
        "rouge-2": {
          "r": 0.039473684210526314,
          "p": 0.036585365853658534,
          "f": 0.03797467855151484
        },
        "rouge-l": {
          "r": 0.22033898305084745,
          "p": 0.2708333333333333,
          "f": 0.2429906492584506
        }
      }
    },
    {
      "paper_id": "math.CA.math/FA/2503.10190v1",
      "true_abstract": "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem.",
      "generated_abstract": "In this article, we study the convergence of a sequence of families of\ngeneralized Sobolev spaces with a fixed metric, defined on a compact space,\nwhich are characterized by the property that the family of corresponding\ngeneralized Banach spaces is also a family of generalized Sobolev spaces with\nthe same metric. We also give a simple proof of the convergence of the\ncorresponding family of generalized Banach spaces.\n  The general setting of our article is the metric space $\\mathcal{X}$ and\nthe Banach space $\\mathcal{B}$, and the generalized Sobolev spaces are defined\nas follows: Let $M\\subseteq \\mathcal{X}$ be a compact set and $V\\subseteq\n\\mathcal{B}$ be a Banach space. The generalized Sobolev space $\\mathcal{S}(M,V)$\nis the space of all sequences $f_n\\in \\mathcal{B}^n(M)$ such that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13793103448275862,
          "p": 0.1935483870967742,
          "f": 0.16107382064411527
        },
        "rouge-2": {
          "r": 0.02727272727272727,
          "p": 0.0297029702970297,
          "f": 0.028436013966443678
        },
        "rouge-l": {
          "r": 0.12643678160919541,
          "p": 0.1774193548387097,
          "f": 0.147651001852169
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.15116v1",
      "true_abstract": "We introduce an empirical functional $\\Psi$ that is an optimal uniform mean\nestimator: Let $F\\subset L_2(\\mu)$ be a class of mean zero functions, $u$ is a\nreal valued function, and $X_1,\\dots,X_N$ are independent, distributed\naccording to $\\mu$. We show that under minimal assumptions, with $\\mu^{\\otimes\nN}$ exponentially high probability, \\[ \\sup_{f\\in F} |\\Psi(X_1,\\dots,X_N,f) -\n\\mathbb{E} u(f(X))| \\leq c R(F) \\frac{ \\mathbb{E} \\sup_{f\\in F } |G_f| }{\\sqrt\nN}, \\] where $(G_f)_{f\\in F}$ is the gaussian processes indexed by $F$ and\n$R(F)$ is an appropriate notion of `diameter' of the class $\\{u(f(X)) : f\\in\nF\\}$.\n  The fact that such a bound is possible is surprising, and it leads to the\nsolution of various key problems in high dimensional probability and high\ndimensional statistics. The construction is based on combining Talagrand's\ngeneric chaining mechanism with optimal mean estimation procedures for a single\nreal-valued random variable.",
      "generated_abstract": "We consider a generalization of the classical $t$-distribution where the\nparameter of the distribution is the number of variables in the sample. We\npropose a new class of distributions based on the $\\chi_1^2$ distribution\nthat generalizes the classical $t$-distribution to a wider class of distributions.\nWe demonstrate the usefulness of the proposed class of distributions in\napplications to financial data. We provide an example using a\n$100$-dimensional financial dataset that illustrates the effectiveness of the\nproposed class of distributions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12037037037037036,
          "p": 0.3170731707317073,
          "f": 0.1744966403062926
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.06153846153846154,
          "f": 0.039215681932430316
        },
        "rouge-l": {
          "r": 0.12037037037037036,
          "p": 0.3170731707317073,
          "f": 0.1744966403062926
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/MN/2408.08503v1",
      "true_abstract": "Research organisms provide invaluable insights into human biology and\ndiseases, serving as essential tools for functional experiments, disease\nmodeling, and drug testing. However, evolutionary divergence between humans and\nresearch organisms hinders effective knowledge transfer across species. Here,\nwe review state-of-the-art methods for computationally transferring knowledge\nacross species, primarily focusing on methods that utilize transcriptome data\nand/or molecular networks. We introduce the term \"agnology\" to describe the\nfunctional equivalence of molecular components regardless of evolutionary\norigin, as this concept is becoming pervasive in integrative data-driven models\nwhere the role of evolutionary origin can become unclear. Our review addresses\nfour key areas of information and knowledge transfer across species: (1)\ntransferring disease and gene annotation knowledge, (2) identifying agnologous\nmolecular components, (3) inferring equivalent perturbed genes or gene sets,\nand (4) identifying agnologous cell types. We conclude with an outlook on\nfuture directions and several key challenges that remain in cross-species\nknowledge transfer.",
      "generated_abstract": "The biological basis of the relationship between the genetic code and\nthe protein-coding region is not fully understood. This study investigates the\npossibility of linking genetic variants and protein-coding regions by\nconstructing a genome-wide association study (GWAS) database. By integrating\ngenetic variants and protein-coding regions, we can analyze the relationship\nbetween genetic variants and protein-coding regions. The database is constructed\nbased on the results of GWAS. We used the R package GWASDB to construct the\ndatabase. In the database, there are 19,000 protein-coding regions and 49,000\ngenetic variants. The database contains 15,865 genetic variants that are not\nfound in the human genome database. The GWAS database is designed to explore\nthe relationship between genetic variants and protein-coding regions",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11504424778761062,
          "p": 0.20634920634920634,
          "f": 0.14772726813081108
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.20634920634920634,
          "f": 0.14772726813081108
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/RM/2412.09662v1",
      "true_abstract": "This article analytically characterizes the impermanent loss for automatic\nmarket makers in decentralized exchanges such as Uniswap or Balancer (CPMM). We\npresent a theoretical static replication formula for the pool value using a\ncombination of European calls and puts. We will formulate a result to guarantee\ncoverage for any final price that falls within a predefined range.",
      "generated_abstract": "We consider the problem of optimal portfolio selection in an infinite\nportfolio space under the model of portfolio selection with no information\ncosts. We show that the portfolio space is infinite because it is infinite if\nand only if the value of the objective function at any given portfolio\ncombination is infinite. Moreover, we show that the value of the objective\nfunction at any given portfolio combination is infinite if and only if the\nvalue of the objective function at any given portfolio combination is infinite\nwhen all values of the objective function are infinite. Finally, we show that\nthe value of the objective function at any given portfolio combination is\ninfinite if and only if the value of the objective function at any given\nportfolio combination is infinite when all values of the objective function are\ninfinite. In other words, the portfolio space is infinite if and only if the\nportfolio space is infinite. Our results answer a question posed by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.2,
          "f": 0.19999999500000015
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.16,
          "f": 0.15999999500000017
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-ex/2503.09045v1",
      "true_abstract": "Electromagnetic (EM) probes, including photons and dileptons, do not interact\nstrongly after their production in heavy-ion collisions, allowing them to carry\nundistorted information from their points of origin. This makes them powerful\ntools for studying early-stage equilibration and the thermodynamic properties\nof the quark-gluon plasma (QGP). In these proceedings, we highlight recent\ntheoretical advancements in EM probes, focusing on their role in probing\nearly-stage dynamics and extracting medium properties. We also discuss the\nemerging multimessenger approach, which combines hadronic and electromagnetic\nprobes to achieve a more comprehensive understanding of the QGP.",
      "generated_abstract": "We calculate the single-nucleon potentials using the mean-field approach,\nand the self-consistent solution of the equations of motion is used to\nconstruct the mean-field potentials. The resulting potentials are then used to\nconstruct the single-nucleon states in the Hartree-Fock-Bogoliubov (HFB)\napproximation. The effect of the Hartree-Fock-Bogoliubov method is studied\nusing the single-nucleon states calculated with the mean-field potentials and\nthe single-nucleon states calculated with the HFB method. The results show\nthat the single-nucleon states calculated with the HFB method are closer to the\nexperimental data than those calculated with the mean-field potentials.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0945945945945946,
          "p": 0.17073170731707318,
          "f": 0.12173912584650301
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.03225806451612903,
          "f": 0.026490061385027856
        },
        "rouge-l": {
          "r": 0.0945945945945946,
          "p": 0.17073170731707318,
          "f": 0.12173912584650301
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.04407v1",
      "true_abstract": "In this paper, we propose a movable antenna (MA)-enabled frequency-hopping\n(FH) multiple-input multiple-output (MIMO) radar system and investigate its\nsensing resolution. Specifically, we derive the expression of the ambiguity\nfunction and analyze the relationship between its main lobe width and the\ntransmit antenna positions. In particular, the optimal antenna distribution to\nachieve the minimum main lobe width in the angular domain is characterized. We\ndiscover that this minimum width is related to the antenna size, the antenna\nnumber, and the target angle. Meanwhile, we present lower bounds of the\nambiguity function in the Doppler and delay domains, and show that the impact\nof the antenna size on the radar performance in these two domains is very\ndifferent from that in the angular domain. Moreover, the performance\nenhancement brought by MAs exhibits a certain trade-off between the main lobe\nwidth and the side lobe peak levels. Therefore, we propose to balance between\nminimizing the side lobe levels and narrowing the main lobe of the ambiguity\nfunction by optimizing the antenna positions. To achieve this goal, we propose\na low-complexity algorithm based on the Rosen's gradient projection method, and\nshow that its performance is very close to the baseline. Simulation results are\npresented to validate the theoretical analysis on the properties of the\nambiguity function, and demonstrate that MAs can reduce the main lobe width and\nsuppress the side lobe levels of the ambiguity function, thereby enhancing\nradar performance.",
      "generated_abstract": "The current trend in the digitalization of the electricity system is to\nelectrify the heating sector. The main goal of this paper is to investigate\nthe feasibility of integrating heat pumps into the power grid, using the\nNordic-Baltic grid as a case study. The Nordic-Baltic grid is a 38,000 km\nlong, 250,000 MW-capacity, single-circuit transmission network with an average\nelectricity consumption of 54 TWh/year, corresponding to approximately 3% of the\nelectricity consumption in the region. This study aims to contribute to the\ncurrent debate on the integration of heat pumps into the electricity system,\nespecially as regards the challenges that must be addressed for their integration\ninto the grid. We conducted a study of the technical feasibility of integrating\nheat pumps",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11304347826086956,
          "p": 0.18571428571428572,
          "f": 0.14054053583637707
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.038461538461538464,
          "f": 0.02739725568774706
        },
        "rouge-l": {
          "r": 0.10434782608695652,
          "p": 0.17142857142857143,
          "f": 0.12972972502556626
        }
      }
    },
    {
      "paper_id": "math.FA.math/CA/2503.04285v1",
      "true_abstract": "We introduce the space SBV$_X$ of special functions with bounded\n$X$-variation in Carnot-Carath\\'eodory spaces and study its main properties.\nOur main outcome is an approximation result, with respect to the BV$_X$\ntopology, for SBV$_X$ functions.",
      "generated_abstract": "We study the equivariant Chern character of the complex vector bundle\n$T^*\\mathbb{C}P^2\\otimes \\Omega^1_X(-1)$ over $X=\\mathbb{C}P^2\\subset\\mathbb{C}\n\\mathbb{P}^3$, with the K\\\"ahler form $\\omega=x\\wedge y$ and K\\\"ahler potential\n$f=x^2+y^2$. We compute the equivariant Chern character of the complex line\nbundle $T^*\\mathbb{C}P^2\\otimes \\Omega^1_X(1)$, which is a complex vector bundle\nover $X$ with K\\\"ahler form $\\omega=x\\wedge y+y\\wedge x$ and K\\\"ahler potential\n$f=x^2+y^2+xy$. In this paper, we obtain the equivariant Chern character of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.18421052631578946,
          "f": 0.20588234801038074
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.15789473684210525,
          "f": 0.1764705833044984
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.08020v2",
      "true_abstract": "We introduce weighted cycles on weaves of general Dynkin types and define a\nskew-symmetrizable intersection pairing between weighted cycles. We prove that\nweighted cycles on a weave form a Laurent polynomial algebra and construct a\nquantization for this algebra using the skew-symmetric intersection pairing in\nthe simply-laced case. We define merodromies along weighted cycles as functions\non the decorated flag moduli space of the weave. We relate weighted cycles with\ncluster variables in a cluster algebra and prove that mutations of weighted\ncycles are compatible with mutations of cluster variables.",
      "generated_abstract": "We prove the existence of a unique, positive solution to the system of\ninterpolation equations\n  \\begin{equation*}\n    (u_{t},u_{xx})= \\left(\n    \\begin{array}{cc}\n      1 + u_x^2 & u_x \\\\\n      u & 1 + u_x^2\n    \\end{array}\n    \\right)\n    (x,t,u) + \\left(\n    \\begin{array}{cc}\n      1 + u_x^2 & u_x \\\\\n      -u & 1 + u_x^2\n    \\end{array}\n    \\right)\n    (x,t,-u)\n  \\end{equation*}\n  with initial data $(u(0,.),u_x(0.,.))$. We then use this solution to\nconstruct a family of solutions to the system",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16326530612244897,
          "p": 0.18604651162790697,
          "f": 0.17391303849952755
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.037037037037037035,
          "f": 0.030303025468320335
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.13953488372093023,
          "f": 0.1304347776299624
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/OS/2503.05117v1",
      "true_abstract": "This paper presents HyperGraph ROS, an open-source robot operating system\nthat unifies intra-process, inter-process, and cross-device computation into a\ncomputational hypergraph for efficient message passing and parallel execution.\nIn order to optimize communication, HyperGraph ROS dynamically selects the\noptimal communication mechanism while maintaining a consistent API. For\nintra-process messages, Intel-TBB Flow Graph is used with C++ pointer passing,\nwhich ensures zero memory copying and instant delivery. Meanwhile,\ninter-process and cross-device communication seamlessly switch to ZeroMQ. When\na node receives a message from any source, it is immediately activated and\nscheduled for parallel execution by Intel-TBB. The computational hypergraph\nconsists of nodes represented by TBB flow graph nodes and edges formed by TBB\npointer-based connections for intra-process communication, as well as ZeroMQ\nlinks for inter-process and cross-device communication. This structure enables\nseamless distributed parallelism. Additionally, HyperGraph ROS provides\nROS-like utilities such as a parameter server, a coordinate transformation\ntree, and visualization tools. Evaluation in diverse robotic scenarios\ndemonstrates significantly higher transmission and throughput efficiency\ncompared to ROS 2. Our work is available at\nhttps://github.com/wujiazheng2020a/hyper_graph_ros.",
      "generated_abstract": "The growing popularity of robotic manipulators in diverse applications\nrequire robust and reliable motion planning solutions. Motion planning in\ncluttered environments requires the integration of a multitude of factors,\nincluding robustness, efficiency, and safety. In this paper, we present a\nframework for motion planning in cluttered environments that addresses all of\nthese factors. The framework integrates a Bayesian hierarchical model of\ncluttered environments and an efficient planning algorithm. It also provides\nrobustness guarantees through a framework that uses a safety-preserving\nrepresentation of the robot's trajectory. To evaluate our framework, we\nintroduce a simulation environment to evaluate our approach and compare it with\nother motion planning approaches, including the recently proposed Bayesian\nHierarchical Planning (BHP) method. The results demonstrate that our approach\noutperforms other motion planning methods in terms of safety and efficiency,\nwhile still",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.20930232558139536,
          "f": 0.16744185566511643
        },
        "rouge-2": {
          "r": 0.006060606060606061,
          "p": 0.00819672131147541,
          "f": 0.006968636227224835
        },
        "rouge-l": {
          "r": 0.10077519379844961,
          "p": 0.1511627906976744,
          "f": 0.12093022775813972
        }
      }
    },
    {
      "paper_id": "math.ST.q-bio/PE/2501.17622v1",
      "true_abstract": "We study the optimization landscape of maximum likelihood estimation for a\nbinary latent tree model with hidden variables at internal nodes and observed\nvariables at the leaves. This model, known as the Cavender-Farris-Neyman (CFN)\nmodel in statistical phylogenetics, is also a special case of the ferromagnetic\nIsing model. While the likelihood function is known to be non-concave with\nmultiple critical points in general, gradient descent-type optimization methods\nhave proven surprisingly effective in practice. We provide theoretical insights\ninto this phenomenon by analyzing the population likelihood landscape in a\nneighborhood of the true parameter vector. Under some conditions on the edge\nparameters, we show that the expected log-likelihood is strongly concave and\nsmooth in a box around the true parameter whose size is independent of both the\ntree topology and number of leaves. The key technical contribution is a careful\nanalysis of the expected Hessian, showing that its diagonal entries are large\nwhile its off-diagonal entries decay exponentially in the graph distance\nbetween the corresponding edges. These results provide the first rigorous\ntheoretical evidence for the effectiveness of optimization methods in this\nsetting and may suggest broader principles for understanding optimization in\nlatent variable models on trees.",
      "generated_abstract": "We study the phase diagram of the Langevin dynamics on a circle of\narbitrary dimension $n$. We prove that the system is in a first-order phase\ntransition at a critical temperature $T_c$. The transition occurs when the\nsystem passes from the regime where the dynamics is ergodic to the regime where\nthe dynamics is non-ergodic. We also show that the critical temperature\n$T_c$ can be expressed in terms of the eigenvalues of the Laplacian of the\ncircular system. The critical temperature $T_c$ is also obtained using the\nHamilton-Jacobi approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.3333333333333333,
          "f": 0.18181817785123974
        },
        "rouge-2": {
          "r": 0.0374331550802139,
          "p": 0.0945945945945946,
          "f": 0.053639842680524664
        },
        "rouge-l": {
          "r": 0.1171875,
          "p": 0.3125,
          "f": 0.1704545414876034
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.11145v1",
      "true_abstract": "The studies of Bonaventura Cavalieri's indivisibles by Giusti, Andersen,\nMancosu and others provide a comprehensive picture of Cavalieri's mathematics,\nas well as of the mathematical objections to it as formulated by Paul Guldin\nand other critics. An issue that has been studied in less detail concerns the\ntheological underpinnings of the contemporary debate over indivisibles, its\nhistorical roots, the geopolitical situation at the time, and its relation to\nthe ultimate suppression of Cavalieri's religious order. We analyze sources\nfrom the 17th through 21st centuries to investigate such a relation.",
      "generated_abstract": "The goal of this paper is to prove that the set of points of the unit circle\nin the complex plane with real part $0$ is dense in the complex plane. We use a\nmethod similar to the one used by G.H. Hardy and R.G. Hartley to prove that the\nset of real numbers with positive real part is dense in the real numbers. We\nalso prove a result that the set of complex numbers with positive real part is\ndense in the complex numbers. We use the same method to prove that the set of\nreal numbers with negative real part is dense in the real numbers. We also\nprove a result that the set of complex numbers with negative real part is\ndense in the complex numbers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14925373134328357,
          "p": 0.23809523809523808,
          "f": 0.1834862337951352
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.030303030303030304,
          "f": 0.026315784560250226
        },
        "rouge-l": {
          "r": 0.13432835820895522,
          "p": 0.21428571428571427,
          "f": 0.16513760994192422
        }
      }
    },
    {
      "paper_id": "cs.HC.stat/CO/2502.08114v2",
      "true_abstract": "The rapid proliferation of data science forced different groups of\nindividuals with different backgrounds to adapt to statistical analysis. We\nhypothesize that conversational agents are better suited for statistical\nanalysis than traditional graphical user interfaces (GUI). In this work, we\npropose a novel conversational agent, StatZ, for statistical analysis. We\nevaluate the efficacy of StatZ relative to established statistical\nsoftware:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time,\nuser experience, and user satisfaction. We combined the proposed analysis\nquestion from state-of-the-art language models with suggestions from\nstatistical analysis experts and tested with 51 participants from diverse\nbackgrounds. Our experimental design assessed each participant's ability to\nperform statistical analysis tasks using traditional statistical analysis tools\nwith GUI and our conversational agent. Results indicate that the proposed\nconversational agents significantly outperform GUI statistical software in all\nassessed metrics, including quantitative (task completion time, accuracy, and\nuser experience), and qualitative (user satisfaction) metrics. Our findings\nunderscore the potential of using conversational agents to enhance statistical\nanalysis processes, reducing cognitive load and learning curves and thereby\nproliferating data analysis capabilities, to individuals with limited knowledge\nof statistics.",
      "generated_abstract": "We consider the problem of learning a set of points in the plane, using only a\n(single) oracle that is able to answer any query with probability at least\n$\\delta$. Our goal is to design an estimator that, in expectation, performs\nas well as the oracle.\n  We consider the case when the oracle only answers queries of size $n$, and\nthe set of points is $n$-dimensional. Our estimator can be seen as an\napproximation to the optimal set of points. We show that the approximation\nerror of our estimator is at most $\\delta$, and furthermore, the number of\npoints needed to achieve this approximation error is linear in $n$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.24193548387096775,
          "f": 0.16759776083518005
        },
        "rouge-2": {
          "r": 0.005847953216374269,
          "p": 0.010101010101010102,
          "f": 0.007407402762965873
        },
        "rouge-l": {
          "r": 0.11965811965811966,
          "p": 0.22580645161290322,
          "f": 0.1564245764776382
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/bio-ph/2503.08818v1",
      "true_abstract": "Molecular motors are in charge of almost every process in the life cycle of\ncells, such as protein synthesis, DNA replication, and cell locomotion, hence\nbeing of crucial importance for understanding the cellular dynamics. However,\ngiven their size scales on the order of nanometers, direct measurements are\nrather challenging, and the information that can be extracted from them is\nlimited. In this work, we propose strategies based on martingale theory in\nstochastic thermodynamics to infer thermodynamic properties of molecular motors\nusing a limited amount of available information. In particular, we use two\nrecent theoretical results valid for systems arbitrary far of equilibrium: the\nintegral fluctuation theorem (IFT) at stopping times, and a family of bounds to\nthe maximal excursions of entropy production. The potential of these strategies\nis illustrated with a simple model for the F1-ATPase rotary molecular motor,\nwhere our approach is able to estimate several quantities determining the\nthermodynamics of the motor, such as the rotational work of the motor performed\nagainst an externally applied force, or the effective environmental\ntemperature.",
      "generated_abstract": "The non-equilibrium dynamics of a single-molecule receptor-ligand\nsystem in the presence of stochastic, local, and non-Markovian perturbations is\nstudied using a mean-field approach. The receptor is assumed to be a\nquantum-mechanical system with an isolated, unbounded energy spectrum, which\nleads to a single-particle system of many-body quantum-mechanical receptors. The\nperturbations considered include (i) local, stochastic interactions between\nmolecules, (ii) non-Markovian stochastic interactions between molecules, and\n(iii) stochastic, non-Markovian interactions between receptors and\nligands. The system is considered in the linear response regime, where the\nperturbations are small compared to the receptor-ligand interaction. The\nsingle-particle dynamics of the system are studied in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1349206349206349,
          "p": 0.3090909090909091,
          "f": 0.18784529963676333
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.0425531914893617,
          "f": 0.03030302571740198
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.21818181818181817,
          "f": 0.13259668085223297
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.07285v1",
      "true_abstract": "The complexity of solving equations over finite groups has been an active\narea of research over the last two decades, starting with Goldmann and Russell,\n\\emph{The complexity of solving equations over finite groups} from 1999. One\nimportant case of a group with unknown complexity is the symmetric group $S_4.$\nIn 2023, Idziak, Kawa{\\l}ek, and Krzaczkowski published $\\exp(\\Omega(\\log^2\nn))$ lower bounds for the satisfiability and equivalence problems over $S_4$\nunder the Exponential Time Hypothesis. In the present note, we prove that the\nsatisfiability problem $\\textsc{PolSat}(S_4)$ can be reduced to the equivalence\nproblem $\\textsc{PolEqv}(S_4)$ and thus, the two problems have the same\ncomplexity. We provide several equivalent formulations of the problem. In\nparticular, we prove that $\\textsc{PolEqv}(S_4)$ is equivalent to the circuit\nequivalence problem for $\\operatorname{CC}[2,3,2]$-circuits, which were\nintroduced by Idziak, Kawe{\\l}ek and Krzaczkowski. Under their strong\nexponential size hypothesis, such circuits cannot compute\n$\\operatorname{AND}_n$ in size $\\exp(o(\\sqrt{n})).$ Our results provide an\nupper bound on the complexity of $\\textsc{PolEqv}(S_4)$ that is based on the\nminimal size of $\\operatorname{AND}_n$ over\n$\\operatorname{CC}[2,3,2]$-circuits.",
      "generated_abstract": "We propose a novel framework for online multi-class classifier training that\nis tailored for the context of multi-label classification. The framework\nconsists of three core components: (i) a model-agnostic context-aware loss\nfunction that dynamically adapts the loss function based on the context of the\ntraining examples; (ii) a context-aware learning rate scheduling algorithm\nthat controls the learning rate by considering both the current context and\nthe current training sample; and (iii) a context-aware learning rate\noptimization algorithm that dynamically optimizes the learning rate based on\nthe context of the training examples. Our experiments demonstrate that the\nframework outperforms the existing methods in terms of both training efficiency\nand generalization performance. Additionally, we introduce an online\ncontext-aware regularization strategy that can effectively mitigate the\neffects of overfitting in online context-aware classification.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1619047619047619,
          "p": 0.23943661971830985,
          "f": 0.19318181336841436
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.05454545454545454,
          "f": 0.0451127771044157
        },
        "rouge-l": {
          "r": 0.1619047619047619,
          "p": 0.23943661971830985,
          "f": 0.19318181336841436
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.14731v1",
      "true_abstract": "Motor skill acquisition in fields like surgery, robotics, and sports involves\nlearning complex task sequences through extensive training. Traditional\nperformance metrics, like execution time and error rates, offer limited insight\nas they fail to capture the neural mechanisms underlying skill learning and\nretention. This study introduces directed functional connectivity (dFC),\nderived from electroencephalography (EEG), as a novel brain-based biomarker for\nassessing motor skill learning and retention. For the first time, dFC is\napplied as a biomarker to map the stages of the Fitts and Posner motor learning\nmodel, offering new insights into the neural mechanisms underlying skill\nacquisition and retention. Unlike traditional measures, it captures both the\nstrength and direction of neural information flow, providing a comprehensive\nunderstanding of neural adaptations across different learning stages. The\nanalysis demonstrates that dFC can effectively identify and track the\nprogression through various stages of the Fitts and Posner model. Furthermore,\nits stability over a six-week washout period highlights its utility in\nmonitoring long-term retention. No significant changes in dFC were observed in\na control group, confirming that the observed neural adaptations were specific\nto training and not due to external factors. By offering a granular view of the\nlearning process at the group and individual levels, dFC facilitates the\ndevelopment of personalized, targeted training protocols aimed at enhancing\noutcomes in fields where precision and long-term retention are critical, such\nas surgical education. These findings underscore the value of dFC as a robust\nbiomarker that complements traditional performance metrics, providing a deeper\nunderstanding of motor skill learning and retention.",
      "generated_abstract": "Genome-wide association studies (GWAS) have identified more than 3,000\ngenetic variants associated with complex traits, yet only a fraction of these\nhave been validated for causal associations with disease. One way to address\nthis gap is to conduct causal variant discovery in genome-wide association\nstudies (GWAS) using causal inference methods. However, the complex nature of\nGWAS makes causal inference challenging. This paper introduces a novel\nframework that leverages the graph-based causal discovery framework\nCausalGraphs to address the causal discovery in GWAS. We first define a\ngeneralized causal graph for GWAS that includes both causal and non-causal\nlinks, and we then provide a methodology for constructing this causal graph and\nextracting causal variables. This methodology is applied to a gene-disease\ninter",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10457516339869281,
          "p": 0.2077922077922078,
          "f": 0.13913043032854455
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.017857142857142856,
          "f": 0.011834315095411482
        },
        "rouge-l": {
          "r": 0.10457516339869281,
          "p": 0.2077922077922078,
          "f": 0.13913043032854455
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/NI/2503.08262v1",
      "true_abstract": "The search for the optimal pair of active and protection paths in a network\nwith Shared Risk Link Groups (SRLG) is a challenging but high-value problem in\nthe industry that is inevitable in ensuring reliable connections on the modern\nInternet. We propose a new approach to solving this problem, with a novel use\nof statistical analysis of the distribution of paths with respect to their\ncost, which is an integral part of our innovation. The key idea in our\nalgorithm is to employ iterative updates of cost bounds, allowing efficient\npruning of suboptimal paths. This idea drives an efficacious exploration of the\nsearch space. We benchmark our algorithms against the state-of-the-art\nalgorithms that exploit the alternative strategy of conflicting links\nexclusion, showing that our approach has the advantage of finding more feasible\nconnections within a set time limit.",
      "generated_abstract": "We present a novel algorithm for solving the Traveling Salesman Problem (TSP)\nin the presence of obstacles. Our approach uses a graph-based representation\nof the obstacle set, based on the Delaunay triangulation of a 2D map. This\napproach enables us to efficiently represent and compute the shortest path\nbetween obstacles in a graph. We then compute the TSP using an iterative\napproach that starts with an initial solution, followed by a series of\nrefinements based on the graph-based representation of the obstacles. This\napproach enables us to compute the optimal solution, along with the optimal\nsolution with respect to the graph-based representation of the obstacles, for\nany given initial solution. Our approach is scalable, as we can compute the\noptimal solution in polynomial time with respect to the size of the graph. We\ndemonstrate the effectiveness of our approach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24731182795698925,
          "p": 0.3150684931506849,
          "f": 0.2771084288075193
        },
        "rouge-2": {
          "r": 0.07352941176470588,
          "p": 0.08849557522123894,
          "f": 0.08032128018322317
        },
        "rouge-l": {
          "r": 0.21505376344086022,
          "p": 0.273972602739726,
          "f": 0.2409638504942663
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.10292v1",
      "true_abstract": "Synchronous consensus protocols offer a significant advantage over their\nasynchronous and partially synchronous counterparts by providing higher fault\ntolerance -- an essential benefit in distributed systems, like blockchains,\nwhere participants may have incentives to act maliciously. However, despite\nthis advantage, synchronous protocols are often met with skepticism due to\nconcerns about their performance, as the latency of synchronous protocols is\ntightly linked to a conservative time bound for message delivery.\n  This paper introduces AlterBFT, a new Byzantine fault-tolerant consensus\nprotocol. The key idea behind AlterBFT lies in the new model we propose, called\nhybrid synchronous system model. The new model is inspired by empirical\nobservations about network behavior in the public cloud environment and\ncombines elements from the synchronous and partially synchronous models.\nNamely, it distinguishes between small messages that respect time bounds and\nlarge messages that may violate bounds but are eventually timely. Leveraging\nthis observation, AlterBFT achieves up to 15$\\times$ lower latency than\nstate-of-the-art synchronous protocols while maintaining similar throughput and\nthe same fault tolerance. Compared to partially synchronous protocols, AlterBFT\nprovides higher fault tolerance, higher throughput, and comparable latency.",
      "generated_abstract": "In this paper, we propose a new method for generating realistic 3D printed\ncameras that are compatible with various 3D printers. Our method uses a\ncombination of real-time rendering, parametric 3D scanning, and 3D printed\ncomponents. We demonstrate the effectiveness of our approach through\ncomparison with existing 3D printed cameras and a high-end CNC-printed camera.\nOur method is also able to generate a high-fidelity 3D printed camera with\nrealistic lighting, color, and texture. Additionally, we provide a detailed\ndescription of the 3D scanning process, and our generated 3D printed camera\nincludes detailed control files that can be used for further customization\nand research. Our work provides a practical, realistic, and scalable solution\nfor generating 3D printed cameras, offering a cost-effective and efficient",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11023622047244094,
          "p": 0.18181818181818182,
          "f": 0.13725489726114973
        },
        "rouge-2": {
          "r": 0.005847953216374269,
          "p": 0.008928571428571428,
          "f": 0.007067133026511247
        },
        "rouge-l": {
          "r": 0.10236220472440945,
          "p": 0.16883116883116883,
          "f": 0.1274509756925223
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10366v1",
      "true_abstract": "It is a common lore that in the thermal leptogenesis in the type-1 seesaw\nscenario with the conventional hierarchy of heavy right-handed neutrinos\n(RHNs), the CP violating, out-of-equilibrium decay of the lightest RHN ($N_1$)\nis the only relevant source of $B-L$ asymmetry. Any asymmetry produced by the\nheavier RHNs ($N_2$ and $N_3$) gets washed out by the lepton number violating\nprocesses mediated by $N_1$. In this paper, we examine this assumption\ncomprehensively, considering decay and inverse decay processes as well as the\ninclusion of scatterings. We find that the above said assumption is true only\nif all the RHNs ($N_1, N_2$ and $N_3$) are in strong washout regime. However,\nwe saw that, to satisfy the neutrino masses and mixing given by the low energy\nneutrino oscillation data, at most one of the RHNs can be in the weak washout\nregime. This leads to, if $N_1$ is in the weak washout regime, then the washout\nparameters of $N_2$ and $N_3$ can be chosen in such a way that the impact of\n$N_2$ and $N_3$ on the final $B-L$ asymmetry is relatively small. On the other\nhand, if $N_2$ or $N_3$ is in weak washout regime, then the asymmetry produced\nby them can affect the final $B-L$ asymmetry even if $N_1$ is in the strong\nwashout regime, which we call the memory effect. We delineated the parameter\nspace where the memory effect is significant.",
      "generated_abstract": "We investigate the phenomenology of electroweak baryogenesis in a supersymmetric\nmicroscopic model based on the G-parity. The model includes a chiral superfield\nthat couples to the gauge bosons through a chiral mass term and is charged under\nthe electroweak gauge group. A Majorana mass term for the right-handed neutrino\nis added to the superpotential, which is responsible for the generation of\nbaryon asymmetry. We show that the superpotential is naturally obtained when\nthe G-parity is implemented on the model. We show that the G-parity is\ninvariant under the $SU(2)_R\\times SU(3)_c\\times U(1)_B\\times U(1)_R$\nsymmetry of the model, and that the superpotential contains no terms that break\nthe symmetry. We then consider the electroweak bary",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11811023622047244,
          "p": 0.234375,
          "f": 0.15706805837120705
        },
        "rouge-2": {
          "r": 0.015306122448979591,
          "p": 0.030927835051546393,
          "f": 0.02047781127048751
        },
        "rouge-l": {
          "r": 0.11811023622047244,
          "p": 0.234375,
          "f": 0.15706805837120705
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CV/2503.10632v1",
      "true_abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
      "generated_abstract": "Recent advancements in computer vision and deep learning have transformed\nthe field of visual question answering (VQA), offering new possibilities for\nenhancing research and applications in AI. However, existing approaches often\nfall short in addressing the fundamental limitations of current models, such as\nhigh computational complexity and insufficient reasoning ability. To address\nthese challenges, we propose VQA-R, a novel approach that integrates the\npower of Reinforcement Learning (RL) and Attention-based Neural Networks\n(ANNs). VQA-R combines the power of RL with ANNs to enhance VQA modeling.\nSpecifically, VQA-R uses a RL agent to learn to predict the correct answer for\neach question, leveraging the interaction between the agent and the model to\nachieve better performance. Additionally, VQA-R incorporates attention\nme",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12987012987012986,
          "p": 0.21505376344086022,
          "f": 0.16194331514301183
        },
        "rouge-2": {
          "r": 0.004807692307692308,
          "p": 0.008620689655172414,
          "f": 0.006172834909316029
        },
        "rouge-l": {
          "r": 0.12337662337662338,
          "p": 0.20430107526881722,
          "f": 0.153846149151109
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.15800v1",
      "true_abstract": "This paper explores how Large Language Models (LLMs) behave in a classic\nexperimental finance paradigm widely known for eliciting bubbles and crashes in\nhuman participants. We adapt an established trading design, where traders buy\nand sell a risky asset with a known fundamental value, and introduce several\nLLM-based agents, both in single-model markets (all traders are instances of\nthe same LLM) and in mixed-model \"battle royale\" settings (multiple LLMs\ncompeting in the same market). Our findings reveal that LLMs generally exhibit\na \"textbook-rational\" approach, pricing the asset near its fundamental value,\nand show only a muted tendency toward bubble formation. Further analyses\nindicate that LLM-based agents display less trading strategy variance in\ncontrast to humans. Taken together, these results highlight the risk of relying\non LLM-only data to replicate human-driven market phenomena, as key behavioral\nfeatures, such as large emergent bubbles, were not robustly reproduced. While\nLLMs clearly possess the capacity for strategic decision-making, their relative\nconsistency and rationality suggest that they do not accurately mimic human\nmarket dynamics.",
      "generated_abstract": "We study the pricing of European options with negative dividends in a\nmarket with a small number of firms and a large number of potential\nunderlying assets. Our model is based on the SABR model with negative dividends,\nwhich has been widely used in the literature to study options pricing in\neconomic models. In our model, dividends are negative and the dividend growth\nrate is unknown. We investigate the effect of dividend growth rate on the\noption price, including the case where dividend growth is exponential. We\nalso consider the case where dividend growth is lognormal. In all cases, we\nobtain a closed-form solution for the option price and the value function.\nOur results show that, in general, the dividend growth rate has a positive\neffect on the option price and value function. The effect of dividend growth\nrate on the value function can be positive or negative. For example, in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14393939393939395,
          "p": 0.24675324675324675,
          "f": 0.18181817716444235
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.024793388429752067,
          "f": 0.02097901609736531
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.2077922077922078,
          "f": 0.1531100431931505
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/AI/2503.10533v1",
      "true_abstract": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nHowever, their relationship to IRT parameters remains underexplored. To address\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\nacross various STEM subjects (e.g., math and biology). Using an automated\napproach, we annotated each question with a 19-criteria IWF rubric and studied\nrelationships to data-driven IRT parameters. Our analysis revealed\nstatistically significant links between the number of IWFs and IRT difficulty\nand discrimination parameters, particularly in life and physical science\ndomains. We further observed how specific IWF criteria can impact item quality\nmore and less severely (e.g., negative wording vs. implausible distractors).\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\nvalidation methods. Our findings highlight the need for further research on\ndomain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.",
      "generated_abstract": "We present a novel framework for modeling semantic changes in language\n(semantic translation). Our approach models semantic changes by integrating\nmulti-stage models, where each stage models the semantic changes in a specific\nlanguage (L1) using a pre-trained L1 model. The modeling is further refined by\na post-processing step that incorporates the predictions of the multi-stage\nmodels. We evaluate our framework on a variety of translation tasks, including\ntranslation from English to Chinese, English to French, and English to Spanish.\nOur approach significantly outperforms baseline methods, demonstrating its\npotential for enhancing translation quality and efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11347517730496454,
          "p": 0.25,
          "f": 0.15609755668102332
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11347517730496454,
          "p": 0.25,
          "f": 0.15609755668102332
        }
      }
    },
    {
      "paper_id": "econ.TH.q-fin/PM/2410.18432v1",
      "true_abstract": "This paper develops a dynamic model to analyze the general equilibrium of the\ninsurance market, focusing on the interaction between insurers' underwriting\nand investment strategies. Three possible equilibrium outcomes are identified:\na positive insurance market, a zero insurance market, and market failure. Our\nfindings reveal why insurers may rationally accept underwriting losses by\nsetting a negative safety loading while relying on investment profits,\nparticularly when there is a negative correlation between insurance gains and\nfinancial returns. Additionally, we explore the impact of regulatory frictions,\nshowing that while imposing a cost on investment can enhance social welfare\nunder certain conditions, it may not always be necessary. Therefore, we\nemphasize the importance of tailoring regulatory interventions to specific\nmarket conditions.",
      "generated_abstract": "This paper introduces a novel methodology for the estimation of the\nparameters of the Black-Scholes option pricing model. The methodology is\nbased on the use of a stochastic volatility model with a fractional Brownian\nmotion. This model is fitted to data using a maximum likelihood estimation\nmethod. The proposed methodology is applied to the option pricing of European\ncall and put options on the S\\&P 500 index, with different volatility\nstructures. The results show that the proposed methodology outperforms the\nBayesian stochastic volatility model and other methods in terms of mean absolute\nerror. The results also suggest that the fractional Brownian motion is a\nrelevant component of the stochastic volatility model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12790697674418605,
          "p": 0.18032786885245902,
          "f": 0.14965985909019408
        },
        "rouge-2": {
          "r": 0.035398230088495575,
          "p": 0.041666666666666664,
          "f": 0.03827750699480388
        },
        "rouge-l": {
          "r": 0.12790697674418605,
          "p": 0.18032786885245902,
          "f": 0.14965985909019408
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/other/2503.05400v1",
      "true_abstract": "We investigate site-controlled In$_{0.25}$Ga$_{0.75}$As quantum dots in\n(111)B GaAs pyramidal recesses as spin qubits. Combining scanning confocal\ncryomicroscopy, magneto-photoluminescence studies and resonant excitation, we\nidentify and isolate a positively charged exciton with a hole-spin in its\nground state. Application of a strong 5 T magnetic field parallel to the growth\naxis, induces a fourfold splitting of the energy levels of the positively\ncharged exciton creating an optically addressable double-lambda system. We\ncombine weak above-band and resonant excitation to demonstrate spin pumping and\nhigh-fidelity spin initialization through all four optical transitions and\nstudy the system behavior as a function of the resonant driving strength\nshowing the existence of a robust spin that can be optically pumped and\ninitialized. These results demonstrate the potential of these quantum dots for\nprecise spin manipulation and their relevance for future quantum hardware.",
      "generated_abstract": "A novel method for studying the electronic structure of topological insulators\nand superconductors based on the time-dependent density functional theory (TDDFT)\nis introduced. The method is based on the simultaneous evaluation of the\ndensity matrix and the Dyson equation. The method enables us to calculate the\ntime-dependent density matrix and the Green's function at the same time. The\ncalculated results are compared with the results obtained by the time-dependent\nBorn-Oppenheimer (TBBO) method. The results show that the method can reproduce\nthe results obtained by the TBBO method. The method is also applicable to other\nsystems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10309278350515463,
          "p": 0.18181818181818182,
          "f": 0.13157894275017326
        },
        "rouge-2": {
          "r": 0.007633587786259542,
          "p": 0.01282051282051282,
          "f": 0.009569373311968587
        },
        "rouge-l": {
          "r": 0.07216494845360824,
          "p": 0.12727272727272726,
          "f": 0.09210525853964704
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2502.20749v1",
      "true_abstract": "Deep learning-based medical image segmentation typically requires large\namount of labeled data for training, making it less applicable in clinical\nsettings due to high annotation cost. Semi-supervised learning (SSL) has\nemerged as an appealing strategy due to its less dependence on acquiring\nabundant annotations from experts compared to fully supervised methods. Beyond\nexisting model-centric advancements of SSL by designing novel regularization\nstrategies, we anticipate a paradigmatic shift due to the emergence of\npromptable segmentation foundation models with universal segmentation\ncapabilities using positional prompts represented by Segment Anything Model\n(SAM). In this paper, we present SemiSAM+, a foundation model-driven SSL\nframework to efficiently learn from limited labeled data for medical image\nsegmentation. SemiSAM+ consists of one or multiple promptable foundation models\nas generalist models, and a trainable task-specific segmentation model as\nspecialist model. For a given new segmentation task, the training is based on\nthe specialist-generalist collaborative learning procedure, where the trainable\nspecialist model delivers positional prompts to interact with the frozen\ngeneralist models to acquire pseudo-labels, and then the generalist model\noutput provides the specialist model with informative and efficient supervision\nwhich benefits the automatic segmentation and prompt generation in turn.\nExtensive experiments on two public datasets and one in-house clinical dataset\ndemonstrate that SemiSAM+ achieves significant performance improvement,\nespecially under extremely limited annotation scenarios, and shows strong\nefficiency as a plug-and-play strategy that can be easily adapted to different\nspecialist and generalist models.",
      "generated_abstract": "Accurate and rapid detection of micro-aneurysms in intravascular ultrasound (IVUS)\nis essential for accurate diagnosis of cardiovascular diseases. However,\ndetection of micro-aneurysms in IVUS images is a challenging task due to the\ncomplex geometry and limited spatial resolution of the micro-aneurysms. In\nthis paper, we present a novel deep learning-based approach for detecting\nmicro-aneurysms in IVUS images. Our approach is based on a deep learning model\nthat uses a convolutional neural network (CNN) to extract feature maps from\nIVUS images. These feature maps are then used to train a classifier to classify\nIVUS images into three groups: no micro-aneurysms, micro-aneurysms, and\nspurious micro-aneurysms. Our model is trained on a dataset of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16233766233766234,
          "p": 0.352112676056338,
          "f": 0.22222221790261734
        },
        "rouge-2": {
          "r": 0.036036036036036036,
          "p": 0.08,
          "f": 0.04968943671154699
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.30985915492957744,
          "f": 0.1955555512359507
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.00658v1",
      "true_abstract": "A new modular approximate Bayesian inferential framework is proposed that\nenables fast calculation of probabilistic predictions of future option prices.\nWe exploit multiple information sources, including daily spot returns,\nhigh-frequency spot data and option prices. A benefit of this modular Bayesian\napproach is that it allows us to work with the theoretical option pricing\nmodel, without needing to specify an arbitrary statistical model that links the\ntheoretical prices to their observed counterparts. We show that our approach\nproduces accurate probabilistic predictions of option prices in realistic\nscenarios and, despite not explicitly modelling pricing errors, the method is\nshown to be robust to their presence. Predictive accuracy based on the Heston\nstochastic volatility model, with predictions produced via rapid real-time\nupdates, is illustrated empirically for short-maturity options.",
      "generated_abstract": "We consider a multi-agent system with two types of agents: traders and\nconsumers. The traders can trade their assets with one another. The consumers\nhave no assets to trade, but they are allowed to borrow assets from the\ntraders. The consumers and traders compete in an auction to borrow assets from\neach other. The traders are efficient in trading their assets, and the\nconsumers are efficient in borrowing assets from the traders. The consumer\nbenefits from borrowing assets from the traders, but the trader can charge\ninterest. We consider two different scenarios: (1) when the traders are\nefficient and the consumers are not efficient, (2) when the consumers are not\nefficient and the traders are efficient. In the first case, the consumers are\nbetter off from borrowing assets from the traders. In the second case, the\nconsumers are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.16666666666666666,
          "f": 0.1333333285333335
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.16666666666666666,
          "f": 0.1333333285333335
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2412.00471v1",
      "true_abstract": "Building a general-purpose task model similar to ChatGPT has been an\nimportant research direction for gene large language models. Instruction\nfine-tuning is a key component in building ChatGPT, but existing instructions\nare primarily based on natural language. Natural language and gene sequences\nhave significant differences in tokenization and encoding. Therefore,\nconstructing a multilingual model that can handle both natural language and\ngene sequences is crucial for solving this problem.In this paper, we expand the\ncapabilities of the LLaMA large language model to include gene language. This\ninvolves expanding the vocabulary using the Byte Pair Encoding (BPE) method,\nspecifically tailored for DNA and protein sequences, and conducting further\npre-training on these sequences. We then convert various downstream gene task\ndata into a unified format for instruction fine-tuning and further fine-tune\nthe model on this data.Our study demonstrates that a mixed model of gene and\nnatural language, fine-tuned with instructions, achieves results comparable to\nthe current state-of-the-art (SOTA) in tasks such as gene classification and\ngene sequence interaction. This provides a promising direction for building a\nunified large language model for gene tasks.",
      "generated_abstract": "This article introduces a novel method for identifying rare alleles using\nhigh-throughput sequencing data. By leveraging the abundance of small RNA\nsequences from a population, we can distinguish between rare and common alleles\nwith a high degree of accuracy. This method can be applied to both RNA and DNA\nsequences, enabling the identification of rare variants in a wide range of\ngenetic and environmental settings. The method is effective even in populations\nwith low sample sizes, making it a practical tool for genetic studies and\npopulation genetics. By integrating high-throughput sequencing data with\ntraditional genetic methods, this approach provides a powerful approach for\nidentifying rare variants in complex populations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.2777777777777778,
          "f": 0.21390373857988515
        },
        "rouge-2": {
          "r": 0.005847953216374269,
          "p": 0.00980392156862745,
          "f": 0.007326002645416185
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.2361111111111111,
          "f": 0.18181817708255896
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00352v1",
      "true_abstract": "This paper reviews statistical methods for hypothesis testing and clustering\nin network models. We analyze the method by Bickel et al. (2016) for deriving\nthe asymptotic null distribution of the largest eigenvalue, noting its slow\nconvergence and the need for bootstrap corrections. The SCORE method by Jin et\nal. (2015) and the NCV method by Chen et al. (2018) are evaluated for their\nefficacy in clustering within Degree-Corrected Block Models, with NCV facing\nchallenges due to its time-intensive nature. We suggest exploring eigenvector\nentry distributions as a potential efficiency improvement.",
      "generated_abstract": "In this paper, we introduce a novel methodology for identifying\ntissue-specific gene expression patterns in cancerous tissues using a\nmultivariate, time-dependent gene expression data. Our methodology incorporates\na novel multi-way interaction term into the logistic regression model, and\nleverages the principle of least squares (PLS) regression to estimate the\ninteraction term. We demonstrate that our methodology can effectively\nidentify tissue-specific gene expression patterns in cancerous tissues. We\nanalyze the expression patterns of 48 genes in three types of human colorectal\ncancer tissues: normal, adenoma, and cancerous. Our analysis shows that the\ntissue-specific expression patterns of these genes are distinct, and there are\nsignificant differences in the expression patterns between normal and cancerous\ntissues. This paper highlights the potential of our methodology in\ncharacterizing t",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19117647058823528,
          "p": 0.17567567567567569,
          "f": 0.18309858655822273
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.02830188679245283,
          "f": 0.031578942435457835
        },
        "rouge-l": {
          "r": 0.19117647058823528,
          "p": 0.17567567567567569,
          "f": 0.18309858655822273
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09525v1",
      "true_abstract": "The complexity of continuous piecewise affine (CPA) functions can be measured\nby the number of pieces $p$ or the number of distinct affine functions $n$. For\nCPA functions on $\\mathbb{R}^d$, this paper shows an upper bound of\n$p=O(n^{d+1})$ and constructs a family of functions achieving a lower bound of\n$p=\\Omega(n^{d+1-\\frac{c}{\\sqrt{\\log_2(n)}}})$.",
      "generated_abstract": "In 1959, J. C. V. C. Veeck, Jr. published a paper on the existence of\nlittle groups, i.e., finite groups with no non-trivial conjugacy class. The\nquestion of the existence of a non-trivial conjugacy class in any finite group\nis known as the Veeck Conjecture. In 1974, A. N. Kashintsev proved the\nnon-existence of non-trivial conjugacy classes in any finite group of prime\npower order. In this paper, we prove the existence of non-trivial conjugacy\nclasses in any finite group of prime power order.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.14,
          "f": 0.1609195353415248
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.1,
          "f": 0.11494252384727198
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.07918v1",
      "true_abstract": "COVID-19 has had a large scale negative impact on the health of opioid users\nexacerbating the health of an already vulnerable population. Critical\ninformation on the total impact of COVID-19 on opioid users is unknown due to a\nlack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and\nlack of data coverage. To assess the impact of COVID-19 on small-area opioid\nmortality, we developed a Bayesian hierarchical excess opioid mortality\nmodeling approach. We incorporate spatio-temporal autocorrelation structures to\nallow for sharing of information across small areas and time to reduce\nuncertainty in small area estimates. Excess mortality is defined as the\ndifference between observed trends after a crisis and expected trends based on\nobserved historical trends, which captures the total increase in observed\nmortality rates compared to what was expected prior to the crisis. We\nillustrate the application of our approach to assess excess opioid mortality\nrisk estimates for 159 counties in GA. Using our proposed approach will help\ninform interventions in opioid-related public health responses, policies, and\nresource allocation. The application of this work also provides a general\nframework for improving the estimation and mapping of health indicators during\ncrisis periods for the opioid user population.",
      "generated_abstract": "The aim of this paper is to introduce and study the notion of generalized\ngeneralized Bott-Samelson system for finite posets. We characterize these\nsystems in terms of their minimal elements. Furthermore, we establish a\ncorrespondence between the generalized Bott-Samelson systems and generalized\nBott-Samelson sequences. We prove that every finite poset admits a generalized\nBott-Samelson system. In addition, we construct examples of finite posets which\ndo not admit a generalized Bott-Samelson system. We also give an example of a\nfinite poset which admits a generalized Bott-Samelson system, but which does\nnot have a minimal element.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13559322033898305,
          "p": 0.2857142857142857,
          "f": 0.18390804161183785
        },
        "rouge-2": {
          "r": 0.005405405405405406,
          "p": 0.012345679012345678,
          "f": 0.007518792756800396
        },
        "rouge-l": {
          "r": 0.1271186440677966,
          "p": 0.26785714285714285,
          "f": 0.17241378873827462
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.03323v1",
      "true_abstract": "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility.",
      "generated_abstract": "This paper examines the effectiveness of a carbon tax in reducing emissions\nunder a scenario of economic growth. We use a two-sector model of a\nprovincial economy, where the economy is split into two sectors: one is\nconsumer-based and the other is industrial-based. The model considers a carbon\ntax rate of 50% with a five-year time horizon. The model assumes that\neconomic growth is driven by investment and that the economy is open to the\nworld. We find that the carbon tax has a positive impact on the economy and\nthat the effect is greater in the longer time horizon. We also find that the\nreduction in emissions is greater in the consumer-based sector than in the\nindustrial-based sector, with the industrial sector emitting more carbon dioxide\nthan the consumer-based sector. The carbon tax has a significant impact on\neconomic growth and em",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.17391304347826086,
          "f": 0.15286623711144484
        },
        "rouge-2": {
          "r": 0.05172413793103448,
          "p": 0.05172413793103448,
          "f": 0.05172413293103497
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.15942028985507245,
          "f": 0.14012738360826013
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.10206v1",
      "true_abstract": "A graph $G$ is perfectly divisible if, for every induced subgraph $H$ of $G$,\neither $V(H)$ is a stable set or admits a partition into two sets $X_1$ and\n$X_2$ such that $\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is a perfect graph.\nIn this article, we propose the following generalisation of perfectly divisible\ngraphs. A graph $G$ is perfectly $1$-divisible if $G$ is perfect and perfectly\n$k$-divisible if, for every induced subgraph $H$ of $G$, either $V(H)$ is a\nstable set or admits a partition into two sets $X_1$ and $X_2$ such that\n$\\omega(H[X_1]) < \\omega(H)$ and $H[X_2]$ is perfectly $(k-1)$-divisible, $k\n\\in \\mathbb{N}_{> 1}$. Our main result establishes that every perfectly\n$k$-divisible graph $G$ satisfies $\\chi(G) \\leq \\binom{\\omega(G)+k-1}{k}$ which\ngeneralises the known bound for perfectly divisible graphs.",
      "generated_abstract": "In this paper, we consider a new class of stochastic partial differential\nequations, namely the stochastic Schr\\\"odinger equation (SSSE) with a\ngeneralised stochastic forcing. We prove that the associated spectral problem\nadmits a unique solution in the Sobolev space $H^1$. Furthermore, we show that\nthis solution can be represented in a unique way by a solution of a\nnon-linear stochastic partial differential equation (NLSPE). We then show that\nthe solution of the NLSPE can be expressed in a unique way by a solution of a\nstochastic Schr\\\"odinger equation with a generalised stochastic forcing. We\nalso show that the SSSE admits a unique solution in the Sobolev space $H^2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.16326530612244897,
          "f": 0.1415929154420865
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.02631578947368421,
          "f": 0.025157227714094576
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.16326530612244897,
          "f": 0.1415929154420865
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.11019v1",
      "true_abstract": "The domain of hedge fund investments is undergoing significant\ntransformation, influenced by the rapid expansion of data availability and the\nadvancement of analytical technologies. This study explores the enhancement of\nhedge fund investment performance through the integration of machine learning\ntechniques, the application of PolyModel feature selection, and the analysis of\nfund size. We address three critical questions: (1) the effect of machine\nlearning on trading performance, (2) the role of PolyModel feature selection in\nfund selection and performance, and (3) the comparative reliability of larger\nversus smaller funds.\n  Our findings offer compelling insights. We observe that while machine\nlearning techniques enhance cumulative returns, they also increase annual\nvolatility, indicating variability in performance. PolyModel feature selection\nproves to be a robust strategy, with approaches that utilize a comprehensive\nset of features for fund selection outperforming more selective methodologies.\nNotably, Long-Term Stability (LTS) effectively manages portfolio volatility\nwhile delivering favorable returns. Contrary to popular belief, our results\nsuggest that larger funds do not consistently yield better investment outcomes,\nchallenging the assumption of their inherent reliability.\n  This research highlights the transformative impact of data-driven approaches\nin the hedge fund investment arena and provides valuable implications for\ninvestors and asset managers. By leveraging machine learning and PolyModel\nfeature selection, investors can enhance portfolio optimization and reassess\nthe dependability of larger funds, leading to more informed investment\nstrategies.",
      "generated_abstract": "This paper presents an efficient and scalable methodology for estimating\nthe price of the S&P 500 index (SPX) based on the stochastic volatility model.\nThis methodology is based on the combination of a neural network (NN)\napproach and the method of moments (MoM). The NN is used to forecast the SPX\nprice at the next trading day, and the MoM is used to estimate the volatility\nlevel. The proposed methodology is tested using historical data from 1984 to\n2022, and the results show that it outperforms existing methods, including the\nMonte Carlo method and the method of moments. The NN-based methodology is also\ncompared with the NN-based methodology of the author in the paper \"A\nneural-network-based approach for the estimation of the price of the S&P 50",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2465753424657534,
          "f": 0.16589861304678388
        },
        "rouge-2": {
          "r": 0.009708737864077669,
          "p": 0.018691588785046728,
          "f": 0.012779548215865785
        },
        "rouge-l": {
          "r": 0.11805555555555555,
          "p": 0.2328767123287671,
          "f": 0.15668202318503274
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.07625v1",
      "true_abstract": "An empirical stochastic analysis of high-frequency, tick-by-tick order data\nof NASDAQ100 listed stocks is conducted using a first-order discrete-time\nMarkov chain model to explore intraday order transition dynamics. This analysis\nfocuses on three market cap categories: High, Medium, and Low. Time-homogeneous\ntransition probability matrices are estimated and compared across time-zones\nand market cap categories, and we found that limit orders exhibit higher degree\nof inertia (DoI), i.e., the probability of placing consecutive limit order is\nhigher, during the opening hour. However, in the subsequent hour, the DoI of\nlimit order decreases, while that of market order increases. Limit order\nadjustments via additions and deletions of limit orders increases significantly\nafter the opening hour. All the order transitions then stabilize during\nmid-hours. As the closing hour approaches, consecutive order executions surge,\nwith decreased placement of buy and sell limit orders following sell and buy\nexecutions, respectively. In terms of the differences in order transitions\nbetween stocks of different market cap, DoI of orders is stronger in high and\nmedium market cap stocks. On the other hand, lower market cap stocks show a\nhigher probability of limit order modifications and greater likelihood of\nsubmitting sell/buy limit orders after buy/sell executions. Further, order\ntransitions are clustered across all stocks, except during opening and closing\nhours. The findings of this study may be useful in understanding intraday order\nplacement dynamics across stocks of varying market cap, thus aiding market\nparticipants in making informed order placements at different times of trading\nhour.",
      "generated_abstract": "We introduce a novel, highly efficient, and scalable methodology for\nquantifying the impact of strategic behaviors in financial markets. Our\napproach is based on the identification of a two-level game that includes\ninvestors, traders, and firms, where investors and traders can influence the\nfirm's behavior through a series of interconnected strategic actions. We\nestablish a mathematical framework for the analysis of the impact of these\nactions, and we propose a methodology for the estimation of the impact of\nstrategic behavior on stock returns. The framework provides a rigorous,\nmathematical basis for analyzing the impact of strategic behaviors on financial\nmarkets, and it facilitates the implementation of practical strategies for\nenhancing financial decision-making. We apply our framework to the analysis of\nthe impact of strategic behavior on stock returns in the context of a\nstrategic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08391608391608392,
          "p": 0.16901408450704225,
          "f": 0.11214952827626885
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.03636363636363636,
          "f": 0.023668634662652684
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.15492957746478872,
          "f": 0.1028037338837455
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2503.05340v1",
      "true_abstract": "Matrix-valued time series analysis has gained prominence in econometrics and\nfinance due to the increasing availability of high-dimensional data with\ninherent matrix structures. Traditional approaches, such as Matrix\nAutoregressive (MAR) models and Dynamic Matrix Factor (DMF) models, often\nimpose restrictive assumptions that may not align with real-world data\ncomplexities. To address this gap, we propose a novel Matrix Autoregressive\nwith Common Factors (MARCF) model, which bridges the gap between MAR and DMF\nframeworks by introducing common bases between predictor and response\nsubspaces. The MARCF model achieves significant dimension reduction and enables\na more flexible and interpretable factor representation of dynamic\nrelationships. We develop a computationally efficient estimator and a gradient\ndescent algorithm. Theoretical guarantees for computational and statistical\nconvergence are provided, and extensive simulations demonstrate the robustness\nand accuracy of the model. Applied to a multinational macroeconomic dataset,\nthe MARCF model outperforms existing methods in forecasting and provides\nmeaningful insights into the interplay between countries and economic factors.",
      "generated_abstract": "This paper introduces a novel methodology for the estimation of\ninteracting stochastic volatility models with multiple latent shocks.\nInteracting stochastic volatility models are widely used in financial\neconometrics, and the identification of the model parameters is of key\ninterest. However, the identification of the model parameters is challenging\nwhen there are multiple latent shocks, which leads to the challenge of\nidentifying the parameters. This paper proposes an approach to address this\nchallenge by using the maximum likelihood estimator (MLE) in the presence of\nmultiple latent shocks. The proposed methodology is applied to the\ninteracting stochastic volatility model with the daily data of S\\&P 500 stocks\nfrom 1987 to 2022. The results show that the proposed methodology effectively\nidentifies the model parameters.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16260162601626016,
          "p": 0.2857142857142857,
          "f": 0.20725388138741987
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.049019607843137254,
          "f": 0.03891050104922163
        },
        "rouge-l": {
          "r": 0.16260162601626016,
          "p": 0.2857142857142857,
          "f": 0.20725388138741987
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DS/2503.09802v1",
      "true_abstract": "We study the task of list-decodable linear regression using batches. A batch\nis called clean if it consists of i.i.d. samples from an unknown linear\nregression distribution. For a parameter $\\alpha \\in (0, 1/2)$, an unknown\n$\\alpha$-fraction of the batches are clean and no assumptions are made on the\nremaining ones. The goal is to output a small list of vectors at least one of\nwhich is close to the true regressor vector in $\\ell_2$-norm. [DJKS23] gave an\nefficient algorithm, under natural distributional assumptions, with the\nfollowing guarantee. Assuming that the batch size $n$ satisfies $n \\geq\n\\tilde{\\Omega}(\\alpha^{-1})$ and the number of batches is $m = \\mathrm{poly}(d,\nn, 1/\\alpha)$, their algorithm runs in polynomial time and outputs a list of\n$O(1/\\alpha^2)$ vectors at least one of which is\n$\\tilde{O}(\\alpha^{-1/2}/\\sqrt{n})$ close to the target regressor. Here we\ndesign a new polynomial time algorithm with significantly stronger guarantees\nunder the assumption that the low-degree moments of the covariates distribution\nare Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant\n$\\delta>0$, as long as the batch size is $n \\geq\n\\Omega_{\\delta}(\\alpha^{-\\delta})$ and the degree-$\\Theta(1/\\delta)$ moments of\nthe covariates are SoS certifiably bounded, our algorithm uses $m =\n\\mathrm{poly}((dn)^{1/\\delta}, 1/\\alpha)$ batches, runs in polynomial-time, and\noutputs an $O(1/\\alpha)$-sized list of vectors one of which is\n$O(\\alpha^{-\\delta/2}/\\sqrt{n})$ close to the target. That is, our algorithm\nachieves substantially smaller minimum batch size and final error, while\nachieving the optimal list size. Our approach uses higher-order moment\ninformation by carefully combining the SoS paradigm interleaved with an\niterative method and a novel list pruning procedure. In the process, we give an\nSoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader\napplicability.",
      "generated_abstract": "In this paper, we introduce a novel approach for optimizing the performance of\n(multi-agent) reinforcement learning agents in multi-robot systems. By leveraging\na deep reinforcement learning agent to optimize the interactions of multiple\nagents, we are able to significantly reduce the number of communication\nmessages required between agents, thereby reducing communication latency and\nincreasing the overall efficiency of the system. Our approach is based on a\nnovel deep reinforcement learning agent, which we term as the \"coordinator\".\nThis agent is trained to optimize the interactions of the agents in a\nmulti-agent system, by leveraging the communication history between the agents.\nWe also introduce a novel technique to handle the communication latency by\noptimizing the weight of the communication history, which allows the coordinator\nto dynamically adjust its weight. Additionally, we introduce a novel technique\nto handle the latency in the communication between the agents by introdu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.28378378378378377,
          "f": 0.17796609739011787
        },
        "rouge-2": {
          "r": 0.024896265560165973,
          "p": 0.05128205128205128,
          "f": 0.03351954867248271
        },
        "rouge-l": {
          "r": 0.11728395061728394,
          "p": 0.25675675675675674,
          "f": 0.161016944847745
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.01864v1",
      "true_abstract": "This paper introduces a novel approach to optimizing portfolio rebalancing by\nintegrating Graph Neural Networks (GNNs) for predicting transaction costs and\nDijkstra's algorithm for identifying cost-efficient rebalancing paths. Using\nhistorical stock data from prominent technology firms, the GNN is trained to\nforecast future transaction costs, which are then applied as edge weights in a\nfinancial asset graph. Dijkstra's algorithm is used to find the least costly\npath for reallocating capital between assets. Empirical results show that this\nhybrid approach significantly reduces transaction costs, offering a powerful\ntool for portfolio managers, especially in high-frequency trading environments.\nThis methodology demonstrates the potential of combining advanced machine\nlearning techniques with classical optimization algorithms to improve financial\ndecision-making processes. Future research will explore expanding the asset\nuniverse and incorporating reinforcement learning for continuous portfolio\noptimization.",
      "generated_abstract": "The evolution of financial markets has been driven by the growth of the\nmarket size, which has increased at a rate of 3.5% per year on average since the\n1990s. However, this growth is not evenly distributed. The growth of the\nmarket size in the United States and Europe is much faster than that in the\nAsian economies. This paper presents a novel approach for analyzing the\nevolution of market size and identifies the key drivers of the market growth.\nWe focus on the period from 1990 to 2022, and we use a multi-spline model to\nobtain a high-resolution series of market size, which is then used to estimate\nthe market growth rate. The results show that the market size in the United\nStates and Europe has grown at a rate of 3.5% per year on average, while the\nmarket size in Asia has grown",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21568627450980393,
          "p": 0.2857142857142857,
          "f": 0.24581005096345315
        },
        "rouge-2": {
          "r": 0.046875,
          "p": 0.05309734513274336,
          "f": 0.049792526139701955
        },
        "rouge-l": {
          "r": 0.20588235294117646,
          "p": 0.2727272727272727,
          "f": 0.23463686660591127
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.00308v1",
      "true_abstract": "We propose a solution to the problem of bargaining with transfers, along with\nan axiomatisation of the solution. Inefficient allocations in the bargaining\nset can influence the solution, but are discounted relative to more efficient\nones. The key axioms are additivity and a property we call \\emph{inverse\nmonotonicity}, which states that adding an allocation to the bargaining set\nthat is worse for a given player than the initial solution cannot benefit that\nplayer.",
      "generated_abstract": "In this paper, we study the equilibrium of a two-sector model with a\nstrategic equilibrium. The model consists of two sectors with heterogeneous\ndemand functions, and the firm has a strategic choice to switch between the two\nsectors. The firm has a positive incentive to maximize its welfare and\nstrategically chooses to switch between the two sectors. We show that if the\nfirm's switching cost is lower than its production cost, the firm has an\nequilibrium with a single sector. If the switching cost is lower than the\nproduction cost, the firm has an equilibrium with two sectors. If the switching\ncost is lower than both the production cost and the switching cost, the firm\nhas an equilibrium with three sectors. Our results can be interpreted as a\nversion of the Nash solution where the firm's switching cost is lower than the\nproduction cost and the switching cost is lower",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28846153846153844,
          "p": 0.24193548387096775,
          "f": 0.2631578897753155
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.021052631578947368,
          "f": 0.024390239028109245
        },
        "rouge-l": {
          "r": 0.2692307692307692,
          "p": 0.22580645161290322,
          "f": 0.2456140301261928
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.09345v1",
      "true_abstract": "This report documents the development, test, and application of Large\nLanguage Models (LLMs) for automated text analysis, with a specific focus on\ngambling-like elements in digital games, such as lootboxes. The project aimed\nnot only to analyse user opinions and attitudes towards these mechanics, but\nalso to advance methodological research in text analysis. By employing\nprompting techniques and iterative prompt refinement processes, the study\nsought to test and improve the accuracy of LLM-based text analysis. The\nfindings indicate that while LLMs can effectively identify relevant patterns\nand themes on par with human coders, there are still challenges in handling\nmore complex tasks, underscoring the need for ongoing refinement in\nmethodologies. The methodological advancements achieved through this study\nsignificantly enhance the application of LLMs in real-world text analysis. The\nresearch provides valuable insights into how these models can be better\nutilized to analyze complex, user-generated content.",
      "generated_abstract": "This paper introduces a novel approach to the study of the evolution of\nmarket power in the presence of exogenous markups, focusing on the\nrelationship between market power and the level of market concentration. By\nintegrating a dynamic stochastic general equilibrium model of the economy with\nadditional information on the level of market concentration, we show how\nmarket power evolves under varying market concentration levels. We show that,\nfor a given level of market concentration, market power may exhibit either a\npreferential or a general equilibrium dynamic. For the preferential case,\nmarket power evolves as a function of the level of market concentration. For\nthe general equilibrium dynamic, market power evolves in the same manner as the\nlevel of market concentration. We find that, under varying market concentration\nlevels, market power evolves in either a preferential or a general equilibrium\nmanner, and that the evolution of market power is in general discontinu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13761467889908258,
          "p": 0.23076923076923078,
          "f": 0.1724137884231736
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.009523809523809525,
          "f": 0.008163260408166203
        },
        "rouge-l": {
          "r": 0.12844036697247707,
          "p": 0.2153846153846154,
          "f": 0.1609195355496104
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-th/2503.10595v1",
      "true_abstract": "We present compact two-loop QCD corrections in the leading-color\napproximation for the production of an electroweak vector boson, $V =\n\\{W^{\\pm}, Z,\\gamma^\\star\\}$, in association with two light jets ($Vjj$) at\nhadron colliders. Leptonic decays of the electroweak boson are included at the\namplitude level. Working in the analytic-reconstruction approach, we develop\ntwo techniques to build compact partial-fraction forms for individual rational\nfunctions. One approach exploits their analytic structure. In the other, we\niteratively construct subtraction terms that match the rational functions in\nsingular limits. Moreover, we show how the singular behavior of the rational\nfunctions can be systematically used to find a more compact basis of the space\nthat they span. We apply our techniques to the $Vjj$ amplitudes, yielding a\nrepresentation that is three orders of magnitude smaller than previous results.\nWe then use these compact expressions to provide an efficient and stable C++\nnumerical implementation suitable for phenomenological applications.",
      "generated_abstract": "We present a novel method for determining the phase of the weak mixing\nclasic mixing angle in the CKM model. Our method relies on the fact that the\nphase of the weak mixing angle in the CKM model can be determined from the\nphase of the neutrino oscillation parameters. We show that the phase of the\nweak mixing angle can be determined from the phase of the neutrino oscillation\nparameters in a way that is compatible with the CKM model. We also discuss the\npossibility of obtaining a value for the phase of the weak mixing angle which\nis compatible with the CKM model and the observed values of the neutrino\noscillation parameters. We show that this possibility is possible if the\ncorresponding phase of the weak mixing angle is not exactly zero. We also show\nthat the phase of the weak mixing angle can be determined from the phase of\nthe neutrino oscillation parameters",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.28,
          "f": 0.17610062461927942
        },
        "rouge-2": {
          "r": 0.04195804195804196,
          "p": 0.0759493670886076,
          "f": 0.05405404946960513
        },
        "rouge-l": {
          "r": 0.11926605504587157,
          "p": 0.26,
          "f": 0.1635220082670782
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.econ/GN/2412.10421v1",
      "true_abstract": "The global food system provides the energy that supports human metabolism,\nwith complex spatial interdependencies between food production, transformation,\nand consumption. Empirical food system data for these global processes are\noften fragmented and inconsistent, with only certain components captured in\nspatially resolved formats. Here we propose a flexible approach to allocating\ncountry-level food system data subnationally. We estimate the spatial patterns\nof food energy production and supply, which we compare to estimates of human\nmetabolism based on average body size. We downscale these rates onto a\none-degree resolution grid with 95 corresponding food types to derive an\ninternally consistent, energy-conserving, and spatially resolved dataset. We\nshow that national food supply varies linearly with metabolism per capita, with\nabout half the variation in food supply explained by metabolic rates. Our data\nprocessing pipeline is openly available and can readily incorporate new inputs\nin order to advance trans-disciplinary food system modeling efforts.",
      "generated_abstract": "We explore the political economy of the COVID-19 pandemic in the United\nStates, with a focus on the effects of the pandemic on voting behavior and\nthe role of democracy in mitigating the effects of economic downturns. We\nemploy a quantitative panel dataset that includes voting data from the 2016\nelection to the 2020 presidential election. We find that the pandemic had a\nnegative impact on voting behavior, with a decline in voting rates of 2.7%\n(standard error: 0.5%). However, we also find that these declines were\npartly offset by a rise in voter turnout, which rose by 1.3 percentage points\n(standard error: 0.3%). The increase in turnout was driven by a rise in\nelection-related activity, including volunteering and donating to political\ncandidates. We find",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16981132075471697,
          "p": 0.225,
          "f": 0.19354838219447348
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16037735849056603,
          "p": 0.2125,
          "f": 0.18279569402243048
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.09898v1",
      "true_abstract": "Dynamic simulation plays a crucial role in power system transient stability\nanalysis, but traditional numerical integration-based methods are\ntime-consuming due to the small time step sizes. Other semi-analytical solution\nmethods, such as the Differential Transformation method, often struggle to\nselect proper orders and steps, leading to slow performance and numerical\ninstability. To address these challenges, this paper proposes a novel adaptive\ndynamic simulation approach for power system transient\n  stability analysis. The approach adds feedback control and optimization to\nselecting the step and order, utilizing the Differential Transformation method\nand a proportional-integral control strategy to control truncation errors.\nOrder selection is formulated as an optimization problem resulting in a\nvariable-step-optimal-order method that achieves significantly larger time step\nsizes without violating numerical stability. It is applied to three systems:\nthe IEEE 9-bus, 3-generator system, IEEE 39-bus, 10-generator system, and a\nPolish 2383-bus, 327-generator system, promising computational efficiency and\nnumerical robustness for large-scale power system is demonstrated in\ncomprehensive case studies.",
      "generated_abstract": "This paper presents a novel hybrid optimization framework for the control of\noptimally-tracking robots using a single on-board AI agent. The proposed\nframework utilizes a multi-agent reinforcement learning (MARL) algorithm to\noptimize the robot's trajectory by learning optimal control policies for the\nrobot's end-effector. The agent is trained using reinforcement learning with\nreal-time feedback from the on-board cameras, which is then used to\noptimally-control the robot's trajectory. The agent is trained by interacting\nwith a virtual environment using simulated data. The agent is trained to\noptimally-control the robot's trajectory using a single camera and virtual\nenvironment. The agent is trained to optimize the robot's trajectory using a\nsingle camera and virtual environment. The performance of the proposed method\nis demonstrated through simulations and experiments. The proposed method\nachieved",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1388888888888889,
          "p": 0.25,
          "f": 0.17857142397959197
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.02197802197802198,
          "f": 0.016806717965893565
        },
        "rouge-l": {
          "r": 0.1388888888888889,
          "p": 0.25,
          "f": 0.17857142397959197
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ph/2503.10347v1",
      "true_abstract": "Through the black hole (BH) superradiance, ultralight bosons can form dense\nclouds around rotating Kerr BHs. Certain ultralight bosons, such as axions and\naxion-like particles (promising dark matter candidates), naturally possess\nself-interactions, and thus may significantly modify the dynamics of the\nsuperradiance process. Previous studies on the detection or constraint of\nultralight bosons through superradiance have largely neglected the\nself-interaction effects of bosons. In this work, we investigate the formation\nand evolution of self-interacting boson clouds in the full Kerr spacetime\nduring BH superradiance. Using numerical methods, we compute the superradiant\ngrowth rate of boson clouds with self-interactions around Kerr BHs and\nquantitatively evaluate how the self-interaction strength of scalar bosons\naffects the growth rate. We also assess the evolution of the BH's mass and\nspin. Our results reveal that, in addition to the superradiance-imposed upper\nbound on the boson cloud mass, self-interactions of ultralight bosons introduce\na new, lower critical mass limit, beyond which the growth rate of the boson\ncloud approaches zero. This implies that the superradiance process terminates\nearlier when self-interactions are considered. Furthermore, we explore how\nself-interactions affect both the oscillation frequency of boson clouds in\ngravitational atoms and the frequency of gravitational wave (GW) emitted\nthrough cloud annihilation. The anticipated frequency shift could be detectable\nby the GW observatories. Given that self-interactions substantially alter the\nevolution of BH superradiance, their effects can significantly relax existing\nconstraints on scalar boson models derived from superradiance. Taking the spin\nmeasurements from GW190412 and GW190517 as examples, we discuss the impact of\nself-interactions on constraint results in details.",
      "generated_abstract": "We consider a system of two massless scalar bosons interacting via a\nconformal scalar field in the presence of a Gaussian cutoff. The bosonic\npropagator is given by the conformal anomaly, which is modified by the cutoff\nand can be written as a sum of two-point functions. We show that the anomalous\npropagator can be obtained from the correlator of the conformal scalar field\nand the Gaussian cutoff, and the latter can be calculated by the\nthree-point function of the conformal scalar field and the Gaussian cutoff.\nThis allows us to compute the anomalous propagator for the first time in the\ncontext of conformal anomalies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12337662337662338,
          "p": 0.3392857142857143,
          "f": 0.18095237704126993
        },
        "rouge-2": {
          "r": 0.029914529914529916,
          "p": 0.08235294117647059,
          "f": 0.04388714342626385
        },
        "rouge-l": {
          "r": 0.12337662337662338,
          "p": 0.3392857142857143,
          "f": 0.18095237704126993
        }
      }
    },
    {
      "paper_id": "hep-ph.hep-ex/2503.09280v1",
      "true_abstract": "An excess observed in the accelerator neutrino experiments in the $\\nu_{\\mu}\n\\rightarrow \\nu_{e}$ channel at high confidence level (CL) has been interpreted\nas due to eV-scale sterile neutrino(s). But, it has been suffered from the\nproblem of ``appearance - disappearance tension'' at the similarly high CL\nbecause the measurements of the $\\nu_{\\mu} \\rightarrow \\nu_{\\mu}$ channel do\nnot observe the expected event number depletion corresponding to the sterile\ncontribution in the appearance channel. We suggest non-unitarity as a simple\nand natural way of resolving the tension, which leads us to construct the\nnon-unitary $(3+1)$ model. With reasonable estimation of the parameters\ngoverning non-unitarity, we perform an illustrative analysis to know if the\ntension is resolved in this model. At the best fit of the appearance signature\nwe have found the unique solution with $\\sin^2 2\\theta_{14} \\approx 0.3$, which\nis consistent with the (reactors + Ga) data combined fit. Unexpectedly, our\ntension-easing mechanism bridges between the two high CL signatures, the BEST\nand LSND-MiniBooNE anomalies.",
      "generated_abstract": "We present the results of a study of the Higgs boson decaying to a\n$ZZ$ final state, using data collected by the CMS experiment at the LHC in 2023.\nThe analysis focuses on the process $ZZ \\to b \\bar b h$, which is expected to\nbe dominant at the LHC. We perform a comprehensive analysis of the Higgs boson\ndecay pattern and its associated backgrounds, and estimate the expected signal\nand background yields. Additionally, we explore the sensitivity of our results\nto the Higgs boson mass, decay branching ratios, and other parameters of the\nmodel. Our results constrain the Higgs boson mass to be in the range of\n$100-130$ GeV, while the branching ratios of $ZZ \\to b \\bar b h$ are constrained\nto be in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15517241379310345,
          "p": 0.2465753424657534,
          "f": 0.19047618573500197
        },
        "rouge-2": {
          "r": 0.03896103896103896,
          "p": 0.05555555555555555,
          "f": 0.04580152187168632
        },
        "rouge-l": {
          "r": 0.12931034482758622,
          "p": 0.2054794520547945,
          "f": 0.1587301539889702
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2503.00640v1",
      "true_abstract": "Laplacian matrices are commonly employed in many real applications, encoding\nthe underlying latent structural information such as graphs and manifolds. The\nuse of the normalization terms naturally gives rise to random matrices with\ndependency. It is well-known that dependency is a major bottleneck of new\nrandom matrix theory (RMT) developments. To this end, in this paper, we\nformally introduce a class of generalized (and regularized) Laplacian matrices,\nwhich contains the Laplacian matrix and the random adjacency matrix as a\nspecific case, and suggest the new framework of the asymptotic theory of\neigenvectors for latent embeddings with generalized Laplacian matrices\n(ATE-GL). Our new theory is empowered by the tool of generalized quadratic\nvector equation for dealing with RMT under dependency, and delicate high-order\nasymptotic expansions of the empirical spiked eigenvectors and eigenvalues\nbased on local laws. The asymptotic normalities established for both spiked\neigenvectors and eigenvalues will enable us to conduct precise inference and\nuncertainty quantification for applications involving the generalized Laplacian\nmatrices with flexibility. We discuss some applications of the suggested ATE-GL\nframework and showcase its validity through some numerical examples.",
      "generated_abstract": "In this work, we consider the problem of estimating the mean and variance of\na function of random variables in a noisy setting. The function of random\nvariables is a function defined over a finite set of random variables, which\ncan be the result of sampling from a probability distribution. The noise\nmodel is a stochastic process that models the uncertainties in the random\nvariables. We propose a Bayesian nonparametric estimator that uses the\ninformation from the noisy data to estimate the parameters of the function of\nrandom variables. Our estimator is based on the posterior distribution of the\nfunction of random variables. We present theoretical results for the\nconvergence of our estimator and show that our estimator is consistent. We\npresent numerical simulations to demonstrate the effectiveness of our\nestimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17543859649122806,
          "p": 0.29411764705882354,
          "f": 0.21978021509962573
        },
        "rouge-2": {
          "r": 0.023668639053254437,
          "p": 0.03669724770642202,
          "f": 0.028776973650174176
        },
        "rouge-l": {
          "r": 0.14912280701754385,
          "p": 0.25,
          "f": 0.1868131821325928
        }
      }
    },
    {
      "paper_id": "cs.AI.stat/OT/2502.14581v1",
      "true_abstract": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models.",
      "generated_abstract": "We present a new framework for the estimation of conditional and\nrelational dependencies in high-dimensional data, based on the notion of\n\"entanglement\". This framework generalizes the existing notion of conditional\nentanglement, which has been extensively used in the literature. We show that\nthe proposed entanglement framework captures the essential characteristics of\nboth conditional and relational dependencies. We propose a novel algorithm for\nthe estimation of entanglement, which is based on a novel algorithm for the\nestimation of conditional entanglement. We apply our framework to several\napplications, including the estimation of entanglement of the joint\ndistribution of the state of a quantum system and the state of a local\nnon-unitary operation, entanglement of the joint distribution of the state of a\nquantum system and a local unitary operation, and entanglement of the joint\ndistribution of the state of a quantum system and a local non-unitary",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15254237288135594,
          "p": 0.15789473684210525,
          "f": 0.15517240879458993
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.010869565217391304,
          "f": 0.012820507981592883
        },
        "rouge-l": {
          "r": 0.15254237288135594,
          "p": 0.15789473684210525,
          "f": 0.15517240879458993
        }
      }
    },
    {
      "paper_id": "physics.acc-ph.physics/acc-ph/2503.10122v1",
      "true_abstract": "Recently, the experimental discovery of a new type of wakefield effect, the\n\"skewed wake effect\", has been reported. We provide an explanation of the\nnature of the skewed wake effect based on a simple three-particle model that we\nhave developed. Taking a step forward, we analyze this effect for the case of a\nhighly elliptical beam, provide a simple estimate of the skew angle, and\nanalyze the wake amplitude effects associated with this effect.",
      "generated_abstract": "The experimental investigation of high-energy cosmic rays (HECRs) and their\ninteractions in the atmosphere has become an important field of research in\nparticle physics. The detection and analysis of HECRs and their interactions\nwith the Earth's atmosphere are challenging due to their high energies, low\ndensity of background interactions, and low signal-to-background ratio.\nHowever, recent advancements in atmospheric muon detectors have allowed for the\ndetection of cosmic rays at energies above 10^{20} eV. These detectors\ncontain a large number of muons that can be used to detect cosmic rays. In this\npaper, we present the results of a comprehensive study of the detector\nperformance and its application to the detection of HECRs. We focus on the\nhigh-energy muon detector, MuonDet, which is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3,
          "p": 0.18072289156626506,
          "f": 0.22556390508225463
        },
        "rouge-2": {
          "r": 0.04411764705882353,
          "p": 0.02631578947368421,
          "f": 0.0329670282864395
        },
        "rouge-l": {
          "r": 0.24,
          "p": 0.14457831325301204,
          "f": 0.18045112312736739
        }
      }
    },
    {
      "paper_id": "econ.TH.q-fin/PM/2410.19030v3",
      "true_abstract": "We present a theory of expected utility with state-dependent linear utility\nfunctions for monetary returns, that incorporates the possibility of\nloss-aversion. Our results relate to first order stochastic dominance,\nmean-preserving spread, increasing-concave linear utility profiles and risk\naversion. As an application of the expected utility theory developed here, we\nanalyze the contract that a monopolist would offer in an insurance market that\nallowed for partial coverage of loss.",
      "generated_abstract": "This study investigates the relationship between the probability of\nand the probability of failure in a stochastic control problem. We show that\nthe probability of failure is a deterministic function of the initial\nprobability of success. We use the results to develop a framework for\npredicting the probability of failure in a stochastic control problem. This\napproach provides a framework for quantifying the impact of uncertainties on\nthe probability of failure, and it allows us to identify potential risks in\ncomplex systems. This approach is particularly useful in situations where\nuncertainty affects the likelihood of a successful outcome. The results in this\npaper provide a theoretical framework for quantifying the impact of\nuncertainty on the probability of failure in complex systems. We demonstrate\nthat the probability of failure is a deterministic function of the initial\nprobability of success, and we propose a framework for predicting the\nprobability of failure in a stochastic control problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23076923076923078,
          "p": 0.19047619047619047,
          "f": 0.20869564721965986
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.010309278350515464,
          "f": 0.01234567420743976
        },
        "rouge-l": {
          "r": 0.23076923076923078,
          "p": 0.19047619047619047,
          "f": 0.20869564721965986
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2412.11211v1",
      "true_abstract": "State-space models (SSMs) offer a powerful framework for dynamical system\nanalysis, wherein the temporal dynamics of the system are assumed to be\ncaptured through the evolution of the latent states, which govern the values of\nthe observations. This paper provides a selective review of recent advancements\nin deep neural network-based approaches for SSMs, and presents a unified\nperspective for discrete time deep state space models and continuous time ones\nsuch as latent neural Ordinary Differential and Stochastic Differential\nEquations. It starts with an overview of the classical maximum likelihood based\napproach for learning SSMs, reviews variational autoencoder as a general\nlearning pipeline for neural network-based approaches in the presence of latent\nvariables, and discusses in detail representative deep learning models that\nfall under the SSM framework. Very recent developments, where SSMs are used as\nstandalone architectural modules for improving efficiency in sequence modeling,\nare also examined. Finally, examples involving mixed frequency and\nirregularly-spaced time series data are presented to demonstrate the advantage\nof SSMs in these settings.",
      "generated_abstract": "We consider the problem of learning a function $f: \\mathbb{R}^d \\rightarrow\n\\mathbb{R}^p$ under the assumption that the data distribution is\nunknown. We assume that the function $f$ can be represented as the sum of\nhigh-order contributions of the data: $f = f_0 + f_1 + \\cdots + f_N$. We\nintroduce a novel objective function that measures the difference between the\ntrue function $f$ and the approximation $f_0 + \\cdots + f_N$ as well as the\ndifference between the approximation $f_0 + \\cdots + f_N$ and the function\n$f$. The proposed objective function is a weighted sum of the squared\ndiscrepancies between the true and the approximation functions and the\ndiscrepancies between the approximation and the true function. We show that the\nproposed objective function is convex and has a unique",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.17543859649122806,
          "f": 0.1197604745455199
        },
        "rouge-2": {
          "r": 0.012422360248447204,
          "p": 0.021505376344086023,
          "f": 0.015748026854425074
        },
        "rouge-l": {
          "r": 0.07272727272727272,
          "p": 0.14035087719298245,
          "f": 0.09580837873713673
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.09463v1",
      "true_abstract": "Co-developing scientific algorithms and hardware accelerators requires\ndomain-specific knowledge and large engineering resources. This leads to a slow\ndevelopment pace and high project complexity, which creates a barrier to entry\nthat is too high for the majority of developers to overcome. We are developing\na reusable end-to-end compiler toolchain for the Julia language entirely built\non permissively-licensed open-source projects. This unifies accelerator and\nalgorithm development by automatically synthesising Julia source code into\nhigh-performance Verilog.",
      "generated_abstract": "We present a novel framework for training and evaluating language models\nin an end-to-end fashion. Our approach, called End-to-End Language Models\n(E2ELMs), builds on the E2E framework for training neural networks, but\nextends it to language models by integrating both a generative model and a\ndiscriminative model. The generative model learns to generate the text data,\nand the discriminative model learns to distinguish between real and fake\ngenerated text. We demonstrate that E2ELMs significantly outperforms\nexisting state-of-the-art approaches for modeling text and language, with\nsignificantly improved performance in many tasks. Our code is available at\nhttps://github.com/alan-turing-institute/E2ELMs.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21311475409836064,
          "p": 0.18840579710144928,
          "f": 0.19999999501893503
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.18032786885245902,
          "p": 0.15942028985507245,
          "f": 0.16923076424970426
        }
      }
    },
    {
      "paper_id": "math.FA.math/AP/2503.09536v1",
      "true_abstract": "We characterise the normal trace space associated to extended\n(measure-valued) divergence-measure fields on the boundary of a set $E \\subset\n\\mathbb R^n$, as the Arens-Eells space $\\mathrm{AE}(\\partial E)$. Such a trace\noperator is constructed for any Borel set $E$, and under a mild regularity\ncondition, which includes Lipschitz domains, this trace operator is shown to\nmoreover be surjective. This relies in part on a new pointwise description of\nthe Anzellotti pairing $\\overline{\\nabla \\phi \\cdot {\\boldsymbol F}}$ between a\n$\\mathrm{W}^{1,\\infty}$ function $\\phi$ and extended divergence-measure field\n${\\boldsymbol F}$. As an application, we prove extension theorems for\ndivergence-measure fields and divergence-free measures. Results for\n$\\mathrm{L}^1$-fields are also obtained.",
      "generated_abstract": "In this paper, we study the asymptotic behavior of the first eigenvalue of a\ncubic matrix with nonnegative entries when the eigenvalues are ordered from\nsmall to large. We establish the lower bound of the first eigenvalue when the\nnumber of eigenvalues is at least $n/2$, and the upper bound of the first\neigenvalue when the number of eigenvalues is at most $n$. We also present a\ncombinatorial characterization of the first eigenvalue for a given number of\neigenvalues.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14634146341463414,
          "p": 0.2727272727272727,
          "f": 0.19047618593096505
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.034482758620689655,
          "f": 0.024999995378125854
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.2727272727272727,
          "f": 0.19047618593096505
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.08670v1",
      "true_abstract": "The Taurid Complex is a large interplanetary system that contains comet\n2P/Encke, several meteoroid streams, and possibly a number of near-Earth\nasteroids. The size and nature of the system has led to the speculation that it\nwas formed through a large-scale cometary breakup. Numerical investigations\nhave suggested that planetary dynamics can create a resonant region with a\nlarge number of objects concentrated in a small segment of the orbit, known as\nthe Taurid swarm, which approaches the Earth in certain years and provides\nfavorable conditions to study the Taurid Complex. Recent meteor observations\nconfirmed the existence of the swarm for mm- to m-sized objects. Here we\npresent a dedicated telescopic search for potentially hazardous asteroids and\nother macroscopic objects in the Taurid swarm using the Zwicky Transient\nFacility survey. We determine from our non-detection that there are no more\nthan 9--14 $H\\leq24$ (equivalent to a diameter of $D\\gtrsim100$~m) objects in\nthe swarm, suggesting that the Encke--Taurid progenitor was $\\sim10$~km in\nsize. A progenitor of such a size is compatible with the prediction of\nstate-of-the-art Solar System dynamical models, which expects $\\sim0.1$\n$D>10$~km objects on Encke-like orbits at any given time.",
      "generated_abstract": "We present a detailed analysis of the spectral energy distribution (SED) of\nthe G39.5+0.5 proto-cluster, a young massive protocluster discovered by\n\\textit{Herschel} in the Perseus molecular cloud. The proto-cluster is a\nmolecular cloud with a central star forming region, containing an embedded\nprotocluster. We investigate the SED of G39.5+0.5 and obtain the best-fit SED\nmodel for the proto-cluster. Our analysis indicates that the proto-cluster is\ndominated by a thermal dust emissivity model with a power-law index of\n$\\beta=1.7\\pm0.1$ and an intrinsic dust reddening of $E(B-V)=0.10\\pm0.05$\nmag. We estimate the intrinsic dust emissivity by fitting the SED with a\nrelativistic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10236220472440945,
          "p": 0.203125,
          "f": 0.13612564999424373
        },
        "rouge-2": {
          "r": 0.03314917127071823,
          "p": 0.06315789473684211,
          "f": 0.04347825635502042
        },
        "rouge-l": {
          "r": 0.10236220472440945,
          "p": 0.203125,
          "f": 0.13612564999424373
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.08802v1",
      "true_abstract": "Recent literature highlights the advantages of implementing social rules via\ndynamic game forms. We characterize when truth-telling remains a dominant\nstrategy in gradual mechanisms implementing strategy-proof social rules, where\nagents gradually reveal their private information while acquiring information\nabout others in the process. Our first characterization hinges on the\nincentive-preservation of a basic transformation on gradual mechanisms called\nilluminating that partitions information sets. The second relies on a single\nreaction-proofness condition. We demonstrate the usefulness of both\ncharacterizations through applications to second-price auctions and the\ntop-trading cycles algorithm.",
      "generated_abstract": "The problem of assigning values to goods is a fundamental task in economic\nreinforcement learning (RL). In this paper, we propose a novel approach for\nreinforcement learning (RL) that integrates value-aware policy evaluation,\nvalue-aware value-based reinforcement learning (VR-RL), and value-aware\npolicy gradient (VR-PG) in a unified framework. We focus on the multi-item\ngoods problem where the agents' utility depends not only on the individual\ngoods but also on their combinations. We develop a value-aware value-based\nreinforcement learning (VR-VR-RL) method for multi-item goods problem, which\nintegrates value-aware policy evaluation, value-aware value-based reinforcement\nlearning, and value-aware policy gradient in a unified framework. The\nvalue-aware policy evaluation is designed to optimize the value of the\ncom",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17142857142857143,
          "p": 0.19047619047619047,
          "f": 0.1804511228333994
        },
        "rouge-2": {
          "r": 0.011627906976744186,
          "p": 0.011111111111111112,
          "f": 0.011363631366221205
        },
        "rouge-l": {
          "r": 0.15714285714285714,
          "p": 0.1746031746031746,
          "f": 0.16541352884843702
        }
      }
    },
    {
      "paper_id": "math.FA.math/FA/2503.10467v1",
      "true_abstract": "This note is to advertise the concept of directed completion of partial\norders as the natural analogue of Cauchy-completion in Lorentzian signature,\nespecially in relation to non-smooth and/or infinite dimensional geometries.\n  A closely related notion appeared in the recent [2] where it was used as\nground for further development of non-smooth calculus in metric spacetimes\nallowing, for instance, for a quite general limit curve theorem in such\nsetting. The proposal in [2] was also in part motivated by the discussion we\nmake here, key points being:\n  - A general existence result, already obtained in the context of theoretical\ncomputer science and for which we give a slightly different presentation.\n  - An example showing that an analogue two-sided completion is unsuitable from\nthe (or at least `some') geometric/analytic perspective.\n  - The existence of natural links with both the the concept of ideal point of\nGeroch--Kronheimer--Penrose and with Beppo Levi's monotone convergence theorem,\nshowing in particular an underlying commonality between these two seemingly far\nconcepts.\n  - The flexibility of the notion, that by nature can cover non-smooth and\ninfinite-dimensional situations.\n  - The fact that the concept emerges spontaneously when investigating the\nduality relations between $L^p$ and $L^q$ spaces where $\\tfrac1p+\\tfrac 1q = 1$\nare H\\\"older conjugate exponents with $p, q < 1$.\n  This note is part of a larger work in progress that aims at laying the\ngrounds of a Lorentzian, or Hyperbolic, theory of Banach spaces. Given that the\nnotion of completion has nothing to do with the linear structure and the\ngrowing interest around this topic, for instance related to convergence of\ngeometric structures, we make available these partial findings.",
      "generated_abstract": "We consider the problem of extending the function $f$ from $\\mathbb{R}$ to the\nrange of the function $g$ defined by $g(x)=\\left\\lfloor x/2\\right\\rfloor$ for\n$x\\in\\mathbb{R}$. The problem is to decide whether the function $g$ can be\nextended to a continuous function on $\\mathbb{R}$ such that $g(x)\\geq f(x)$\nfor all $x\\in\\mathbb{R}$. The problem is NP-complete and we provide a\nreduction from the Knapsack problem to the problem of extending the function\n$f$ to the range of the function $g$. We also present a reduction from the\nKnapsack problem to the problem of extending the function $f$ to the range of\nthe function $g$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08771929824561403,
          "p": 0.35714285714285715,
          "f": 0.1408450672564968
        },
        "rouge-2": {
          "r": 0.011627906976744186,
          "p": 0.04918032786885246,
          "f": 0.01880877433633761
        },
        "rouge-l": {
          "r": 0.08187134502923976,
          "p": 0.3333333333333333,
          "f": 0.13145539589499447
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.17810v2",
      "true_abstract": "In recent years, with advances in large language models (LLMs), end-to-end\nspoken dialogue models (SDMs) have made significant strides. Compared to\ntext-based LLMs, the evaluation of SDMs needs to take speech-related aspects\ninto account, such as paralinguistic information and speech quality. However,\nthere is still a lack of comprehensive evaluations for SDMs in speech-to-speech\n(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive\nbenchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers\nevaluations about multilingualism, multi-round dialogues, and paralinguistics.\nOur benchmark is divided into two difficulty levels: basic track and pro track,\nconsisting of 16 and 20 datasets respectively, evaluating the model's abilities\nin Understanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can effectively facilitate the development of spoken\ndialogue models by providing a multifaceted evaluation of existing models and\nhelping to track progress in this area.",
      "generated_abstract": "In the context of dialogue management, the task of dialogue management is to\nconduct two-way conversation between two or more speakers, where the goal is\nto deliver messages for the other speaker. This task is not simply a textual\ntask but requires a comprehensive understanding of the other speaker's\nintention, which is often determined by their previous utterances. To address\nthis challenge, we propose a hierarchical transformer model for dialogue\nmanagement, which consists of a hierarchical encoder and a hierarchical\ndecoder. The hierarchical encoder encodes the dialogue context into a\nhierarchical representation, and the hierarchical decoder decodes the\nhierarchical representation into a hierarchical response. We first introduce a\nhierarchical attention mechanism to enhance the representation of the\nhierarchical context. Then, we propose a hierarchical attention mechanism to\nen",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16551724137931034,
          "p": 0.3380281690140845,
          "f": 0.22222221780907073
        },
        "rouge-2": {
          "r": 0.015706806282722512,
          "p": 0.02830188679245283,
          "f": 0.020202015611559956
        },
        "rouge-l": {
          "r": 0.14482758620689656,
          "p": 0.29577464788732394,
          "f": 0.194444440031293
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.05676v1",
      "true_abstract": "Multidimensional poverty measurement is crucial for capturing deprivation\nbeyond income-based metrics. This study compares the Alkire-Foster (AF) method\nand a Markov Random Field (MRF) approach for classifying multidimensional\npoverty using a simulation-based analysis. The AF method applies a\ndeterministic threshold-based classification, while the MRF approach leverages\nprobabilistic graphical modelling to account for correlations between\ndeprivation indicators. Using a synthetic dataset of 50,000 individuals with\nten binary deprivation indicators, we assess classification accuracy, false\npositive/negative trade-offs, and agreement between the methods. Results show\nthat AF achieves higher classification accuracy (89.5%) compared to MRF\n(75.4%), with AF minimizing false negatives and MRF reducing false positives.\nThe overall agreement between the two methods is 65%, with discrepancies\nprimarily occurring when AF classifies individuals as poor while MRF does not.\nWhile AF is transparent and easy to implement, it does not capture\ninterdependencies among indicators, potentially leading to misclassification.\nMRF, though computationally intensive, offers a more nuanced understanding of\ndeprivation clusters. These findings highlight the trade-offs in\nmultidimensional poverty measurement and provide insights for policymakers on\nmethod selection based on data availability and policy objectives. Future\nresearch should extend these approaches to non-binary indicators and real-world\ndatasets.",
      "generated_abstract": "This article discusses the use of the maximum likelihood estimator in\nnon-parametric and semi-parametric models. We examine the use of the maximum\nlikelihood estimator in non-parametric models, where the underlying distribution\nis unknown. We then discuss the use of the maximum likelihood estimator in\nsemi-parametric models, where the underlying distribution is known. In\nparticular, we examine the use of the maximum likelihood estimator in the\nclassical parametric model, where the unknown distribution is a Gaussian. We\ndiscuss the use of the maximum likelihood estimator in the non-parametric\nestimator of the inverse Wishart distribution, where the unknown distribution\nis a multivariate Gaussian. We also discuss the use of the maximum likelihood\nestimator in the non-parametric estimator of the inverse Cauchy distribution,\nwhere the unknown distribution is a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.058394160583941604,
          "p": 0.20512820512820512,
          "f": 0.09090908745932348
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.058394160583941604,
          "p": 0.20512820512820512,
          "f": 0.09090908745932348
        }
      }
    },
    {
      "paper_id": "cs.SI.cs/SI/2503.07695v1",
      "true_abstract": "On February 7, 2024, Russian President Vladimir Putin gave a two-hour\ninterview with conservative political commentator, Tucker Carlson. This study\ninvestigated the impact of the Carlson- Putin interview on the US X audience.\nWe proposed a framework of social media impact using machine learning (ML) and\nnatural language processing (NLP) by measuring changes in audience, structure,\nand content. Triangulation methods were used to validate the process and\nresults. The interview had a considerable impact among segments of the American\npublic: 1) the reach and engagement of far-right influencers increased after\nthe interview, suggesting Kremlin narratives gained traction within these\ncircles, 2) the communication structure became more vulnerable to\ndisinformation spread after the interview, and 3) the public discourse changed\nfrom support for Ukraine funding to conversations about Putin, Russia, and the\nissue of \"truth\" or the veracity of Putin's claims. This research contributes\nto methods development for social media studies and aids scholars in analyzing\nhow public opinion shapes policy debates. The Carlson-Putin interview sparked a\nbroader discussion about truth-telling. Far from being muted, the broad impact\nof the interview appears considerable and poses challenges for foreign affairs\nleaders who depend on public support and buy-in when formulating national\npolicy.",
      "generated_abstract": "We investigate the applicability of Bayesian network-based methods to\nprove the existence of solutions to stochastic control problems with\nnonlinear dynamics. To this end, we introduce a novel approach based on\nBayesian Networks (BNs) that allows us to express the dynamics of the system in\nterms of a finite number of variables. The BN representation of the system\nallows us to explicitly compute the probability of each possible state of the\nsystem under the assumption of a given set of parameters. Using the BN method,\nwe are able to prove the existence of a solution to a stochastic control\nproblem, in particular, the existence of a solution to the control problem\nassociated with the system. Furthermore, we show that the solution of the\ncontrol problem associated with the system is unique. We validate our method on\nthe context of a control problem associated with the linear stochastic\nStockhausen system. The results obtained confirm that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0851063829787234,
          "p": 0.15789473684210525,
          "f": 0.11059907378963257
        },
        "rouge-2": {
          "r": 0.010362694300518135,
          "p": 0.01639344262295082,
          "f": 0.012698407952433117
        },
        "rouge-l": {
          "r": 0.07801418439716312,
          "p": 0.14473684210526316,
          "f": 0.10138248392788146
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.21268v1",
      "true_abstract": "Gene drive alleles bias their own inheritance to offspring. They can fix in a\nwild-type population in spite of a fitness cost, and even lead to the\neradication of the target population if the fitness cost is high. However, this\noutcome may be prevented or delayed if areas previously cleared by the drive\nare recolonised by wild-type individuals. Here, we investigate the conditions\nunder which these stochastic wild-type recolonisation events are likely and\nwhen they are unlikely to occur in one spatial dimension. More precisely, we\nexamine the conditions ensuring that the last individual carrying a wild-type\nallele is surrounded by a large enough number of drive homozygous individuals,\nresulting in a very low chance of wild-type recolonisation. To do so, we make a\ndeterministic approximation of the distribution of drive alleles within the\nwave, and we split the distribution of wild-type alleles into a deterministic\npart and a stochastic part. Our analytical and numerical results suggest that\nthe probability of wild-type recolonisation events increases with lower fitness\nof drive individuals, with smaller migration rate, and also with smaller local\ncarrying capacity. Numerical simulations show that these results extend to two\nspatial dimensions. We also demonstrate that, if a wild-type recolonisation\nevent were to occur, the probability of a following drive reinvasion event\ndecreases with smaller values of the intrinsic growth rate of the population.\nOverall, our study paves the way for further analysis of wild-type\nrecolonisation at the back of eradication traveling waves.",
      "generated_abstract": "This paper presents a new computational framework for the study of\nmanipulability in multi-agent systems. The framework consists of a graphical\nmodel and a method for quantifying the manipulability of an agent in terms of\nthe degree of freedom of its action. The graphical model captures the\ninteraction of all agents, while the method for quantifying manipulability\nperforms the necessary operations on the graph to produce a manipulable\nagent. This method is based on the concept of a \"difference graph\" that is\nconstructed from the graph by removing edges that connect agents with degrees\nthat are less than a threshold. The method has been applied to two\nwell-known models of multi-agent systems: the Bounded-Memory Shared Memory\nModel and the Bounded-Memory Shared Memory-Adaptive Model. The results show\nthat the manipulability of agents in the Bounded-Memory Shared Memory Model\nremains",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11428571428571428,
          "p": 0.21052631578947367,
          "f": 0.14814814358710576
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.04878048780487805,
          "f": 0.035398225464798164
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.19736842105263158,
          "f": 0.13888888432784652
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.09495v1",
      "true_abstract": "The calibration of the CR39 and Makrofol Nuclear Track Detectors of the\nMoEDAL experiment at the CERN-LHC was performed by exposing stacks of detector\nfoils to heavy ion beams with energies ranging from 340 MeV/nucleon to 150\nGeV/nucleon. After chemical etching, the base areas and lengths of etch-pit\ncones were measured using automatic and manual optical microscopes. The\nresponse of the detectors, as measured by the ratio of the track-etching rate\nover the bulk-etching rate, was determined over a range extending from their\nthreshold at Z/$\\beta\\sim7$ and $\\sim50$ for CR39 and Makrofol, respectively,\nup to Z/$\\beta\\sim92$",
      "generated_abstract": "This work focuses on the use of the Widow-Friend algorithm in the analysis of\nnon-uniformly-smeared charged particle events. The algorithm was originally\ndeveloped for the analysis of the PARADISE-II experiment, where it was applied to\nanalyze the events of charged particles originating from the decay of a\nproton-antiproton annihilation. In this paper, we present the application of the\nWidow-Friend algorithm to the analysis of a large number of events from the\nLEAP-IIB experiment, which was constructed at CERN for the study of the\nnuclear interaction with positron beams. In order to perform the analysis of\nthe events, the software package WIDOW has been used. WIDOW is an open-source\ntoolkit that allows the user to analyze non-uniformly-smeared events in\nparallel, using a GPU",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17142857142857143,
          "p": 0.16666666666666666,
          "f": 0.16901407950803427
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.009523809523809525,
          "f": 0.01020407665816569
        },
        "rouge-l": {
          "r": 0.15714285714285714,
          "p": 0.1527777777777778,
          "f": 0.15492957246578076
        }
      }
    },
    {
      "paper_id": "math.DG.math-ph/2503.10208v1",
      "true_abstract": "We explore the Jordan-Chevalley decomposition problem for an operator field\nin small dimensions. In dimensions three and four, we find tensorial conditions\nfor an operator field $L$, similar to a nilpotent Jordan block, to possess\nlocal coordinates in which $L$ takes a strictly upper triangular form. We prove\nthe Tempesta-Tondo conjecture for higher order brackets of\nFr\\\"olicher-Nijenhuis type.",
      "generated_abstract": "We present a new proof of the main theorem of C. W. Simpson and G. M. Williams\nin their book \"Differential geometry and the geometry of groups\". In this\narticle we are going to give a new proof of the main theorem of Simpson and\nWilliams which uses a new approach, a new tool, the differential geometry of\nthe universal cover of the group. We will also give a new proof of the main\ntheorem of F. C. H. C. K. Chern, \"Differential geometry of the universal cover\nof a group\".",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2127659574468085,
          "p": 0.2222222222222222,
          "f": 0.21739129935018917
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2127659574468085,
          "p": 0.2222222222222222,
          "f": 0.21739129935018917
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/RM/2502.16364v1",
      "true_abstract": "As the developed world replaces Defined Benefit (DB) pension plans with\nDefined Contribution (DC) plans, there is a need to develop decumulation\nstrategies for DC plan holders. Optimal decumulation can be viewed as a problem\nin optimal stochastic control. Formulation as a control problem requires\nspecification of an objective function, which in turn requires a definition of\nreward and risk. An intuitive specification of reward is the total withdrawals\nover the retirement period. Most retirees view risk as the possibility of\nrunning out of savings. This paper investigates several possible left tail risk\nmeasures, in conjunction with DC plan decumulation. The risk measures studied\ninclude (i) expected shortfall (ii) linear shortfall and (iii) probability of\nshortfall. We establish that, under certain assumptions, the set of optimal\ncontrols associated with all expected reward and expected shortfall Pareto\nefficient frontier curves is identical to the set of optimal controls for all\nexpected reward and linear shortfall Pareto efficient frontier curves. Optimal\nefficient frontiers are determined computationally for each risk measure, based\non a parametric market model. Robustness of these strategies is determined by\ntesting the strategies out-of-sample using block bootstrapping of historical\ndata.",
      "generated_abstract": "We introduce a novel technique for constructing portfolio strategies that\noptimize a nonlinear function of the portfolio weights with respect to the\nportfolio risk. This is a significant departure from the conventional approach\nin which a linear function is employed. We derive a novel formulation of this\nproblem, characterizing the optimal portfolio weights as the zeros of a\ncertain linear operator. We also discuss the relationship between our approach\nand the recently introduced method of Lietz and van der Schaft. Finally, we\npresent numerical experiments to illustrate the effectiveness of our\nproposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15126050420168066,
          "p": 0.2903225806451613,
          "f": 0.1988950231201735
        },
        "rouge-2": {
          "r": 0.01744186046511628,
          "p": 0.033707865168539325,
          "f": 0.022988501252771684
        },
        "rouge-l": {
          "r": 0.13445378151260504,
          "p": 0.25806451612903225,
          "f": 0.17679557560636133
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.21505v2",
      "true_abstract": "The Gulf Cooperation Council countries -- Oman, Bahrain, Kuwait, UAE, Qatar,\nand Saudi Arabia -- holds strategic significance due to its large oil reserves.\nHowever, these nations face considerable challenges in shifting from\noil-dependent economies to more diversified, knowledge-based systems. This\nstudy examines the progress of Gulf Cooperation Council (GCC) countries in\nachieving economic diversification and social development, focusing on the\nSocial Progress Index (SPI), which provides a broader measure of societal\nwell-being beyond just economic growth. Using data from the World Bank,\ncovering 2010 to 2023, the study employs the XGBoost machine learning model to\nforecast SPI values for the period of 2024 to 2026. Key components of the\nmethodology include data preprocessing, feature selection, and the simulation\nof independent variables through ARIMA modeling. The results highlight\nsignificant improvements in education, healthcare, and women's rights,\ncontributing to enhanced SPI performance across the GCC countries. However,\nnotable challenges persist in areas like personal rights and inclusivity. The\nstudy further indicates that despite economic setbacks caused by global\ndisruptions, including the COVID-19 pandemic and oil price volatility, GCC\nnations are expected to see steady improvements in their SPI scores through\n2027. These findings underscore the critical importance of economic\ndiversification, investment in human capital, and ongoing social reforms to\nreduce dependence on hydrocarbons and build knowledge-driven economies. This\nresearch offers valuable insights for policymakers aiming to strengthen both\nsocial and economic resilience in the region while advancing long-term\nsustainable development goals.",
      "generated_abstract": "We study a simple model of endogenous growth with the existence of a\nintertemporal choice problem. Our model contains an exogenous variable\nrepresenting the size of the uncertainty associated with the choice problem.\nThe size of uncertainty is endogenous and is affected by the choice\nprobability. We show that the choice probability is endogenous and is\ndetermined by the exogenous variable. We also show that the model can be\ndescribed by a simple vector autoregressive model with a single parameter.\nFurthermore, we show that the parameter of the simple vector autoregressive\nmodel is determined by the choice probability and the uncertainty in the choice\nprobability. We then show that the choice probability can be estimated by\nmeans of the empirical distribution of the choice probability. We show that\nthe uncertainty in the choice probability can be estimated by means of the\nempirical distribution of the choice probability",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06432748538011696,
          "p": 0.22916666666666666,
          "f": 0.10045661758178531
        },
        "rouge-2": {
          "r": 0.012711864406779662,
          "p": 0.03488372093023256,
          "f": 0.018633536457699214
        },
        "rouge-l": {
          "r": 0.06432748538011696,
          "p": 0.22916666666666666,
          "f": 0.10045661758178531
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19557v2",
      "true_abstract": "We study how social image concerns affect information sharing patterns\nbetween peers. An individual receives a signal (\"news\") about the state of the\nworld and can either share it with a peer or not. This signal has two\nattributes: a headline -- e.g., arguing for or against human-induced climate\nchange -- and a veracity status, indicating if the signal is based on facts or\nmade-up. The headline is observable at no cost by everyone, while observing the\nveracity status is costly and the cost depends on an individual's type. We\nstudy the sharing patterns induced by two different types of social image\nconcern: wanting to be perceived as talented, which implies being able to\ndistinguish proper from fake news, and wanting to signal one's worldview. Our\nmodel can rationalize the empirical finding that fake news may be shared with a\nhigher propensity than proper news (e.g., Vosoughi et al., 2018). We show that\nboth a veracity and a worldview concern may rationalize this finding, though\nsharing patterns are empirically distinguishable and welfare implications\ndiffer.",
      "generated_abstract": "We study the optimal timing of a firm's announcement of a new product\nand its decision to scale up production in response to a demand shock. We\nassume that the firm's production capacity is a function of its past\nproduction and the demand shock. We characterize the optimal timing of the\nannouncement and the production decisions as a function of the firm's\nproduction capacity and the firm's past production. We find that the optimal\ntiming of the announcement and the production decisions depend on the\nproduct-specific demand shock. The optimal timing of the announcement is\nearlier when the demand shock is small and later when the demand shock is\nlarge.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09917355371900827,
          "p": 0.2926829268292683,
          "f": 0.14814814436747456
        },
        "rouge-2": {
          "r": 0.023668639053254437,
          "p": 0.05555555555555555,
          "f": 0.033195016556877996
        },
        "rouge-l": {
          "r": 0.09917355371900827,
          "p": 0.2926829268292683,
          "f": 0.14814814436747456
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.01362v1",
      "true_abstract": "This paper describes a streaming audio-to-MIDI piano transcription approach\nthat aims to sequentially translate a music signal into a sequence of note\nonset and offset events. The sequence-to-sequence nature of this task may call\nfor the computationally-intensive transformer model for better performance,\nwhich has recently been used for offline transcription benchmarks and could be\nextended for streaming transcription with causal attention mechanisms. We\nassume that the performance limitation of this naive approach lies in the\ndecoder. Although time-frequency features useful for onset detection are\nconsiderably different from those for offset detection, the single decoder is\ntrained to output a mixed sequence of onset and offset events without guarantee\nof the correspondence between the onset and offset events of the same note. To\novercome this limitation, we propose a streaming encoder-decoder model that\nuses a convolutional encoder aggregating local acoustic features, followed by\nan autoregressive Transformer decoder detecting a variable number of onset\nevents and another decoder detecting the offset events for the active pitches\nwith validation of the sustain pedal at each time frame. Experiments using the\nMAESTRO dataset showed that the proposed streaming method performed comparably\nwith or even better than the state-of-the-art offline methods while\nsignificantly reducing the computational cost.",
      "generated_abstract": "This paper introduces a novel method for estimating the spectral energy\ndistribution (SED) of a scene using a single measurement, namely the spectral\nenergy map (SEM). SEM is a function that maps the spectral energy density of a\nscene into a one-dimensional spectrum. The SEM is calculated by combining a\nspectral imaging technique with a spectral sensing method. The method we propose\nuses the multi-frequency imaging (MFI) technique to measure the spectral\nenergy densities of the scene, and then applies a spectral sensing method to\ncalculate the SEDs of the scene. A method is proposed to calculate the SEM for\na scene from a single measurement. In addition, the method we propose is able to\nimprove the SEM of the scene when the scene has significant non-uniformity in\nits spectral energy density. This paper also proposes a method for calculating\nthe SEM using the multi",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.183206106870229,
          "p": 0.34285714285714286,
          "f": 0.23880596560976222
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.043478260869565216,
          "f": 0.033333328605556226
        },
        "rouge-l": {
          "r": 0.1450381679389313,
          "p": 0.2714285714285714,
          "f": 0.18905472182866773
        }
      }
    },
    {
      "paper_id": "nucl-ex.physics/ed-ph/2502.14612v1",
      "true_abstract": "The Modular Neutron Array (MoNA) collaboration was initiated in 2000 at the\nNational Superconducting Cyclotron Laboratory (NSCL) at Michigan State\nUniversity (MSU). Since then, the collaboration studied properties of nuclides\nat and beyond the neutron dripline discovering seven new isotopes between\nlithium to fluorine. The collaboration included liberal arts colleges, regional\ncomprehensive universities, and major research universities with the focus of\ngiving undergraduate students meaningful research experiences. Over the last 25\nyears, the combined efforts of hundreds of undergraduates, dozens of graduate\nstudents and research associates, and faculty from more than a dozen colleges\nand universities produced over fifty publications, won awards for research, and\ncombined research and teaching in new and interesting ways.",
      "generated_abstract": "The T2K experiment has detected a slight excess of $K^+\\to\\pi^+\\nu_e\\nu_\\mu$\nevent over the background-only-hypothesis (BOH) expectation in the\n$\\nu_\\mu\\to\\nu_e\\mu^+\\mu^-$ decay channel. The excess is consistent with\n$K^+\\to\\pi^+\\mu^+\\mu^-$, and has not yet been confirmed in the\n$\\nu_\\mu\\to\\nu_e\\mu^+\\mu^-$ decay channel. We present the first measurement of\nthe $K^+\\to\\pi^+\\mu^+\\mu^-$ branching ratio and discuss the implications for\n$K\\to\\pi\\mu^+\\mu^-$ decay. We also present a new measurement of the $K\\to\n\\pi\\mu^+\\mu^-$ branching ratio, which is consistent with the BOH expectation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.2127659574468085,
          "f": 0.15151514692952264
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.015384615384615385,
          "f": 0.011235950419772144
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.2127659574468085,
          "f": 0.15151514692952264
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.09310v1",
      "true_abstract": "The failure of a system can result from the simultaneous effects of multiple\ncauses, where assigning a specific cause may be inappropriate or unavailable.\nExamples include contributing causes of death in epidemiology and the aetiology\nof neurodegenerative diseases like Alzheimer's. We propose a parametric Weibull\naccelerated failure time model for multiple causes, incorporating a\ndata-driven, individualized, and time-varying winning probability (relative\nimportance) matrix. Using maximum likelihood estimation and the\nexpectation-maximization (EM) algorithm, our approach enables simultaneous\nestimation of regression coefficients and relative cause importance, ensuring\nconsistency and asymptotic normality. A simulation study and an application to\nAlzheimer's disease demonstrate its effectiveness in addressing cause-mixture\nproblems and identifying informative biomarker combinations, with comparisons\nto Weibull and Cox proportional hazards models.",
      "generated_abstract": "The use of machine learning in healthcare has been increasing, with a\nparticular focus on image analysis. The main challenge in image analysis is\nthe lack of a clear understanding of the data. This paper addresses this\nchallenge by developing a methodology to integrate the clinical domain into the\nmachine learning pipeline. The methodology combines image analysis with\nclinical reasoning. The approach consists of four stages: (i) data\npreprocessing, (ii) feature extraction, (iii) model training, and (iv)\nevaluation. In the preprocessing stage, the image is resized and centered and\nthe image is converted into grayscale images. In the feature extraction stage,\nthe features are extracted using different machine learning algorithms. The\nmodel training stage consists of training a deep learning model with the\nfeatures extracted from the previous stage. The evaluation stage consists of\ntesting the model on the validation dataset to assess its performance. The\npro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.14634146341463414,
          "f": 0.13714285216261243
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.022388059701492536,
          "f": 0.023999995025921032
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.14634146341463414,
          "f": 0.13714285216261243
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.09981v1",
      "true_abstract": "This study investigates possibility and impossibility results of the\nrepugnant and sadistic conclusions in population ethics and economics. The\nrepugnant conclusion says that an enormous population with very low well-being\nis socially better than any smaller population with sufficiently high\nwell-being. The sadistic conclusion says that adding individuals with negative\nwell-being to a society is socially better than adding individuals with\npositive well-being to it. Previous studies have often found it challenging to\navoid both undesirable conclusions. However, I demonstrate that a class of\nacceptable social welfare orderings can easily prevent these conclusions while\nadhering to standard axioms, such as anonymity, strong Pareto, Pigou-Dalton\ntransfer, and extended continuity. Nevertheless, if the avoidance requirements\nfor the repugnant and sadistic conclusions are strengthened, it is possible to\nencounter new impossibility results. These results reveal essential conflicts\nbetween the independence axiom and the avoidance of the weak repugnant\nconclusion when evaluating well-being profiles with different populations.",
      "generated_abstract": "We study the dynamic allocation problem of a public good with heterogeneous\nconsumers, each of whom has a private utility function. The good is provided by\na firm with a fixed budget. The firm is interested in allocating the good\nefficiently to the consumers, with the goal of maximizing the total utility of\nthe firm. We characterize the optimal allocation by a single-peaked utility\ncurve. In particular, the optimal allocation is a single-peaked curve that\nchanges shape only under certain conditions. Our results demonstrate that the\nfirm's optimal allocation strategy can be understood as an alternative to\nleast-cost production. We also show that our results are robust to assumptions\nabout the firm's utility function, and that the optimal allocation is\ndifferentiable. Finally, we provide a heuristic algorithm that produces an\nefficient allocation in a wide range of settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.2125,
          "f": 0.18994412913454647
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.015748031496062992,
          "f": 0.01526717057718245
        },
        "rouge-l": {
          "r": 0.16161616161616163,
          "p": 0.2,
          "f": 0.17877094477700464
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.06587v1",
      "true_abstract": "This article presents a comprehensive methodology for processing financial\ndatasets of Apple Inc., encompassing quarterly income and daily stock prices,\nspanning from March 31, 2009, to December 31, 2023. Leveraging 60 observations\nfor quarterly income and 3774 observations for daily stock prices, sourced from\nMacrotrends and Yahoo Finance respectively, the study outlines five distinct\ndatasets crafted through varied preprocessing techniques. Through detailed\nexplanations of aggregation, interpolation (linear, polynomial, and cubic\nspline) and lagged variables methods, the study elucidates the steps taken to\ntransform raw data into analytically rich datasets. Subsequently, the article\ndelves into regression analysis, aiming to decipher which of the five data\nprocessing methods best suits capital market analysis, by employing both linear\nand polynomial regression models on each preprocessed dataset and evaluating\ntheir performance using a range of metrics, including cross-validation score,\nMSE, MAE, RMSE, R-squared, and Adjusted R-squared. The research findings reveal\nthat linear interpolation with polynomial regression emerges as the\ntop-performing method, boasting the lowest validation MSE and MAE values,\nalongside the highest R-squared and Adjusted R-squared values.",
      "generated_abstract": "This study investigates the empirical properties of the GMM estimator of the\noptimality condition for the mean function of the empirical distribution of\nexogenous variables. The GMM estimator is characterized by a maximum likelihood\nestimator and the Fisher information matrix. The asymptotic distribution of the\nGMM estimator is obtained by establishing the asymptotic normality of the\nmaximum likelihood estimator and the Fisher information matrix, and a\nnonasymptotic central limit theorem is derived. The asymptotic normality of the\nGMM estimator is established by using the method of moments. The asymptotic\nnormality of the Fisher information matrix is established by using the method\nof moments. The asymptotic normality of the GMM estimator is established by\nusing the method of moments. The asymptotic normality of the Fisher information\nmatrix is established by using the method of moments. The asymptotic normal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.088,
          "p": 0.2558139534883721,
          "f": 0.13095237714356586
        },
        "rouge-2": {
          "r": 0.006097560975609756,
          "p": 0.015151515151515152,
          "f": 0.008695648081665442
        },
        "rouge-l": {
          "r": 0.072,
          "p": 0.20930232558139536,
          "f": 0.10714285333404208
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.hep-lat/2503.08414v1",
      "true_abstract": "This work examines the effect of disclinations on the scattering of\nquasipaticles in graphene with the presence of a topological defect. Using the\ntight-binding method, the electronic properties of graphene with disclination\nare described, where the topological defects are introduced in the lattice via\ngeometric theory. The massless Dirac equation is modified to account for the\ncurvature induced by these defects, incorporating a gauge field. The results\nshow that disclinations significantly affect the scattering process, altering\nphase shifts and interference patterns. The differential cross-section and its\ndependence on the scattering angle are analyzed, highlighting the role of\ngeometric factors like the parameter {\\alpha} in shaping the scattering\ndynamics.",
      "generated_abstract": "We study the dynamical structure factor of two-dimensional (2D) disordered\nChern insulators in the presence of a magnetic field, using a mean-field\napproach. In the absence of the magnetic field, the dynamical structure factor\nexhibits a rich pattern of peaks and troughs, reflecting the presence of\ntopological defects. When the magnetic field is turned on, the peaks and troughs\nof the dynamical structure factor shift and merge, while the overall\nstructure of the spectrum remains unchanged. The magnetic field induces a\nrotation of the band structure, which leads to the emergence of new peaks and\ntroughs in the dynamical structure factor. Our results indicate that the\ndynamical structure factor is sensitive to the magnetic field, and that the\ndynamical structure factor exhibits rich structures under magnetic field\nperturbations. The results of this study provide important ins",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18666666666666668,
          "p": 0.21212121212121213,
          "f": 0.1985815553040593
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.058823529411764705,
          "f": 0.05882352441176514
        },
        "rouge-l": {
          "r": 0.18666666666666668,
          "p": 0.21212121212121213,
          "f": 0.1985815553040593
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/GN/2407.20377v1",
      "true_abstract": "This paper explores an innovative approach to Environmental, Social, and\nGovernance (ESG) scoring by integrating Natural Language Processing (NLP)\ntechniques with Item Response Theory (IRT), specifically the Rasch model. The\nstudy utilizes a comprehensive dataset of news articles in Portuguese related\nto Petrobras, a major oil company in Brazil, collected from 2022 and 2023. The\ndata is filtered and classified for ESG-related sentiments using advanced NLP\nmethods. The Rasch model is then applied to evaluate the psychometric\nproperties of these ESG measures, providing a nuanced assessment of ESG\nsentiment trends over time. The results demonstrate the efficacy of this\nmethodology in offering a more precise and reliable measurement of ESG factors,\nhighlighting significant periods and trends. This approach may enhance the\nrobustness of ESG metrics and contribute to the broader field of sustainability\nand finance by offering a deeper understanding of the temporal dynamics in ESG\nreporting.",
      "generated_abstract": "In this work, we propose a novel methodology to solve a multi-armed bandit\nsystem with multiple arms, each of which is associated with an agent. In each\niteration, the agent chooses an arm, and the algorithm selects an arm based on\nits reward. We introduce a novel reward model that considers both the agent's\nreward and the agent's cost. We then propose a novel algorithm that uses\ngradient descent to update the reward model. We evaluate our method on two\nwell-known multi-armed bandit problems, i.e., the online lasso problem and the\nonline bandit problem, and demonstrate that our method outperforms existing\nalgorithms. Finally, we showcase the application of our methodology in the\nmulti-agent system problem and demonstrate that our methodology can enhance\nthe performance of existing algorithms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2191780821917808,
          "f": 0.1807909556053498
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.017857142857142856,
          "f": 0.01574802656581467
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.1917808219178082,
          "f": 0.15819208554885267
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/MF/2411.18397v1",
      "true_abstract": "We study optimal payoff choice for an expected utility maximizer under the\nconstraint that their payoff is not allowed to deviate ``too much'' from a\ngiven benchmark. We solve this problem when the deviation is assessed via a\nBregman-Wasserstein (BW) divergence, generated by a convex function $\\phi$.\nUnlike the Wasserstein distance (i.e., when $\\phi(x)=x^2$). The inherent\nasymmetry of the BW divergence makes it possible to penalize positive\ndeviations different than negative ones. As a main contribution, we provide the\noptimal payoff in this setting. Numerical examples illustrate that the choice\nof $\\phi$ allow to better align the payoff choice with the objectives of\ninvestors.",
      "generated_abstract": "We develop a novel linear-quadratic control framework for an infinite-dimensional\nsystem of financial agents, where agents' financial information is represented\nby a stochastic process. The control problem is formulated as an optimal\ncontrol problem with a general linear quadratic regulator (LQR) constraint,\nwhere the LQR constraint is a quadratic cost function. We show that the\ncontrol problem is equivalent to a standard optimal control problem with\nquadratic cost function, and propose a Lyapunov function for the LQR\nconstraint that can be efficiently computed. We also derive a nonlinear\noptimization problem to solve the optimal control problem, and show that the\nsolution is a quadratic control function. Numerical examples demonstrate the\neffectiveness of the proposed framework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2375,
          "p": 0.296875,
          "f": 0.2638888839506173
        },
        "rouge-2": {
          "r": 0.06796116504854369,
          "p": 0.07,
          "f": 0.06896551224247167
        },
        "rouge-l": {
          "r": 0.225,
          "p": 0.28125,
          "f": 0.24999999506172846
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2409.13236v1",
      "true_abstract": "Collective decision-making is the process through which diverse stakeholders\nreach a joint decision. Within societal settings, one example is participatory\nbudgeting, where constituents decide on the funding of public projects. How to\nmost efficiently aggregate diverse stakeholder inputs on a portfolio of\nprojects with uncertain long-term benefits remains an open question. We address\nthis problem by studying collective decision-making through the integration of\npreference aggregation and knapsack allocation methods. Since different\nstakeholder groups may evaluate projects differently,we examine several\naggregation methods that combine their diverse inputs. The aggregated\nevaluations are then used to fill a ``collective'' knapsack. Among the methods\nwe consider are the arithmetic mean, Borda-type rankings, and delegation to\nexperts. We find that the factors improving an aggregation method's ability to\nidentify projects with the greatest expected long-term value include having\nmany stakeholder groups, moderate variation in their expertise levels, and some\ndegree of delegation or bias favoring groups better positioned to objectively\nassess the projects. We also discuss how evaluation errors and heterogeneous\ncosts impact project selection. Our proposed aggregation methods are relevant\nnot only in the context of funding public projects but also, more generally,\nfor organizational decision-making under uncertainty.",
      "generated_abstract": "We study a two-sided matching problem in a single-good, two-person market\nwith a constant-elasticity of substitution between goods. The market is\nnon-convex, but the utility of the consumer can be approximated by a\npolynomial-time computable convex function. We characterize the value of the\nutility function in terms of the expected value of the total cost. We also\nintroduce a class of matching policies that are guaranteed to be optimal and\nconsistent with the consumer's preferences. We prove that the existence of such\npolicies is guaranteed by a monotonicity property of the utility function and\nshow that they are optimal in a class of matching policies that is\nself-contained and easily interpretable. We also present an application of our\nfindings to a two-sided matching problem in the financial industry.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1386861313868613,
          "p": 0.2835820895522388,
          "f": 0.18627450539263757
        },
        "rouge-2": {
          "r": 0.021052631578947368,
          "p": 0.037383177570093455,
          "f": 0.026936022326520752
        },
        "rouge-l": {
          "r": 0.13138686131386862,
          "p": 0.26865671641791045,
          "f": 0.1764705838240101
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.10352v1",
      "true_abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
      "generated_abstract": "Accurate and efficient prediction of the next time step in speech recognition\nremains a challenge. In this paper, we propose a novel speech recognition\nframework, SpeechNet, which integrates a graph neural network (GNN) with a\nlong short-term memory (LSTM) recurrent neural network (RNN). Our model\naddresses the challenges of high latency and large vocabulary size, leveraging\nthe GNN to capture temporal dependencies, and the LSTM to model long-term\nsequences. We validate our approach on the LibriSpeech dataset, achieving\nstate-of-the-art performance across three evaluation metrics: 1) speech\nrecognition accuracy, 2) speech segmentation accuracy, and 3) model size. Our\nframework demonstrates strong generalization capabilities across different\nspeech datasets, achieving 80.1% accuracy on LibriSpeech, 72",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16161616161616163,
          "p": 0.18604651162790697,
          "f": 0.17297296799766265
        },
        "rouge-2": {
          "r": 0.03816793893129771,
          "p": 0.045871559633027525,
          "f": 0.04166666170868115
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.1744186046511628,
          "f": 0.16216215718685187
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.19574v1",
      "true_abstract": "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
      "generated_abstract": "The use of a single-cell RNA sequencing (scRNA-seq) dataset to predict\nbiological phenotypes, such as disease progression, is a promising approach for\ncomprehensive understanding of complex biological systems. However, the\ncomputational challenge of predicting complex biological phenotypes, such as\ndisease progression, from a single-cell RNA sequencing dataset is significant.\nWe introduce a novel approach to predicting disease progression from a\nsingle-cell RNA sequencing dataset that leverages a deep learning framework and\nemploys a novel feature selection process. Our approach consists of two\ncritical steps. First, we employ a feature selection process to remove\nredundant and irrelevant features from the scRNA-seq dataset. Second, we\nimplement a deep learning model, which predicts the probability of disease\nprogression in a given patient based on the selected features and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12345679012345678,
          "p": 0.28169014084507044,
          "f": 0.1716738155051669
        },
        "rouge-2": {
          "r": 0.02,
          "p": 0.047619047619047616,
          "f": 0.02816900991866755
        },
        "rouge-l": {
          "r": 0.11728395061728394,
          "p": 0.2676056338028169,
          "f": 0.16309012451804245
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.08370v1",
      "true_abstract": "This paper presents Ev-Layout, a novel large-scale event-based multi-modal\ndataset designed for indoor layout estimation and tracking. Ev-Layout makes key\ncontributions to the community by: Utilizing a hybrid data collection platform\n(with a head-mounted display and VR interface) that integrates both RGB and\nbio-inspired event cameras to capture indoor layouts in motion. Incorporating\ntime-series data from inertial measurement units (IMUs) and ambient lighting\nconditions recorded during data collection to highlight the potential impact of\nmotion speed and lighting on layout estimation accuracy. The dataset consists\nof 2.5K sequences, including over 771.3K RGB images and 10 billion event data\npoints. Of these, 39K images are annotated with indoor layouts, enabling\nresearch in both event-based and video-based indoor layout estimation. Based on\nthe dataset, we propose an event-based layout estimation pipeline with a novel\nevent-temporal distribution feature module to effectively aggregate the\nspatio-temporal information from events. Additionally, we introduce a\nspatio-temporal feature fusion module that can be easily integrated into a\ntransformer module for fusion purposes. Finally, we conduct benchmarking and\nextensive experiments on the Ev-Layout dataset, demonstrating that our approach\nsignificantly improves the accuracy of dynamic indoor layout estimation\ncompared to existing event-based methods.",
      "generated_abstract": "This paper introduces the SOTA Mixed-Reality (MR) system for medical\nexpertise assessment, called MREA, developed by the Medical Robotics Research\nCenter at the University of Illinois at Chicago (MREA-UIC). MREA-UIC is the\nfirst academic center in the world to implement a full-fledged MR system for\nexpertise assessment. MREA-UIC's MR system is based on the recently\nintroduced, high-fidelity MR-enabled, multi-modal, and multi-device MR\nexpertise assessment framework, MREA-VR. MREA-VR comprises three components:\n(1) a virtual reality (VR) system with a virtual reality headset, (2) a\nmulti-modal system, comprising a mobile device for data capture and a\ncomputer-based system, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11023622047244094,
          "p": 0.2028985507246377,
          "f": 0.1428571382949814
        },
        "rouge-2": {
          "r": 0.016216216216216217,
          "p": 0.03296703296703297,
          "f": 0.0217391260147562
        },
        "rouge-l": {
          "r": 0.10236220472440945,
          "p": 0.18840579710144928,
          "f": 0.13265305666232835
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08557v1",
      "true_abstract": "This paper studies Integrated Sensing, Communication, and Powering (ISCAP) as\na novel framework designed to enhance Internet of Things (IoT) applications\nwithin sixth-generation wireless networks. In these applications, in addition\nto IoT devices requiring an energy supply and receiving information or control\ndata to perform their tasks, the base station serving them must sense the\ndevices and their environment to localize them, thereby improving data\ntransmission and enabling simultaneous power delivery. In our multi-node ISCAP\nIoT system, we optimize base station beamforming alongside the receiver's\npower-splitting factor to maximize energy harvesting while adhering to strict\ncommunication and sensing constraints. To effectively tackle this non-convex\noptimization problem, we decompose it into three manageable subproblems and\nemploy several techniques such as semidefinite relaxation and Rayleigh quotient\nmethods to find an efficient solution. Simulation results demonstrate the\neffectiveness of the proposed design, highlighting performance trade-offs among\nsensing accuracy, communication reliability, and power transfer efficiency.",
      "generated_abstract": "This paper proposes a novel algorithm for the design of an ultra-wideband (UWB)\nhigh-gain antenna array with low-noise figure (LNF) to enhance the receiver\nsensitivity for low-density parity-check (LDPC) error-correcting codes. By\nintroducing a non-orthogonal multiple access (NOMA)-based phase shifter to\nimprove the spatial multiplexing gain (SMG), the proposed array performs\nsub-carrier-assisted LDPC detection, which is a highly non-convex problem\nwith limited number of subcarriers. To address this, we propose a novel\nsub-carrier-assisted LDPC detection algorithm that utilizes a novel\nmulti-step optimization method. The proposed algorithm first performs\nsub-carrier-assisted LDPC detection with a fixed number of subcarriers and\nthen iter",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12605042016806722,
          "p": 0.20833333333333334,
          "f": 0.1570680581299856
        },
        "rouge-2": {
          "r": 0.026845637583892617,
          "p": 0.04395604395604396,
          "f": 0.033333328625347894
        },
        "rouge-l": {
          "r": 0.1092436974789916,
          "p": 0.18055555555555555,
          "f": 0.1361256497530223
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.06079v1",
      "true_abstract": "Despite the significance of probabilistic time-series forecasting models,\ntheir evaluation metrics often involve intractable integrations. The most\nwidely used metric, the continuous ranked probability score (CRPS), is a\nstrictly proper scoring function; however, its computation requires\napproximation. We found that popular CRPS estimators--specifically, the\nquantile-based estimator implemented in the widely used GluonTS library and the\nprobability-weighted moment approximation--both exhibit inherent estimation\nbiases. These biases lead to crude approximations, resulting in improper\nrankings of forecasting model performance when CRPS values are close. To\naddress this issue, we introduced a kernel quadrature approach that leverages\nan unbiased CRPS estimator and employs cubature construction for scalable\ncomputation. Empirically, our approach consistently outperforms the two widely\nused CRPS estimators.",
      "generated_abstract": "In this paper, we present a novel approach for the estimation of\ninverse density functions using the multivariate Gauss-Markov kernel. The\nproposed method combines a kernel density estimator with a Gauss-Markov kernel\nto generate a multivariate density function. We prove that the proposed\nestimator is asymptotically normal and its asymptotic variance is finite.\nAdditionally, we prove that the proposed estimator can be efficiently computed\nby the use of the maximum likelihood estimator. We apply the proposed estimator\nto the estimation of the density function of a Gaussian process.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1827956989247312,
          "p": 0.3333333333333333,
          "f": 0.2361111065364584
        },
        "rouge-2": {
          "r": 0.008928571428571428,
          "p": 0.013157894736842105,
          "f": 0.010638293055683484
        },
        "rouge-l": {
          "r": 0.1827956989247312,
          "p": 0.3333333333333333,
          "f": 0.2361111065364584
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.10599v1",
      "true_abstract": "Recently, a thermodynamic bound on correlation times was formulated in [A.\nDechant, J. Garnier-Brun, S.-i. Sasa, Phys. Rev. Lett. 131, 167101 (2023)],\nshowing how the decay of correlations in Langevin dynamics is bounded by\nshort-time fluctuations and dissipation. Whereas these original results only\naddress very long observation times in steady-state dynamics, we here\ngeneralize the respective inequalities to finite observations and general\ninitial conditions. We utilize the connection between correlations and the\nfluctuations of time-integrated density functionals and generalize the direct\nstochastic calculus approach from [C. Dieball and A. Godec, Phys. Rev. Lett.\n130, 087101 (2023)] which paves the way for further generalizations. We address\nthe connection between short and long time scales, as well as the saturation of\nthe bounds via complementary spectral-theoretic arguments. Motivated by the\nspectral insight, we formulate all results also for complex-valued observables.",
      "generated_abstract": "We investigate the thermodynamic limit of the Ising spin glass model on\nthe square lattice, a paradigmatic model in statistical physics. In this\nmodel, the Ising spin glass is defined by a random binary interaction\nstrength matrix. We present a new proof of the existence of the Gibbs\nequilibrium for this model, generalizing the proof of the existence of the\nequilibrium for the Ising model. We also derive the thermodynamic limit of the\nIsing spin glass model, and show that the thermodynamic limit is the\nferromagnetic Ising spin glass model. Moreover, we show that the Ising spin\nglass model on the square lattice is dual to a model on the hypercube,\ndemonstrating the connection between the Ising spin glass model and the\nspin-glass model on the hypercube. Finally, we show that the thermodynamic\nlimit of the Ising spin glass",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2909090909090909,
          "f": 0.2012578571100828
        },
        "rouge-2": {
          "r": 0.03007518796992481,
          "p": 0.046511627906976744,
          "f": 0.03652967559558871
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.2909090909090909,
          "f": 0.2012578571100828
        }
      }
    },
    {
      "paper_id": "math.AP.math/FA/2503.07569v1",
      "true_abstract": "In this paper, we consider second order degenerate parabolic equations with\ncomplex, measurable, and time-dependent coefficients. The degenerate\nellipticity is dictated by a spatial $A_2$-weight. We prove that having a\ngeneralized fundamental solution with upper Gaussian bounds is equivalent to\nMoser's $L^2$-$L^\\infty$ estimates for local weak solutions. In the special\ncase of real coefficients, Moser's $L^2$-$L^\\infty$ estimates are known, which\nprovide an easier proof of Gaussian upper bounds, and a known Harnack\ninequality is then used to derive Gaussian lower bounds.",
      "generated_abstract": "In this paper, we study the existence and uniqueness of a solution to the\nenhanced Fokker-Planck equation with a localization term in the limit of a\nscaled-down version of the underlying diffusion process. We show that, in\nsufficiently regular settings, the solution of the enhanced Fokker-Planck\nequation is in fact the unique solution to the original Fokker-Planck equation\nwith a localization term. As a consequence, the solution of the enhanced\nFokker-Planck equation can be used to approximate the solution of the\noriginal Fokker-Planck equation. We provide an application of this result to\nthe study of the limiting process of the localized Markov-Chain model, and to\nthe study of the limiting process of the localized Brownian motion.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3076923076923077,
          "f": 0.275862064019025
        },
        "rouge-2": {
          "r": 0.0641025641025641,
          "p": 0.06329113924050633,
          "f": 0.06369426251612681
        },
        "rouge-l": {
          "r": 0.25,
          "p": 0.3076923076923077,
          "f": 0.275862064019025
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.01509v1",
      "true_abstract": "A key step in the Bayesian workflow for model building is the graphical\nassessment of model predictions, whether these are drawn from the prior or\nposterior predictive distribution. The goal of these assessments is to identify\nwhether the model is a reasonable (and ideally accurate) representation of the\ndomain knowledge and/or observed data. There are many commonly used visual\npredictive checks which can be misleading if their implicit assumptions do not\nmatch the reality. Thus, there is a need for more guidance for selecting,\ninterpreting, and diagnosing appropriate visualizations. As a visual predictive\ncheck itself can be viewed as a model fit to data, assessing when this model\nfails to represent the data is important for drawing well-informed conclusions.\n  We present recommendations and diagnostic tools to mitigate ad-hoc\ndecision-making in visual predictive checks. These contributions aim to improve\nthe robustness and interpretability of Bayesian model criticism practices. We\noffer recommendations for appropriate visual predictive checks for observations\nthat are: continuous, discrete, or a mixture of the two. We also discuss\ndiagnostics to aid in the selection of visual methods. Specifically, in the\ndetection of an incorrect assumption of continuously-distributed data:\nidentifying when data is likely to be discrete or contain discrete components,\ndetecting and estimating possible bounds in data, and a diagnostic of the\ngoodness-of-fit to data for density plots made through kernel density\nestimates.",
      "generated_abstract": "We consider the problem of detecting a hidden latent variable in a multivariate\ngeneralized linear model. We introduce the concept of hidden latent variable\nmodeling and show that under certain conditions, it is equivalent to the\nproblem of detecting the latent variable in a latent variable model. We\nprovide an algorithm for the hidden latent variable modeling problem and show\nthat under a certain regularity condition, the algorithm is asymptotically\noptimal. We then extend the algorithm to the case where the data are generated\nby a multivariate normal distribution and apply it to the problem of detecting\nthe location of a hidden latent variable in a multivariate normal model. We\nshow that under a certain regularity condition, the algorithm is asymptotically\noptimal. We also compare the asymptotic behavior of the hidden latent variable\nmodeling algorithm with that of the latent variable modeling algorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1223021582733813,
          "p": 0.32075471698113206,
          "f": 0.17708332933648013
        },
        "rouge-2": {
          "r": 0.014150943396226415,
          "p": 0.033707865168539325,
          "f": 0.019933550652201176
        },
        "rouge-l": {
          "r": 0.1223021582733813,
          "p": 0.32075471698113206,
          "f": 0.17708332933648013
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2502.16118v1",
      "true_abstract": "We study the relationship between the rank of the prior covariance matrix and\nthe local false sign rate in a multivariate empirical Bayes normal mean model.\nIt has been observed that the false sign rate is inflated when the prior\nassigns weight to low-rank covariance matrices. We show that this issue arises\ndue to the rank deficiency of prior covariance matrices and propose an\nadjustment to mitigate it.",
      "generated_abstract": "The statistical significance of a randomized trial is evaluated using the\ntest statistic. The test statistic is also used in the analysis of\nnon-randomized trials to determine the difference between two treatment\ngroups. This paper reviews the statistical significance of the test statistic\nand the difference between two treatment groups in non-randomized trials. It\nprovides an overview of the statistical analysis of non-randomized trials. The\nrecommendations for the design and analysis of non-randomized trials are also\nprovided.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2127659574468085,
          "p": 0.2564102564102564,
          "f": 0.23255813457815047
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.03508771929824561,
          "f": 0.03389830009049197
        },
        "rouge-l": {
          "r": 0.1702127659574468,
          "p": 0.20512820512820512,
          "f": 0.18604650667117376
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/EM/2503.05800v1",
      "true_abstract": "Understanding consumer choice is fundamental to marketing and management\nresearch, as firms increasingly seek to personalize offerings and optimize\ncustomer engagement. Traditional choice modeling frameworks, such as\nmultinomial logit (MNL) and mixed logit models, impose rigid parametric\nassumptions that limit their ability to capture the complexity of consumer\ndecision-making. This study introduces the Mixture of Experts (MoE) framework\nas a machine learning-driven alternative that dynamically segments consumers\nbased on latent behavioral patterns. By leveraging probabilistic gating\nfunctions and specialized expert networks, MoE provides a flexible,\nnonparametric approach to modeling heterogeneous preferences.\n  Empirical validation using large-scale retail data demonstrates that MoE\nsignificantly enhances predictive accuracy over traditional econometric models,\ncapturing nonlinear consumer responses to price variations, brand preferences,\nand product attributes. The findings underscore MoEs potential to improve\ndemand forecasting, optimize targeted marketing strategies, and refine\nsegmentation practices. By offering a more granular and adaptive framework,\nthis study bridges the gap between data-driven machine learning approaches and\nmarketing theory, advocating for the integration of AI techniques in managerial\ndecision-making and strategic consumer insights.",
      "generated_abstract": "We study the problem of estimating the mean and variance of a multivariate\ndistribution from observations in the presence of correlated noise. We show that\nif the distribution is identically and independently distributed (i.i.d.) and\nthe noise follows an i.i.d. distribution, then the mean and variance can be\nestimated using the sample mean and variance of the observed data. We show that\nif the noise follows a multivariate normal distribution with known covariance\nmatrix, then the mean and variance can be estimated by solving a least squares\nproblem. In addition, we show that the mean and variance can be estimated by\nsolving a linear program for the case when the noise follows a Gaussian\ndistribution. We show that if the noise follows a multivariate normal\ndistribution with known covariance matrix, then the mean and variance can be\nestimated by solving a linear program for the case when the noise follows a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08270676691729323,
          "p": 0.1896551724137931,
          "f": 0.11518324184424783
        },
        "rouge-2": {
          "r": 0.005847953216374269,
          "p": 0.011627906976744186,
          "f": 0.007782096714260145
        },
        "rouge-l": {
          "r": 0.07518796992481203,
          "p": 0.1724137931034483,
          "f": 0.10471203765576619
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.01708v1",
      "true_abstract": "We develop a pseudo-likelihood theory for rank one matrix estimation problems\nin the high dimensional limit. We prove a variational principle for the\nlimiting pseudo-maximum likelihood which also characterizes the performance of\nthe corresponding pseudo-maximum likelihood estimator. We show that this\nvariational principle is universal and depends only on four parameters\ndetermined by the corresponding null model. Through this universality, we\nintroduce a notion of equivalence for estimation problems of this type and, in\nparticular, show that a broad class of estimation tasks, including community\ndetection, sparse submatrix detection, and non-linear spiked matrix models, are\nequivalent to spiked matrix models. As an application, we obtain a complete\ndescription of the performance of the least-squares (or ``best rank one'')\nestimator for any rank one matrix estimation problem.",
      "generated_abstract": "We consider the problem of recovering the joint distribution of two random\nvariables $X$ and $Y$ from a single observed value $X$ or $Y$, under\nassumptions on the distribution of the error $Z$. We focus on the case where\n$Z$ is distributed as a Gaussian mixture with mean zero and covariance\n$\\Sigma$, where $\\Sigma$ has non-negative eigenvalues and is positive semidefinite\nwith non-negative trace. The goal is to recover the parameters $\\beta$ and\n$\\sigma$ of $\\Sigma$, which are assumed to be unknown. In the high-dimensional\ncase where $p$ is large compared to the sample size $n$, we show that the\nmaximum likelihood estimator is asymptotically normal, with asymptotic\nnormality given by a generalized inverse Gaussian distribution. We prove that\nthe maximum likelihood estimator is consistent and asymptotically normal,\nproviding a statistical guarantee",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2375,
          "p": 0.2235294117647059,
          "f": 0.23030302530762176
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.03278688524590164,
          "f": 0.03418802919716634
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.18823529411764706,
          "f": 0.19393938894398546
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.physics/flu-dyn/2503.10478v1",
      "true_abstract": "A multiscale stochastic-deterministic coupling method is proposed to\ninvestigate the complex interactions between turbulent and rarefied gas flows\nwithin a unified framework. This method intermittently integrates the general\nsynthetic iterative scheme with the shear stress transport turbulence model\ninto the direct simulation Monte Carlo (DSMC) approach, enabling the simulation\nof gas flows across the free-molecular, transition, slip, and turbulent\nregimes. First, the macroscopic synthetic equations, derived directly from\nDSMC, are coupled with the turbulence model to establish a constitutive\nrelation that incorporates not only turbulent and laminar transport\ncoefficients but also higher-order terms accounting for rarefaction effects.\nSecond, the macroscopic properties, statistically sampled over specific time\nintervals in DSMC, along with the turbulent properties provided by the\nturbulence model, serve as initial conditions for solving the macroscopic\nsynthetic equations. Finally, the simulation particles in DSMC are updated\nbased on the macroscopic properties obtained from the synthetic equations.\nNumerical simulations demonstrate that the proposed method asymptotically\nconverges to either the turbulence model or DSMC results, adaptively adjusting\nto different flow regimes. Then, this coupling method is applied to simulate an\nopposing jet surrounded by hypersonic rarefied gas flows, revealing significant\nvariations in surface properties due to the interplay of turbulent and rarefied\neffects. This study presents an efficient methodology for simulating the\ncomplex interplay between rarefied and turbulent flows, establishing a\nfoundational framework for investigating the coupled effects of turbulence,\nhypersonic conditions, and chemical reactions in rarefied gas dynamics in the\nfuture.",
      "generated_abstract": "The flow of water in a pipe is often modeled by the Navier-Stokes equations,\nwhich are nonlinear and non-local in nature. This paper studies the nonlinear\nand nonlocal flow of water in a pipe using the Lagrangian method. The flow\nsolution is obtained by integrating the Navier-Stokes equations in time, and\nthe solution is discretized using a finite element method. The effect of the\nnonlinearity and nonlocality on the flow is studied through numerical\nexperiments. The results show that the nonlinearity and nonlocality significantly\nincrease the computational complexity of the problem and increase the\ncomputational time required to solve the problem. The effects of the\nnonlinearity and nonlocality on the solution are also studied using the\nLagrange-Euler method. The results show that the nonlinearity and nonlocality\nenhance the solution accuracy and decrease",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1388888888888889,
          "p": 0.32786885245901637,
          "f": 0.19512194703914346
        },
        "rouge-2": {
          "r": 0.01834862385321101,
          "p": 0.041237113402061855,
          "f": 0.025396821134593812
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.29508196721311475,
          "f": 0.17560975191719225
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.19862v1",
      "true_abstract": "Decentralized lending protocols within the decentralized finance ecosystem\nenable the lending and borrowing of crypto-assets without relying on\ntraditional intermediaries. Interest rates in these protocols are set\nalgorithmically and fluctuate according to the supply and demand for liquidity.\nIn this study, we propose an agent-based model tailored to a decentralized\nlending protocol and determine the optimal interest rate model. When the\nresponses of the agents are linear with respect to the interest rate, the\noptimal solution is derived from a system of Riccati-type ODEs. For nonlinear\nbehaviors, we propose a Monte-Carlo estimator, coupled with deep learning\ntechniques, to approximate the optimal solution. Finally, after calibrating the\nmodel using block-by-block data, we conduct a risk-adjusted profit and loss\nanalysis of the liquidity pool under industry-standard interest rate models and\nbenchmark them against the optimal interest rate model.",
      "generated_abstract": "This paper investigates the implications of the use of different numerical\nassets in the evaluation of the efficient frontiers of portfolios. We\nconsider two commonly used numerical assets: the capital asset pricing model\n(CAPM) and the generalized CAPM (GCAPM), and study their effect on the\nefficiency of the portfolio. We show that the GCAPM is more efficient than the\nCAPM in the long run, and that the GCAPM is more efficient than the CAPM in the\nshort run. Our results highlight the importance of carefully choosing the\nnumerical asset in order to obtain accurate and reliable information on the\nefficiency of portfolios.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0898876404494382,
          "p": 0.13793103448275862,
          "f": 0.10884353263732724
        },
        "rouge-2": {
          "r": 0.008064516129032258,
          "p": 0.011904761904761904,
          "f": 0.009615379800298267
        },
        "rouge-l": {
          "r": 0.0898876404494382,
          "p": 0.13793103448275862,
          "f": 0.10884353263732724
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09773v1",
      "true_abstract": "Current template-based gravitational-wave searches for compact binary mergers\nneglect the general relativistic phenomenon of spin-induced orbital precession.\nOwing to their asymmetric masses, gravitational-waves from neutron star-black\nhole (NSBH) binaries are prime candidates for displaying strong imprints of\nspin-precession. As a result, current searches may be missing a significant\nfraction of the astrophysical population, and the detected NSBH population may\nbe significantly suppressed or biased. Here we report the most sensitive search\nfor NSBH binaries to date by including spin-precession for the first time. We\nanalyze data from the entirety of the third LIGO-Virgo-KAGRA gravitational-wave\nobserving run and show that when accounting for spin-precession, our search is\nup to 100% more sensitive than the search techniques currently adopted by the\nLIGO-Virgo-KAGRA collaboration (for systems with strong precessional effects).\nThis allows us to more tightly constrain the rate of NSBH mergers in the local\nUniverse. Firstly, we focus on a precessing subpopulation of NSBH mergers; the\nlack of observed candidates allows us to place an upper limit on the merger\nrate of $R_{90} = 79\\, \\mathrm{Gpc}^{-3}\\mathrm{yr}^{-1}$ with 90% confidence.\nSecondly, we tighten the overall rate of NSBH mergers; we show that if there is\nno preferred direction of component spin, the rate of NSBH mergers is on\naverage 16% smaller than previously believed. Finally, we report four new\nsubthreshold NSBH candidates, all with strong imprints of spin precession, but\nnote that these are most likely to be of terrestrial origin.",
      "generated_abstract": "We present a study of the effects of cosmic-ray-induced ionization on\nthe properties of high-Z elements in the iron-peak group, based on the\nhigh-resolution synchrotron spectroscopy of 14 elements obtained with the\nRHIC-UPGRADE facility at RWTH Aachen University. In the case of iron, we find\nthat cosmic-ray ionization leads to an increase in the abundance of the\noxygen-isotope 12O/16O by 10\\%. This effect is not seen in the other elements.\nThe cosmic-ray ionization also modifies the abundances of the non-neutron\nstabilized elements. We attribute the observed enhancement of the abundance of\n12O/16O to the effect of the cosmic-ray induced ionization, and we argue that the\nex",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10596026490066225,
          "p": 0.24615384615384617,
          "f": 0.148148143940758
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.031578947368421054,
          "f": 0.019108276034525657
        },
        "rouge-l": {
          "r": 0.09271523178807947,
          "p": 0.2153846153846154,
          "f": 0.1296296254222395
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06020v1",
      "true_abstract": "Using a reaction-diffusion model with free boundaries in one space dimension\nfor a single population species with density $u(t,x)$ and population range\n$[g(t), h(t)]$, we demonstrate that the Allee effects can be eliminated if the\nspecies maintains its population density at a suitable level at the range\nboundary by advancing or retreating the fronts. It is proved that with such a\nstrategy at the range edge the species can invade the environment successfully\nwith all admissible initial populations, exhibiting the dynamics of super\ninvaders. Numerical simulations are used to help understand what happens if the\npopulation density level at the range boundary is maintained at other levels.\nIf the invading cane toads in Australia used this strategy at the range\nboundary to become a super invader, then our results may explain why toads near\nthe invading front evolve to have longer legs and run faster.",
      "generated_abstract": "A method is introduced for the generation of a wide range of organisms from a\nsingle genotype. The method relies on the coalescent of a single population\ncontaining a fixed number of individuals, and the use of an evolutionary\ndynamics model to simulate the coalescent. This method is validated by\nanalyzing the evolution of a model organism with a simple system of\nreaction-diffusion equations. The model organism was used as a test-bed to\nvalidate the method, which provides a powerful and flexible tool for the\ndevelopment of new models and applications of evolutionary theory.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1702127659574468,
          "p": 0.2711864406779661,
          "f": 0.20915032205903725
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.023255813953488372,
          "f": 0.018518513725995753
        },
        "rouge-l": {
          "r": 0.13829787234042554,
          "p": 0.22033898305084745,
          "f": 0.16993463578452747
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/GA/2503.10532v1",
      "true_abstract": "Type II Cepheids (T2Ceps), alongside RR Lyrae stars, serve as important\ndistance indicators for old population II stars due to their period-luminosity\n(PL) relations. However, studies of these relations in the Sloan photometric\nsystem are rather limited in the literature. Our goal is to calibrate PL\nrelations (and their counterparts in Wesenheit magnitudes) in the\nSloan-Pan-STARSS gP1rP1iP1 bands for Galactic T2Ceps located in the vicinity of\nthe Sun. We collected data for 16 T2Ceps of the BLHer type and 17 of the WVir\ntype using 40 cm telescopes of the Las Cumbres Observatory Global Telescope\nNetwork. Geometric parallaxes were adopted from Gaia Data Release 3. We have\ncalibrated PL and period-Wesenheit relations for Milky Way BLHer and WVir stars\nin the solar neighborhood, as well as for a combined sample of both types. The\nrelationships derived here will allow to determine the distances to T2Ceps that\nwill be discovered by the Legacy Survey of Space and Time survey and, in turn,\nto probe the extended halo of the Milky Way, as well as the halos of nearby\ngalaxies. To the best of our knowledge, the relations derived in this study are\nthe first for Milky Way T2Ceps in the Sloan bands.",
      "generated_abstract": "The rapid expansion of exoplanetary systems has been linked to planetary\nnatal kicks, but it is unclear how these kicks evolve over time. Here, we\ninvestigate the long-term evolution of planetary kicks in exoplanetary systems\nusing the GAIA-DR2 data release. We find that, within the first 10 Myr after\ndiscovery, kicks are on average 0.14 km/s, increasing to 0.25 km/s by 10 Gyr.\nThis is comparable to the kicks found for field stars in the solar neighborhood\n(0.19 km/s). We show that the kicks are correlated with the stellar mass of the\nhost star, with the highest kicks found in the most massive stars. We also\nidentify a strong correlation between kicks and the stellar effective temperature\nof the host",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14960629921259844,
          "p": 0.2235294117647059,
          "f": 0.17924527821511227
        },
        "rouge-2": {
          "r": 0.026881720430107527,
          "p": 0.04424778761061947,
          "f": 0.033444811351551536
        },
        "rouge-l": {
          "r": 0.13385826771653545,
          "p": 0.2,
          "f": 0.1603773536868104
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2411.13559v1",
      "true_abstract": "Financial markets are nonlinear with complexity, where different types of\nassets are traded between buyers and sellers, each having a view to maximize\ntheir Return on Investment (ROI). Forecasting market trends is a challenging\ntask since various factors like stock-specific news, company profiles, public\nsentiments, and global economic conditions influence them. This paper describes\na daily price directional predictive system of financial instruments,\naddressing the difficulty of predicting short-term price movements. This paper\nwill introduce the development of a novel trading system methodology by\nproposing a two-layer Composing Ensembles architecture, optimized through grid\nsearch, to predict whether the price will rise or fall the next day. This\nstrategy was back-tested on a wide range of financial instruments and time\nframes, demonstrating an improvement of 20% over the benchmark, representing a\nstandard investment strategy.",
      "generated_abstract": "This study examines the impact of the COVID-19 pandemic on the stock market\nperformance of S\\&P 500 Index. The study utilized panel data for the 10-year\nperiod between January 1, 2012 and December 31, 2021. The results indicate that\nthe stock market performance of S\\&P 500 Index was significantly affected by\nthe COVID-19 pandemic, which resulted in a negative impact on its performance.\nThe findings also indicate that the impact of the COVID-19 pandemic on the\nstock market performance of S\\&P 500 Index is more pronounced in the first 18\nmonths of the pandemic, suggesting that investors should be more cautious in\nselecting stocks to invest in during this period.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11538461538461539,
          "p": 0.19047619047619047,
          "f": 0.14371257015167285
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09615384615384616,
          "p": 0.15873015873015872,
          "f": 0.11976047434328967
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.16584v1",
      "true_abstract": "Recent advancements in audio tokenization have significantly enhanced the\nintegration of audio capabilities into large language models (LLMs). However,\naudio understanding and generation are often treated as distinct tasks,\nhindering the development of truly unified audio-language models. While\ninstruction tuning has demonstrated remarkable success in improving\ngeneralization and zero-shot learning across text and vision, its application\nto audio remains largely unexplored. A major obstacle is the lack of\ncomprehensive datasets that unify audio understanding and generation. To\naddress this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset\ncovering 80 diverse tasks across speech, music, and sound domains, with over\n100 million instances. Audio-FLAN lays the foundation for unified\naudio-language models that can seamlessly handle both understanding (e.g.,\ntranscription, comprehension) and generation (e.g., speech, music, sound) tasks\nacross a wide range of audio domains in a zero-shot manner. The Audio-FLAN\ndataset is available on HuggingFace and GitHub and will be continuously\nupdated.",
      "generated_abstract": "This paper introduces a novel approach for music information retrieval\n(MIR) by integrating multi-modal music representations. The proposed method\nutilizes the fusion of multimodal information, including music audio, textual\nmetadata, and visual representations, to enhance retrieval performance. The\nframework leverages multi-task learning to optimize audio-text, audio-visual,\nand visual-visual representations. This approach facilitates a more holistic\nunderstanding of the music content, enabling accurate retrieval results.\nAdditionally, the proposed method leverages the fusion of music audio, textual\nmetadata, and visual representations to enhance retrieval performance. The\nframework leverages multi-task learning to optimize audio-text, audio-visual,\nand visual-visual representations. This approach facilitates a more holistic\nunderstanding of the music content, enabling accurate retrieval results.\nAdditionally, a data augmentation technique",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08108108108108109,
          "p": 0.16666666666666666,
          "f": 0.10909090468760349
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.16666666666666666,
          "f": 0.10909090468760349
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.15131v1",
      "true_abstract": "We study the fundamental problem of calibrating a linear binary classifier of\nthe form $\\sigma(\\hat{w}^\\top x)$, where the feature vector $x$ is Gaussian,\n$\\sigma$ is a link function, and $\\hat{w}$ is an estimator of the true linear\nweight $w^\\star$. By interpolating with a noninformative $\\textit{chance\nclassifier}$, we construct a well-calibrated predictor whose interpolation\nweight depends on the angle $\\angle(\\hat{w}, w_\\star)$ between the estimator\n$\\hat{w}$ and the true linear weight $w_\\star$. We establish that this angular\ncalibration approach is provably well-calibrated in a high-dimensional regime\nwhere the number of samples and features both diverge, at a comparable rate.\nThe angle $\\angle(\\hat{w}, w_\\star)$ can be consistently estimated.\nFurthermore, the resulting predictor is uniquely $\\textit{Bregman-optimal}$,\nminimizing the Bregman divergence to the true label distribution within a\nsuitable class of calibrated predictors. Our work is the first to provide a\ncalibration strategy that satisfies both calibration and optimality properties\nprovably in high dimensions. Additionally, we identify conditions under which a\nclassical Platt-scaling predictor converges to our Bregman-optimal calibrated\nsolution. Thus, Platt-scaling also inherits these desirable properties provably\nin high dimensions.",
      "generated_abstract": "We consider a family of nonlinear regression models with linear and nonlinear\ninteractions, where the nonlinearity is expressed as an operator on the space of\nfunctions on $\\mathbb{R}^d$. The aim is to design a class of estimators that\nachieve both the minimax optimality and the minimax lower bounds. We propose a\ntwo-step strategy based on a first-order optimality criterion for estimating\nthe linear model and a second-order optimality criterion for estimating the\nnonlinear model. Our estimator achieves both minimax optimality and\nminimax lower bounds. We establish that the estimator in the first step\nachieves the minimax optimality, and in the second step, the estimator in the\nsecond step achieves the minimax lower bound. We establish the minimax lower\nbounds and the minimax optimality of our estimator under some mild conditions.\nWe derive the minim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23008849557522124,
          "p": 0.4,
          "f": 0.2921348268242646
        },
        "rouge-2": {
          "r": 0.049079754601226995,
          "p": 0.07547169811320754,
          "f": 0.05947954912784549
        },
        "rouge-l": {
          "r": 0.20353982300884957,
          "p": 0.35384615384615387,
          "f": 0.2584269616557254
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.physics/data-an/2503.09328v1",
      "true_abstract": "A Schr\\\"odinger bridge is the most probable time-dependent probability\ndistribution that connects an initial probability distribution $w_{i}$ to a\nfinal one $w_{f}$. The problem has been solved and widely used for the case of\nsimple Brownian evolution (non-interacting particles). It is related to the\nproblem of entropy regularized Wasserstein optimal transport. In this article,\nwe generalize Brownian bridges to systems of interacting particles. We derive\nsome equations for the forward and backward single particle ``wave-functions''\nwhich allow to compute the most probable evolution of the single-particle\nprobability between the initial and final distributions.",
      "generated_abstract": "We propose a model for non-Markovian memory effects in open quantum\nsystems driven by a time-dependent Hamiltonian. This model, based on a\nquasi-classical approach, is intended to describe the effect of the\ntime-dependent driving on the correlations of the system's density matrix. We\nassess the accuracy of the model by comparing its predictions with numerical\nsimulations of the time-dependent dynamics of the system's density matrix. We\nfind that the memory kernel obtained from the model exhibits a pronounced\nlong-time tail, and we interpret this as a signature of the non-Markovian\neffects. Our model also allows us to compute the memory kernel in the limit of\ninfinite driving strength, where it converges to the memory kernel obtained\nfrom the master equation. In this limit, our model predicts a single peak in\nthe memory kernel. We discuss the implications of our results for the\ndet",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23880597014925373,
          "p": 0.19753086419753085,
          "f": 0.21621621126095697
        },
        "rouge-2": {
          "r": 0.06818181818181818,
          "p": 0.049586776859504134,
          "f": 0.05741626306723789
        },
        "rouge-l": {
          "r": 0.22388059701492538,
          "p": 0.18518518518518517,
          "f": 0.20270269774744348
        }
      }
    },
    {
      "paper_id": "eess.SP.math/GR/2503.09398v1",
      "true_abstract": "Incorporating mathematical properties of a wireless policy to be learned into\nthe design of deep neural networks (DNNs) is effective for enhancing learning\nefficiency. Multi-user precoding policy in multi-antenna system, which is the\nmapping from channel matrix to precoding matrix, possesses a permutation\nequivariance property, which has been harnessed to design the parameter sharing\nstructure of the weight matrix of DNNs. In this paper, we study a stronger\nproperty than permutation equivariance, namely unitary equivariance, for\nprecoder learning. We first show that a DNN with unitary equivariance designed\nby further introducing parameter sharing into a permutation equivariant DNN is\nunable to learn the optimal precoder. We proceed to develop a novel non-linear\nweighting process satisfying unitary equivariance and then construct a joint\nunitary and permutation equivariant DNN. Simulation results demonstrate that\nthe proposed DNN not only outperforms existing learning methods in learning\nperformance and generalizability but also reduces training complexity.",
      "generated_abstract": "We study the problem of minimizing the total variation distance between two\ndistributions $P$ and $Q$ in a bounded convex domain. The key result of this\nwork is the following: for any $P$ and $Q$ such that $P$ and $Q$ are supported\non disjoint subsets of the domain, there exists a distribution $R$ such that\n$P \\le R \\le Q$ and $P - R$ and $Q - R$ are equal in distribution. This\nresult has several interesting consequences, including the following: the\nminimum of the total variation distance between two distributions is\ndetermined by the minimum of the total variation distance between their\ndistributions of support, and the minimum of the total variation distance is\ndetermined by the minimum of the total variation distance between their\ndistributions of support and their distributions of support with the\ndistributions of support removed.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1414141414141414,
          "p": 0.2222222222222222,
          "f": 0.17283950141975318
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.02127659574468085,
          "f": 0.016806717909753497
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.1746031746031746,
          "f": 0.13580246438271623
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09150v1",
      "true_abstract": "Personalization is a critical yet often overlooked factor in boosting\nproductivity and wellbeing in knowledge-intensive workplaces to better address\nindividual preferences. Existing tools typically offer uniform guidance whether\nauto-generating email responses or prompting break reminders without accounting\nfor individual behavioral patterns or stress triggers. We introduce AdaptAI, a\nmultimodal AI solution combining egocentric vision and audio, heart and motion\nactivities, and the agentic workflow of Large Language Models LLMs to deliver\nhighly personalized productivity support and context-aware well-being\ninterventions. AdaptAI not only automates peripheral tasks (e.g. drafting\nsuccinct document summaries, replying to emails etc.) but also continuously\nmonitors the users unique physiological and situational indicators to\ndynamically tailor interventions such as micro-break suggestions or exercise\nprompts, at the exact point of need. In a preliminary study with 15\nparticipants, AdaptAI demonstrated significant improvements in task throughput\nand user satisfaction by anticipating user stressors and streamlining daily\nworkflows.",
      "generated_abstract": "This paper presents a novel framework for predicting the evolution of\ncultural norms in complex societies, by integrating textual and socio-economic\ndata. Using a novel approach based on the theory of structural complexity, we\nshow that a culture's evolution is directly linked to the quality of its\ninfrastructure, and to the existence of a network of interconnected social\nstructures. We present a novel method for evaluating the quality of a society's\ninfrastructure, based on the concept of complexity. We show that a culture's\nevolution is directly linked to the quality of its infrastructure, and to the\nexistence of a network of interconnected social structures. Using a\nmulti-modal dataset, we show that the evolution of cultural norms can be\npredictable and explainable. We demonstrate that the evolution of cultural\nnorms is affected by the quality of a society's infrastructure and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08,
          "p": 0.16393442622950818,
          "f": 0.10752687731240625
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08,
          "p": 0.16393442622950818,
          "f": 0.10752687731240625
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2407.10284v2",
      "true_abstract": "``Self-Organised Criticality'' (SOC) is the mechanism by which complex\nsystems spontaneously settle close to a *critical point*, at the edge between\nstability and chaos, and characterized by fat-tailed fluctuations and\nlong-memory correlations. Such a scenario may explain why insignificant\nperturbations can generate large disruptions, through the propagation of\n``avalanches'' across the system. In this short review, we discuss how SOC\ncould offer a plausible solution to the excess volatility puzzle in financial\nmarkets and the analogue ``small shocks, large business cycle puzzle'' for the\neconomy at large, as initially surmised by Per Bak et al. in 1993. We argue\nthat in general the quest for efficiency and the necessity of *resilience* may\nbe mutually incompatible and require specific policy considerations.",
      "generated_abstract": "This paper presents a new methodology for determining the optimal\nfunding strategy for the acquisition of assets in a portfolio. The proposed\nframework is based on a system of nonlinear ordinary differential equations\n(ODEs), which are solved numerically using the B\\'ezier spline method. The\nmethodology is tested on a real dataset of portfolio acquisitions of\nmultinational companies in the U.S. market over the period from 1998 to 2018.\nThe results show that the proposed methodology yields accurate and reliable\nresults, with the system of ODEs providing a clear and comprehensive\nunderstanding of the dynamics of the portfolio.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11578947368421053,
          "p": 0.16417910447761194,
          "f": 0.13580246428517012
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.02127659574468085,
          "f": 0.01886791959238292
        },
        "rouge-l": {
          "r": 0.09473684210526316,
          "p": 0.13432835820895522,
          "f": 0.11111110626047879
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/CY/2503.10264v1",
      "true_abstract": "While peer review enhances writing and research quality, harsh feedback can\nfrustrate and demotivate authors. Hence, it is essential to explore how\ncritiques should be delivered to motivate authors and enable them to keep\niterating their work. In this study, we explored the impact of appending an\nautomatically generated positive summary to the peer reviews of a writing task,\nalongside varying levels of overall evaluations (high vs. low), on authors'\nfeedback reception, revision outcomes, and motivation to revise. Through a 2x2\nonline experiment with 137 participants, we found that adding an AI-reframed\npositive summary to otherwise harsh feedback increased authors' critique\nacceptance, whereas low overall evaluations of their work led to increased\nrevision efforts. We discuss the implications of using AI in peer feedback,\nfocusing on how AI-driven critiques can influence critique acceptance and\nsupport research communities in fostering productive and friendly peer feedback\npractices.",
      "generated_abstract": "The rise of the Metaverse has sparked a global conversation about the\nintersection of augmented reality (AR) and virtual reality (VR). This paper\nexplores how VR can enhance AR experiences by leveraging its spatial\nrepresentation capabilities. We present a framework for designing VR AR\nexperiences that integrate AR features such as location-based content,\ninteractivity, and immersive visualization. This approach allows users to\nobtain a more holistic view of the physical world, allowing them to explore\nspaces and interact with objects in real time. The paper examines various\nVR technologies, such as motion tracking, spatial tracking, and immersive\nvisualization, and how they can be used to enhance AR experiences. We also\ndiscuss how these technologies can be integrated into VR AR experiences to\ncreate more immersive and interactive environments. This study provides\ninsights into how VR",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.16853932584269662,
          "f": 0.1587301537470957
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.016129032258064516,
          "f": 0.015209120491551315
        },
        "rouge-l": {
          "r": 0.13,
          "p": 0.14606741573033707,
          "f": 0.13756613258307457
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00725v1",
      "true_abstract": "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
      "generated_abstract": "We study the causal effect of a binary treatment $Z$ on a continuous\ntarget $Y$ through a vector of multinomial logit (MNL) models. We derive the\ndistribution of the causal effect of $Z$ on $Y$ via the marginal likelihood and\nestimate it using maximum likelihood estimation. We provide a general\ntheoretical justification for the estimation procedure, including a\nrepresentation of the causal effect as a multivariate linear combination of\nthe estimated coefficients. We apply our method to a dataset of 13,645\nadults from the U.S. Census Bureau's American Community Survey (ACS), with\n$Z$ indicating whether the respondent has ever been arrested or been\narrested within the past 12 months. We find that the estimated causal effect of\n$Z$ on $Y$ is approximately 14.4% (95",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.19753086419753085,
          "f": 0.18079095548788676
        },
        "rouge-2": {
          "r": 0.007575757575757576,
          "p": 0.00909090909090909,
          "f": 0.008264457851242643
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.12345679012345678,
          "f": 0.11299434531839532
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/GN/2503.07498v1",
      "true_abstract": "We examine the problem of optimal portfolio allocation within the framework\nof utility theory. We apply exponential utility to derive the optimal\ndiversification strategy and logarithmic utility to determine the optimal\nleverage. We enhance existing methodologies by incorporating compound\nprobability distributions to model the effects of both statistical and\nnon-stationary uncertainties. Additionally, we extend the maximum expected\nutility objective by including the variance of utility in the objective\nfunction, which we term generalized mean-variance. In the case of logarithmic\nutility, it provides a natural explanation for the half-Kelly criterion, a\nconcept widely used by practitioners.",
      "generated_abstract": "This study investigates the influence of the price of Bitcoin on the price of\ncopper in the global financial market. We use data from the CME exchange\n(https://www.cME.com), which provides the price of Bitcoin in the US dollar\ncurrency, to test the hypothesis that the price of Bitcoin will have an\ninfluence on the price of copper. Our results show that the price of Bitcoin\nhas a significant impact on the price of copper. The price of Bitcoin has a\nnegative correlation with the price of copper, with a coefficient of -0.2168.\nThis indicates that the price of Bitcoin will negatively affect the price of\ncopper. In addition, we find that the correlation between the price of Bitcoin\nand the price of copper is not significant, with a coefficient of 0.0540. This\nind",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16923076923076924,
          "p": 0.16923076923076924,
          "f": 0.1692307642307694
        },
        "rouge-2": {
          "r": 0.01098901098901099,
          "p": 0.01098901098901099,
          "f": 0.010989005989013265
        },
        "rouge-l": {
          "r": 0.13846153846153847,
          "p": 0.13846153846153847,
          "f": 0.13846153346153867
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.01704v1",
      "true_abstract": "Discrete Choice Modelling serves as a robust framework for modelling human\nchoice behaviour across various disciplines. Building a choice model is a semi\nstructured research process that involves a combination of a priori\nassumptions, behavioural theories, and statistical methods. This complex set of\ndecisions, coupled with diverse workflows, can lead to substantial variability\nin model outcomes. To better understand these dynamics, we developed the\nSerious Choice Modelling Game, which simulates the real world modelling process\nand tracks modellers' decisions in real time using a stated preference dataset.\nParticipants were asked to develop choice models to estimate Willingness to Pay\nvalues to inform policymakers about strategies for reducing noise pollution.\nThe game recorded actions across multiple phases, including descriptive\nanalysis, model specification, and outcome interpretation, allowing us to\nanalyse both individual decisions and differences in modelling approaches.\nWhile our findings reveal a strong preference for using data visualisation\ntools in descriptive analysis, it also identifies gaps in missing values\nhandling before model specification. We also found significant variation in the\nmodelling approach, even when modellers were working with the same choice\ndataset. Despite the availability of more complex models, simpler models such\nas Multinomial Logit were often preferred, suggesting that modellers tend to\navoid complexity when time and resources are limited. Participants who engaged\nin more comprehensive data exploration and iterative model comparison tended to\nachieve better model fit and parsimony, which demonstrate that the\nmethodological choices made throughout the workflow have significant\nimplications, particularly when modelling outcomes are used for policy\nformulation.",
      "generated_abstract": "In this paper, we develop a novel method for estimating stochastic volatility\nmodels with both daily and annual returns using a single data set of stock\nprices. The method combines a novel approach for handling the non-stationary\ndaily returns and a novel approach for handling the long-memory property of\nannual returns. We propose a novel approach for handling the non-stationary\ndaily returns and a novel approach for handling the long-memory property of\nannual returns. The method is based on the use of a time series autoregressive\nmodel for the daily returns and an autoregressive moving average model for the\nannual returns. The estimated models are then used to construct an\nautoregressive integrated moving average (ARIMA) model for the annual returns.\nThe resulting ARIMA model is estimated using a data-driven approach. We show\nthat the proposed method can handle both daily and annual returns, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14035087719298245,
          "p": 0.375,
          "f": 0.20425531518551385
        },
        "rouge-2": {
          "r": 0.020080321285140562,
          "p": 0.05102040816326531,
          "f": 0.028818439750849762
        },
        "rouge-l": {
          "r": 0.14035087719298245,
          "p": 0.375,
          "f": 0.20425531518551385
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10196v1",
      "true_abstract": "In this paper, we present an error estimate for the filtered Lie splitting\nscheme applied to the Zakharov system, characterized by solutions exhibiting\nvery low regularity across all dimensions. Our findings are derived from the\napplication of multilinear estimates established within the framework of\ndiscrete Bourgain spaces. Specifically, we demonstrate that when the solution\n$(E,z,z_t) \\in H^{s+r+1/2}\\times H^{s+r}\\times H^{s+r-1}$, the error in\n$H^{r+1/2}\\times H^{r}\\times H^{r-1}$ is $\\mathcal{O}(\\tau^{s/2})$ for\n$s\\in(0,2]$, where $r=\\max(0,\\frac d2-1)$. To the best of our knowledge, this\nrepresents the first explicit error estimate for the splitting method based on\nthe original Zakharov system, as well as the first instance where low\nregularity error estimates for coupled equations have been considered within\nthe Bourgain framework. Furthermore, numerical experiments confirm the validity\nof our theoretical results.",
      "generated_abstract": "We study the convergence of a non-conforming finite element method for the\nsolution of the two-phase Navier-Stokes equations. Our approach is based on\nthe representation of the divergence of the pressure tensor as a weighted sum\nof a scalar function and a velocity-dependent pressure gradient tensor. The\npressure-gradient tensor is constructed by an iterative procedure based on a\nsuitable interpolation of the velocity field, and it is defined on a\nsufficiently fine mesh of the domain. We prove that the pressure-gradient\ntensor converges to the real pressure tensor in the sense of trace and\npointwise. Furthermore, we derive a sufficient condition for the convergence of\nthe iterative procedure. We demonstrate the convergence of the iterative\nprocedure by numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2247191011235955,
          "p": 0.30303030303030304,
          "f": 0.258064511239126
        },
        "rouge-2": {
          "r": 0.042735042735042736,
          "p": 0.050505050505050504,
          "f": 0.04629629133101905
        },
        "rouge-l": {
          "r": 0.20224719101123595,
          "p": 0.2727272727272727,
          "f": 0.23225805962622278
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14877v1",
      "true_abstract": "In this paper is studied the problem concerning the angle between two\nsubspaces of arbitrary dimensions in Euclidean space $E_{n}$. It is proven that\nthe angle between two subspaces is equal to the angle between their orthogonal\nsubspaces. Using the eigenvalues and eigenvectors of corresponding matrix\nrepresentations, there are introduced principal values and principal subspaces.\nTheir geometrical interpretation is also given together with the canonical\nrepresentation of the two subspaces. The canonical matrix for the two subspaces\nis introduced and its properties of duality are obtained. Here obtained results\nexpand the classic results given in [1,2].",
      "generated_abstract": "We prove that any non-trivial homogeneous polynomial of degree 2 in the\ncommutative ring of Laurent polynomials with real coefficients can be written\nas a linear combination of the Laurent polynomials themselves. This result\nfollows from an application of the Gelfand-Levitan theorem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08620689655172414,
          "p": 0.14285714285714285,
          "f": 0.10752687702624603
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.02564102564102564,
          "f": 0.016129027946150995
        },
        "rouge-l": {
          "r": 0.06896551724137931,
          "p": 0.11428571428571428,
          "f": 0.08602150068216004
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.16277v1",
      "true_abstract": "Machine learning is critical for innovation and efficiency in financial\nmarkets, offering predictive models and data-driven decision-making. However,\nchallenges such as missing data, lack of transparency, untimely updates,\ninsecurity, and incompatible data sources limit its effectiveness. Blockchain\ntechnology, with its transparency, immutability, and real-time updates,\naddresses these challenges. We present a framework for integrating\nhigh-frequency on-chain data with low-frequency off-chain data, providing a\nbenchmark for addressing novel research questions in economic mechanism design.\nThis framework generates modular, extensible datasets for analyzing economic\nmechanisms such as the Transaction Fee Mechanism, enabling multi-modal insights\nand fairness-driven evaluations. Using four machine learning techniques,\nincluding linear regression, deep neural networks, XGBoost, and LSTM models, we\ndemonstrate the framework's ability to produce datasets that advance financial\nresearch and improve understanding of blockchain-driven systems. Our\ncontributions include: (1) proposing a research scenario for the Transaction\nFee Mechanism and demonstrating how the framework addresses previously\nunexplored questions in economic mechanism design; (2) providing a benchmark\nfor financial machine learning by open-sourcing a sample dataset generated by\nthe framework and the code for the pipeline, enabling continuous dataset\nexpansion; and (3) promoting reproducibility, transparency, and collaboration\nby fully open-sourcing the framework and its outputs. This initiative supports\nresearchers in extending our work and developing innovative financial\nmachine-learning models, fostering advancements at the intersection of machine\nlearning, blockchain, and economics.",
      "generated_abstract": "This study examines the impact of the 2020 COVID-19 pandemic on the\ngovernment's spending priorities. Using a stochastic frontier analysis, we\ninvestigate the impact of the pandemic on government expenditure in the\nGhanaian context. We find that the pandemic had a significant impact on\ngovernment spending, with a decline in expenditure on health and education\nfollowed by an increase in expenditure on social protection and infrastructure.\nThese findings underscore the need to adapt fiscal policies to account for\npandemic-related shocks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09722222222222222,
          "p": 0.2545454545454545,
          "f": 0.1407035135880408
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09722222222222222,
          "p": 0.2545454545454545,
          "f": 0.1407035135880408
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.08018v1",
      "true_abstract": "In this paper we consider the Toda lattice $(\\boldsymbol{p}(t);\n\\boldsymbol{q}(t))$ at thermal equilibrium, meaning that its variables $(p_i)$\nand $(e^{q_i-q_{i+1}})$ are independent Gaussian and Gamma random variables,\nrespectively. We justify the notion from the physics literature that this model\ncan be thought of as a dense collection of solitons (or \"soliton gas'') by, (i)\nprecisely defining the locations of these solitons; (ii) showing that local\ncharges and currents for the Toda lattice are well-approximated by simple\nfunctions of the soliton data; and (iii) proving an asymptotic scattering\nrelation that governs the dynamics of the soliton locations. Our arguments are\nbased on analyzing properties about eigenvector entries of the Toda lattice's\n(random) Lax matrix, particularly, their rates of exponential decay and their\nevolutions under inverse scattering.",
      "generated_abstract": "We present a complete analytical solution for the non-monotonic evolution\nof a linear system of differential equations, subject to a sinusoidal forcing\nthat follows an exponential decay. The solution is obtained via the\nHamilton-Jacobi equation, which allows for a closed form solution for the\nsystem's probability density function. In the case where the forcing has a\nsymmetric distribution, we find that the system follows a non-monotonic\nevolution. For asymmetric distributions, the system exhibits a monotonic\nevolution. We show that the solution can be extended to non-linear systems,\nwhich we demonstrate with a simple model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1595744680851064,
          "p": 0.2459016393442623,
          "f": 0.19354838232341323
        },
        "rouge-2": {
          "r": 0.025423728813559324,
          "p": 0.03409090909090909,
          "f": 0.029126208698275872
        },
        "rouge-l": {
          "r": 0.13829787234042554,
          "p": 0.21311475409836064,
          "f": 0.16774193071051002
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.03019v1",
      "true_abstract": "In this paper, we define an underlying data generating process that allows\nfor different magnitudes of cross-sectional dependence, along with time series\nautocorrelation. This is achieved via high-dimensional moving average processes\nof infinite order (HDMA($\\infty$)). Our setup and investigation integrates and\nenhances homogenous and heterogeneous panel data estimation and testing in a\nunified way. To study HDMA($\\infty$), we extend the Beveridge-Nelson\ndecomposition to a high-dimensional time series setting, and derive a complete\ntoolkit set. We exam homogeneity versus heterogeneity using Gaussian\napproximation, a prevalent technique for establishing uniform inference. For\npost-testing inference, we derive central limit theorems through Edgeworth\nexpansions for both homogenous and heterogeneous settings. Additionally, we\nshowcase the practical relevance of the established asymptotic properties by\nrevisiting the common correlated effects (CCE) estimators, and a classic\nnonstationary panel data process. Finally, we verify our theoretical findings\nvia extensive numerical studies using both simulated and real datasets.",
      "generated_abstract": "This paper proposes a novel approach to estimating the marginal structural\nparameters of the Cox-Ingersoll-Ross (CIR) model in high-dimensional settings\nby leveraging a multilevel coordinate descent (MDLC) algorithm. The MDLC\nalgorithm is tailored to the CIR model and is shown to outperform the\nstandard multilevel algorithm in terms of computational efficiency and\nperformance. The MDLC algorithm is applied to three high-dimensional CIR models\nto estimate the parameters of the model in the presence of serial correlation\nand heteroskedasticity. The results indicate that the proposed MDLC algorithm\noutperforms existing methods in terms of computational efficiency, accuracy, and\npower, particularly in high-dimensional settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10619469026548672,
          "p": 0.20689655172413793,
          "f": 0.14035087271023577
        },
        "rouge-2": {
          "r": 0.006993006993006993,
          "p": 0.011363636363636364,
          "f": 0.008658003941457939
        },
        "rouge-l": {
          "r": 0.10619469026548672,
          "p": 0.20689655172413793,
          "f": 0.14035087271023577
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.05649v1",
      "true_abstract": "Recent advancements in controllable expressive speech synthesis, especially\nin text-to-speech (TTS) models, have allowed for the generation of speech with\nspecific styles guided by textual descriptions, known as style prompts. While\nthis development enhances the flexibility and naturalness of synthesized\nspeech, there remains a significant gap in understanding how these models\nhandle vague or abstract style prompts. This study investigates the potential\ngender bias in how models interpret occupation-related prompts, specifically\nexamining their responses to instructions like \"Act like a nurse\". We explore\nwhether these models exhibit tendencies to amplify gender stereotypes when\ninterpreting such prompts. Our experimental results reveal the model's tendency\nto exhibit gender bias for certain occupations. Moreover, models of different\nsizes show varying degrees of this bias across these occupations.",
      "generated_abstract": "Large language models (LLMs) have demonstrated remarkable success in a\npractical setting, yet their application to human-centered tasks has been\nlimited. To address this gap, we propose a novel framework, LLM-Human, that\ncombines LLMs with human-centered techniques to develop a robust, scalable\nhuman-centered language model (HCLM). Our approach integrates human-centered\nknowledge with LLMs by leveraging human-centered datasets and reasoning\nframeworks, enabling LLMs to understand human-centered tasks beyond their\ntraditional capabilities. We evaluate LLM-Human across two real-world\nhuman-centered tasks: (1) human-centered question generation, and (2)\nhuman-centered dialogue management, using a human-centered benchmark dataset\ndeveloped for this purpose. Our results demonstrate that LLM-Human significantly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.19480519480519481,
          "f": 0.1764705832795849
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.18181818181818182,
          "f": 0.16470587739723197
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.08988v1",
      "true_abstract": "We propose a new method for investigating atmospheric inhomogeneities in\nexoplanets through transmission spectroscopy. Our approach links chromatic\nvariations in conventional transit model parameters (central transit time,\ntotal and full durations, and transit depth) to atmospheric asymmetries. By\nseparately analyzing atmospheric asymmetries during ingress and egress, we can\nderive clear connections between these variations and the underlying\nasymmetries of the planetary limbs. Additionally, this approach enables us to\ninvestigate differences between the limbs slightly offset from the terminator\non the dayside and the nightside. We applied this method to JWST's\nNIRSpec/G395H observations of the hot Saturn exoplanet WASP-39 b. Our analysis\nsuggests a higher abundance of CO2 on the evening limb compared to the morning\nlimb and indicates a greater probability of SO2 on the limb slightly offset\nfrom the terminator on the dayside relative to the nightside. These findings\nhighlight the potential of our method to enhance the understanding of\nphotochemical processes in exoplanetary atmospheres.",
      "generated_abstract": "This work investigates the application of a newly developed multi-scale\nmodel for the treatment of multi-frequency data of solar-like oscillations in\nthe context of the G3 network. The model is based on the analysis of\nhigh-resolution solar spectra and is designed to take into account the\ncomplex interplay between the stellar surface and the stellar atmosphere. The\nmodel employs a novel approach for the spectral decomposition of the stellar\nspectrum, which allows for a more accurate treatment of the stellar continuum\nat high frequencies, as well as for a more efficient processing of the\nhigh-frequency data. The model is applied to a dataset of 227 G3-type stars\nobserved with the G3 network in the 2023-2024 observing season. We analyze the\nstellar parameters, the radial velocities, and the stellar activity indicators\nof the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14432989690721648,
          "p": 0.18181818181818182,
          "f": 0.16091953529594413
        },
        "rouge-2": {
          "r": 0.02857142857142857,
          "p": 0.036036036036036036,
          "f": 0.0318725050269051
        },
        "rouge-l": {
          "r": 0.13402061855670103,
          "p": 0.16883116883116883,
          "f": 0.1494252824223809
        }
      }
    },
    {
      "paper_id": "cs.CC.stat/CO/2502.15024v1",
      "true_abstract": "We investigate implications of the (extended) low-degree conjecture (recently\nformalized in [MW23]) in the context of the symmetric stochastic block model.\nAssuming the conjecture holds, we establish that no polynomial-time algorithm\ncan weakly recover community labels below the Kesten-Stigum (KS) threshold. In\nparticular, we rule out polynomial-time estimators that, with constant\nprobability, achieve correlation with the true communities that is\nsignificantly better than random. Whereas, above the KS threshold,\npolynomial-time algorithms are known to achieve constant correlation with the\ntrue communities with high probability[Mas14,AS15].\n  To our knowledge, we provide the first rigorous evidence for the sharp\ntransition in recovery rate for polynomial-time algorithms at the KS threshold.\nNotably, under a stronger version of the low-degree conjecture, our lower bound\nremains valid even when the number of blocks diverges. Furthermore, our results\nprovide evidence of a computational-to-statistical gap in learning the\nparameters of stochastic block models.\n  In contrast to prior work, which either (i) rules out polynomial-time\nalgorithms for hypothesis testing with 1-o(1) success probability [Hopkins18,\nBBK+21a] under the low-degree conjecture, or (ii) rules out low-degree\npolynomials for learning the edge connection probability matrix [LG23], our\napproach provides stronger lower bounds on the recovery and learning problem.\n  Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with\ngraph splitting and cross-validation techniques. In order to rule out general\nrecovery algorithms, we employ the correlation preserving projection method\ndeveloped in [HS17].",
      "generated_abstract": "We introduce a novel framework for analyzing large-scale multi-agent\nsystems. This framework, called Cohesion, is designed to capture the\ninter-agent interactions within a multi-agent system, while preserving the\ninter-agent interactions among the agents themselves. In this work, we focus on\nthe problem of multi-agent system performance evaluation, which typically\ninvolves a series of cost-function evaluations, where the cost function\nrepresents the agents' interaction and the cost function evaluation represents\nthe number of interactions between the agents. Our proposed Cohesion framework\nis designed to capture the multiple inter-agent interactions while maintaining\nthe cost-function evaluation process. Furthermore, the Cohesion framework\nprovides an effective tool for analyzing the multi-agent system performance.\nWe provide theoretical analysis of Cohesion and its performance evaluation\nmechanism, which can be used to evaluate the multi-agent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.28,
          "f": 0.19444443991126553
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.02727272727272727,
          "f": 0.018749995488282333
        },
        "rouge-l": {
          "r": 0.1347517730496454,
          "p": 0.25333333333333335,
          "f": 0.17592592139274704
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/CB/2410.14612v2",
      "true_abstract": "High-throughput image analysis in the biomedical domain has gained\nsignificant attention in recent years, driving advancements in drug discovery,\ndisease prediction, and personalized medicine. Organoids, specifically, are an\nactive area of research, providing excellent models for human organs and their\nfunctions. Automating the quantification of organoids in microscopy images\nwould provide an effective solution to overcome substantial manual\nquantification bottlenecks, particularly in high-throughput image analysis.\nHowever, there is a notable lack of open biomedical datasets, in contrast to\nother domains, such as autonomous driving, and, notably, only few of them have\nattempted to quantify annotation uncertainty. In this work, we present MultiOrg\na comprehensive organoid dataset tailored for object detection tasks with\nuncertainty quantification. This dataset comprises over 400 high-resolution 2d\nmicroscopy images and curated annotations of more than 60,000 organoids. Most\nimportantly, it includes three label sets for the test data, independently\nannotated by two experts at distinct time points. We additionally provide a\nbenchmark for organoid detection, and make the best model available through an\neasily installable, interactive plugin for the popular image visualization tool\nNapari, to perform organoid quantification.",
      "generated_abstract": "Cellular automata (CA) are a class of self-reproducing patterns that can be\ngenerated by local interactions, and have been extensively studied for their\napplications in biology and engineering. However, CA have also been used to\npredict cellular outcomes, such as gene expression, based on the global\ninteractions. In this work, we introduce an extension of CA to capture both\nlocal and global interactions. We show that the resulting CA, called\nself-consistent CA (sCA), can be used to generate patterns of interest in\ndifferent experimental settings, such as gene expression, which is a\nhighly-interconnected network of genes. We further show that sCA can be used\nto predict gene expression from global interactions. We also show that the\nrepresentation of sCA as a CA can be used to generate new CA patterns, which\ncan be used to generate new patterns of interest. This work is an extension",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15714285714285714,
          "p": 0.27848101265822783,
          "f": 0.20091323739705189
        },
        "rouge-2": {
          "r": 0.028089887640449437,
          "p": 0.042735042735042736,
          "f": 0.033898300298535584
        },
        "rouge-l": {
          "r": 0.15714285714285714,
          "p": 0.27848101265822783,
          "f": 0.20091323739705189
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.13982v1",
      "true_abstract": "Natural Language Processing (NLP) and Voice Recognition agents are rapidly\nevolving healthcare by enabling efficient, accessible, and professional patient\nsupport while automating grunt work. This report serves as my self project\nwherein models finetuned on medical call recordings are analysed through a\ntwo-stage system: Automatic Speech Recognition (ASR) for speech transcription\nand a Large Language Model (LLM) for context-aware, professional responses.\nASR, finetuned on phone call recordings provides generalised transcription of\ndiverse patient speech over call, while the LLM matches transcribed text to\nmedical diagnosis. A novel audio preprocessing strategy, is deployed to provide\ninvariance to incoming recording/call data, laden with sufficient augmentation\nwith noise/clipping to make the pipeline robust to the type of microphone and\nambient conditions the patient might have while calling/recording.",
      "generated_abstract": "This paper proposes an energy-efficient and real-time algorithm for a\nnonlinear state-of-charge (SOC) estimation of lithium-ion batteries (LIBs)\nusing a single charge sensor. The proposed algorithm is based on a\ncombination of a gradient descent (GD) algorithm and a gradient descent (GD)\nwith momentum (GDM) technique. The GD algorithm is used to estimate the SOC\nusing the first-order derivative of the voltage as a function of the current.\nThe GDM technique is used to adjust the learning rate of the GD algorithm to\nachieve better convergence. The performance of the proposed algorithm is\nevaluated using a lithium-ion battery simulator. The results show that the\nproposed algorithm achieves a significant reduction in the computational\ncomplexity of the SOC estimation compared to previous methods, while maintaining\na high level of accuracy. The algorithm",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12631578947368421,
          "p": 0.16,
          "f": 0.14117646565743963
        },
        "rouge-2": {
          "r": 0.008264462809917356,
          "p": 0.008849557522123894,
          "f": 0.008547003552855573
        },
        "rouge-l": {
          "r": 0.11578947368421053,
          "p": 0.14666666666666667,
          "f": 0.1294117597750867
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2501.18909v1",
      "true_abstract": "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
      "generated_abstract": "We introduce a novel approach for the analysis of gene expression data that\nintroduces a time dimension. We define an extension of the Markov Chain Monte\nCarlo (MCMC) sampling method that enables the efficient integration of\ntime-dependent transcription factors (TFs) into the Markov Chain. This approach\nis used to generate a continuous-time Markov chain of TF states for a given\ngenome. This model is then used to derive the distribution of gene expression\nvalues for each TF. The method is applied to the TF network of the human\ngenome, and we show that the distribution of gene expression values is well\ndescribed by a multivariate normal distribution. Additionally, we use the\nmethod to obtain a statistical model for the time-dependent expression of the\nhuman genome, enabling the analysis of temporal gene expression dynamics in\ncells and organisms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08490566037735849,
          "p": 0.12162162162162163,
          "f": 0.09999999515802492
        },
        "rouge-2": {
          "r": 0.006944444444444444,
          "p": 0.00847457627118644,
          "f": 0.007633582835502308
        },
        "rouge-l": {
          "r": 0.07547169811320754,
          "p": 0.10810810810810811,
          "f": 0.08888888404691386
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.03608v1",
      "true_abstract": "This paper studies games of voluntary disclosure in which a sender discloses\nevidence to a receiver who then offers an allocation and transfers. We\ncharacterize the set of equilibrium payoffs in this setting. Our main result\nestablishes that any payoff profile that can be achieved through information\ndesign can also be supported by an equilibrium of the disclosure game. Hence,\nour analysis suggests an equivalence between disclosure and design in these\nsettings. We apply our results to monopoly pricing, bargaining over policies,\nand insurance markets.",
      "generated_abstract": "We consider the problem of maximizing the total expected payoff by a group\nof agents under a monotone utility function. We show that, under certain\nconditions, the optimal group size is always at least two. We also show that the\nproblem is NP-hard when the utility function is monotone. This result is\ncontrary to the result of Hajek and Oswald (2013), who showed that the problem\nis NP-hard for monotone utility functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.29545454545454547,
          "f": 0.23853210527733368
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16923076923076924,
          "p": 0.25,
          "f": 0.20183485757091166
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/MF/2411.10386v2",
      "true_abstract": "The fragility of financial systems was starkly demonstrated in early 2023\nthrough a cascade of major bank failures in the United States, including the\nsecond, third, and fourth largest collapses in the US history. The highly\ninterdependent financial networks and the associated high systemic risk have\nbeen deemed the cause of the crashes. The goal of this paper is to enhance\nexisting systemic risk analysis frameworks by incorporating essential debt\nvaluation factors. Our results demonstrate that these additional elements\nsubstantially influence the outcomes of risk assessment. Notably, by modeling\nthe dynamic relationship between interest rates and banks' credibility, our\nframework can detect potential cascading failures that standard approaches\nmight miss. The proposed risk assessment methodology can help regulatory bodies\nprevent future failures, while also allowing companies to more accurately\npredict turmoil periods and strengthen their survivability during such events.",
      "generated_abstract": "We propose a simple and efficient method for pricing American-style options\nwith an unbounded payoff in the presence of a finite time horizon. Our method\nrelies on a new martingale representation of the payoff, which we derive from\nthe canonical martingale representation of the option. We then present a\nsimple, straightforward and efficient numerical algorithm to solve the\nfinite-dimensional optimization problem, and validate the proposed method\nthrough a number of examples. The numerical results show that our method\noutperforms the existing approaches in terms of both computational efficiency\nand accuracy.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13761467889908258,
          "p": 0.234375,
          "f": 0.17341039996257826
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.03529411764705882,
          "f": 0.027272722530992555
        },
        "rouge-l": {
          "r": 0.11009174311926606,
          "p": 0.1875,
          "f": 0.13872831903772279
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.09918v1",
      "true_abstract": "Quasi-normal modes (QNMs) and greybody factors are some of the most\ncharacteristic features of the dynamics of black holes (BHs) and represent the\nbasis for a number of fundamental physics tests with gravitational wave\nobservations. It is therefore important to understand the properties of these\nquantities, naturally introduced within BH perturbation theory, in particular\nthe stability properties under modifications of the BH potential. Instabilities\nin the QNMs have been recently shown to appear in the BH pseudospectrum under\ncertain circumstances. In this work, we give a novel point of view based on the\nexistence of some recently discovered hidden symmetries in BH dynamics and the\nassociated infinite series of conserved quantities, the Korteweg-de Vries (KdV)\nintegrals. We provide different motivations to use the KdV integrals as\nindicators of some crucial BH spectral properties. In particular, by studying\nthem in different scenarios described by modified BH barriers, we find strong\nevidence that the KdV conserved quantities represent a useful tool to look for\ninstabilities in the BH spectrum of QNMs and in their greybody factors.",
      "generated_abstract": "In this work, we revisit the phenomenon of the superradiance of a charged\nspinless particle in a spherically symmetric background. We consider a\nspin-1/2 particle in a magnetic field, which is in a state of superradiance,\nwhere the particle is strongly influenced by the background magnetic field. We\nshow that the superradiance occurs due to the creation of a bound state of the\nspin-1/2 particle, which is driven by the magnetic field. The bound state is\ncharacterized by a binding energy, which is inversely proportional to the\nmagnetic field, and it is also proportional to the particle mass. We show that\nthe particle mass is the most important parameter that controls the strength of\nthe superradiance. The bound state is also characterized by a radius, which is\ninversely proportional to the magnetic field, and it is also proportional to\nthe mass of the spin-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14414414414414414,
          "p": 0.2807017543859649,
          "f": 0.19047618599277225
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.061855670103092786,
          "f": 0.045977006823740584
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.2631578947368421,
          "f": 0.17857142408801033
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.02040v1",
      "true_abstract": "This paper introduces a distributed contingency detection algorithm for\ndetecting unobservable contingencies in power distribution systems using\nstochastic hybrid system (SHS) models. We aim to tackle the challenge of\nlimited measurement capabilities in distribution networks that restrict the\nability to detect contingencies promptly. We incorporate the dynamics of\ndistribution network connections, load feeders, PV, and battery energy storage\nsystem (BESS) hybrid resources into a fully correlated SHS model representing\nthe distribution system as a randomly switching system between different\nstructures during contingency occurrence. We show that jumps in the SHS model\ncorrespond to contingencies in the physical power grid. We propose a probing\napproach based on magnitude-modulation inputs (MaMI) to make contingencies\ndetectable. The effectiveness of the proposed approach is validated through\nsimulations on a sample distribution system.",
      "generated_abstract": "This paper presents a distributed optimal control (DOC) framework for the\ncontrol of two parallel wind turbines (WTs) connected in series. The DOC\nframework is designed to achieve a balance of power (BOP) in a wind farm\nsystem. The proposed control scheme is based on a cooperative control strategy\nthat utilizes the optimal control of the individual WTs to balance the\ngenerated power of the WTs. To solve the DOC problem, an adaptive multiobjective\ndual-stage optimization algorithm is proposed. The proposed algorithm combines\na global objective function to achieve a balance of power and a local objective\nfunction to minimize the power losses in the wires connecting the WTs. The\nproposed algorithm is then used to solve the DOC problem. The DOC framework\nutilizes the optimal control of the individual WTs to balance the generated\npower of the WTs, thus ensuring",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2159090909090909,
          "p": 0.2835820895522388,
          "f": 0.24516128541436016
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.056074766355140186,
          "f": 0.052173908067675336
        },
        "rouge-l": {
          "r": 0.2159090909090909,
          "p": 0.2835820895522388,
          "f": 0.24516128541436016
        }
      }
    },
    {
      "paper_id": "physics.space-ph.physics/space-ph/2503.04965v1",
      "true_abstract": "We present the first full-wavelength numerical simulations of the electric\nfield generated by cosmic ray impacts into the Moon. Billions of cosmic rays\nfall onto the Moon every year. Ultra-high energy cosmic ray impacts produce\nsecondary particle cascades within the regolith and subsequent coherent,\nwidebandwidth, linearly-polarized radio pulses by the Askaryan Effect.\nObservations of the cosmic ray particle shower radio emissions can reveal\nsubsurface structure on the Moon and enable the broad and deep prospecting\nnecessary to confirm or refute the existence of polar ice deposits. Our\nsimulations show that the radio emissions and reflections could reveal ice\nlayers as thin as 10 cm and buried under regolith as deep as 9 m. The Askaryan\nEffect presents a novel and untapped opportunity for characterizing buried\nlunar ice at unprecedented depths and spatial scales.",
      "generated_abstract": "This study investigates the atmospheric electric field (AEF) in the vicinity\nof the Earth's ionosphere. AEF is a critical parameter in various applications\nsuch as communications, navigation, and geophysical exploration. The AEF is\nessential in space missions to ensure safe and successful operations. In\naddition, the AEF also affects the Earth's magnetosphere and heliosphere,\nimpacting the Earth's climate and radiation environment. AEF is sensitive to\nspace weather events such as geomagnetic storms and solar flares, and it can\nbe used to monitor these events. In this study, the AEF was determined using\nthe magnetometer on board the RS-B3 spacecraft. The AEF was calculated using\nthe Faraday's law and the induction equation. The AEF was measured in three\nlocations: the ionosphere (5",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12359550561797752,
          "p": 0.14102564102564102,
          "f": 0.13173652196780108
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.01834862385321101,
          "f": 0.0171673769949728
        },
        "rouge-l": {
          "r": 0.10112359550561797,
          "p": 0.11538461538461539,
          "f": 0.10778442615941793
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.02351v1",
      "true_abstract": "Concept-selective regions within the human cerebral cortex exhibit\nsignificant activation in response to specific visual stimuli associated with\nparticular concepts. Precisely localizing these regions stands as a crucial\nlong-term goal in neuroscience to grasp essential brain functions and\nmechanisms. Conventional experiment-driven approaches hinge on manually\nconstructed visual stimulus collections and corresponding brain activity\nrecordings, constraining the support and coverage of concept localization.\nAdditionally, these stimuli often consist of concept objects in unnatural\ncontexts and are potentially biased by subjective preferences, thus prompting\nconcerns about the validity and generalizability of the identified regions. To\naddress these limitations, we propose a data-driven exploration approach. By\nsynthesizing extensive brain activity recordings, we statistically localize\nvarious concept-selective regions. Our proposed MindSimulator leverages\nadvanced generative technologies to learn the probability distribution of brain\nactivity conditioned on concept-oriented visual stimuli. This enables the\ncreation of simulated brain recordings that reflect real neural response\npatterns. Using the synthetic recordings, we successfully localize several\nwell-studied concept-selective regions and validate them against empirical\nfindings, achieving promising prediction accuracy. The feasibility opens\navenues for exploring novel concept-selective regions and provides prior\nhypotheses for future neuroscience research.",
      "generated_abstract": "This study investigates the use of multi-agent reinforcement learning (MARL)\nwith a multi-agent system (MAS) composed of 136 individual neurons to control\nthe activity of 200 neurons in an artificial neuronal network (ANN). The ANN\nneurons were modeled as multi-layer perceptrons (MLPs) with 50 neurons in the\nlast layer. The ANN was trained to respond to the input signal of neurons\ncontrolling the spiking activity of the 136 neurons in the MAS. The ANN was\ntrained using backpropagation (BP) algorithm and the MAS was trained using\nMARL. The neurons in the MAS were controlled using the BP algorithm. The\nMARL-MAS system was trained using reinforcement learning (RL) using a\nmulti-armed bandit (MA",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08088235294117647,
          "p": 0.18333333333333332,
          "f": 0.11224489371095392
        },
        "rouge-2": {
          "r": 0.005681818181818182,
          "p": 0.010638297872340425,
          "f": 0.007407402868589886
        },
        "rouge-l": {
          "r": 0.08088235294117647,
          "p": 0.18333333333333332,
          "f": 0.11224489371095392
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.08703v1",
      "true_abstract": "Event cameras provide superior temporal resolution, dynamic range, power\nefficiency, and pixel bandwidth. Spiking Neural Networks (SNNs) naturally\ncomplement event data through discrete spike signals, making them ideal for\nevent-based tracking. However, current approaches that combine Artificial\nNeural Networks (ANNs) and SNNs, along with suboptimal architectures,\ncompromise energy efficiency and limit tracking performance. To address these\nlimitations, we propose the first Transformer-based spike-driven tracking\npipeline. Our Global Trajectory Prompt (GTP) method effectively captures global\ntrajectory information and aggregates it with event streams into event images\nto enhance spatiotemporal representation. We then introduce SDTrack, a\nTransformer-based spike-driven tracker comprising a Spiking MetaFormer backbone\nand a simple tracking head that directly predicts normalized coordinates using\nspike signals. The framework is end-to-end, does not require data augmentation\nor post-processing. Extensive experiments demonstrate that SDTrack achieves\nstate-of-the-art performance while maintaining the lowest parameter count and\nenergy consumption across multiple event-based tracking benchmarks,\nestablishing a solid baseline for future research in the field of neuromorphic\nvision.",
      "generated_abstract": "In this paper, we propose a new framework to solve the 2D and 3D nonlinear\nnonlocal pressure-driven incompressible Navier-Stokes (NIS) equations on\nnonuniform meshes. In particular, we consider the case of 2D nonlocal NIS,\nwhere the nonlocal term is given by the Helmholtz free energy density and the\nlocal term is given by the 2D nonlocal NIS equation. The 3D nonlocal NIS case\nis a special case of the 2D nonlocal NIS case. We prove that the 2D and 3D\nsolutions of the new NIS equations are the same as the solutions of the\ncorresponding local NIS equations. We also prove that the new NIS equations\nyield the same solutions as the 2D and 3D nonlocal NIS equations if the\nHelmholtz free energy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09923664122137404,
          "p": 0.24074074074074073,
          "f": 0.14054053640672035
        },
        "rouge-2": {
          "r": 0.006369426751592357,
          "p": 0.011494252873563218,
          "f": 0.008196716722993701
        },
        "rouge-l": {
          "r": 0.09923664122137404,
          "p": 0.24074074074074073,
          "f": 0.14054053640672035
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/ET/2503.07558v1",
      "true_abstract": "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
      "generated_abstract": "This paper introduces a novel methodology to analyze and assess the impact\nof the COVID-19 pandemic on global economic growth and its impact on various\neconomic indicators. This research considers the impact of the pandemic on the\neconomic performance of the United States and China, assessing their resilience\nand effectiveness in managing the economic crisis. The analysis is conducted\nusing a comprehensive framework incorporating economic indicators, sectoral\nimpacts, and the impact of the pandemic on the global economy. The results\nshow that China was more resilient and able to maintain its economic\nperformance during the pandemic, while the United States was less prepared and\nhad a more difficult time recovering. The analysis also highlights the\nsignificant economic impact of the pandemic on various sectors, with health\nand education being the hardest hit. The findings provide valuable insights for\npolicy mak",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09248554913294797,
          "p": 0.1951219512195122,
          "f": 0.12549019171518663
        },
        "rouge-2": {
          "r": 0.007380073800738007,
          "p": 0.01694915254237288,
          "f": 0.01028277212310428
        },
        "rouge-l": {
          "r": 0.09248554913294797,
          "p": 0.1951219512195122,
          "f": 0.12549019171518663
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.19002v1",
      "true_abstract": "This paper studies the stochastic setting in cooperative games and suggests a\nsolution concept based on second order stochastic dominance (SSD), which is\noften applied to robustly model risk averse behaviour of players in different\neconomic and game theoretic models as it enables to model not specified levels\nof risk aversion among players. The main result of the paper connects this\nsolution concept, \\emph{SSD-core}, in case of uniform distribution of the game\nto cores of two deterministic cooperative games. Interestingly, balancedness of\nboth of these games and convexity of one of these implies non-emptiness of the\nSSD-core. The opposite implication does not, in general, hold and leads to\nquestions about intersections of cores of two games and their relations.\nFinally, we present an application of the SSD-core to the multiple newsvendors\nproblem, where we provide a characterization of risk averse behaviour of\nplayers with an interpretation in terms of the model.",
      "generated_abstract": "We study a sequential game with two strategies, where the agents must\nplay the game until they have reached a predefined stopping time. We provide a\ncharacterization of the optimal strategy. This characterization is based on\nthe notion of expected value of the strategy profile and can be used to derive\noptimal strategies in a number of situations. We also show that the expected\nvalue of the optimal strategy is the same as the expected value of the\nstrategies of the two agents, which is an interesting result.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21505376344086022,
          "p": 0.37735849056603776,
          "f": 0.27397259811503105
        },
        "rouge-2": {
          "r": 0.05223880597014925,
          "p": 0.09210526315789473,
          "f": 0.06666666204807288
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.2830188679245283,
          "f": 0.20547944743009952
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/GN/2501.06248v2",
      "true_abstract": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.",
      "generated_abstract": "In this paper, we analyze the dynamic effects of a policy change on\neconomic outcomes, focusing on the impact of the introduction of a new tax. We\nshow that the policy change can lead to a significant increase in the share of\neconomic agents with high income. The impact of the tax is not constant across\nthe economic agents, but rather exhibits a long-term declining trend. This\ndifference in the tax's long-term impact on different economic agents is\nexplained by the heterogeneity of economic agents' income distributions. We\nfind that the new tax decreases the inequality of economic agents' incomes.\nFinally, we demonstrate that the tax's effect on economic agents' incomes can be\nsignificantly amplified by the introduction of a dynamic feature in the tax\nscheme. This feature enables the tax to react to economic agents' changes in\nincome. Therefore, the new",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.29577464788732394,
          "f": 0.23999999517779602
        },
        "rouge-2": {
          "r": 0.03571428571428571,
          "p": 0.04310344827586207,
          "f": 0.03906249504394595
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.28169014084507044,
          "f": 0.2285714237492246
        }
      }
    },
    {
      "paper_id": "cs.CC.cs/CC/2503.10393v1",
      "true_abstract": "Oredango puzzle, one of the pencil puzzles, was originally created by\nKanaiboshi and published in the popular puzzle magazine Nikoli. In this paper,\nwe show NP- and ASP-completeness of Oredango by constructing a reduction from\nthe 1-in-3SAT problem. Next, we formulate Oredango as an 0-1\ninteger-programming problem, and present numerical results obtained by solving\nOredango puzzles from Nikoli and PuzzleSquare JP using a 0-1 optimization\nsolver.",
      "generated_abstract": "We present a novel method for training large language models (LLMs) using\nsmall amounts of labeled data. Our method is based on a recently proposed\nmethodology for training LLMs called the Adaptation Loss (AL). This approach\nenables LLMs to learn complex tasks by adapting to their new environment. We\npropose using AL to adapt the LLM to the training data. We show that our\nmethodology improves the performance of LLMs on the Penn Treebank (PTB) dataset\nby 3.3\\% on a 10-fold cross-validation task. We compare our methodology with\nother approaches, including the Adaptation Loss and the Adaptation Loss with\nLearned Adaptation (ALLA) methodology. Our methodology performs on par with\nALLA and significantly outperforms other methods, such as the Adaptation Loss\nand AL",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18,
          "p": 0.11842105263157894,
          "f": 0.142857138070043
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.10526315789473684,
          "f": 0.12698412219702712
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/MF/2502.17417v1",
      "true_abstract": "In this paper, we propose an event-driven Limit Order Book (LOB) model that\ncaptures twelve of the most observed LOB events in exchange-based financial\nmarkets. To model these events, we propose using the state-of-the-art Neural\nHawkes process, a more robust alternative to traditional Hawkes process models.\nMore specifically, this model captures the dynamic relationships between\ndifferent event types, particularly their long- and short-term interactions,\nusing a Long Short-Term Memory neural network. Using this framework, we\nconstruct a midprice process that captures the event-driven behavior of the LOB\nby simulating high-frequency dynamics like how they appear in real financial\nmarkets. The empirical results show that our model captures many of the broader\ncharacteristics of the price fluctuations, particularly in terms of their\noverall volatility. We apply this LOB simulation model within a Deep\nReinforcement Learning Market-Making framework, where the trading agent can now\ncomplete trade order fills in a manner that closely resembles real-market trade\nexecution. Here, we also compare the results of the simulated model with those\nfrom real data, highlighting how the overall performance and the distribution\nof trade order fills closely align with the same analysis on real data.",
      "generated_abstract": "We consider the problem of estimating the drift and diffusion parameters of a\nmarginal-efficient portfolio, where the portfolio is composed of a market\nportfolio and a risk-free asset, and the drift and diffusion parameters are\nestimated from daily data. We develop a semiparametric estimator for the\nestimated drift and diffusion parameters, which is based on the method of\nconditional moments. We also derive the asymptotic distribution of the\nestimated parameters. As a special case, the estimator can be used to\nestimate the drift and diffusion parameters of the market portfolio. Numerical\nexperiments demonstrate the effectiveness of our estimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.112,
          "p": 0.2641509433962264,
          "f": 0.1573033666045955
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.04878048780487805,
          "f": 0.030651336686778545
        },
        "rouge-l": {
          "r": 0.088,
          "p": 0.20754716981132076,
          "f": 0.12359550143605617
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.07259v1",
      "true_abstract": "Egocentric video-based models capture rich semantic information and have\ndemonstrated strong performance in human activity recognition (HAR). However,\ntheir high power consumption, privacy concerns, and dependence on lighting\nconditions limit their feasibility for continuous on-device recognition. In\ncontrast, inertial measurement unit (IMU) sensors offer an energy-efficient and\nprivacy-preserving alternative, yet they suffer from limited large-scale\nannotated datasets, leading to weaker generalization in downstream tasks. To\nbridge this gap, we propose COMODO, a cross-modal self-supervised distillation\nframework that transfers rich semantic knowledge from the video modality to the\nIMU modality without requiring labeled annotations. COMODO leverages a\npretrained and frozen video encoder to construct a dynamic instance queue,\naligning the feature distributions of video and IMU embeddings. By distilling\nknowledge from video representations, our approach enables the IMU encoder to\ninherit rich semantic information from video while preserving its efficiency\nfor real-world applications. Experiments on multiple egocentric HAR datasets\ndemonstrate that COMODO consistently improves downstream classification\nperformance, achieving results comparable to or exceeding fully supervised\nfine-tuned models. Moreover, COMODO exhibits strong cross-dataset\ngeneralization. Benefiting from its simplicity, our method is also generally\napplicable to various video and time-series pre-trained models, offering the\npotential to leverage more powerful teacher and student foundation models in\nfuture research. The code is available at https://github.com/Breezelled/COMODO .",
      "generated_abstract": "In this paper, we present a novel framework that leverages the power of\nlarge language models (LLMs) for the task of scene text detection in\nmultimodal videos. The proposed framework uses the textual information in the\nvideo and the textual information in the LLM to enhance the performance of\nscene text detection. The LLM is employed as a visual feature extractor that\nextracts visual features from the video frames. These visual features are then\nused as input to the LLM to generate textual features, which are used as input\nto the scene text detector. The proposed method demonstrates significant\nperformance improvements over existing state-of-the-art methods for scene text\ndetection in videos, particularly in scenarios where the LLM achieves superior\nperformance. The proposed framework also provides a framework for future work\nin the field of multimodal video processing. The code for the proposed\nframework is available",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17197452229299362,
          "p": 0.35064935064935066,
          "f": 0.23076922635364167
        },
        "rouge-2": {
          "r": 0.034653465346534656,
          "p": 0.059322033898305086,
          "f": 0.04374999534453174
        },
        "rouge-l": {
          "r": 0.16560509554140126,
          "p": 0.33766233766233766,
          "f": 0.22222221780663315
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05766v1",
      "true_abstract": "Audio-visual representation learning is crucial for advancing multimodal\nspeech processing tasks, such as lipreading and audio-visual speech\nrecognition. Recently, speech foundation models (SFMs) have shown remarkable\ngeneralization capabilities across various speech-related tasks. Building on\nthis progress, we propose an audio-visual representation learning model that\nleverages cross-modal knowledge distillation from SFMs. In our method, SFMs\nserve as teachers, from which multi-layer hidden representations are extracted\nusing clean audio inputs. We also introduce a multi-teacher ensemble method to\ndistill the student, which receives audio-visual data as inputs. A novel\nrepresentational knowledge distillation loss is employed to train the student\nduring pretraining, which is also applied during finetuning to further enhance\nthe performance on downstream tasks. Our experiments utilized both a\nself-supervised SFM, WavLM, and a supervised SFM, iFLYTEK-speech. The results\ndemonstrated that our proposed method achieved superior or at least comparable\nperformance to previous state-of-the-art baselines across automatic speech\nrecognition, visual speech recognition, and audio-visual speech recognition\ntasks. Additionally, comprehensive ablation studies and the visualization of\nlearned representations were conducted to evaluate the effectiveness of our\nproposed method.",
      "generated_abstract": "This paper presents a novel approach for estimating the position of a\ntarget object in a 3D scene using a single RGB image. The proposed method\nincorporates an ensemble of Convolutional Neural Networks (CNNs) for\ndifferent object categories. The network architecture utilizes a single\nconvolutional layer for each category. We propose a novel training methodology\nthat ensures the network converges to a stable state and provides the\nnecessary information to the classifiers to recognize the object category.\nAdditionally, we propose a novel approach for classifying the object categories\nbased on the similarity between the object feature vectors and the learned\nfeature embeddings. The proposed methodology is evaluated through\nexperiments on the DIRTY dataset, which contains 400 images of 10 different\ntarget objects. The results show that the proposed methodology achieves an\naverage precision of 92.1%,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.2823529411764706,
          "f": 0.2296650669444382
        },
        "rouge-2": {
          "r": 0.023952095808383235,
          "p": 0.032520325203252036,
          "f": 0.02758620201165366
        },
        "rouge-l": {
          "r": 0.18548387096774194,
          "p": 0.27058823529411763,
          "f": 0.22009568895400755
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/SC/2408.17188v2",
      "true_abstract": "We introduce a generalized promotion time cure model motivated by a new\nbiological consideration. The new approach is flexible to model heterogeneous\nsurvival data, in particular for addressing intra-sample heterogeneity. We also\nindicate that the new approach is suited to model a series or parallel system\nconsisting of multiple subsystems in reliability analysis.",
      "generated_abstract": "The study of spatial patterns in large-scale biological data is of vital\ninterest in various disciplines. In ecology and evolutionary biology, spatial\npatterns are often represented by the so-called Delaunay tessellation, which\nis a set of triangles whose interior angles sum to 180 degrees. A well-known\nresult states that for a given Delaunay tessellation of a sphere, there exists\na unique isometric embedding into $\\mathbb{R}^2$ such that the triangle\ninteriors coincide with the Euclidean distance vectors. The Delaunay\nembedding is used to study the topological structure of the Delaunay tessellation\nand to define the Euclidean metric. In this work, we extend the notion of the\nEuclidean metric to the metric space of Delaunay tessellations. We provide an\nexistence result for the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.15384615384615385,
          "f": 0.19999999545000013
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.008771929824561403,
          "f": 0.012269934445407742
        },
        "rouge-l": {
          "r": 0.2857142857142857,
          "p": 0.15384615384615385,
          "f": 0.19999999545000013
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10635v1",
      "true_abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
      "generated_abstract": "The popularity of 3D face reconstruction is increasing rapidly, yet\ncurrent methods are often based on a single 3D face model and therefore face\nreconstruction errors. In this paper, we present a novel method that leverages\na 3D face model, a single 3D face model, and a large-scale face database to\ngenerate a 3D face model that is highly accurate, robust to model inaccuracies,\nand highly expressive. We first introduce a new 3D face model that can be\nconstructed by merging multiple 3D face models. We then propose a method that\nuses the generated 3D face model to reconstruct a 3D face model that is highly\naccurate. Finally, we introduce a method that generates a large-scale face\ndatabase, which we refer to as the Face Database Generator, by merging\nexisting large-scale face databases. We evaluate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0972972972972973,
          "p": 0.2535211267605634,
          "f": 0.14062499599151623
        },
        "rouge-2": {
          "r": 0.00411522633744856,
          "p": 0.009615384615384616,
          "f": 0.005763684563117111
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.2112676056338028,
          "f": 0.11718749599151626
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2501.16659v1",
      "true_abstract": "Considering the continuous-time Mean-Variance (MV) portfolio optimization\nproblem, we study a regime-switching market setting and apply reinforcement\nlearning (RL) techniques to assist informed exploration within the control\nspace. We introduce and solve the Exploratory Mean Variance with Regime\nSwitching (EMVRS) problem. We also present a Policy Improvement Theorem.\nFurther, we recognize that the widely applied Temporal Difference (TD) learning\nis not adequate for the EMVRS context, hence we consider Orthogonality\nCondition (OC) learning, leveraging the martingale property of the induced\noptimal value function from the analytical solution to EMVRS. We design a RL\nalgorithm that has more meaningful parameterization using the market parameters\nand propose an updating scheme for each parameter. Our empirical results\ndemonstrate the superiority of OC learning over TD learning with a clear\nconvergence of the market parameters towards their corresponding ``grounding\ntrue\" values in a simulated market scenario. In a real market data study, EMVRS\nwith OC learning outperforms its counterparts with the highest mean and\nreasonably low volatility of the annualized portfolio returns.",
      "generated_abstract": "This study investigates the impact of a 50\\% tax on capital gains and the\ntax rate on capital gains on the wealth of a population of U.S. households.\nBased on the results of a dynamic analysis of the impact of tax policy, the\nstudy identifies the optimal tax rate on capital gains and the optimal tax\nrate on capital gains and dividends. The results suggest that the optimal\ntax rate on capital gains is 45\\% and the optimal tax rate on capital gains and\ndividends is 50\\%. These findings provide insights into the optimal tax rates\nfor different economic conditions, which can be used to design tax policies that\nmaximize wealth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09836065573770492,
          "p": 0.24,
          "f": 0.13953487959707964
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.02702702702702703,
          "f": 0.0169491482375765
        },
        "rouge-l": {
          "r": 0.09016393442622951,
          "p": 0.22,
          "f": 0.12790697262033546
        }
      }
    },
    {
      "paper_id": "math.SP.math/SP/2502.18307v1",
      "true_abstract": "The notion of eta invariant is traditionally defined by means of analytic\ncontinuation. We prove, by examining the particular case of the operator curl,\nthat the eta invariant can equivalently be obtained as the trace of the\ndifference of positive and negative spectral projections, appropriately\nregularised. Our construction is direct, in the sense that it does not involve\nanalytic continuation, and is based on the use of pseudodifferential\ntechniques. This provides a novel approach to the study of spectral asymmetry\nof non-semibounded (pseudo)differential systems on manifolds which encompasses\nand extends previous results.",
      "generated_abstract": "We prove that the limit of the sequence of probabilities of the event\n$A_{n,m}$ that $n$ distinct vertices of a graph are in a clique of size $m$\nexists and is positive for large $n$. Moreover, we show that $A_{n,m}$ is\nequivalent to the event that $n$ distinct vertices of a graph are in a\n$m$-clique. We also give an example of a graph such that $A_{n,m}$ fails to\nhold for some $m$ but $A_{n,m}$ holds for all $m$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14925373134328357,
          "p": 0.23255813953488372,
          "f": 0.1818181770561985
        },
        "rouge-2": {
          "r": 0.0449438202247191,
          "p": 0.0625,
          "f": 0.052287576832842526
        },
        "rouge-l": {
          "r": 0.11940298507462686,
          "p": 0.18604651162790697,
          "f": 0.14545454069256214
        }
      }
    },
    {
      "paper_id": "cs.OS.cs/OS/2502.10923v2",
      "true_abstract": "The emergence of symmetric multi-processing (SMP) systems with non-uniform\nmemory access (NUMA) has prompted extensive research on process and data\nplacement to mitigate the performance impact of NUMA on applications. However,\nexisting solutions often overlook the coordination between the CPU scheduler\nand memory manager, leading to inefficient thread and page table placement.\nMoreover, replication techniques employed to improve locality suffer from\nredundant replicas, scalability barriers, and performance degradation due to\nmemory bandwidth and inter-socket interference. In this paper, we present\nPhoenix, a novel integrated CPU scheduler and memory manager with on-demand\npage table replication mechanism. Phoenix integrates the CPU scheduler and\nmemory management subsystems, allowing for coordinated thread and page table\nplacement. By differentiating between data and page table pages, Phoenix\nenables direct migration or replication of page tables based on application\nbehavior. Additionally, Phoenix employs memory bandwidth management mechanism\nto maintain Quality of Service (QoS) while mitigating coherency maintenance\noverhead. We implemented Phoenix as a loadable kernel module for Linux,\nensuring compatibility with legacy applications and ease of deployment. Our\nevaluation on real hardware demonstrates that Phoenix reduces CPU cycles by\n2.09x and page-walk cycles by 1.58x compared to state-of-the-art solutions.",
      "generated_abstract": "This paper examines the scalability of the Intel SGX enclave model to\nintroduce and evaluate the SGX-enabled AMD EPYC 7551P, a new generation of\nintel-based servers. The EPYC 7551P is the first Intel-based server platform to\ninclude SGX support and is designed for high-performance computing (HPC) and\ndata analytics workloads. We evaluated the performance of the AMD EPYC 7551P\non an Intel SGX-enabled bare-metal system, as well as on an Intel Xeon Scalable\nplatform, to evaluate the impact of SGX on performance. Our results show that\nSGX performance improvements are limited by the size of the SGX domain, with\nperformance gains increasing as the size of the SGX domain increases. Additionally,\nwe found",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15503875968992248,
          "p": 0.28169014084507044,
          "f": 0.19999999542050012
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.0297029702970297,
          "f": 0.02158272918663729
        },
        "rouge-l": {
          "r": 0.14728682170542637,
          "p": 0.2676056338028169,
          "f": 0.18999999542050008
        }
      }
    },
    {
      "paper_id": "math.CV.math/OA/2503.09112v1",
      "true_abstract": "In this paper, we provide a complete characterization of bounded Toeplitz\noperators $T_f$ on the harmonic Bergman space of the unit disk, where the\nsymbol $f$ has a polar decomposition truncated above, that commute with\n$T_{z+\\bar{g}}$, for a bounded analytic function $g$.",
      "generated_abstract": "This article studies the asymptotic behavior of the non-autonomous\nevolution equation $\\partial_t u-\\Delta_x u=f(u)$ for a bounded domain $\\Omega\n\\subset {\\mathbb R}^d$, $d\\in\\{1,2,3\\}$, in the case that $f$ is a harmonic\nfunction, i.e. $f$ is the solution of a free boundary problem. We show that\n$u$ converges to zero as $t\\to\\infty$ in $L^p(\\Omega)$ whenever $p>1$, where\n$p$ is the conjugate exponent of the Sobolev space $H^1(\\Omega)$, if the\nboundary condition $f(0)=0$ is replaced by the condition $f(0)=f_\\infty$ where\n$f_\\infty$ is a non-negative function. We also give an upper bound on the\nexponent $p$ in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3333333333333333,
          "p": 0.17647058823529413,
          "f": 0.23076922624260363
        },
        "rouge-2": {
          "r": 0.0975609756097561,
          "p": 0.044444444444444446,
          "f": 0.06106869798962795
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.11764705882352941,
          "f": 0.15384614931952675
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09457v1",
      "true_abstract": "The effective conductivity ($T^{eff}$) of 2D and 3D Random Resistor Networks\n(RRNs) with random edge conductivity is studied. The combined influence of\ngeometrical disorder, which controls the overall connectivity of the medium and\nleads to percolation effects, and conductivity randomness is investigated. A\nformula incorporating connectivity aspects and second-order averaging methods,\nwidely used in the stochastic hydrology community, is derived and extrapolated\nto higher orders using a power averaging formula based on a mean-field\nargument. This approach highlights the role of the so-called resistance\ndistance introduced by graph theorists. Simulations are performed on various\nRRN geometries constructed from 2D and 3D bond-percolation lattices. The\nresults confirm the robustness of the power averaging technique and the\nrelevance of the mean-field assumption.",
      "generated_abstract": "We present a new approach to the simulation of two-dimensional quantum\nchains with an arbitrary number of sites. We introduce a new class of models,\nwhich we term ``mixed models'' and describe their properties. We demonstrate\nthat this class of models is not only useful for simulating a wide range of\nchains, but also for a number of other tasks. For example, we show that it is\npossible to simulate a chain of any finite number of sites, using a single\nmixed model, and that the model can be used to simulate a single chain of any\nfinite number of sites. We also show that the mixed model can be used to\nsimulate a chain of any finite number of sites, with a finite number of\ntransitions per unit time, using a single mixed model. We also show that a\nchain of finite length can be simulated using a single mixed model, using a\nsingle mixed model, and that the model can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13095238095238096,
          "p": 0.15714285714285714,
          "f": 0.14285713789846535
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.009345794392523364,
          "f": 0.009090904094630843
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.14285714285714285,
          "f": 0.1298701249114524
        }
      }
    },
    {
      "paper_id": "math.DG.math/AG/2503.10517v1",
      "true_abstract": "We consider the group $\\mathcal G$ which is the semidirect product of the\ngroup of analytic functions with values in ${\\mathbb C}^*$ on the circle and\nthe group of analytic diffeomorphisms of the circle that preserve the\norientation. Then we construct the central extensions of the group $\\mathcal G$\nby the group ${\\mathbb C}^*$. The first central extension, so-called the\ndeterminant central extension, is constructed by means of determinants of\nlinear operators acting in infinite-dimensional locally convex topological\n$\\mathbb C$-vector spaces. Other central extensions are constructed by\n$\\cup$-products of group $1$-cocycles with the application to them the map\nrelated with algebraic $K$-theory. We prove in the second cohomology group,\ni.e. modulo of a group $2$-coboundary, the equality of the $12$th power of the\n$2$-cocycle constructed by the first central extension and the product of\ninteger powers of the $2$-cocycles constructed above by means of\n$\\cup$-products (in multiplicative notation). As an application of this result\nwe obtain the new topological Riemann-Roch theorem for a complex line bundle\n$L$ on a smooth manifold $M$, where $\\pi :M \\to B$ is a fibration in oriented\ncircles. More precisely, we prove that in the group $H^3(B, {\\mathbb Z})$ the\nelement $12 \\, [ {\\mathcal Det} (L)]$ is equal to the element $6 \\, \\pi_* (\nc_1(L) \\cup c_1(L))$, where $[{\\mathcal Det} (L)]$ is the class of the\ndeterminant gerbe on $B$ constructed by $L$ and the determinant central\nextension.",
      "generated_abstract": "In this paper, we prove the first non-trivial result on the convergence of\nthe solution of the linear Stokes system with Dirichlet boundary conditions\ngenerated by the Laplace operator and the divergence-free part of the\nNewtonian gravitational potential. The space of test functions is the Sobolev\nspace $H^1(\\Omega)$ and the norm is induced by the $L^2$ norm. We prove that\nthe sequence of the norm of the solution converges to zero as the number of\niterations tends to infinity. The proof is based on the trace inequality and\nthe non-homogeneity of the Stokes problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.140625,
          "p": 0.3157894736842105,
          "f": 0.19459459033104465
        },
        "rouge-2": {
          "r": 0.04568527918781726,
          "p": 0.10975609756097561,
          "f": 0.06451612488174638
        },
        "rouge-l": {
          "r": 0.1328125,
          "p": 0.2982456140350877,
          "f": 0.18378377952023384
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/OT/2407.11518v1",
      "true_abstract": "In this paper, we present a new ensemble-based filter method by\nreconstructing the analysis step of the particle filter through a transport\nmap, which directly transports prior particles to posterior particles. The\ntransport map is constructed through an optimization problem described by the\nMaximum Mean Discrepancy loss function, which matches the expectation\ninformation of the approximated posterior and reference posterior. The proposed\nmethod inherits the accurate estimation of the posterior distribution from\nparticle filtering. To improve the robustness of Maximum Mean Discrepancy, a\nvariance penalty term is used to guide the optimization. It prioritizes\nminimizing the discrepancy between the expectations of highly informative\nstatistics for the approximated and reference posteriors. The penalty term\nsignificantly enhances the robustness of the proposed method and leads to a\nbetter approximation of the posterior. A few numerical examples are presented\nto illustrate the advantage of the proposed method over the ensemble Kalman\nfilter.",
      "generated_abstract": "The development of statistical models for multivariate time series has\nbeen a crucial area of research in the last two decades. While significant\nprogress has been made in the last few years, there are still a few key\nchallenges that need to be addressed in order to fully leverage the power of\ntime series data analysis. One of the key challenges is the lack of a\nconsistent and principled approach to handling missing data in multivariate\ntime series. Traditional methods for missing data analysis, such as imputation\nand prediction, often rely on heuristic approaches that are prone to\nartificially inflating the statistical significance of outliers. Another key\nchallenge is the lack of a comprehensive and practical approach to dealing with\nnon-stationarity in multivariate time series. Traditional methods such as\nseasonal decomposition and ARMA models often fail to capture the complex\nnon-stationary dynamics present",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1348314606741573,
          "p": 0.14285714285714285,
          "f": 0.1387283187035987
        },
        "rouge-2": {
          "r": 0.007518796992481203,
          "p": 0.008130081300813009,
          "f": 0.007812495007632584
        },
        "rouge-l": {
          "r": 0.11235955056179775,
          "p": 0.11904761904761904,
          "f": 0.11560693142036173
        }
      }
    },
    {
      "paper_id": "hep-th.gr-qc/2503.10598v1",
      "true_abstract": "The basic observables in cosmology are known as in-in correlators. Recent\ncalculations have revealed that in-in correlators in four dimensional de Sitter\nspace exhibit hidden simplicity stemming from a close relation to scattering\namplitudes in flat space. In this paper we explain how to make this property\nmanifest by dressing flat space Feynman diagrams with certain auxiliary\npropagators. These dressing rules hold for any order in perturbation theory and\ncan be derived for a broad range of scalar theories, including those with\ninfrared divergences. In the latter case we show that they reproduce the same\ninfrared divergences predicted by the Schwinger-Keldysh formalism.",
      "generated_abstract": "In this paper we present a novel approach to the calculation of\nthe correlation function of two fermion currents using the functional\nrenormalization group (FRG) method. In contrast to other methods based on\nperturbation theory, this approach avoids the need to solve the renormalization\nproblem and focuses instead on the structure of the correlator. By\nintroducing a new auxiliary field, we obtain a closed formula for the\ncorrelation function of two currents at any renormalization point. This\nrepresentation is not only useful for studying the renormalization properties\nof two-point functions but also facilitates the derivation of the\nself-consistent equation for the bare coupling constant. In particular, we\nobtain a closed formula for the bare coupling constant in the two-point\nfunction of the current vertex and find that it is proportional to the\ndeterminant of the renormalization group transformation. The results",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.1951219512195122,
          "f": 0.1975308591982931
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.02586206896551724,
          "f": 0.02803737821294524
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.18292682926829268,
          "f": 0.18518518018594743
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2501.03269v1",
      "true_abstract": "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
      "generated_abstract": "We present a novel framework for the simultaneous estimation of asset\nparameters and their joint evolution, using a Bayesian nonparametric approach\nto address the data scarcity issue. This approach relies on a novel\nnonparametric Bayesian model for the joint evolution of the mean and the\ncovariance of the asset returns. In contrast to the traditional parametric\nmodels, our approach does not require the knowledge of the joint distribution\nof the asset returns, but only the joint distribution of their covariances.\nFurthermore, the joint evolution model is fully characterized by a single\nparameter, which can be obtained through a Bayesian nonparametric inference\nstrategy. We develop a novel Bayesian nonparametric estimation framework for\nthe joint evolution of the mean and the covariance of the asset returns,\nwhich is based on a novel kernel density estimation (KDE) algorithm. This\napproach allows for a fast,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08411214953271028,
          "p": 0.12857142857142856,
          "f": 0.10169491047272516
        },
        "rouge-2": {
          "r": 0.007194244604316547,
          "p": 0.009345794392523364,
          "f": 0.008130076385421703
        },
        "rouge-l": {
          "r": 0.07476635514018691,
          "p": 0.11428571428571428,
          "f": 0.09039547544447661
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2503.00965v1",
      "true_abstract": "G protein-coupled receptors (GPCRs) represent a diverse and vital family of\nmembrane proteins that mediate intracellular signaling in response to\nextracellular stimuli, playing critical roles in physiology and disease.\nTraditionally recognized as chemical signal transducers, GPCRs have recently\nbeen implicated in mechanotransduction, the process of converting mechanical\nstimuli into cellular responses. This review explores the emerging role of\nGPCRs in sensing and responding to mechanical forces, with a particular focus\non the cardiovascular system. Cardiovascular homeostasis is heavily influenced\nby mechanical forces such as shear stress, cyclic stretch, and pressure, which\nare central to both normal physiology and the pathogenesis of diseases like\nhypertension and atherosclerosis. GPCRs, including the angiotensin II type 1\nreceptor (AT1R) and the $\\beta$2-adrenergic receptor ($\\beta$2-AR), have\ndemonstrated the ability to integrate mechanical and chemical signals,\npotentially through conformational changes and/or modulation of lipid\ninteractions, leading to biased signaling. Recent studies highlight the dual\nactivation mechanisms of GPCRs, with $\\beta$2-AR now serving as a key example\nof how mechanical and ligand-dependent pathways contribute to cardiovascular\nregulation. This review synthesizes current knowledge of GPCR\nmechanosensitivity, emphasizing its implications for cardiovascular health and\ndisease, and explores advancements in methodologies poised to further unravel\nthe mechanistic intricacies of these receptors.",
      "generated_abstract": "This paper describes the development of an advanced in-vitro cell culture\nsystem based on a novel microfluidic approach, enabling the study of the\nbiological properties of cells in suspension. The system allows for the\nconcentration control of cells in a single drop of medium, in real time,\nproviding a unique opportunity to study cell behavior in a controlled and\nrealistic environment. The system includes a microfluidic chamber with a\npneumatic valve, a magnetic stirrer, a micro-channel plate (MCP), and a\ncomputer with a microcontroller. The system is controlled by a microcontroller\nprogrammed to control the pneumatic valve, the magnetic stirrer, and the\nMCP. The control system is based on the use of an analog-digital converter (ADC)\nto monitor the pressure of the medium and the temperature of the cells.\nThe experimental results obtained in this",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.18055555555555555,
          "f": 0.12093022810340742
        },
        "rouge-2": {
          "r": 0.015306122448979591,
          "p": 0.025210084033613446,
          "f": 0.01904761434638564
        },
        "rouge-l": {
          "r": 0.08391608391608392,
          "p": 0.16666666666666666,
          "f": 0.11162790252201207
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.06769v1",
      "true_abstract": "This paper proposes an innovative solution to the growing issue of greenhouse\ngas emissions: a closed photobioreactor (PBR) fa\\c{c}ade system to mitigate\ngreenhouse gas (GHG) concentrations. With digital fabrication technology, this\nstudy explores the transition from traditional, single function building\nfacades to multifunctional, integrated building systems. It introduces a\nphotobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG)\nconcentrations while addressing the challenge of large-scale prefabricated\ncomponents transportation. This research introduces a novel approach by\ndesigning the fa\\c{c}ade system as modular, user-friendly and\ntransportation-friendly bricks, enabling the creation of a user-customized and\nself-assembled photobioreactor (PBR) system. The single module in the system is\nproposed to be \"neutralization bricks\", which embedded with algae and equipped\nwith an air circulation system, facilitating the photobioreactor (PBR)'s\nfunctionality. A connection system between modules allows for easy assembly by\nusers, while a limited variety of brick styles ensures modularity in\nmanufacturing without sacrificing customization and diversity. The system is\nalso equipped with an advanced microalgae status detection algorithm, which\nallows users to monitor the condition of the microalgae using monocular camera.\nThis functionality ensures timely alerts and notifications for users to replace\nthe algae, thereby optimizing the operational efficiency and sustainability of\nthe algae cultivation process.",
      "generated_abstract": "This paper investigates the design of low-complexity, high-throughput\nclusters for the high-dimensional data analysis task. The key challenge is to\nachieve high throughput while preserving low complexity, which is particularly\nimportant in data-intensive tasks like image classification. We first introduce\na novel algorithm for constructing low-complexity clusters in a high-dimensional\nspace, and then propose a novel method for constructing low-complexity and\nhigh-throughput clusters for the image classification task. The proposed\napproach is based on the idea of partitioning the data into subspaces that\nare close to each other in the high-dimensional space. We demonstrate that our\nproposed approach outperforms existing methods in terms of throughput and\ncomplexity. The proposed approach is applied to the ImageNet dataset, which\ncontains 1.2 million images. The results show that our approach can significantly\nreduce the time required to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13385826771653545,
          "p": 0.20238095238095238,
          "f": 0.1611374359659488
        },
        "rouge-2": {
          "r": 0.02197802197802198,
          "p": 0.03333333333333333,
          "f": 0.0264900614359028
        },
        "rouge-l": {
          "r": 0.12598425196850394,
          "p": 0.19047619047619047,
          "f": 0.1516587629801668
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.09069v2",
      "true_abstract": "The rapid expansion of e-commerce and the widespread use of credit cards in\nonline purchases and financial transactions have significantly heightened the\nimportance of promptly and accurately detecting credit card fraud (CCF). Not\nonly do fraudulent activities in financial transactions lead to substantial\nmonetary losses for banks and financial institutions, but they also undermine\nuser trust in digital services. This study presents a new stacking-based\napproach for CCF detection by adding two extra layers to the usual\nclassification process: an attention layer and a confidence-based combination\nlayer. In the attention layer, we combine soft outputs from a convolutional\nneural network (CNN) and a recurrent neural network (RNN) using the dependent\nordered weighted averaging (DOWA) operator, and from a graph neural network\n(GNN) and a long short-term memory (LSTM) network using the induced ordered\nweighted averaging (IOWA) operator. These weighted outputs capture different\npredictive signals, increasing the model's accuracy. Next, in the\nconfidence-based layer, we select whichever aggregate (DOWA or IOWA) shows\nlower uncertainty to feed into a meta-learner. To make the model more\nexplainable, we use shapley additive explanations (SHAP) to identify the top\nten most important features for distinguishing between fraud and normal\ntransactions. These features are then used in our attention-based model.\nExperiments on three datasets show that our method achieves high accuracy and\nrobust generalization, making it effective for CCF detection.",
      "generated_abstract": "This paper proposes a novel framework to assess the performance of\nindividual investors and investment portfolios by employing a novel\nmulti-objective framework, namely the Multiobjective Optimization of Weighted\nSum of Squares (MOWSS). The framework is based on the sum of squared weights\nof the investment objectives, which are weighted according to their\nimportance. The proposed framework is a generalization of the weighted sum of\nsquares (WSS) objective function, which has been widely used in the literature.\nThe framework is formulated as a constrained quadratic programming problem,\nwhere the objective function is convex and the constraints are linear. The\nconvexity of the objective function facilitates the use of existing\noptimization techniques such as gradient descent and conjugate gradient, which\nare well-suited for solving constrained quadratic programming problems. The\nframework is implemented through the Mathemat",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10256410256410256,
          "p": 0.1951219512195122,
          "f": 0.134453776995975
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.03389830508474576,
          "f": 0.02439023929580694
        },
        "rouge-l": {
          "r": 0.08974358974358974,
          "p": 0.17073170731707318,
          "f": 0.11764705430689941
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.07796v1",
      "true_abstract": "Recently, a new frontier in computing has emerged with physical neural\nnetworks(PNNs) harnessing intrinsic physical processes for learning. Here, we\nexplore topological mechanical neural networks(TMNNs) inspired by the quantum\nspin Hall effect(QSHE) in topological metamaterials, for machine learning\nclassification tasks. TMNNs utilize pseudospin states and the robustness of the\nQSHE, making them damage-tolerant for binary classification. We first\ndemonstrate data clustering using untrained TMNNs. Then, for specific tasks, we\nderive an in situ backpropagation algorithm - a two-step, local-rule method\nthat updates TMNNs using only local information, enabling in situ physical\nlearning. TMNNs achieve high accuracy in classifications of Iris flowers,\nPenguins, and Seeds while maintaining robustness against bond pruning.\nFurthermore, we demonstrate parallel classification via frequency-division\nmultiplexing, assigning different tasks to distinct frequencies for enhanced\nefficiency. Our work introduces in situ backpropagation for wave-based\nmechanical neural networks and positions TMNNs as promising neuromorphic\ncomputing hardware for classification tasks.",
      "generated_abstract": "We study the two-dimensional (2D) square lattice Hubbard model on the\nsurface of a 2D honeycomb lattice using a mean-field theory approach. Our\nresults reveal that the 2D Hubbard model on the honeycomb lattice is\ntopological. By analyzing the surface states, we demonstrate that the 2D Hubbard\nmodel on the honeycomb lattice is topologically protected and is in a\ntopological phase. Furthermore, we show that the 2D Hubbard model on the\nhoneycomb lattice has a topological phase transition at the edge of the\ntopological phase. This transition is characterized by a finite value of the\norder parameter at the edge. By analyzing the surface states, we also demonstrate\nthat the 2D Hubbard model on the honeycomb lattice is gapped at the\ntopological phase transition. Our results demonstrate that the 2D",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14150943396226415,
          "p": 0.3,
          "f": 0.19230768795200534
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.038461538461538464,
          "f": 0.02714931669949505
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.3,
          "f": 0.19230768795200534
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2502.05336v1",
      "true_abstract": "This paper introduces Monotone Delta, an order-theoretic measure designed to\nenhance the reliability assessment of survey-based instruments in human-machine\ninteractions. Traditional reliability measures, such as Cronbach's Alpha and\nMcDonald's Omega, often yield misleading estimates due to their sensitivity to\nredundancy, multidimensional constructs, and assumptions of normality and\nuncorrelated errors. These limitations can compromise decision-making in\nhuman-centric evaluations, where survey instruments inform adaptive interfaces,\ncognitive workload assessments, and human-AI trust models. Monotone Delta\naddresses these issues by quantifying internal consistency through the\nminimization of ordinal contradictions and alignment with a unidimensional\nlatent order using weighted tournaments. Unlike traditional approaches, it\noperates without parametric or model-based assumptions. We conducted\ntheoretical analyses and experimental evaluations on four challenging\nscenarios: tau-equivalence, redundancy, multidimensionality, and non-normal\ndistributions, and proved that Monotone Delta provides more stable reliability\nassessments compared to existing methods. The Monotone Delta is a valuable\nalternative for evaluating questionnaire-based assessments in psychology, human\nfactors, healthcare, and interactive system design, enabling organizations to\noptimize survey instruments, reduce costly redundancies, and enhance confidence\nin human-system interactions.",
      "generated_abstract": "A large number of research studies have highlighted the importance of\nrepresentative samples in applied research. The use of representative samples\nis particularly important in public health research. This paper examines the\ndifference in the prevalence of diabetes mellitus between individuals from\ndifferent social groups. The aim of the paper is to examine the differences in\nthe prevalence of diabetes mellitus between individuals from different social\ngroups. The statistical analysis used in the study is descriptive. The results\nof the study show that the prevalence of diabetes mellitus is higher in\nindividuals from the lowest social groups. The results also show that the\nprevalence of diabetes mellitus is lower in individuals from the highest social\ngroups.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06569343065693431,
          "p": 0.17647058823529413,
          "f": 0.0957446768973519
        },
        "rouge-2": {
          "r": 0.005952380952380952,
          "p": 0.012658227848101266,
          "f": 0.008097161641071689
        },
        "rouge-l": {
          "r": 0.06569343065693431,
          "p": 0.17647058823529413,
          "f": 0.0957446768973519
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2501.07135v1",
      "true_abstract": "We present a systematic, trend-following strategy, applied to commodity\nfutures markets, that combines univariate trend indicators with cross-sectional\ntrend indicators that capture so-called {\\em momentum spillover}, which can\noccur when there is a lead-lag relationship between the trending behaviour of\ndifferent markets. Our strategy utilises two methods for detecting lead-lag\nrelationships, with a method for computing {\\em network momentum}, to produce a\nnovel trend-following indicator. We use our new trend indicator to construct a\nportfolio whose performance we compare to a baseline model which uses only\nunivariate indicators, and demonstrate statistically significant improvements\nin Sharpe ratio, skewness of returns, and downside performance, using synthetic\nbootstrapped data samples taken from time-series of actual prices.",
      "generated_abstract": "This paper investigates the joint stochastic volatility models with the\nnon-Gaussian Brownian motion. We derive the closed-form solution for the\nvolatility function and the covariance function. Moreover, we propose an\nefficient Monte Carlo method to solve the stochastic volatility model. The\nnumerical results show that the proposed method can achieve better performance\nthan the conventional Monte Carlo method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1348314606741573,
          "p": 0.27906976744186046,
          "f": 0.1818181774253904
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1348314606741573,
          "p": 0.27906976744186046,
          "f": 0.1818181774253904
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.physics/chem-ph/2503.08880v1",
      "true_abstract": "Density matrix embedding theory (DMET) provides a framework to describe\nground-state expectation values in strongly correlated systems, but its\nextension to dynamical quantities is still an open problem. We demonstrate one\nroute to obtaining excitations and dynamical spectral functions by using the\ntechniques of DMET to approximate the matrix elements that arise in a\nsingle-mode inspired excitation ansatz. We demonstrate this approach in the 1D\nHubbard model, comparing the neutral excitations, single-particle density of\nstates, charge, and spin dynamical structure factors to benchmarks from the\nBethe ansatz and density matrix renormalization group. Our work highlights the\npotential of these ideas in building computationally efficient approaches for\ndynamical quantities.",
      "generated_abstract": "We report the first observation of ultrafast electronic excitations of a\ncopper(I) complex in solution. The observed frequency of the electronic\nexcitations is in good agreement with previous theoretical predictions, and\nthe excitation energy of the electronic excitation is found to be comparable to\nthe electronic binding energy of the complex. We also report the first\nexperimental observation of a similar electronic excitation in a zinc(II)\ncomplex. The results provide valuable insight into the electronic structure of\nmetallic complexes, which could be of relevance to the understanding of their\ninteractions with ligands and electrolyte solutions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13580246913580246,
          "p": 0.20754716981132076,
          "f": 0.1641790996959235
        },
        "rouge-2": {
          "r": 0.009523809523809525,
          "p": 0.012345679012345678,
          "f": 0.010752683255291877
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.20754716981132076,
          "f": 0.1641790996959235
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02729v1",
      "true_abstract": "This paper introduces a novel low-complexity memoryless linearizer for\nsuppression of distortion in analog frontends. It is based on our recently\nintroduced linearizer which is inspired by neural networks, but with\norders-of-magnitude lower complexity than conventional neural-networks\nconsidered in this context, and it can also outperform the conventional\nparallel memoryless Hammerstein linearizer. Further, it can be designed through\nmatrix inversion and thereby the costly and time consuming numerical\noptimization traditionally used when training neural networks is avoided. The\nlinearizer proposed in this paper is different in that it uses 1-bit\nquantizations as nonlinear activation functions and different bias values.\nThese features enable a look-up table implementation which eliminates all but\none of the multiplications and additions required for the linearization.\nExtensive simulations and comparisons are included in the paper, for distorted\nmulti-tone signals and bandpass filtered white noise, which demonstrate the\nefficacy of the proposed linearizer.",
      "generated_abstract": "The emergence of large-scale 6G networks requires novel methods for resource\nmanagement. In this paper, we present a novel energy-efficient scheduling\nmethod for 6G backhaul networks, which utilizes a novel energy-efficient\nscheduling algorithm for resource allocation. The proposed method enables\nnetwork operators to balance traffic and energy consumption while minimizing\nthe total energy consumption. Additionally, the proposed method enables\noptimization of the system's energy efficiency. The proposed method is\nanalyzed using a system model that simulates a 6G backhaul network. The results\ndemonstrate that the proposed method can effectively balance the energy and\ntraffic consumption while reducing the energy consumption. The proposed method\nalso enhances the network's energy efficiency, thereby improving the system's\nperformance. The proposed method is also evaluated using the IEEE 802.16j\nstandard, which is widely",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.2857142857142857,
          "f": 0.22857142377142864
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.037383177570093455,
          "f": 0.032128509155014376
        },
        "rouge-l": {
          "r": 0.18095238095238095,
          "p": 0.2714285714285714,
          "f": 0.21714285234285727
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09489v1",
      "true_abstract": "Integrated sensing and communications (ISAC) has emerged as a promising\nparadigm to unify wireless communications and radar sensing, enabling efficient\nspectrum and hardware utilization. A core challenge with realizing the gains of\nISAC stems from the unique challenges of dual purpose beamforming design due to\nthe highly non-convex nature of key performance metrics such as sum rate for\ncommunications and the Cramer-Rao lower bound (CRLB) for sensing. In this\npaper, we propose a low-complexity structured approach to ISAC beamforming\noptimization to simultaneously enhance spectral efficiency and estimation\naccuracy. Specifically, we develop a successive convex approximation (SCA)\nbased algorithm which transforms the original non-convex problem into a\nsequence of convex subproblems ensuring convergence to a locally optimal\nsolution. Furthermore, leveraging the proposed SCA framework and the Lagrange\nduality, we derive the optimal beamforming structure for CRLB optimization in\nISAC systems. Our findings characterize the reduction in radar streams one can\nemploy without affecting performance. This enables a dimensionality reduction\nthat enhances computational efficiency. Numerical simulations validate that our\napproach achieves comparable or superior performance to the considered\nbenchmarks while requiring much lower computational costs.",
      "generated_abstract": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, but their computational costs have limited their\napplications to resource-constrained settings. To address this, we propose\nLearning-Based Large Language Models (LBLMs), a novel framework that leverages\nlarge language models (LLMs) as a model-agnostic encoder for efficient\ntranslation and translation-based summarization. LBLMs first encodes the source\ndocument into an LLM-aligned vector representation and then utilizes a\nmulti-headed LLM-based decoder to generate the target document. We propose two\nencoder-decoder models, a simple LBLM and a larger LBLM, both of which use\nsmaller LLMs with improved performance. Our experiments demonstrate that LBLMs\nachieve state-of-the-art performance in translation and summarization tasks,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.23809523809523808,
          "f": 0.18691588308149196
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.02857142857142857,
          "f": 0.021126755902847683
        },
        "rouge-l": {
          "r": 0.13076923076923078,
          "p": 0.20238095238095238,
          "f": 0.1588784999039219
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.01640v1",
      "true_abstract": "We study certain properties of modules over 1-dimensional local integral\ndomains. First, we examine the order of the conductor ideal and its expected\nrelationship with multiplicity. Next, we investigate the reflexivity of certain\ncolength-two ideals. Finally, we consider the freeness problem of the absolute\nintegral closure of a DVR, and connect this to the reflexivity problem of\n$R^{\\frac{1}{p^n}}$.",
      "generated_abstract": "Let $G$ be a finite group, $X$ a complex projective variety, and $Y$ a\nnon-algebraic variety. We define the \\emph{relative Gorenstein dimension of\n$X$ over $Y$}, denoted by $\\operatorname{rGD}(X/Y)$, as the supremum of the\ndimensions of the relative singularities of $X$ over $Y$. We show that this\ndimension is equal to the dimension of the relative singularities of $X$ over\n$Y$ when $X$ is smooth and $Y$ is a closed subvariety of $X$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.1951219512195122,
          "f": 0.19512194621951232
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.03389830508474576,
          "f": 0.035398225098285614
        },
        "rouge-l": {
          "r": 0.14634146341463414,
          "p": 0.14634146341463414,
          "f": 0.1463414584146343
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.19436v2",
      "true_abstract": "In economic analysis, rational decision-makers often take actions to reduce\ntheir risk exposure. These actions include purchasing market insurance and\nimplementing prevention measures to modify the shape of the loss distribution.\nUnder the assumption that the insureds' actions are fully observed by the\ninsurer, this paper investigates the interaction between self-protection and\ninsurance demand when insurance premiums are determined by convex premium\nprinciples within the framework of distortion risk measures. Specifically, the\ninsured selects an optimal proportional insurance share and prevention effort\nto minimize the risk measure of their end-of-period exposure. We explicitly\ncharacterize the optimal combination of prevention effort and insurance demand\nin a self-protection model when the insured adopts tail value-at-risk or a\nsubclass with strictly concave distortion functions. Additionally, we conduct\ncomparative static analyses to illustrate our main findings under various\npremium structures, risk aversion levels, and loss distributions. Our results\nindicate that market insurance and self-protection are complementary,\nsupporting classical insights from the literature regarding corner insurance\npolicies (i.e., null and full insurance) in the absence of ex ante moral\nhazard. Finally, we consider the effects of moral hazard on the interaction\nbetween self-protection and insurance demand. Our findings show that ex ante\nmoral hazard shifts the complementary effect into substitution effect.",
      "generated_abstract": "We consider the stochastic volatility model with a moving average (MA)\nstress-inducing\n  term, which is commonly used in option pricing and risk management. We\nintroduce a new methodology to deal with the time-varying volatility and\nprice-dependent MA coefficients, and we propose a novel estimator for the\nvolatility. We show that the estimator is consistent and asymptotically normal\nunder some conditions. We also derive the asymptotic distribution of the\nestimator. We illustrate the methodology with a simple example and then apply\nit to the option pricing and risk management of European call and put options.\n  The proposed methodology is applied to the option pricing and risk\nmanagement of European call and put options, and the results are compared with\nthose obtained using the conventional methodology. The results show that the\nproposed methodology can improve the accuracy of option pr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14728682170542637,
          "p": 0.25333333333333335,
          "f": 0.18627450515426774
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.03418803418803419,
          "f": 0.025974021262650547
        },
        "rouge-l": {
          "r": 0.11627906976744186,
          "p": 0.2,
          "f": 0.14705881887975794
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2411.16617v1",
      "true_abstract": "Quanto options allow the buyer to exchange the foreign currency payoff into\nthe domestic currency at a fixed exchange rate. We investigate quanto options\nwith multiple underlying assets valued in different foreign currencies each\nwith a different strike price in the payoff function. We carry out a\ncomparative performance analysis of different stochastic volatility (SV),\nstochastic correlation (SC), and stochastic exchange rate (SER) models to\ndetermine the best combination of these models for Monte Carlo (MC) simulation\npricing. In addition, we test the performance of all model variants with\nconstant correlation as a benchmark. We find that a combination of GARCH-Jump\nSV, Weibull SC, and Ornstein Uhlenbeck (OU) SER performs best. In addition, we\nanalyze different discretization schemes and their results. In our simulations,\nthe Milstein scheme yields the best balance between execution times and lower\nstandard deviations of price estimates. Furthermore, we find that incorporating\nmean reversion into stochastic correlation and stochastic FX rate modeling is\nbeneficial for MC simulation pricing. We improve the accuracy of our\nsimulations by implementing antithetic variates variance reduction. Finally, we\nderive the correlation risk parameters Cora and Gora in our framework so that\ncorrelation hedging of quanto options can be performed.",
      "generated_abstract": "In the era of big data, financial data is becoming increasingly available.\nHowever, the availability of such data is only one of the challenges faced by\ndata analysts. In this work, we consider the case of binary data, which is\nfrequently encountered in financial applications. We propose a novel methodology\nbased on a genetic algorithm to tackle the problem of data imbalance. Our\nmethodology is based on a hybrid approach combining two genetic algorithms, one\nfor the majority class and the other for the minority class. The proposed\nmethodology is applied to the problem of classifying binary data, and we show\nthat it can outperform existing methods in terms of classification accuracy.\nMoreover, we show that our methodology can be easily adapted to other problems\nwhere binary data is available.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13709677419354838,
          "p": 0.21794871794871795,
          "f": 0.16831682694245675
        },
        "rouge-2": {
          "r": 0.0053475935828877,
          "p": 0.00847457627118644,
          "f": 0.006557372305082712
        },
        "rouge-l": {
          "r": 0.12096774193548387,
          "p": 0.19230769230769232,
          "f": 0.148514846744437
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2407.05866v1",
      "true_abstract": "We introduce generalizations of the COGARCH model of Kl\\\"uppelberg et al.\nfrom 2004 and the volatility and price model of Barndorff-Nielsen and Shephard\nfrom 2001 to a Markov-switching environment. These generalizations allow for\nexogeneous jumps of the volatility at times of a regime switch. Both models are\nstudied within the framework of Markov-modulated generalized Ornstein-Uhlenbeck\nprocesses which allows to derive conditions for stationarity, formulas for\nmoments, as well as the autocovariance structure of volatility and price\nprocess. It turns out that both models inherit various properties of the\noriginal models and therefore are able to capture basic stylized facts of\nfinancial time-series such as uncorrelated log-returns, correlated squared\nlog-returns and non-existence of higher moments in the COGARCH case.",
      "generated_abstract": "This study investigates the effect of the use of short-term and long-term\nlong-short-equity (LTSE) strategies on the volatility of the S\\&P 500 index\nduring the COVID-19 pandemic. The results show that the use of LTSE strategies\nincreases the volatility of the index and that the use of a short-term strategy\nwith a 3-month look-back is more effective than a long-term strategy with a 6-\nmonth look-back. The results also show that the use of LTSE strategies is more\neffective during periods of higher volatility, such as the COVID-19 pandemic.\nThese results suggest that LTSE strategies could be effective in managing\nvolatility during periods of high volatility, such as during the COVID-19\npandemic, and may provide",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13580246913580246,
          "p": 0.2037037037037037,
          "f": 0.1629629581629631
        },
        "rouge-2": {
          "r": 0.05454545454545454,
          "p": 0.07058823529411765,
          "f": 0.06153845662064471
        },
        "rouge-l": {
          "r": 0.12345679012345678,
          "p": 0.18518518518518517,
          "f": 0.1481481433481483
        }
      }
    },
    {
      "paper_id": "math.LO.math/CO/2503.09246v1",
      "true_abstract": "We introduce the notion of Ramsey partition regularity, a generalisation of\npartition regularity involving infinitary configurations. We provide\ncharacterisations of this notion in terms of certain ultrafilters related to\ntensor products and dubbed Ramsey's witnesses; and we also consider their\nnonstandard counterparts as pairs of hypernatural numbers, called Ramsey pairs.\nThese characterisations are then used to determine whether various\nconfigurations involving polynomials and exponentials are Ramsey partition\nregular over the natural numbers.",
      "generated_abstract": "In this paper, we study the localization of the boundary of a compact\nsubmanifold $M$ in the unit disk. We first prove that $M$ has a smooth boundary\nwhen $M$ is a compact connected submanifold of the unit disk with a fixed\npositive boundary value. Then we construct a sequence of compact connected\nsubmanifolds $M_n$, with $n \\geq 1$, such that $M_n$ converges to $M$ as $n\n\\to \\infty$ in the Gromov-Hausdorff topology. We then prove that for each $n$\nthere is a smooth boundary $S_n$ in $M_n$ such that the restriction of the\ninterior of $M$ to $S_n$ coincides with the interior of $M$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.18181818181818182,
          "f": 0.1851851801869
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.18867924528301888,
          "p": 0.18181818181818182,
          "f": 0.1851851801869
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2412.05508v1",
      "true_abstract": "Experimentation in online digital platforms is used to inform decision\nmaking. Specifically, the goal of many experiments is to optimize a metric of\ninterest. Null hypothesis statistical testing can be ill-suited to this task,\nas it is indifferent to the magnitude of effect sizes and opportunity costs.\nGiven access to a pool of related past experiments, we discuss how\nexperimentation practice should change when the goal is optimization. We survey\nthe literature on empirical Bayes analyses of A/B test portfolios, and single\nout the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which\ntreats experimentation as a constrained optimization problem. We show that the\nframework can be solved with dynamic programming and implemented by\nappropriately tuning $p$-value thresholds. Furthermore, we develop several\nextensions of the A/B Testing Problem and discuss the implications of these\nresults on experimentation programs in industry. For example, under no-cost\nassumptions, firms should be testing many more ideas, reducing test allocation\nsizes, and relaxing $p$-value thresholds away from $p = 0.05$.",
      "generated_abstract": "The impact of the 2008 financial crisis on employment has been the focus of\nmany studies. However, the impact on employment in the financial sector has\nreceived less attention. We use data from the Financial Services Survey to\nexplore how the crisis affected employment in this sector. Our main findings\nare that the crisis led to a decline in employment in this sector, with a\nsignificant decline in employment in the banking and insurance sector. The\nfindings also suggest that the decline in employment was more pronounced in\nlow-skill jobs. However, our analysis also highlights that some sectors, such\nas construction, experienced an increase in employment. These findings highlight\nthe complexity of the impact of the financial crisis on employment and\nhighlight the need for further research into the long-term impact of the\ncrisis on employment in the financial sector.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13445378151260504,
          "p": 0.2191780821917808,
          "f": 0.16666666195366767
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.037037037037037035,
          "f": 0.029629624829630403
        },
        "rouge-l": {
          "r": 0.13445378151260504,
          "p": 0.2191780821917808,
          "f": 0.16666666195366767
        }
      }
    },
    {
      "paper_id": "astro-ph.CO.astro-ph/CO/2503.09718v1",
      "true_abstract": "The strongly lensed Supernova (SN) Encore at a redshift of $z = 1.949$,\ndiscovered behind the galaxy cluster MACS J0138$-$2155 at $z=0.336$, provides a\nrare opportunity for time-delay cosmography and studies of the SN host galaxy,\nwhere previously another SN, called SN Requiem, had appeared. To enable these\nstudies, we combine new James Webb Space Telescope (JWST) imaging, archival\nHubble Space Telescope (HST) imaging, and new Very Large Telescope (VLT)\nspectroscopic data to construct state-of-the-art lens mass models that are\ncomposed of cluster dark-matter (DM) halos and galaxies. We determine the\nphotometric and structural parameters of the galaxies across six JWST and five\nHST filters. We use the color-magnitude and color-color relations of\nspectroscopically-confirmed cluster members to select additional cluster\nmembers, identifying a total of 84 galaxies belonging to the galaxy cluster. We\nconstruct seven different mass models using a variety of DM halo mass profiles,\nand explore both multi-plane and approximate single-plane lens models. As\nconstraints, we use the observed positions of 23 multiple images from eight\nmultiply lensed sources at four distinct spectroscopic redshifts. In addition,\nwe use stellar velocity dispersion measurements to obtain priors on the galaxy\nmass distributions. We find that six of the seven models fit well to the\nobserved image positions. Mass models with cored-isothermal DM profiles fit\nwell to the observations, whereas the mass model with a Navarro-Frenk-White\ncluster DM profile has an image-position $\\chi^2$ value that is four times\nhigher. We build our ultimate model by combining four multi-lens-plane mass\nmodels and predict the image positions and magnifications of SN Encore and SN\nRequiem. Our work lays the foundation for building state-of-the-art mass models\nof the cluster for future cosmological analysis and SN host galaxy studies.",
      "generated_abstract": "In this work, we present a novel approach for the analysis of the\ncold gas in nearby dwarf galaxies using mid-infrared spectra, combining\nspectral synthesis with a Bayesian framework. We combine data from the\nSpitzer Space Telescope with the near-infrared spectra from the InfraRed\nMulti-Object Spectrograph (IRMOS) on the 4.2 m telescope at Mauna Kea. We\nadopt a Bayesian approach to determine the physical properties of the cold gas\n(temperature, density, metallicity, and molecular content) using the IRMOS\ndata as priors. We use this approach to perform a Bayesian analysis of the\nspectra of the dwarf galaxies NGC 1097 and NGC 1316, both of which exhibit\nsignificant cold gas components, and to estimate their physical properties.\nSpecific",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14367816091954022,
          "p": 0.3333333333333333,
          "f": 0.20080320864179618
        },
        "rouge-2": {
          "r": 0.022641509433962263,
          "p": 0.057692307692307696,
          "f": 0.03252032115510366
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.28,
          "f": 0.1686746945855713
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/CG/2503.06833v1",
      "true_abstract": "The Hausdorff distance is a fundamental measure for comparing sets of\nvectors, widely used in database theory and geometric algorithms. However, its\nexact computation is computationally expensive, often making it impractical for\nlarge-scale applications such as multi-vector databases. In this paper, we\nintroduce an approximation framework that efficiently estimates the Hausdorff\ndistance while maintaining rigorous error bounds. Our approach leverages\napproximate nearest-neighbor (ANN) search to construct a surrogate function\nthat preserves essential geometric properties while significantly reducing\ncomputational complexity. We provide a formal analysis of approximation\naccuracy, deriving both worst-case and expected error bounds. Additionally, we\nestablish theoretical guarantees on the stability of our method under\ntransformations, including translation, rotation, and scaling, and quantify the\nimpact of non-uniform scaling on approximation quality. This work provides a\nprincipled foundation for integrating Hausdorff distance approximations into\nlarge-scale data retrieval and similarity search applications, ensuring both\ncomputational efficiency and theoretical correctness.",
      "generated_abstract": "We study the problem of computing a $\\tau$-approximation of the maximum\ndiameter in a $d$-regular graph $G$ for a parameter $\\tau \\in (0, 1)$. We show\nthat this problem is in $\\text{NP}\\xspace$-hard even if the graph is edge-colored\nwith constant degree. In contrast, we show that this problem is in $\\text{NP}\\xspace$\nif the graph is bipartite. Furthermore, we present a polynomial-time algorithm\nfor computing a $\\tau$-approximation of the maximum diameter in a $d$-regular\ngraph with bounded degree. Our algorithm runs in $O(n \\log n + n^2)$ time and\nuses $O(n \\log n + n^2)$ space. We also show that the problem is in\n$\\text{NP}\\xspace$-hard even if the graph is bipartite and bounded degree.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11403508771929824,
          "p": 0.24528301886792453,
          "f": 0.15568861842160003
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.22641509433962265,
          "f": 0.1437125705174084
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.10478v1",
      "true_abstract": "Existing studies have shown that the monetization of silver in the Ming\nDynasty effectively promoted the prosperity of trade in the Ming Dynasty, while\nthe prices of labor, handicraft products and grain were long suppressed by the\ndeformed economic structure. With the expansion of silver application, the\nfluctuation of silver supply and demand exacerbated the above contradictions.\nCapital accumulation that should have been obtained through the marketization\nof labor was easily plundered by the landlord gentry class through silver. This\narticle re-discusses the issue from the perspective of supply and demand.\nCompared with the increase and then decrease of silver supply, the evolution of\nsilver demand is more complicated: at the tax level, the widespread use of\nsilver leads to a huge difference in the elasticity of production and trade\ntaxes. When government spending surges, the increase in tax burden will be\nmainly borne by agriculture and handicrafts. At the production level, the high\nliquidity of silver makes the concentration of social wealth more convenient,\nwhile the reduction in silver supply and the expansion of demand have rapidly\nexpanded deflation, further exacerbating the gap between the rich and the poor.\nSuch combined effect of supply and demand factors has caused the monetization\nof silver to become an accelerator of the economic collapse of the Ming\nDynasty.",
      "generated_abstract": "The COVID-19 pandemic has brought about significant changes in how individuals\nand firms interact with each other and the world around them. This paper\nexplores how the pandemic has affected the interactions between firms and\nfirm-specific customers, focusing on the impact on customer service. We\nanalyze the response of firms to customer demands by studying the effect of\nCOVID-19 on customer service. We use a unique dataset that combines data from\nthe United States, the United Kingdom, and India to analyze the effects of\nCOVID-19 on customer service and identify the critical factors affecting the\nresponse of firms. Our analysis shows that firms respond to customer demands\nby adjusting their service levels, implementing temporary staffing solutions,\nand improving the quality of their service. This study provides valuable\ninsights into the effects of COVID-19 on customer service and the strategic\nimplications for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.19753086419753085,
          "f": 0.157635463184256
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.02608695652173913,
          "f": 0.019933550095474675
        },
        "rouge-l": {
          "r": 0.09836065573770492,
          "p": 0.14814814814814814,
          "f": 0.11822659618918216
        }
      }
    },
    {
      "paper_id": "math.PR.math/MP/2503.09486v1",
      "true_abstract": "The directed landscape, the central object in the Kardar-Parisi-Zhang\nuniversality class, is shown to be the scaling limit of various models by\nDauvergne and Vir\\'ag (2022) and Dauvergne, Ortmann and Vir\\'ag (2018). In his\nstudy of geodesics in upper tail deviations of the directed landscape, Liu\n(2022) put forward a conjecture about the rate of the lowest rate metric under\nwhich a geodesic between two points passes through a particular point between\nthem. Das, Dauvergne and Vir\\'ag (2024) disproved his conjecture, and made a\nconjecture of their own. This paper disproves that conjecture and puts the\nquestion to rest with an answer and a proof.",
      "generated_abstract": "We construct a new representation of the Heisenberg-Weyl algebra $H_n$\n(the Heisenberg-Weyl algebra in a finite-dimensional space) with a\nquasi-triangular Hopf algebra structure. The algebra is a quasi-triangular\nHopf algebra, with a quasi-triangular Hopf algebra structure on its comodule\nalgebra. The comodule algebra is a subalgebra of the comodule algebra of the\nquasi-triangular Hopf algebra. This is the first example of a quasi-triangular\nHopf algebra that admits a comodule algebra. We also construct a new\nrepresentation of the Heisenberg-Weyl algebra $H_n$ with a quasi-triangular\nHopf algebra structure on a comodule algebra. The comodule algebra is the\nalgebra of the projective cover of the algebra of the quasi-triangular Hopf",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2727272727272727,
          "f": 0.17142856711836746
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.017241379310344827,
          "f": 0.012820508149245619
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.24242424242424243,
          "f": 0.15238094807074842
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.00085v1",
      "true_abstract": "This study examines the influence of employee education and health on\nfirm-level Total Factor Productivity (TFP) in China, using panel data from\nA-share listed companies spanning from 2007 to 2022. The analysis shows that\nlife expectancy and higher education have a significant impact on TFP. More\noptimal health conditions can result in increased productivity through\ndecreased absenteeism and improved work efficiency. Similarly, higher levels of\neducation can support technological adaptation, innovation, and managerial\nefficiency. Nevertheless, the correlation between health and higher education\nindicates that there may be a point where further improvements in health yield\ndiminishing returns in terms of productivity for individuals with advanced\neducation. These findings emphasise the importance of implementing\ncomprehensive policies that improve both health and education, maximising their\nimpact on productivity. This study adds to the current body of research by\npresenting empirical evidence at the firm-level in China. It also provides\npractical insights for policymakers and business leaders who want to improve\neconomic growth and competitiveness. Future research should take into account\nwider datasets, more extensive health metrics, and delve into the mechanisms\nthat contribute to the diminishing returns observed in the relationship between\nhealth and education.",
      "generated_abstract": "This paper explores the impact of the COVID-19 pandemic on the labor\nmarket of four countries: Denmark, Finland, Germany, and Sweden. We analyze\nthe effects of the pandemic on the unemployment rate, employment, and labor\nforce participation. We also explore the effects of the pandemic on wage\ngrowth. Using a difference-in-differences approach, we find that the\npandemic's effects on the unemployment rate and employment are larger in\nDenmark and Sweden than in Finland and Germany. However, the pandemic's effects\non wage growth are similar in all countries. The pandemic's effects on\nunemployment rates and employment rates are larger in Denmark and Sweden than\nin Finland and Germany. However, the pandemic's effects on wage growth are\nsimilar in all countries. Our results suggest that the pandemic's effects on\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09230769230769231,
          "p": 0.21428571428571427,
          "f": 0.12903225385593722
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09230769230769231,
          "p": 0.21428571428571427,
          "f": 0.12903225385593722
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/QM/2503.05448v1",
      "true_abstract": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
      "generated_abstract": "This study presents a novel approach for the prediction of genome-wide\nsyndromic trait association using a data-driven Bayesian framework. We\ninvestigate a novel gene-environment interaction model for the prediction of\nsyndromic diseases using gene expression profiles and gene set enrichment\nanalysis. Our findings highlight the value of incorporating gene set enrichment\nanalysis into Bayesian modeling frameworks. We propose a novel Bayesian\nprediction method that incorporates gene set enrichment analysis to improve\nprediction accuracy and robustness. Our results demonstrate that incorporating\ngene set enrichment analysis into Bayesian modeling frameworks can improve\nprediction accuracy and robustness. We provide a detailed analysis of the\nproposed methodology and its application to a gene-environment interaction\nmodel for the prediction of syndromic diseases. The proposed methodology\ndemonstrates significant improvements in prediction accuracy and robust",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.3548387096774194,
          "f": 0.2075471656728374
        },
        "rouge-2": {
          "r": 0.005405405405405406,
          "p": 0.010752688172043012,
          "f": 0.0071942401519100065
        },
        "rouge-l": {
          "r": 0.14666666666666667,
          "p": 0.3548387096774194,
          "f": 0.2075471656728374
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.physics/ao-ph/2503.05288v1",
      "true_abstract": "Like Johnson noise, where thermal fluctuations of charge carriers in a\nresistor lead to measurable current fluctuations, the internal variability of\nEarth's atmosphere leads to fluctuations in the infrared radiation emitted to\nspace, creating \"Earth's infrared background\" (EIB). This background consists\nof fluctuations that are isotropic in space and red in time, with an upper\nbound of 400 km and 2.5 days on their spatiotemporal decorrelation, between\nmeso-scale and synoptic-scale weather. Like the anisotropies in the Cosmic\nMicrowave Background (CMB), which represent features of interest in the\nUniverse, the anisotropies in Earth's infrared radiation represent features of\ninterest in Earth's atmosphere. Unlike the CMB, which represents a historical\nrecord of the Universe since the Big Bang, the EIB represents Earth's climate\nin steady state.",
      "generated_abstract": "This study investigates the impact of cloud-to-ground lightning (CGL)\nrainfall\n  events on the atmospheric moisture field and its impact on the atmospheric\ntemperature profile. We focus on the role of CGL rainfall events in regulating\nthe moisture budget of the atmosphere and the impact of the moisture\ndistribution on the temperature profile. The results show that the cumulative\nCGL rainfall events have a significant impact on the atmospheric moisture\nfield, which has a significant impact on the atmospheric temperature profile.\nIn particular, when the cumulative CGL rainfall event frequency is less than 2\nevents per year, the cumulative rainfall event frequency will not affect the\natmospheric moisture field and the atmospheric temperature profile. However,\nwhen the cumulative CGL rainfall event frequency is more than 2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13580246913580246,
          "p": 0.2,
          "f": 0.1617647010650953
        },
        "rouge-2": {
          "r": 0.008849557522123894,
          "p": 0.012658227848101266,
          "f": 0.010416661823461452
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.2,
          "f": 0.1617647010650953
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/CL/2503.10533v1",
      "true_abstract": "High-quality test items are essential for educational assessments,\nparticularly within Item Response Theory (IRT). Traditional validation methods\nrely on resource-intensive pilot testing to estimate item difficulty and\ndiscrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a\ndomain-general approach for evaluating test items based on textual features.\nHowever, their relationship to IRT parameters remains underexplored. To address\nthis gap, we conducted a study involving over 7,000 multiple-choice questions\nacross various STEM subjects (e.g., math and biology). Using an automated\napproach, we annotated each question with a 19-criteria IWF rubric and studied\nrelationships to data-driven IRT parameters. Our analysis revealed\nstatistically significant links between the number of IWFs and IRT difficulty\nand discrimination parameters, particularly in life and physical science\ndomains. We further observed how specific IWF criteria can impact item quality\nmore and less severely (e.g., negative wording vs. implausible distractors).\nOverall, while IWFs are useful for predicting IRT parameters--particularly for\nscreening low-difficulty MCQs--they cannot replace traditional data-driven\nvalidation methods. Our findings highlight the need for further research on\ndomain-general evaluation rubrics and algorithms that understand\ndomain-specific content for robust item validation.",
      "generated_abstract": "In this paper, we propose a novel framework for multilingual question\ntransformer models by introducing a multilingual tokenizer. Our approach\nincorporates a multilingual tokenizer into a pretrained language model to\ngenerate a language-invariant tokenizer for multilingual data. The tokenizer\nis designed to capture multilingual characteristics such as word order,\nphonology, and grammar. We introduce a multilingual tokenizer based on\nTransformer-XL, which outperforms existing tokenizers. This multilingual tokenizer\nis also evaluated on two datasets: WMT16 and WMT17, which are widely used for\nevaluating multilingual language models. The results show that our multilingual\ntokenizer significantly improves the performance of the pretrained model. This\napproach has the potential to enhance multilingual translation and generation\nby reducing the linguistic and cultural differences between",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1347517730496454,
          "p": 0.24358974358974358,
          "f": 0.17351597714893363
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.01818181818181818,
          "f": 0.013888884167632776
        },
        "rouge-l": {
          "r": 0.1276595744680851,
          "p": 0.23076923076923078,
          "f": 0.16438355705760943
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10107v1",
      "true_abstract": "Perceptive mobile networks (PMNs), integrating ubiquitous sensing\ncapabilities into mobile networks, represent an important application of\nintegrated sensing and communication (ISAC) in 6G. In this paper, we propose a\npractical framework for uplink sensing of angle-of-arrival (AoA), Doppler, and\ndelay in millimeter-wave (mmWave) communication systems, which addresses\nchallenges posed by clock asynchrony and hybrid arrays, while being compatible\nwith existing communication protocols. We first introduce a beam scanning\nmethod and a corresponding AoA estimation algorithm, which utilizes frequency\nsmoothing to effectively estimate AoAs for both static and dynamic paths. We\nthen propose several methods for constructing a ``clean'' reference signal,\nwhich is subsequently used to cancel the effect caused by the clock asynchrony.\nWe further develop a signal ratio-based joint AoA-Doppler-delay estimator and\npropose an AoA-based 2D-FFT-MUSIC (AB2FM) algorithm that applies 2D-FFT\noperations on the signal subspace, which accelerates the computation process\nwith low complexity. Our proposed framework can estimate parameters in pairs,\nremoving the complicated parameter association process. Simulation results\nvalidate the effectiveness of our proposed framework and demonstrate its\nrobustness in both low and high signal-to-noise ratio (SNR) conditions.",
      "generated_abstract": "In this paper, we propose a novel distributed adaptive estimation strategy for\na linear system with multiple input-output (MIoU) matrices. The proposed\nstrategy is based on the distributed estimation of the MIoU matrices, and\nenables the estimation of the system state vector and output vector from\nexisting measurements. By leveraging the MIoU matrices, the proposed strategy\nachieves a significant reduction in the number of measurements required for\nestimation. Furthermore, we propose a distributed adaptive estimation\nstrategy for a linear system with multiple input-output (MIoU) matrices. The\nproposed strategy is based on the distributed estimation of the MIoU matrices,\nand enables the estimation of the system state vector and output vector from\nexisting measurements. By leveraging the MIoU matrices, the proposed strategy\nachieves a significant reduction in the number of measurements required for\nestimation. Furthermore, we propose a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12878787878787878,
          "p": 0.37777777777777777,
          "f": 0.19209039168821224
        },
        "rouge-2": {
          "r": 0.033707865168539325,
          "p": 0.09230769230769231,
          "f": 0.04938271213060371
        },
        "rouge-l": {
          "r": 0.12878787878787878,
          "p": 0.37777777777777777,
          "f": 0.19209039168821224
        }
      }
    },
    {
      "paper_id": "math.OC.econ/TH/2502.17012v2",
      "true_abstract": "We introduce a model of infinite horizon linear dynamic optimization and\nobtain results concerning existence of solution and satisfaction of the Euler\ncondition and transversality condition being unconditionally sufficient for\noptimality of a trajectory. We show that the optimal value function is concave\nand continuous and the optimal trajectory satisfies the functional equation of\ndynamic programming. Linearity bites when it comes to the definition of optimal\ndecision rules which can no longer be guaranteed to be single-valued. We show\nthat the optimal decision rule is an upper semi-continuous correspondence. For\nlinear cake-eating problems, we obtain monotonicity results for the optimal\nvalue function and a conditional monotonicity result for optimal decision\nrules. We also introduce the concept of a two-phase linear cake eating problem\nand obtain a necessary condition that must be satisfied by all solutions of\nsuch problems.",
      "generated_abstract": "We study the optimal matching problem for a network of agents with preferences\nover the set of agents and a matching with capacity constraints. We show that\nthe optimal matching depends only on the agent preferences and the network\nstructure, and can be computed in polynomial time in the number of agents. We\nalso provide a general framework to model preferences over the set of agents,\nenabling a unified analysis of matching problems with preferences over sets,\nand preferences over the union of sets, which are both well-studied. Our\nresults also imply a generalization of the Nash Equilibrium concept to\npreferences over the set of agents.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2345679012345679,
          "p": 0.3392857142857143,
          "f": 0.27737225794022063
        },
        "rouge-2": {
          "r": 0.06451612903225806,
          "p": 0.09090909090909091,
          "f": 0.07547169325738728
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.32142857142857145,
          "f": 0.2627737177942352
        }
      }
    },
    {
      "paper_id": "math.OC.math/HO/2503.07134v1",
      "true_abstract": "This paper is dedicated to the elementary proof of Pontryagin's maximum\nprinciple for problems with free right end point. The proof for the standard\nproblem is taken from the monography of Ioffe and Tichomirov. We assume\npiecewise continuous controls and the proof turns out to be very simple. We\ngeneralize the concept to the problem of optimal multiprocesses, to control\nproblems with delays and to the control of Volterra integral equations.\nFurthermore, we discuss the problem on infinite horizon. Moreover, we state\nArrow type sufficiency conditions. The optimality conditions are demonstrated\non illustrative examples.",
      "generated_abstract": "We introduce a new notion of $\\varepsilon$-approximation for discrete\nfunctions on an interval. The approximation is based on the well-known\napproximation of the Riemann integral with a rectangular window. This notion\nleads to a new class of discrete functions, called $\\varepsilon$-approximation\nfunctions, which are functions that are approximately equal to their\n$\\varepsilon$-approximation functions for a fixed $\\varepsilon > 0$. We show\nthat the $\\varepsilon$-approximation functions are a compact class of\n$\\varepsilon$-approximation functions, and that a $\\varepsilon$-approximation\nfunction is a $\\varepsilon$-approximation function for a fixed $\\varepsilon > 0$\nif and only if it is a $\\varepsilon$-approximation function for a fixed\n$\\varepsilon_0 > 0$ and a fixed $0 < \\varepsilon < \\varepsilon_0$.\n  We introduce a new notion of $\\varepsilon$-appro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.203125,
          "p": 0.24528301886792453,
          "f": 0.22222221726641841
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.203125,
          "p": 0.24528301886792453,
          "f": 0.22222221726641841
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/CB/2501.09546v1",
      "true_abstract": "Bacteria can form a great variety of spatially heterogeneous cell density\npatterns, ranging from simple concentric rings to dynamical spiral waves\nappearing in growing colonies. These pattern formation phenomena are important\nas they reflect how cellular processes such as metabolism operate in\nheterogeneous chemical environments. In the laboratory, they can be studied in\nsimplified set-ups, where spatial gradients of oxygen and nutrients are\nexternally imposed, and cells are immobilized in a gel matrix. An intriguing\nexample, observed in such set-ups over 80 years ago, is the sequential\nformation of narrow bands of high cell density, taking place even for a clonal\npopulation. However, key aspects of the dynamics of band formation remained\nobscure. Using time-lapse imaging of replicate transparent columns in\nsimplified growth media, we first quantify the precision of the positioning and\ntiming of band formation. We also show that the appearance and position of\ndifferent bands can be modulated independently. This \"modularity\" is suggested\nby the observation that different bands differ in their gene expression, and it\nis reproduced by a theoretical model based on the existence of internal\nmetabolic states and the induction of a pH gradient. Finally, we can also\nmodify the observed pattern formation by introducing genetic modifications that\nimpair selected metabolic pathways. In our opinion, the possibility of precise\nmeasurements and controls, together with the simplicity and richness of the\n\"proliferation pattern formation\" phenomenon, can make it a model system to\nstudy the response of cellular processes to heterogeneous environments.",
      "generated_abstract": "The advent of next-generation sequencing (NGS) has revolutionized our\nunderstanding of microbial ecology. While much progress has been made, there\nremains a significant gap between our understanding of microbial communities\nand their functional roles. This paper discusses the challenges in\ncharacterizing functional communities of microbes and introduces a novel\napproach to do so. We first review the existing methods for characterizing\nmicrobial functional communities, including community assembly analysis,\ncommunity similarity analysis, and community composition analysis. We then\ndiscuss the advantages and limitations of these methods. Next, we introduce\nthe community functional profiling (CFP) approach, a novel method that\nenables us to analyze functional communities in a comprehensive manner. CFP\nutilizes community-level metabolic profiling data to identify the functional\nrelationships between microbes, including the key metabolic processes,\nfunctions,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10062893081761007,
          "p": 0.18823529411764706,
          "f": 0.13114753644349655
        },
        "rouge-2": {
          "r": 0.00423728813559322,
          "p": 0.008264462809917356,
          "f": 0.005602236415197113
        },
        "rouge-l": {
          "r": 0.0880503144654088,
          "p": 0.16470588235294117,
          "f": 0.11475409382054572
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2501.13136v1",
      "true_abstract": "Digital currencies have become popular in the last decade due to their\nnon-dependency and decentralized nature. The price of these currencies has seen\na lot of fluctuations at times, which has increased the need for prediction. As\ntheir most popular, Bitcoin(BTC) has become a research hotspot. The main\nchallenge and trend of digital currencies, especially BTC, is price\nfluctuations, which require studying the basic price prediction model. This\nresearch presents a classification and regression model based on stack deep\nlearning that uses a wavelet to remove noise to predict movements and prices of\nBTC at different time intervals. The proposed model based on the stacking\ntechnique uses models based on deep learning, especially neural networks and\ntransformers, for one, seven, thirty and ninety-day forecasting. Three feature\nselection models, Chi2, RFE and Embedded, were also applied to the data in the\npre-processing stage. The classification model achieved 63\\% accuracy for\npredicting the next day and 64\\%, 67\\% and 82\\% for predicting the seventh,\nthirty and ninety days, respectively. For daily price forecasting, the\npercentage error was reduced to 0.58, while the error ranged from 2.72\\% to\n2.85\\% for seven- to ninety-day horizons. These results show that the proposed\nmodel performed better than other models in the literature.",
      "generated_abstract": "We investigate the possibility of integrating the information contained in\ntrade flow data with that of other market data in order to enhance the\nunderstanding of trading activities. To this end, we use the trading volume\nmeasurement of the Swiss Stock Exchange and the trading volume of other\nsecurities in the Swiss Stock Exchange. We employ the logarithm of trading\nvolume and the logarithm of trading volume of other securities as predictors in\na multi-step logistic regression model. This approach allows us to explore the\nrelationship between trading volumes and the trading volumes of other\nsecurities. The results show that the logarithm of trading volume of other\nsecurities has a significant impact on the probability of trade flow in the\ncase of the Swiss Stock Exchange. The results also show that the logarithm of\ntrading volume of other securities is a significant predictor of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13970588235294118,
          "p": 0.30158730158730157,
          "f": 0.19095476954218338
        },
        "rouge-2": {
          "r": 0.04020100502512563,
          "p": 0.08163265306122448,
          "f": 0.05387204945028322
        },
        "rouge-l": {
          "r": 0.13970588235294118,
          "p": 0.30158730158730157,
          "f": 0.19095476954218338
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01265v1",
      "true_abstract": "Contrast-enhanced magnetic resonance imaging (CE-MRI) is crucial for tumor\ndetection and diagnosis, but the use of gadolinium-based contrast agents\n(GBCAs) in clinical settings raises safety concerns due to potential health\nrisks. To circumvent these issues while preserving diagnostic accuracy, we\npropose a novel Transformer with Localization Prompts (TLP) framework for\nsynthesizing CE-MRI from non-contrast MR images. Our architecture introduces\nthree key innovations: a hierarchical backbone that uses efficient Transformer\nto process multi-scale features; a multi-stage fusion system consisting of\nLocal and Global Fusion modules that hierarchically integrate complementary\ninformation via spatial attention operations and cross-attention mechanisms,\nrespectively; and a Fuzzy Prompt Generation (FPG) module that enhances the TLP\nmodel's generalization by emulating radiologists' manual annotation through\nstochastic feature perturbation. The framework uniquely enables interactive\nclinical integration by allowing radiologists to input diagnostic prompts\nduring inference, synergizing artificial intelligence with medical expertise.\nThis research establishes a new paradigm for contrast-free MRI synthesis while\naddressing critical clinical needs for safer diagnostic procedures. Codes are\navailable at https://github.com/ChanghuiSu/TLP.",
      "generated_abstract": "This paper presents a deep learning-based framework for evaluating the\nphysical similarity of brain tissues. Our approach leverages a pre-trained\nneural network to classify the tissue type of an anatomical MRI image, and then\nevaluates the similarity between the predicted tissue type and the ground truth\none. We demonstrate that our approach outperforms existing methods, particularly\nwhen there is no ground truth for the tissue type. The results highlight the\npotential of deep learning in improving tissue classification in MRI. The\nframework can be easily extended to other types of tissues and to more complex\nscenarios such as those involving multiple classes of tissues.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10638297872340426,
          "p": 0.20833333333333334,
          "f": 0.14084506594723284
        },
        "rouge-2": {
          "r": 0.012121212121212121,
          "p": 0.02040816326530612,
          "f": 0.015209120799781683
        },
        "rouge-l": {
          "r": 0.09929078014184398,
          "p": 0.19444444444444445,
          "f": 0.1314553945857305
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2411.17750v1",
      "true_abstract": "This paper statistically analysed pensioner longevity in Ghana. It\nfundamentally sought to ascertain the significant determinants of longevity\namongst Ghanaian pensioners, specifically, SSNIT pensioners by estimating the\nmortality rate of SSNIT pensioners, determining the factors that significantly\naffect the longevity of the average SSNIT pensioner and constructing a\npredictive model for predicting the longevity of SSNIT pensioners in Ghana.\nSecondary data was obtained from the leading pension provider in Ghana, SSNIT.\nThe results of the study revealed that the total number of male deaths was\nsignificantly greater, about four times more, than the total number of female\ndeaths. There was sufficient evidence that there exists a lower rate of death\namong female pensioners as compared to male pensioners. Furthermore, a\nsignificant number of the SSNIT pensioners used in the analysis survived less\nthan 8 years after retirement before their death and the average basic salary\nof the pensioners who lived less than 8 years after retirement was GH 7741.827\ncedis and served for 35.65 years on average. On the contrary, it was observed\nthat pensioners who lived more than 8 years after retirement had an average\nbasic salary of GH 5544.20 cedis and served for 31.49 years on average. In\nconclusion, predictors such as basic salary, the number of years of service in\nthe workforce, the age of the pensioner before death and the gender of the\npensioner were statistically significant in predicting the number of years a\nSSNIT pensioner survived after retirement before death that is, whether a\nparticular SSNIT pensioner survived less than 8 years or 8 years and above\nafter retirement. The authors recommend to employees that, their health should\nbe of great priority as every increase in the service year increases the\nlikelihood of a SSNIT pensioner surviving less than 8 years after retirement by\n22.36%.",
      "generated_abstract": "The role of nucleus accumbens (NAc) shell and core in the regulation of\ncocaine-seeking behavior is poorly understood. The NAc shell contains\nneurochemicals that influence reward and motivation. These include dopamine,\nglutamate, and GABA, all of which are involved in motivational processes.\nGlutamate and GABA are involved in reward-related processes, while dopamine\nplays a role in motivational processes. The NAc core contains glutamate and\nGABA, and these neurotransmitters are involved in reward-related processes.\nHowever, it is unclear whether the NAc shell and core are involved in\ncocaine-seeking behavior. To address this, we examined the effects of\ndopamine, glutamate, and GABA on cocaine-seeking behavior",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07142857142857142,
          "p": 0.17543859649122806,
          "f": 0.10152283852714593
        },
        "rouge-2": {
          "r": 0.0041841004184100415,
          "p": 0.012048192771084338,
          "f": 0.0062111762977917935
        },
        "rouge-l": {
          "r": 0.06428571428571428,
          "p": 0.15789473684210525,
          "f": 0.09137055426318656
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01448v1",
      "true_abstract": "This paper introduces Agency-Driven Labor Theory as a new theoretical\nframework for understanding human work in AI-augmented environments. While\ntraditional labor theories have focused primarily on task execution and labor\ntime, ADLT proposes that human labor value is increasingly derived from agency\n- the capacity to make informed judgments, provide strategic direction, and\ndesign operational frameworks for AI systems. The paper presents a mathematical\nframework expressing labor value as a function of agency quality, direction\neffectiveness, and outcomes, providing a quantifiable approach to analyzing\nhuman value creation in AI-augmented workplaces. Drawing on recent work in\norganizational economics and knowledge worker productivity, ADLT explains how\nhuman workers create value by orchestrating complex systems that combine human\nand artificial intelligence. The theory has significant implications for job\ndesign, compensation structures, professional development, and labor market\ndynamics. Through applications across various sectors, the paper demonstrates\nhow ADLT can guide organizations in managing the transition to AI-augmented\noperations while maximizing human value creation. The framework provides\npractical tools for policymakers and educational institutions as they prepare\nworkers for a labor market where value creation increasingly centers on agency\nand direction rather than execution.",
      "generated_abstract": "This paper explores the interplay between innovation and international\ntrade using a two-sector model of a developed economy and a developing\neconomy. The model features an innovation-driven technology spillover from a\ndeveloped economy to a developing one, which leads to the development of a\ncompetitive industry. The competitive industry's innovation is exported to the\ndeveloped economy, leading to an increase in the developed economy's\ninternational trade. The developed economy's innovation is exported to the\ndeveloping economy, leading to an increase in the developing economy's\ninternational trade. The model also features a trade barrier. The trade barrier\nis raised from zero to a significant level to induce the developed economy to\nimpose an export tax on the exported product from the developing economy. The\ndeveloped economy uses the export tax to offset the increased import of the\ndeveloping",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.104,
          "p": 0.23214285714285715,
          "f": 0.14364640456640532
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.009900990099009901,
          "f": 0.007142852530870326
        },
        "rouge-l": {
          "r": 0.104,
          "p": 0.23214285714285715,
          "f": 0.14364640456640532
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.hep-ex/2503.10521v1",
      "true_abstract": "The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.",
      "generated_abstract": "We present the first detector-level measurements of the charged-particle\nspectra and shower development of cosmic-ray air showers generated with the\nMu2e detector simulation software. The mu2e detector was operated in the 2024\nrun of the Fermi National Accelerator Laboratory (Fermilab) Tevatron collider\nand in the 2025 run of the Large Hadron Collider (LHC). The detector was\noperated in the mu2e 2025-04-01-001 run, which was a 2000-muon-hour run at the\nLHC, and in the mu2e 2025-04-01-002 run, which was a 1000-muon-hour run at the\nTevatron. The mu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.16,
          "f": 0.13913042986767502
        },
        "rouge-2": {
          "r": 0.03614457831325301,
          "p": 0.043478260869565216,
          "f": 0.03947367925294383
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.16,
          "f": 0.13913042986767502
        }
      }
    },
    {
      "paper_id": "physics.data-an.physics/data-an/2503.09771v1",
      "true_abstract": "We propose a method of estimating the uncertainty of a result obtained\nthrough extrapolation to the complete basis set limit. The method is based on\nan ensemble of random walks which simulate all possible extrapolation outcomes\nthat could have been obtained if results from larger basis sets had been\navailable. The results assembled from a large collection of random walks can be\nthen analyzed statistically, providing a route for uncertainty prediction at a\nconfidence level required in a particular application. The method is free of\nempirical parameters and compatible with any extrapolation scheme. The proposed\ntechnique is tested in a series of numerical trials by comparing the determined\nconfidence intervals with reliable reference data. We demonstrate that the\npredicted error bounds are reliable, tight, yet conservative at the same time.",
      "generated_abstract": "This paper introduces a novel approach to identify the most informative\ndata features for the classification of different classes of nonlinear\npartial differential equations (PDEs). We formulate the problem as a\nmulti-classifier ensemble learning problem, which is tackled by a\nmultilayer perceptron (MLP) with two hidden layers, and a binary classification\nlayer. We propose a new approach to construct the MLP by integrating the\nproposed feature selection methodology. This approach is demonstrated through\nnumerical experiments using the three-dimensional Navier-Stokes equations,\ndiscretized using the finite element method, and the nonlinear Schr\\\"odinger\nequation, discretized using the finite difference method. The results show\nthat the proposed approach is able to identify the most informative features\nfor the classification of different classes of PDEs. The approach is also\napplicable to the classification of the flow of other fluids",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22826086956521738,
          "p": 0.25925925925925924,
          "f": 0.24277456149420304
        },
        "rouge-2": {
          "r": 0.04032258064516129,
          "p": 0.044642857142857144,
          "f": 0.04237287636886011
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.24691358024691357,
          "f": 0.23121386785258458
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.09434v1",
      "true_abstract": "We derive nonlinear stability results for numerical integrators on Riemannian\nmanifolds, by imposing conditions on the ODE vector field and the step size\nthat makes the numerical solution non-expansive whenever the exact solution is\nnon-expansive over the same time step. Our model case is a geodesic version of\nthe explicit Euler method. Precise bounds are obtained in the case of\nRiemannian manifolds of constant sectional curvature. The approach is based on\na cocoercivity property of the vector field adapted to manifolds from Euclidean\nspace. It allows us to compare the new results to the corresponding well-known\nresults in flat spaces, and in general we find that a non-zero curvature will\ndeteriorate the stability region of the geodesic Euler method. The step size\nbounds depend on the distance traveled over a step from the initial point.\nNumerical examples for spheres and hyperbolic 2-space confirm that the bounds\nare tight.",
      "generated_abstract": "The present paper investigates the evolution of the numerical solutions of\nthe three-dimensional (3D) Navier-Stokes equations subject to non-linear\nviscous stress tensor. In particular, we focus on the impact of the\nviscosity-to-stress coefficient, $\\mu$, on the flow. We perform a series of\nnumerical experiments on both the two- and three-dimensional (2D and 3D)\nscenarios. Our findings reveal that the impact of the non-linear viscous\nstress tensor on the numerical solutions is significant. In particular, the\nhigher the non-linear viscosity is, the more pronounced the difference in\nnumerical results is compared to the linear case. Moreover, we demonstrate that\nthe effect of the non-linear viscosity on the numerical solutions is more\npronounced in the three-dimensional scenarios compared to the two-dimensional\nscen",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.25806451612903225,
          "f": 0.21333332848355566
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.06451612903225806,
          "f": 0.051502141126195444
        },
        "rouge-l": {
          "r": 0.17045454545454544,
          "p": 0.24193548387096775,
          "f": 0.19999999515022235
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2407.18572v1",
      "true_abstract": "An approach to amputation, the process of introducing missing values to a\ncomplete dataset, is presented. It allows to construct missingness indicators\nin a flexible and principled way via copulas and Bernoulli margins and to\nincorporate dependence in missingness patterns. Besides more classical\nmissingness models such as missing completely at random, missing at random, and\nmissing not at random, the approach is able to model structured missingness\nsuch as block missingness and, via mixtures, monotone missingness, which are\npatterns of missing data frequently found in real-life datasets. Properties\nsuch as joint missingness probabilities or missingness correlation are derived\nmathematically. The approach is demonstrated with mathematical examples and\nempirical illustrations in terms of a well-known dataset.",
      "generated_abstract": "This paper presents a novel framework for the analysis of dynamic\ndistributions, focusing on the temporal evolution of their moments. The\nframework is based on a generalization of the Markov-chain Monte Carlo\n(MCMC) method, which is called the Markov-chain-based Moment (MCM) method.\nTheoretical properties of the method are discussed, including its\nasymptotic convergence and the existence of a unique stationary distribution.\nThe framework is applied to the analysis of dynamic distributions over time,\nincluding the analysis of the distribution of the time between two consecutive\nevents in the context of temporal data analysis and the estimation of the\ncumulative distribution function of a dynamic distribution over time. The\nframework is implemented using the Metropolis-Hastings (MH) algorithm to\ngenerate simulated data from dynamic distributions, and is tested on simulated\ndata generated from simulated and real-world dynamic distributions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14864864864864866,
          "p": 0.1506849315068493,
          "f": 0.1496598589458098
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.008547008547008548,
          "f": 0.008849552528391884
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.136986301369863,
          "f": 0.13605441676893906
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.13140v2",
      "true_abstract": "Tensor decomposition has emerged as a powerful framework for feature\nextraction in multi-modal biomedical data. In this review, we present a\ncomprehensive analysis of tensor decomposition methods such as Tucker,\nCANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse\napplications across biomedical domains such as imaging, multi-omics, and\nspatial transcriptomics. To systematically investigate the literature, we\napplied a topic modeling-based approach that identifies and groups distinct\nthematic sub-areas in biomedicine where tensor decomposition has been used,\nthereby revealing key trends and research directions. We evaluated challenges\nrelated to the scalability of latent spaces along with obtaining the optimal\nrank of the tensor, which often hinder the extraction of meaningful features\nfrom increasingly large and complex datasets. Additionally, we discuss recent\nadvances in quantum algorithms for tensor decomposition, exploring how quantum\ncomputing can be leveraged to address these challenges. Our study includes a\npreliminary resource estimation analysis for quantum computing platforms and\nexamines the feasibility of implementing quantum-enhanced tensor decomposition\nmethods on near-term quantum devices. Collectively, this review not only\nsynthesizes current applications and challenges of tensor decomposition in\nbiomedical analyses but also outlines promising quantum computing strategies to\nenhance its impact on deriving actionable insights from complex biomedical\ndata.",
      "generated_abstract": "The study of the mechanisms that govern the transition between normal\nresidual and tumor states is crucial for the development of precision\ntherapeutics. In this context, we present a novel approach that integrates\nneural network models and a dynamic molecular model to investigate the\ndynamics of tumor progression in the context of a patient's healthy tissues.\nThe approach is based on the assumption that the dynamic interactions between\nthe patient's tumor and its surrounding tissues influence the evolution of the\ntumor. The models are trained on a set of real data from a patient with a\nsolid tumor, and the performance of the model is evaluated by comparing its\nprediction to the observed tumor progression. The results of the study show that\nthe model's predictions are in good agreement with the observed dynamics,\nhighlighting its potential for guiding therapeutic strategies in the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16176470588235295,
          "p": 0.275,
          "f": 0.20370369903978067
        },
        "rouge-2": {
          "r": 0.03763440860215054,
          "p": 0.05223880597014925,
          "f": 0.04374999513203179
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.2125,
          "f": 0.15740740274348436
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.14069v2",
      "true_abstract": "We study the problem of estimating the barycenter of a distribution given\ni.i.d. data in a geodesic space. Assuming an upper curvature bound in\nAlexandrov's sense and a support condition ensuring the strong geodesic\nconvexity of the barycenter problem, we establish finite-sample error bounds in\nexpectation and with high probability. Our results generalize Hoeffding- and\nBernstein-type concentration inequalities from Euclidean to geodesic spaces.\nBuilding on these concentration inequalities, we derive statistical guarantees\nfor two efficient algorithms for the computation of barycenters.",
      "generated_abstract": "This paper considers the design of parametric and non-parametric\nestimators for the marginal and conditional distributions of a functional\nvariable. The estimators are based on the least squares estimator of the\nconditional expectation and are asymptotically normal in a distributional sense\nunder certain assumptions. We also consider the case of non-normal data and\ndesign non-parametric estimators based on the least squares estimator of the\nconditional expectation. The asymptotic normality of these estimators is\nproved. We present simulation results to assess the performance of the\nestimators.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.28,
          "f": 0.247787605685645
        },
        "rouge-2": {
          "r": 0.06172839506172839,
          "p": 0.0684931506849315,
          "f": 0.06493505994855832
        },
        "rouge-l": {
          "r": 0.19047619047619047,
          "p": 0.24,
          "f": 0.21238937559714943
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.05087v2",
      "true_abstract": "This paper studies the formation of the grand coalition of a cooperative game\nby investigating its possible internal dynamics. Each coalition is capable of\nforcing all players to reconsider the current state of the game when it does\nnot provide sufficient payoff. Different coalitions may ask for contradictory\nevolutions, leading to the impossibility of the grand coalition forming. In\nthis paper, we give a characterization of the impossibility, for a given state,\nof finding a new state dominating the previous one such that each aggrieved\ncoalition has a satisfactory payoff. To do so, we develop new polyhedral tools\nrelated to a new family of polyhedra, appearing in numerous situations in\ncooperative game theory.",
      "generated_abstract": "We study the problem of allocating a fixed number of units of a good to a\nallocator who can assign them to any number of agents. We assume that the\nnumber of agents is infinite, and that the agent's utility is bounded below by\nzero. We show that this problem is PSPACE-hard even when the number of\nagents is $O(1)$. We also show that, even when the number of agents is\nexponential in $n$, the problem is PSPACE-hard for $n=O(1)$ and PSPACE-complete\nfor $n=\\omega(1)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14102564102564102,
          "p": 0.24444444444444444,
          "f": 0.17886178397779115
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.030303030303030304,
          "f": 0.023391808125578062
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.2,
          "f": 0.14634145877453913
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/RM/2503.06806v1",
      "true_abstract": "An important step in the Financial Benchmarks Reform was taken on 13th\nSeptember 2018, when the ECB Working Group on Euro Risk-Free Rates recommended\nthe Euro Short-Term Rate ESTR as the new benchmark rate for the euro area, to\nreplace the Euro OverNight Index Average (EONIA) which will be discontinued at\nthe end of 2021. This transition has a number of important consequences on\nfinancial instruments, OTC derivatives in particular.\n  In this paper we show in detail how the switch from EONIA to ESTR affects the\npricing of OIS, IRS and XVAs. We conclude that the adoption of the \"clean\ndiscounting\" approach recommended by the the ECB, based on ESTR only, is\ntheoretically sound and leads to very limited impacts on financial valuations.\n  This finding ensures the possibility, for the financial industry, to switch\nall EUR OTC derivatives, either cleared with Central Counterparties, or subject\nto bilateral collateral agreements, or non-collateralised, in a safe and\nconsistent manner. The transition to such EONIA-free pricing framework is\nessential for the complete elimination of EONIA before its discontinuation\nscheduled on 31st December 2021.",
      "generated_abstract": "We introduce a novel stochastic volatility model for equity returns based on\nthe Black-Scholes model and the Hull-White model. The model is based on a\nrepresentation of the log-return as the sum of an asset-specific log-return and\na stochastic volatility term, which can be interpreted as the market-implied\nvolatility of the log-return. This approach leads to a model that is more\nindependent of the underlying volatility model than the Black-Scholes model,\nwhile maintaining the same level of volatility consistency as the Black-Scholes\nmodel. The model is based on the solution of a stochastic differential\nequation (SDE) and is parameterized by a Black-Scholes-like volatility\nstructure. The model is used to analyze the dynamics of the log-return and its\nimpact on stock returns. We also compare the performance of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15873015873015872,
          "p": 0.31746031746031744,
          "f": 0.2116402071957673
        },
        "rouge-2": {
          "r": 0.022857142857142857,
          "p": 0.038834951456310676,
          "f": 0.02877697375265331
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.2698412698412698,
          "f": 0.17989417544973554
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.08920v1",
      "true_abstract": "A distributed integrated sensing and communication (D-ISAC) system offers\nsignificant cooperative gains for both sensing and communication performance.\nThese gains, however, can only be fully realized when the distributed nodes are\nperfectly synchronized, which is a challenge that remains largely unaddressed\nin current ISAC research. In this paper, we propose an over-the-air\ntime-frequency synchronization framework for the D-ISAC system, leveraging the\nreciprocity of bistatic sensing channels. This approach overcomes the\nimpractical dependency of traditional methods on a direct line-of-sight (LoS)\nlink, enabling the estimation of time offset (TO) and carrier frequency offset\n(CFO) between two ISAC nodes even in non-LoS (NLOS) scenarios. To achieve this,\nwe introduce a bistatic signal matching (BSM) technique with delay-Doppler\ndecoupling, which exploits offset reciprocity (OR) in bistatic observations.\nThis method compresses multiple sensing links into a single offset for\nestimation. We further present off-grid super-resolution estimators for TO and\nCFO, including the maximum likelihood estimator (MLE) and the matrix pencil\n(MP) method, combined with BSM processing. These estimators provide accurate\noffset estimation compared to spectral cross-correlation techniques. Also, we\nextend the pairwise synchronization leveraging OR between two nodes to the\nsynchronization of $N$ multiple distributed nodes, referred to as centralized\npairwise synchronization. We analyze the Cramer-Rao bounds (CRBs) for TO and\nCFO estimates and evaluate the impact of D-ISAC synchronization on the\nbottom-line target localization performance. Simulation results validate the\neffectiveness of the proposed algorithm, confirm the theoretical analysis, and\ndemonstrate that the proposed synchronization approach can recover up to 96% of\nthe bottom-line target localization performance of the fully-synchronous\nD-ISAC.",
      "generated_abstract": "In this paper, we propose a novel deep learning (DL) framework for non-orthogonality\nprediction (NOP) in wireless networks. The DL framework uses a multi-layer\nfeedforward neural network (MFNN) to model the interference and pathloss\ndistributions. The MFNN learns these distributions from historical data using\na multi-task learning approach. The MFNN is integrated with a convolutional\nneural network (CNN) to predict the non-orthogonality indices of interference\nand pathloss. The proposed framework is evaluated using the IEEE 802.11e\ninterference-and-pathloss measurements, which are collected from 1610 MHz\nchannels in the U.S. The results show that the proposed DL framework achieves\nstate-of-the-art performance in terms of the root mean square error (RMSE) and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14545454545454545,
          "p": 0.3116883116883117,
          "f": 0.19834710309917364
        },
        "rouge-2": {
          "r": 0.03292181069958848,
          "p": 0.0761904761904762,
          "f": 0.045977007280519615
        },
        "rouge-l": {
          "r": 0.1393939393939394,
          "p": 0.2987012987012987,
          "f": 0.1900826402892563
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/hist-ph/2503.07666v1",
      "true_abstract": "The correspondence principle states that classical mechanics emerges from\nquantum mechanics in the appropriate limits. However, beyond this heuristic\nrule, an information-theoretic perspective reveals that classical mechanics is\na compressed, lower-information representation of quantum reality. Quantum\nmechanics encodes significantly more information through superposition,\nentanglement, and phase coherence, which are lost due to decoherence, phase\naveraging, and measurement, reducing the system to a classical probability\ndistribution. This transition is quantified using Kolmogorov complexity, where\nclassical systems require \\( O(N) \\) bits of information, while quantum\ndescriptions require \\( O(2^N) \\), showing an exponential reduction in\ncomplexity. Further justification comes from Ehrenfest's theorem, which ensures\nthat quantum expectation values obey Newton's laws, and path integral\nsuppression, which eliminates non-classical trajectories when \\( S \\gg \\hbar\n\\). Thus, rather than viewing quantum mechanics as an extension of classical\nmechanics, we argue that classical mechanics is a lossy, computationally\nreduced encoding of quantum physics, emerging from a systematic loss of quantum\ncorrelations.",
      "generated_abstract": "The theory of quantum error-correcting codes (QECCs) has been the backbone\nof information processing for over 50 years. This article reviews the\nfoundational principles of QECCs and their applications in quantum computing.\nWe show how these foundations are used to derive a new class of codes known as\nquantum error-correcting codes (QECCs). We also explore the potential of QECCs\nin quantum computing, including their potential as an error-correction layer\nand a key building block for quantum architectures. Finally, we discuss the\nchallenges and future directions of this emerging area of quantum computing.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14035087719298245,
          "p": 0.25396825396825395,
          "f": 0.18079095586708813
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.023809523809523808,
          "f": 0.017316012687919376
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.23809523809523808,
          "f": 0.16949152083883953
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/NA/2503.09892v1",
      "true_abstract": "As inverter-based resources (IBRs) penetrate power systems, the dynamics\nbecome more complex, exhibiting multiple timescales, including electromagnetic\ntransient (EMT) dynamics of power electronic controllers and electromechanical\ndynamics of synchronous generators. Consequently, the power system model\nbecomes highly stiff, posing a challenge for efficient simulation using\nexisting methods that focus on dynamics within a single timescale. This paper\nproposes a Heterogeneous Multiscale Method for highly efficient multi-timescale\nsimulation of a power system represented by its EMT model. The new method\nalternates between the microscopic EMT model of the system and an automatically\nreduced macroscopic model, varying the step size accordingly to achieve\nsignificant acceleration while maintaining accuracy in both fast and slow\ndynamics of interests. It also incorporates a semi-analytical solution method\nto enable a more adaptive variable-step mechanism. The new simulation method is\nillustrated using a two-area system and is then tested on a detailed EMT model\nof the IEEE 39-bus system.",
      "generated_abstract": "We consider the problem of estimating the mean of a Gaussian process (GP)\nfrom a limited number of samples. This is a fundamental task in machine\nlearning and signal processing, where the goal is to infer the mean of the\nunderlying unknown process. The mean estimation problem is of particular\ninterest in the context of radar, where the goal is to estimate the signal\namplitude from a limited number of samples. In radar, the signal amplitude is\noften represented as a multivariate Gaussian process (GP) with known mean and\ncovariance. In this work, we propose a novel approach for the mean estimation\nproblem, which we call GP-MAP, to estimate the mean of a GP. Our approach\ncombines a gradient descent algorithm with a localization strategy to ensure\nthat the estimated mean is close to the true mean. The algorithm is\ncomputationally efficient and can be implemented in a hardware-friendly way,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12745098039215685,
          "p": 0.16666666666666666,
          "f": 0.1444444395333335
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.015748031496062992,
          "f": 0.01486988349138515
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.15384615384615385,
          "f": 0.1333333284222224
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PR/2501.05232v1",
      "true_abstract": "Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.",
      "generated_abstract": "This paper examines the relationship between the S&P 500 index and the\ncryptocurrency market. We find that the S&P 500 index has a negative\nrelationship with cryptocurrencies, with a negative correlation of -0.35.\nMoreover, the correlation is stronger when the index is trading lower, and\nstronger when the index is trading higher. We also find that the correlation\ndecreases when the cryptocurrency market is more volatile. We provide a\ndynamic model that incorporates market sentiment and volatility to explain\nthis phenomenon. Finally, we examine the relationship between the index and\nspecific cryptocurrencies, and find that the correlation between the index and\ncryptocurrencies is stronger when the index is trading higher. We also find\nthat the correlation between the index and specific cryptocurrencies decreases\nwhen the index is more volatile",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.28,
          "f": 0.22580644680020823
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.0379746835443038,
          "f": 0.03448275366296809
        },
        "rouge-l": {
          "r": 0.17567567567567569,
          "p": 0.26,
          "f": 0.20967741454214375
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/GN/2411.13762v1",
      "true_abstract": "This paper delves into the spectrum of credit risks associated with\ndecentralized stablecoin issuance, ranging from overcollateralized lending to\nbusiness-to-business credit. It examines the mechanisms, risks, and mitigation\nstrategies at each layer, highlighting the potential for scaling decentralized\nstablecoins while ensuring systemic health.",
      "generated_abstract": "In this paper, we present an algorithm that can be used to identify\nprospective investment opportunities by analyzing the data of a portfolio of\nstocks. The algorithm, based on the kernel-based machine learning methodology,\nuses the kernel ridge regression to predict the stock's price using a\ncombination of information from the past, current and future stocks. The\nalgorithm is implemented on a large dataset of 1,100 stocks that is\ncomprehensive, diverse, and high-quality. The results of the proposed algorithm\nshow that it is able to identify potential investment opportunities that are\nworth further analysis. It is also shown that the algorithm's performance\nimproves as the number of stocks in the dataset increases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.09333333333333334,
          "f": 0.12280701304247477
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.09333333333333334,
          "f": 0.12280701304247477
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.13395v1",
      "true_abstract": "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
      "generated_abstract": "The rise of synthetic audio data, combined with the emergence of large\nsynthetic audio datasets, has fueled a boom in generative audio models. This\nincrease in accessibility has also led to a proliferation of research papers\npresenting novel approaches to generating audio. However, existing work often\nfocuses on a limited set of audio tasks, limiting their practical\napplicability. To address this, we introduce the \\textbf{GENASynth}, a\ncomprehensive benchmark that covers a wide range of audio generation tasks. The\nGENASynth benchmark includes audio datasets, evaluation metrics, and\narchitectures, enabling researchers to compare their models' performance across\ndiverse tasks. By providing a comprehensive dataset and evaluation framework,\nGENASynth aims to foster the development of robust, efficient, and\ntransferable generative audio models. We provide a detailed description of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12716763005780346,
          "p": 0.2558139534883721,
          "f": 0.16988416544833868
        },
        "rouge-2": {
          "r": 0.008547008547008548,
          "p": 0.01680672268907563,
          "f": 0.011331440289868466
        },
        "rouge-l": {
          "r": 0.12138728323699421,
          "p": 0.2441860465116279,
          "f": 0.16216215772633097
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/BM/2502.12638v2",
      "true_abstract": "3D molecule generation is crucial for drug discovery and material design.\nWhile prior efforts focus on 3D diffusion models for their benefits in modeling\ncontinuous 3D conformers, they overlook the advantages of 1D SELFIES-based\nLanguage Models (LMs), which can generate 100% valid molecules and leverage the\nbillion-scale 1D molecule datasets. To combine these advantages for 3D molecule\ngeneration, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D\nLanguage Modeling for 3D Molecule Generation. NExT-Mol uses an extensively\npretrained molecule LM for 1D molecule generation, and subsequently predicts\nthe generated molecule's 3D conformers with a 3D diffusion model. We enhance\nNExT-Mol's performance by scaling up the LM's model size, refining the\ndiffusion neural architecture, and applying 1D to 3D transfer learning.\nNotably, our 1D molecule LM significantly outperforms baselines in\ndistributional similarity while ensuring validity, and our 3D diffusion model\nachieves leading performances in conformer prediction. Given these improvements\nin 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD\nfor de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for\nconditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are\navailable at https://github.com/acharkq/NExT-Mol.",
      "generated_abstract": "Neuronal activity is a key component of the brain's information processing\nsystem, yet the mechanisms underlying this dynamic behavior remain poorly\nunderstood. In this work, we propose a neural network model that integrates\ninformation from multiple electroencephalography (EEG) signals to generate\nunsupervised clustering results, thereby capturing the inter-neuronal\ninteractions and the brain's overall activity patterns. Our model leverages\ndeep learning techniques to learn from multi-modal EEG data and integrate\ninformation from both time-ordered and time-disjoint signals. It achieves\nsuperior clustering performance compared to existing neural network-based\nclustering approaches, such as the K-Means algorithm and the Hierarchical\nAgglomerative Clustering algorithm. Our model also demonstrates superior\nperformance when applied to EEG data from healthy subjects, revealing the\nassociation between specific brain regions and certain",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.17045454545454544,
          "f": 0.1408450655734093
        },
        "rouge-2": {
          "r": 0.011049723756906077,
          "p": 0.017241379310344827,
          "f": 0.013468008707503183
        },
        "rouge-l": {
          "r": 0.112,
          "p": 0.1590909090909091,
          "f": 0.131455394211907
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.stat/AP/2503.03990v1",
      "true_abstract": "Accurately quantifying air-sea fluxes is important for understanding air-sea\ninteractions and improving coupled weather and climate systems. This study\nintroduces a probabilistic framework to represent the highly variable nature of\nair-sea fluxes, which is missing in deterministic bulk algorithms. Assuming\nGaussian distributions conditioned on the input variables, we use artificial\nneural networks and eddy-covariance measurement data to estimate the mean and\nvariance by minimizing negative log-likelihood loss. The trained neural\nnetworks provide alternative mean flux estimates to existing bulk algorithms,\nand quantify the uncertainty around the mean estimates. Stochastic\nparameterization of air-sea turbulent fluxes can be constructed by sampling\nfrom the predicted distributions. Tests in a single-column forced upper-ocean\nmodel suggest that changes in flux algorithms influence sea surface temperature\nand mixed layer depth seasonally. The ensemble spread in stochastic runs is\nmost pronounced during spring restratification.",
      "generated_abstract": "We investigate the sensitivity of the mean sea level pressure to changes in\nthe phase of the atmospheric pressure in the Northern Hemisphere. We find that\nchanges in the phase of the pressure, which affects the intensity of the\natmospheric circulation, have a large impact on the mean sea level pressure,\nwith an increase of about 2.6 hPa for every 10\u00b0 of phase shift. This effect is\nmore pronounced in the tropics. We also find that the sensitivity is\ntemperature-dependent, with a larger sensitivity for cooler temperatures.\nThese findings highlight the importance of phase-dependent atmospheric\ncirculation in the global mean sea level.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14563106796116504,
          "p": 0.2631578947368421,
          "f": 0.18749999541328136
        },
        "rouge-2": {
          "r": 0.03007518796992481,
          "p": 0.047619047619047616,
          "f": 0.03686635470194799
        },
        "rouge-l": {
          "r": 0.1262135922330097,
          "p": 0.22807017543859648,
          "f": 0.1624999954132814
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/CR/2503.09727v1",
      "true_abstract": "Knowledge graph reasoning (KGR), which answers complex, logical queries over\nlarge knowledge graphs (KGs), represents an important artificial intelligence\ntask with a range of applications. Many KGs require extensive domain expertise\nand engineering effort to build and are hence considered proprietary within\norganizations and enterprises. Yet, spurred by their commercial and research\npotential, there is a growing trend to make KGR systems, (partially) built upon\nprivate KGs, publicly available through reasoning APIs.\n  The inherent tension between maintaining the confidentiality of KGs while\nensuring the accessibility to KGR systems motivates our study of KG extraction\nattacks: the adversary aims to \"steal\" the private segments of the backend KG,\nleveraging solely black-box access to the KGR API. Specifically, we present\nKGX, an attack that extracts confidential sub-KGs with high fidelity under\nlimited query budgets. At a high level, KGX progressively and adaptively\nqueries the KGR API and integrates the query responses to reconstruct the\nprivate sub-KG. This extraction remains viable even if any query responses\nrelated to the private sub-KG are filtered. We validate the efficacy of KGX\nagainst both experimental and real-world KGR APIs. Interestingly, we find that\ntypical countermeasures (e.g., injecting noise into query responses) are often\nineffective against KGX. Our findings suggest the need for a more principled\napproach to developing and deploying KGR systems, as well as devising new\ndefenses against KG extraction attacks.",
      "generated_abstract": "This paper introduces a novel approach for online data collection by\nusing a simple and intuitive approach, i.e. the user just needs to click on a\ncertain image, and the system will automatically record the user's voice and\nvideo. This method has the following advantages: 1) It is easy to use and\neffective in collecting data, 2) it can be used for various tasks, 3) it can\nbe used for both online and offline recording, and 4) it can be used in any\nenvironment, including the Internet, smart phones, and even the cloud. This\nstudy also introduces a new method for online data collection using images and\nvideos, and shows the feasibility of using this method in various fields.\nFurthermore, the feasibility of using the system in the context of education\nwill be demonstrated.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09259259259259259,
          "p": 0.18518518518518517,
          "f": 0.12345678567901248
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07407407407407407,
          "p": 0.14814814814814814,
          "f": 0.09876542765432118
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CE/2503.06647v1",
      "true_abstract": "Accurate food intake monitoring is crucial for maintaining a healthy diet and\npreventing nutrition-related diseases. With the diverse range of foods consumed\nacross various cultures, classic food classification models have limitations\ndue to their reliance on fixed-sized food datasets. Studies show that people\nconsume only a small range of foods across the existing ones, each consuming a\nunique set of foods. Existing class-incremental models have low accuracy for\nthe new classes and lack personalization. This paper introduces a personalized,\nclass-incremental food classification model designed to overcome these\nchallenges and improve the performance of food intake monitoring systems. Our\napproach adapts itself to the new array of food classes, maintaining\napplicability and accuracy, both for new and existing classes by using\npersonalization. Our model's primary focus is personalization, which improves\nclassification accuracy by prioritizing a subset of foods based on an\nindividual's eating habits, including meal frequency, times, and locations. A\nmodified version of DSN is utilized to expand on the appearance of new food\nclasses. Additionally, we propose a comprehensive framework that integrates\nthis model into a food intake monitoring system. This system analyzes meal\nimages provided by users, makes use of a smart scale to estimate food weight,\nutilizes a nutrient content database to calculate the amount of each\nmacro-nutrient, and creates a dietary user profile through a mobile\napplication. Finally, experimental evaluations on two new benchmark datasets\nFOOD101-Personal and VFN-Personal, personalized versions of well-known datasets\nfor food classification, are conducted to demonstrate the effectiveness of our\nmodel in improving the classification accuracy of both new and existing\nclasses, addressing the limitations of both conventional and class-incremental\nfood classification models.",
      "generated_abstract": "We introduce a novel method for generating synthetic audio in a multimodal\nenvironment, where both the image and audio modalities are generated simultaneously\nusing a latent-space approach. Our method consists of three stages: 1)\ngenerating a synthetic image, 2) generating a synthetic audio via a generative\nmodel, and 3) merging the audio and image to generate the final audio-image\npair. We propose a novel multimodal audio-image alignment loss to align the\ngenerated image and audio. Additionally, we propose a novel image-audio\ntransformation loss to improve the image quality of the generated audio.\nExperimental results show that our method outperforms existing approaches in\naudio quality and image quality. Code will be released upon publication.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12578616352201258,
          "p": 0.30303030303030304,
          "f": 0.17777777363200006
        },
        "rouge-2": {
          "r": 0.0199203187250996,
          "p": 0.050505050505050504,
          "f": 0.028571424514449554
        },
        "rouge-l": {
          "r": 0.11320754716981132,
          "p": 0.2727272727272727,
          "f": 0.15999999585422234
        }
      }
    },
    {
      "paper_id": "cs.FL.cs/FL/2503.05006v1",
      "true_abstract": "Markov decision process over vector addition system with states (VASS MDP) is\na finite state model combining non-deterministic and probabilistic behavior,\naugmented with non-negative integer counters that can be incremented or\ndecremented during each state transition. VASS MDPs can be used as abstractions\nof probabilistic programs with many decidable properties. In this paper, we\ndevelop techniques for analyzing the asymptotic behavior of VASS MDPs. That is,\nfor every initial configuration of size \\(n\\), we consider the number of\ntransitions needed to reach a configuration with some counter negative. We show\nthat given a strongly connected VASS MDP there either exists an integer \\(k\\leq\n2^d\\cdot 3^{|T|} \\), where \\(d \\) is the dimension and \\(|T|\\) the number of\ntransitions of the VASS MDP, such that for all \\(\\epsilon>0 \\) and all\nsufficiently large \\(n\\) it holds that the complexity of the VASS MDP lies\nbetween \\(n^{k-\\epsilon} \\) and \\(n^{k+\\epsilon} \\) with probability at least\n\\(1-\\epsilon \\), or it holds for all \\(\\epsilon>0 \\) and all sufficiently large\n\\(n\\) that the complexity of the VASS MDP is at least \\(2^{n^{1-\\epsilon}} \\)\nwith probability at least \\(1-\\epsilon \\). We show that it is decidable which\ncase holds and the \\(k\\) is computable in time polynomial in the size of the\nconsidered VASS MDP. We also provide a full classification of asymptotic\ncomplexity for VASS Markov chains.",
      "generated_abstract": "This paper introduces the Fuzzy Linear Autoencoder (FLAE) framework, which\nis a novel approach for fuzzy modeling and prediction. The FLAE architecture\nconsists of two key components: (i) a fuzzy autoencoder, which encodes the\ninput data into a fuzzy vector space, and (ii) a fuzzy classification layer,\nwhich processes the fuzzy vectors to predict the target variable. The\nencoder-decoder architecture of the fuzzy autoencoder enables it to learn\nfuzzy representations that are more interpretable than standard representations.\nThe fuzzy classification layer is designed to leverage the fuzzy semantics of\nthe input data to generate more accurate predictions. The performance of the\nframework is evaluated through several experiments, including classification\nand regression tasks. The results show that the FLAS model outperforms the\nstate-of-the-art approaches",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11475409836065574,
          "p": 0.17721518987341772,
          "f": 0.1393034778158958
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.035398230088495575,
          "f": 0.02657806840012886
        },
        "rouge-l": {
          "r": 0.09836065573770492,
          "p": 0.1518987341772152,
          "f": 0.11940298030345801
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.06308v1",
      "true_abstract": "Computable phenotypes are used to characterize patients and identify outcomes\nin studies conducted using healthcare claims and electronic health record data.\nChart review studies establish reference labels against which computable\nphenotypes are compared to understand their measurement characteristics, the\nquantity of interest, for instance the positive predictive value. We describe a\nmethod to adaptively evaluate a quantity of interest over sequential samples of\ncharts, with the goal to minimize the number of charts reviewed. With the help\nof a simultaneous confidence band, we stop the reviewing once the confidence\nband meets a pre-specified stopping threshold. The contribution of this article\nis threefold. First, we tested the use of an adaptive approach called Neyman's\nsampling of charts versus random or stratified random sampling. Second, we\npropose frequentist confidence bands and Bayesian credible intervals to\nsequentially evaluate the quantity of interest. Third, we propose a tool to\npredict the stopping time (defined as the number of charts reviewed) at which\nthe chart review would be complete. We observe that Bayesian credible intervals\nproved to be tighter than its frequentist confidence band counterparts.\nMoreover, we observe that simple random sampling is often performing similarly\nto Neyman's sampling.",
      "generated_abstract": "We study a novel method to predict the joint distribution of two random\nvariables. The method is based on the assumption that the two random variables\nare mutually correlated. We show that the joint distribution can be\napproximated by means of a multivariate normal distribution with the covariance\nmatrix being a simple correlation matrix. We derive the closed-form solution\nfor the mean of the multivariate normal distribution and discuss its\nproperties. Finally, we compare our method with existing methods and show that\nour method performs better in terms of the mean squared error.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.3448275862068966,
          "f": 0.21978021543774914
        },
        "rouge-2": {
          "r": 0.028409090909090908,
          "p": 0.06172839506172839,
          "f": 0.038910501519781246
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.3448275862068966,
          "f": 0.21978021543774914
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.09865v1",
      "true_abstract": "We are concerned with the likelihood ratio tests in the $p_0$ model for\ntesting degree heterogeneity in directed networks. It is an exponential family\ndistribution on directed graphs with the out-degree sequence and the in-degree\nsequence as naturally sufficient statistics. For two growing dimensional null\nhypotheses: a specified null $H_{0}: \\theta_{i}=\\theta_{i}^{0}$ for\n$i=1,\\ldots,r$ and a homogenous null $H_{0}: \\theta_{1}=\\cdots=\\theta_{r}$, we\nreveal high dimensional Wilks' phenomena that the normalized log-likelihood\nratio statistic,\n$[2\\{\\ell(\\widehat{\\bs\\theta})-\\ell(\\widehat{\\bs\\theta}^{0})\\}-r]/(2r)^{1/2}$,\nconverges in distribution to a standard normal distribution as $r\\rightarrow\n\\infty$. Here, $\\ell( \\bs{\\theta})$ is the log-likelihood function,\n$\\widehat{\\bs{\\theta}}$ is the unrestricted maximum likelihood estimator (MLE)\nof $\\bs\\theta$, and $\\widehat{\\bs{\\theta}}^0$ is the restricted MLE for\n$\\bs\\theta$ under the null $H_{0}$. For the homogenous null $H_0:\n\\theta_1=\\cdots=\\theta_r$ with a fixed $r$, we establish the Wilks-type theorem\nthat $2\\{\\ell(\\widehat{\\bs{\\theta}}) - \\ell(\\widehat{\\bs{\\theta}}^0)\\}$\nconverges in distribution to a chi-square distribution with $r-1$ degrees of\nfreedom as $n\\rightarrow \\infty$, not depending on the nuisance parameters.\nThese results extend a recent work by \\cite{yan2023likelihood} to directed\ngraphs. Simulation studies and real data analyses illustrate the theoretical\nresults.",
      "generated_abstract": "We study the estimation of the mean of a random vector, where the random\n$n\\times k$ matrix of variables has a Gaussian distribution with $k$ columns\nwithin each row. We consider two cases: when $k$ is either a constant or a\nconstant multiple of the number of columns. The first case arises when the\ncovariance matrix of the random vector is known, and the second when it is not.\nWe propose a novel estimator, based on a low-rank approximation of the\ncovariance matrix. Our estimator achieves a rate of $\\sqrt{n}$ when $k$ is a\nconstant multiple of the number of columns, and a rate of $\\sqrt{k}$ when\n$k$ is either a constant or a constant multiple of the number of columns. We\nalso provide a convergence rate of $\\sqrt{k\\log(n)}$ for the estimator when\n$k$ is a constant multiple",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11504424778761062,
          "p": 0.20634920634920634,
          "f": 0.14772726813081108
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.030303030303030304,
          "f": 0.022988501038447254
        },
        "rouge-l": {
          "r": 0.09734513274336283,
          "p": 0.1746031746031746,
          "f": 0.1249999954035384
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08970v1",
      "true_abstract": "Previous studies on echocardiogram segmentation are focused on the left\nventricle in parasternal long-axis views. In this study, deep-learning models\nwere evaluated on the segmentation of the ventricles in parasternal short-axis\nechocardiograms (PSAX-echo). Segmentation of the ventricles in complementary\nechocardiogram views will allow the computation of important metrics with the\npotential to aid in diagnosing cardio-pulmonary diseases and other\ncardiomyopathies. Evaluating state-of-the-art models with small datasets can\nreveal if they improve performance on limited data. PSAX-echo were performed on\n33 volunteer women. An experienced cardiologist identified end-diastole and\nend-systole frames from 387 scans, and expert observers manually traced the\ncontours of the cardiac structures. Traced frames were pre-processed and used\nto create labels to train 2 specific-domain (Unet-Resnet101 and Unet-ResNet50),\nand 4 general-domain (3 Segment Anything (SAM) variants, and the Detectron2)\ndeep-learning models. The performance of the models was evaluated using the\nDice similarity coefficient (DSC), Hausdorff distance (HD), and difference in\ncross-sectional area (DCSA). The Unet-Resnet101 model provided superior\nperformance in the segmentation of the ventricles with 0.83, 4.93 pixels, and\n106 pixel2 on average for DSC, HD, and DCSA respectively. A fine-tuned MedSAM\nmodel provided a performance of 0.82, 6.66 pixels, and 1252 pixel2, while the\nDetectron2 model provided 0.78, 2.12 pixels, and 116 pixel2 for the same\nmetrics respectively. Deep-learning models are suitable for the segmentation of\nthe left and right ventricles in PSAX-echo. This study demonstrated that\nspecific-domain trained models such as Unet-ResNet provide higher accuracy for\necho segmentation than general-domain segmentation models when working with\nsmall and locally acquired datasets.",
      "generated_abstract": "We introduce a novel framework for segmenting brain tumors in magnetic\nresonance imaging (MRI) images. Our approach, which combines a segmentation\nmodel with a generative model, leverages both image-level and semantic\ninformation. The segmentation model is trained using a cross-entropy loss\nfunction, which ensures that the model predicts the correct label for each\nregion. The generative model is trained using the mean squared error (MSE)\nloss function, which encourages the model to generate plausible segmentations.\nWe validate our framework on a publicly available dataset of 185 patients with\nmalignant gliomas. Our approach outperforms the state-of-the-art segmentation\nmethod, with a mean Dice score of 0.718 and a mean Dice score of 0.707 on the\ntest set. Our framework also generates plausible",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1125,
          "p": 0.23076923076923078,
          "f": 0.15126049979521233
        },
        "rouge-2": {
          "r": 0.012711864406779662,
          "p": 0.027777777777777776,
          "f": 0.01744185615738343
        },
        "rouge-l": {
          "r": 0.10625,
          "p": 0.21794871794871795,
          "f": 0.14285713845067452
        }
      }
    },
    {
      "paper_id": "q-bio.NC.eess/SP/2503.02685v1",
      "true_abstract": "Precise parcellation of functional networks (FNs) of early developing human\nbrain is the fundamental basis for identifying biomarker of developmental\ndisorders and understanding functional development. Resting-state fMRI\n(rs-fMRI) enables in vivo exploration of functional changes, but adult FN\nparcellations cannot be directly applied to the neonates due to incomplete\nnetwork maturation. No standardized neonatal functional atlas is currently\navailable. To solve this fundamental issue, we propose TReND, a novel and fully\nautomated self-supervised transformer-autoencoder framework that integrates\nregularized nonnegative matrix factorization (RNMF) to unveil the FNs in\nneonates. TReND effectively disentangles spatiotemporal features in voxel-wise\nrs-fMRI data. The framework integrates confidence-adaptive masks into\ntransformer self-attention layers to mitigate noise influence. A self\nsupervised decoder acts as a regulator to refine the encoder's latent\nembeddings, which serve as reliable temporal features. For spatial coherence,\nwe incorporate brain surface-based geodesic distances as spatial encodings\nalong with functional connectivity from temporal features. The TReND clustering\napproach processes these features under sparsity and smoothness constraints,\nproducing robust and biologically plausible parcellations. We extensively\nvalidated our TReND framework on three different rs-fMRI datasets: simulated,\ndHCP and HCP-YA against comparable traditional feature extraction and\nclustering techniques. Our results demonstrated the superiority of the TReND\nframework in the delineation of neonate FNs with significantly better spatial\ncontiguity and functional homogeneity. Collectively, we established TReND, a\nnovel and robust framework, for neonatal FN delineation. TReND-derived neonatal\nFNs could serve as a neonatal functional atlas for perinatal populations in\nhealth and disease.",
      "generated_abstract": "This study examined the effects of music on the response of rat motor\nmechanoreceptors (MMRs). Experimental set-up consisted of 100 rats with\ndistal and proximal intracutaneous (i.c.) and distal and proximal extracutaneous\n(e.c.) electrodes. Rats were exposed to either non-musical (control) or music\n(piano, jazz, and classical) for 30 minutes. Average motor responses of\ndistal-i.c. and proximal-e.c. MMRs were recorded at 50 and 100 Hz and 1000 and\n3000 Hz. Piano music produced statistically significant increases in motor\nresponses of distal-i.c. MMRs, but not of proximal-e.c. MMRs. Jazz music, on",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05421686746987952,
          "p": 0.13846153846153847,
          "f": 0.07792207387792603
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.05421686746987952,
          "p": 0.13846153846153847,
          "f": 0.07792207387792603
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/PL/2503.05849v1",
      "true_abstract": "The quality of software products tends to correlate with the quality of the\nabstractions adopted early in the design process. Acknowledging this tendency\nhas led to the development of various tools and methodologies for modeling\nsystems thoroughly before implementing them. However, creating effective\nabstract models of domain problems is difficult, especially if the models are\nalso expected to exhibit qualities such as intuitiveness, being seamlessly\nintegrable with other models, or being easily translatable into code.\n  This thesis describes Conceptual, a DSL for modeling the behavior of software\nsystems using self-contained and highly reusable units of functionally known as\nconcepts. The language's syntax and semantics are formalized based on previous\nwork. Additionally, the thesis proposes a strategy for mapping language\nconstructs from Conceptual into the Alloy modeling language. The suggested\nstrategy is then implemented with a simple compiler, allowing developers to\naccess and utilize Alloy's existing analysis tools for program reasoning.\n  The utility and expressiveness of Conceptual is demonstrated qualitatively\nthrough several practical case studies. Using the implemented compiler, a few\nerroneous specifications are identified in the literature. Moreover, the thesis\nestablishes preliminary tool support in the Visual Studio Code IDE.",
      "generated_abstract": "This paper introduces the first general framework for describing and\ntesting software that interacts with a set of heterogeneous,\nnon-interoperable services. The framework focuses on the behavior of the\nsoftware under the impact of service failures and the effect of the\nsoftware's ability to adapt to changing service environments. We call this\nframework Service-Driven Software Testing (SDST). SDST is built on the\nnotion of adaptive software that adapts to changes in the service environment\nand, thus, the software undergoes a continuous evolution. We describe how SDST\ncan be applied to different types of software, from web applications to\nenterprise systems, and illustrate how SDST helps developers improve their\nsoftware's resilience and adaptability. SDST is also useful in practice by\nproviding a systematic approach for testing software under service\nfailures. We provide an overview of SDST, the main components",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13636363636363635,
          "p": 0.2,
          "f": 0.16216215734112505
        },
        "rouge-2": {
          "r": 0.0273224043715847,
          "p": 0.0390625,
          "f": 0.03215433599239121
        },
        "rouge-l": {
          "r": 0.12878787878787878,
          "p": 0.18888888888888888,
          "f": 0.15315314833211605
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.10528v1",
      "true_abstract": "The first-order model theory of modules has been studied for decades. More\nrecently, the model theoretic study of nonelementary classes of\nmodules--especially Abstract Elementary Classes of modules--has produced\ninteresting results. This survey aims to discuss these recent results and give\nan introduction to the framework of Abstract Elementary Classes for module\ntheorists.",
      "generated_abstract": "We prove that the set of all $k$-tuples of the integers is homeomorphic to\nthe unit interval. This is the first result of its kind in the literature,\nwhereas the existence of a $k$-tuple in a topological space $X$ is a\nwell-known fact. Our proof is based on the theory of compact metric spaces\nand the theory of representations of groups, and we give a simple proof for the\ncase $k=1$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.1702127659574468,
          "f": 0.18390804100938052
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.029411764705882353,
          "f": 0.03418802931989258
        },
        "rouge-l": {
          "r": 0.15,
          "p": 0.1276595744680851,
          "f": 0.13793102951512765
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2412.13040v1",
      "true_abstract": "Epithelial tissues are continuously exposed to cyclic stretch. Physiological\nstretching has been found to regulate soft tissue function at the molecular,\ncellular, and tissue scales, allowing tissues to preserve their homeostasis and\nadapt to challenges. In contrast, dysregulated or pathological stretching can\ninduce damage and tissue fragilisation. Many mechanisms have been described for\nthe repair of epithelial tissues across a range of time-scales. In this review,\nwe present the timescales of (i) physiological cyclic loading regimes, (ii)\nstrain-regulated remodelling and damage accumulation, and (iii) repair\nmechanisms in epithelial tissues. We discuss how fatigue in biological tissues\ndiffers from synthetic materials, in that damage can be partially or fully\nreversed by repair mechanisms acting on timescales shorter than cyclic loading.\nWe highlight that timescales are critical to understanding the interplay\nbetween damage and repair in tissues that experience cyclic loading, opening up\nnew avenues for exploring soft tissue homeostasis.",
      "generated_abstract": "Membrane proteins are key components of cellular membranes and are essential\nfor cellular function. They are typically formed by a large central domain\ncontaining the catalytic site and a peripheral domain that interacts with the\nmembrane. The central domain is usually comprised of a large number of\namino-acid residues, which can be organized into multiple types of domains.\nThese types of domains, known as structural modules, are thought to play a\nrole in the functional organization of membrane proteins. The central domain of\nmany membrane proteins is organized into multiple structural modules. These\nmodules are thought to play a role in the functional organization of the\nmembrane protein. In this review, we discuss the structural modules of\nmembrane proteins, their role in the functional organization of the membrane\nprotein, and their potential for therapeutic applications. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.2777777777777778,
          "f": 0.23809523319727902
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.037383177570093455,
          "f": 0.03225805961010741
        },
        "rouge-l": {
          "r": 0.19791666666666666,
          "p": 0.2638888888888889,
          "f": 0.2261904712925171
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.13076v1",
      "true_abstract": "Machine learning predictions are typically interpreted as the sum of\ncontributions of predictors. Yet, each out-of-sample prediction can also be\nexpressed as a linear combination of in-sample values of the predicted\nvariable, with weights corresponding to pairwise proximity scores between\ncurrent and past economic events. While this dual route leads nowhere in some\ncontexts (e.g., large cross-sectional datasets), it provides sparser\ninterpretations in settings with many regressors and little training data-like\nmacroeconomic forecasting. In this case, the sequence of contributions can be\nvisualized as a time series, allowing analysts to explain predictions as\nquantifiable combinations of historical analogies. Moreover, the weights can be\nviewed as those of a data portfolio, inspiring new diagnostic measures such as\nforecast concentration, short position, and turnover. We show how weights can\nbe retrieved seamlessly for (kernel) ridge regression, random forest, boosted\ntrees, and neural networks. Then, we apply these tools to analyze post-pandemic\nforecasts of inflation, GDP growth, and recession probabilities. In all cases,\nthe approach opens the black box from a new angle and demonstrates how machine\nlearning models leverage history partly repeating itself.",
      "generated_abstract": "We propose a new method for inference on the stochastic frontier using\nthe generalized method of moments (GMOM) and the maximum likelihood estimator\n(MLE). We show that the GMOM estimator is consistent and asymptotically\nnormal, and that the MLE is consistent and asymptotically normal, when the\nheteroskedasticity is in the forward direction and the serial correlation is\nin the backward direction. We derive asymptotic covariance matrices for the\ntwo estimators and show that, under mild conditions, the covariance matrices\nmatch each other. The asymptotic covariance matrix of the GMOM estimator can\nbe expressed as a special case of the covariance matrix of the MLE. We\nintroduce a simple estimator of the covariance matrix of the GMOM estimator and\nuse it to test for serial correlation in the stochastic frontier model. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11510791366906475,
          "p": 0.24615384615384617,
          "f": 0.15686274075595938
        },
        "rouge-2": {
          "r": 0.03954802259887006,
          "p": 0.06930693069306931,
          "f": 0.050359707603902924
        },
        "rouge-l": {
          "r": 0.1079136690647482,
          "p": 0.23076923076923078,
          "f": 0.1470588191873319
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2411.09517v1",
      "true_abstract": "We study a setting where agents use no-regret learning algorithms to\nparticipate in repeated auctions. \\citet{kolumbus2022auctions} showed, rather\nsurprisingly, that when bidders participate in second-price auctions using\nno-regret bidding algorithms, no matter how large the number of interactions\n$T$ is, the runner-up bidder may not converge to bidding truthfully. Our first\nresult shows that this holds for \\emph{general deterministic} truthful\nauctions. We also show that the ratio of the learning rates of the bidders can\n\\emph{qualitatively} affect the convergence of the bidders. Next, we consider\nthe problem of revenue maximization in this environment. In the setting with\nfully rational bidders, \\citet{myerson1981optimal} showed that revenue can be\nmaximized by using a second-price auction with reserves.We show that, in stark\ncontrast, in our setting with learning bidders, \\emph{randomized} auctions can\nhave strictly better revenue guarantees than second-price auctions with\nreserves, when $T$ is large enough. Finally, we study revenue maximization in\nthe non-asymptotic regime. We define a notion of {\\em auctioneer regret}\ncomparing the revenue generated to the revenue of a second price auction with\ntruthful bids. When the auctioneer has to use the same auction throughout the\ninteraction, we show an (almost) tight regret bound of $\\smash{\\widetilde\n\\Theta(T^{3/4})}.$ If the auctioneer can change auctions during the\ninteraction, but in a way that is oblivious to the bids, we show an (almost)\ntight bound of $\\smash{\\widetilde \\Theta(\\sqrt{T})}.$",
      "generated_abstract": "In this paper, we propose a novel model of the coordination problem in\nthe context of stakeholder management. The model considers a set of stakeholders\nwho are willing to cooperate in an interdependent manner to achieve a common\ngoal. Each stakeholder is assigned a cost function that measures the\neffects of the cooperation on their individual payoffs. We formulate the\ncoordination problem as a zero-sum game, and solve it by constructing a\nrepresentative equilibrium. Our findings reveal that, under the optimal\ncoordination rule, the cooperating stakeholders can obtain a larger payoff than\ntheir individual payoffs if the individual payoffs are sufficiently large, and\nthat the cooperating stakeholders can obtain a larger payoff than their\nindividual payoffs if the individual payoffs are sufficiently small. Additionally,\nour results reveal that the payoff of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15079365079365079,
          "p": 0.25,
          "f": 0.18811880718753074
        },
        "rouge-2": {
          "r": 0.014423076923076924,
          "p": 0.02830188679245283,
          "f": 0.019108275782385727
        },
        "rouge-l": {
          "r": 0.15079365079365079,
          "p": 0.25,
          "f": 0.18811880718753074
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06913v1",
      "true_abstract": "Given a finite collection of stochastic alternatives, we study the problem of\nsequentially allocating a fixed sampling budget to identify the optimal\nalternative with a high probability, where the optimal alternative is defined\nas the one with the smallest value of extreme tail risk. We particularly\nconsider a situation where these alternatives generate heavy-tailed losses\nwhose probability distributions are unknown and may not admit any specific\nparametric representation. In this setup, we propose data-driven sequential\nsampling policies that maximize the rate at which the likelihood of falsely\nselecting suboptimal alternatives decays to zero. We rigorously demonstrate the\nsuperiority of the proposed methods over existing approaches, which is further\nvalidated via numerical studies.",
      "generated_abstract": "We propose a novel approach for non-parametric Bayesian model selection in\ngeneral high-dimensional high-dimensional regression models. Our approach is\nbased on the use of a Gaussian process prior over the parameter space of the\nmodel and a novel Bayesian model selection criterion that combines information\nfrom the model's likelihood and the prior. We derive an efficient likelihood\nestimator for our model, and show that its properties allow for a fast\nestimation of the model's posterior, even in high-dimensional settings. We\nprovide an algorithm for selecting the number of priors and the number of\nchains required for inference. We illustrate our approach through a simulation\nstudy and two empirical applications, one in linear regression and the other in\ntime-series analysis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1511627906976744,
          "p": 0.18309859154929578,
          "f": 0.1656050905870422
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.009174311926605505,
          "f": 0.009174306926608228
        },
        "rouge-l": {
          "r": 0.11627906976744186,
          "p": 0.14084507042253522,
          "f": 0.12738853007748813
        }
      }
    },
    {
      "paper_id": "eess.SP.stat/TH/2502.17981v1",
      "true_abstract": "This work deals with the generation of theoretical correlation matrices with\nspecific sparsity patterns, associated to graph structures. We present a novel\napproach based on convex optimization, offering greater flexibility compared to\nexisting techniques, notably by controlling the mean of the entry distribution\nin the generated correlation matrices. This allows for the generation of\ncorrelation matrices that better represent realistic data and can be used to\nbenchmark statistical methods for graph inference.",
      "generated_abstract": "This paper studies the joint distribution of the conditional moments of the\ndistributions generated by a Markov chain with an explicit transition matrix.\nWe show that, for a large class of transition matrices, the joint distribution\nof the conditional moments converges to a normal distribution. Furthermore, we\nprove that, under certain assumptions, the distribution of the conditional\nmoments of the first $k$ moments converges to a Cauchy distribution.\n  We also investigate the joint distribution of the first $k$ moments and\nshow that, under a certain assumption, the joint distribution of the first $k$\nmoments converges to a chi-squared distribution with $k$ degrees of freedom.\n  Finally, we show that, under certain assumptions, the joint distribution of\nthe first $k$ moments and the conditional moments converges to a Cauchy\ndistribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21818181818181817,
          "p": 0.2553191489361702,
          "f": 0.2352941126778163
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.013888888888888888,
          "f": 0.014388484215104469
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.23404255319148937,
          "f": 0.21568626954056144
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.05591v1",
      "true_abstract": "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
      "generated_abstract": "The concept of functional connectomes has emerged as a promising approach to\nstudy the brain's network dynamics. In functional connectomes, connections\nbetween neurons are defined by their interactions during spiking, while the\nneural dynamics are described by their spike trains. However, the number of\nspikes recorded in the neurological record is often limited, making it difficult\nto obtain a complete representation of the dynamic brain activity. To overcome\nthis limitation, we introduce a novel approach to modeling spiking dynamics in\nthe brain based on spike-based neural networks (SBNNs), which are neural\nnetworks that model spiking dynamics by learning the dynamics directly from the\nrecorded data. We show that SBNNs can successfully model the spiking dynamics of\nthe neurological record, capturing the complex dynamics of the brain. We\nexplain the advantages of SBNNs in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18867924528301888,
          "p": 0.23809523809523808,
          "f": 0.2105263108565098
        },
        "rouge-2": {
          "r": 0.020134228187919462,
          "p": 0.024793388429752067,
          "f": 0.022222217275995614
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.21428571428571427,
          "f": 0.18947367927756245
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/EC/2502.17044v1",
      "true_abstract": "Supply chain disruptions constitute an often underestimated risk for\nfinancial stability. As in financial networks, systemic risks in production\nnetworks arises when the local failure of one firm impacts the production of\nothers and might trigger cascading disruptions that affect significant parts of\nthe economy. Here, we study how systemic risk in production networks translates\ninto financial systemic risk through a mechanism where supply chain contagion\nleads to correlated bank-firm loan defaults. We propose a financial\nstress-testing framework for micro- and macro-prudential applications that\nfeatures a national firm level supply chain network in combination with\ninterbank network layers. The model is calibrated by using a unique data set\nincluding about 1 million firm-level supply links, practically all bank-firm\nloans, and all interbank loans in a small European economy. As a showcase we\nimplement a real COVID-19 shock scenario on the firm level. This model allows\nus to study how the disruption dynamics in the real economy can lead to\ninterbank solvency contagion dynamics. We estimate to what extent this\namplifies financial systemic risk. We discuss the relative importance of these\ncontagion channels and find an increase of interbank contagion by 70% when\nproduction network contagion is present. We then examine the financial systemic\nrisk firms bring to banks and find an increase of up to 28% in the presence of\nthe interbank contagion channel. This framework is the first financial systemic\nrisk model to take agent-level dynamics of the production network and shocks of\nthe real economy into account which opens a path for directly, and event-driven\nunderstanding of the dynamical interaction between the real economy and\nfinancial systems.",
      "generated_abstract": "The financial market has been experiencing a dynamic growth and evolution over\ntime. This evolution is driven by several factors such as technological\nadvancements, the emergence of new actors and the evolution of the regulation.\nThis evolution is particularly significant in the field of artificial\nintelligence (AI), which is capable of enhancing the performance of financial\ninstruments and making them more efficient and effective. This paper aims to\nanalyse the role of AI in financial markets and its potential impact on\neconomic growth and development. It discusses the opportunities and challenges\nof implementing AI in the financial sector and identifies the key areas of\nresearch for future studies. The paper also provides a comprehensive review of\nthe existing literature and highlights key areas for future research. The\nresearch provides a comprehensive overview of the use of AI in financial\nmarkets, identifying key areas for future research",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10416666666666667,
          "p": 0.18072289156626506,
          "f": 0.13215858566942904
        },
        "rouge-2": {
          "r": 0.016666666666666666,
          "p": 0.031496062992125984,
          "f": 0.021798360596634668
        },
        "rouge-l": {
          "r": 0.09027777777777778,
          "p": 0.1566265060240964,
          "f": 0.11453744029497971
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.03699v4",
      "true_abstract": "We study a multivariate autoregressive stochastic volatility model for the\nfirst 3 principal components (level, slope, curvature) of 10 series of\nzero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this\nmodel using monthly data from 1990. Unlike classic models with hidden\nstochastic volatility, here it is observed as VIX: the volatility index for the\nS&P 500 stock market index. Surprisingly, this stock index volatility works for\nTreasury bonds, too. Next, we prove long-term stability and the Law of Large\nNumbers. We express total returns of zero-coupon bonds using these principal\ncomponents. We prove the Law of Large Numbers for these returns. All results\nare done for discrete and continuous time.",
      "generated_abstract": "This paper proposes a novel risk management strategy for the investment\nbanks that focus on the allocation of resources within the firm. The proposed\nstrategy focuses on the risk allocation within the investment banks and\nprovides a framework for the bank management to select the appropriate risk\nmanagement strategies and allocate resources to minimize the overall risk.\nThis paper also discusses the application of the proposed strategy in\nreal-world scenarios, which includes a case study on the allocation of risk\nwithin the investment banking industry. The case study demonstrates the\neffectiveness of the proposed strategy in minimizing the overall risk in the\ninvestment banking industry.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.14285714285714285,
          "f": 0.11111110635802489
        },
        "rouge-2": {
          "r": 0.009345794392523364,
          "p": 0.012345679012345678,
          "f": 0.010638292967974191
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.14285714285714285,
          "f": 0.11111110635802489
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01352v1",
      "true_abstract": "Polarization, as a new optical imaging tool, has been explored to assist in\nthe diagnosis of pathology. Moreover, converting the polarimetric Mueller\nMatrix (MM) to standardized stained images becomes a promising approach to help\npathologists interpret the results. However, existing methods for\npolarization-based virtual staining are still in the early stage, and the\ndiffusion-based model, which has shown great potential in enhancing the\nfidelity of the generated images, has not been studied yet. In this paper, a\nRegulated Bridge Diffusion Model (RBDM) for polarization-based virtual staining\nis proposed. RBDM utilizes the bidirectional bridge diffusion process to learn\nthe mapping from polarization images to other modalities such as H\\&E and\nfluorescence. And to demonstrate the effectiveness of our model, we conduct the\nexperiment on our manually collected dataset, which consists of 18,000 paired\npolarization, fluorescence and H\\&E images, due to the unavailability of the\npublic dataset. The experiment results show that our model greatly outperforms\nother benchmark methods. Our dataset and code will be released upon acceptance.",
      "generated_abstract": "In this paper, we propose a novel method for estimating 3D brain tumor\ncovariates from CT scans. The method is based on the multilayer perceptron\n(MLP) neural network, which has been successfully employed to extract features\nfrom CT scans. The MLP neural network is trained using the tumor and\nbackground-derived features, and the estimated tumor parameters are used to\nestimate the tumor volume from the CT scans. We evaluated the proposed method\non the TCGA dataset, and the results demonstrate the effectiveness of the\nmethod in estimating tumor parameters. Our method can be applied to other\nbiomedical imaging datasets, and its application in the clinical setting is\ndesirable. In addition, our method can be applied to other biomedical imaging\ndatasets, and its application in the clinical setting is desirable.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.3972602739726027,
          "f": 0.30687830213711825
        },
        "rouge-2": {
          "r": 0.075,
          "p": 0.11650485436893204,
          "f": 0.09125474808657082
        },
        "rouge-l": {
          "r": 0.21551724137931033,
          "p": 0.3424657534246575,
          "f": 0.2645502598090759
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2501.07737v1",
      "true_abstract": "Understanding how molecular changes caused by genetic variation drive disease\nrisk is crucial for deciphering disease mechanisms. However, interpreting\ngenome sequences is challenging because of the vast size of the human genome,\nand because its consequences manifest across a wide range of cells, tissues and\nscales -- spanning from molecular to whole organism level. Here, we present\nPhenformer, a multi-scale genetic language model that learns to generate\nmechanistic hypotheses as to how differences in genome sequence lead to\ndisease-relevant changes in expression across cell types and tissues directly\nfrom DNA sequences of up to 88 million base pairs. Using whole genome\nsequencing data from more than 150 000 individuals, we show that Phenformer\ngenerates mechanistic hypotheses about disease-relevant cell and tissue types\nthat match literature better than existing state-of-the-art methods, while\nusing only sequence data. Furthermore, disease risk predictors enriched by\nPhenformer show improved prediction performance and generalisation to diverse\npopulations. Accurate multi-megabase scale interpretation of whole genomes\nwithout additional experimental data enables both a deeper understanding of\nmolecular mechanisms involved in disease and improved disease risk prediction\nat the level of individuals.",
      "generated_abstract": "The development of the next-generation sequencing (NGS) technologies has\nprovided unprecedented opportunities for the study of genetic diversity and\ndisease-associated polymorphisms. However, the complex and dynamic nature of\ndisease-associated polymorphisms (DAPs) makes it challenging to interpret the\nresults of NGS-based genotyping data. To address this issue, we developed\nDAP-NGS, a novel framework to identify and quantify DAPs. DAP-NGS is a\nmulti-step process that integrates multiple computational and statistical\napproaches. The first step is to identify DAPs from NGS data using a novel\nmulti-step approach that combines sequence-based, motif-based, and\ntranscriptome-based methods. The second step uses a novel hierarchical\ndiscriminant analysis (HDA) approach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.2191780821917808,
          "f": 0.16580310410480834
        },
        "rouge-2": {
          "r": 0.0056179775280898875,
          "p": 0.010309278350515464,
          "f": 0.0072727227065152635
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.1917808219178082,
          "f": 0.14507771550377205
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.04278v1",
      "true_abstract": "This study addresses the challenge of access point (AP) and user equipment\n(UE) association in cell-free massive MIMO networks. It introduces a deep\nlearning algorithm leveraging Bidirectional Long Short-Term Memory cells and a\nhybrid probabilistic methodology for weight updating. This approach enhances\nscalability by adapting to variations in the number of UEs without requiring\nretraining. Additionally, the study presents a training methodology that\nimproves scalability not only with respect to the number of UEs but also to the\nnumber of APs. Furthermore, a variant of the proposed AP-UE algorithm ensures\nrobustness against pilot contamination effects, a critical issue arising from\npilot reuse in channel estimation. Extensive numerical results validate the\neffectiveness and adaptability of the proposed methods, demonstrating their\nsuperiority over widely used heuristic alternatives.",
      "generated_abstract": "The study of large language models (LLMs) has led to the development of\nmany powerful models that are capable of complex, natural language understanding\nand generation tasks. However, these models often exhibit severe limitations in\ntheir ability to model complex dependencies between words, which can be crucial\nfor tasks such as question answering. This paper presents a novel approach to\nmodeling dependencies between words using neural language models. Our approach\nemploys a novel layer that we call an auxiliary attention layer, which serves\nas a mechanism to capture and model dependencies between words within a\nsentence. By integrating this auxiliary attention layer with the standard\nattention layer, we are able to better model complex dependencies between\nwords and enhance the model's ability to perform complex tasks such as question\nanswering. Our approach demonstrates significant improvements in question\nanswering performance compared to existing methods, and also outperforms other\nstate-of-the-art",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.17582417582417584,
          "f": 0.17391303847885178
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.015503875968992248,
          "f": 0.016326525626323887
        },
        "rouge-l": {
          "r": 0.17204301075268819,
          "p": 0.17582417582417584,
          "f": 0.17391303847885178
        }
      }
    },
    {
      "paper_id": "q-bio.QM.quant-ph/2503.10510v1",
      "true_abstract": "Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.",
      "generated_abstract": "We present a theoretical framework for investigating the role of quantum\ncomputing in the exploration of the non-equilibrium statistical mechanics of\nbiological systems. The framework relies on a generalization of the mean-field\napproach for the description of collective behavior in non-equilibrium\nstatistical mechanics, which we call the mean-field quantum-classical\napproach. In this approach, the mean-field approximation is replaced by a\nmean-field quantum approximation. We demonstrate that the mean-field quantum\napproach can be used to investigate the properties of biological systems in\ntheir non-equilibrium statistical mechanics. We show that, for instance, the\nmean-field quantum approximation of the Ising model in the Ising-like\nHamiltonian of a biological system can be used to describe the effect of\nquantum computing on the dynamics of biological systems. We also show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1488095238095238,
          "p": 0.4098360655737705,
          "f": 0.21834060744531952
        },
        "rouge-2": {
          "r": 0.036885245901639344,
          "p": 0.08737864077669903,
          "f": 0.051873194672823794
        },
        "rouge-l": {
          "r": 0.13690476190476192,
          "p": 0.3770491803278688,
          "f": 0.20087335853702265
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.09209v1",
      "true_abstract": "Time-dependent Stark-Zeeman systems describe the motion of an electron\nattracted by a proton subject to a magnetic and a time-dependent electric\nfield. For instance the study of the dynamics of a gateway around the moon\nwhich is subject to the joint attraction of the moon, the earth and the sun\nleads to time-dependent Stark-Zeeman systems. In the time-dependent case there\nis no preserved energy. Therefore collisions cannot be regularized by blowing\nup the energy hypersurface. A new regularization technique of blowing up\ninstead of the energy hypersurface the loop space was recently discovered by\nBarutello, Ortega, and Verzini. In this article we explain how this new\nregularization technique can be applied to the study of periodic orbits in\ntime-dependent planar Stark-Zeeman systems. Since the regularization by\nblowing-up the loop space is nonlocal the regularized periodic orbits will not\nsatisfy an ODE anymore but a delay equation.",
      "generated_abstract": "We prove that, given a closed surface $S$ with a single connected component\nand a closed surface $S'$ with two connected components, there is a unique\nhomeomorphism between $S$ and $S'$, which is not constant. This generalizes a\nresult of C. R. McMullen from 1999. Our proof is based on a new proof of the\nLeray-Serre theorem. The main novelty of our proof is the use of the theory of\n$K$-homology.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.1836734693877551,
          "f": 0.13533834121092223
        },
        "rouge-2": {
          "r": 0.031007751937984496,
          "p": 0.06060606060606061,
          "f": 0.04102563654753501
        },
        "rouge-l": {
          "r": 0.08333333333333333,
          "p": 0.14285714285714285,
          "f": 0.10526315324099744
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2503.08533v1",
      "true_abstract": "Advancements in audio foundation models (FMs) have fueled interest in\nend-to-end (E2E) spoken dialogue systems, but different web interfaces for each\nsystem makes it challenging to compare and contrast them effectively. Motivated\nby this, we introduce an open-source, user-friendly toolkit designed to build\nunified web interfaces for various cascaded and E2E spoken dialogue systems.\nOur demo further provides users with the option to get on-the-fly automated\nevaluation metrics such as (1) latency, (2) ability to understand user input,\n(3) coherence, diversity, and relevance of system response, and (4)\nintelligibility and audio quality of system output. Using the evaluation\nmetrics, we compare various cascaded and E2E spoken dialogue systems with a\nhuman-human conversation dataset as a proxy. Our analysis demonstrates that the\ntoolkit allows researchers to effortlessly compare and contrast different\ntechnologies, providing valuable insights such as current E2E systems having\npoorer audio quality and less diverse responses. An example demo produced using\nour toolkit is publicly available here:\nhttps://huggingface.co/spaces/Siddhant/Voice_Assistant_Demo.",
      "generated_abstract": "Audio-visual dialogue generation (AVDG) models have demonstrated remarkable\ncapabilities, yet their large-scale deployment in real-world settings remains\nchallenging. To address this, we introduce a novel large-scale AVDG benchmark\n-- the 2025 Audio-Visual Dialogue Generation (AVDG-25) Challenge. This\ncompetition aims to assess the capabilities of AVDG models across a range of\nspeaker-dialogue pairs, with a focus on robustness to unseen speakers and\ndialogue styles. We introduce a new dataset, AVDG-25, which includes over 200,000\nspeaker-dialogue pairs from 140,000 hours of audio and video data. We also\nintroduce a new evaluation protocol and benchmarking framework, aimed at\nensuring the validity and reliability of AVDG models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.20253164556962025,
          "f": 0.1675392621649627
        },
        "rouge-2": {
          "r": 0.020689655172413793,
          "p": 0.030927835051546393,
          "f": 0.02479338362646087
        },
        "rouge-l": {
          "r": 0.13392857142857142,
          "p": 0.189873417721519,
          "f": 0.15706805797648107
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2411.00071v2",
      "true_abstract": "Musculoskeletal (MSK) models offer a non-invasive way to understand\nbiomechanical loads on joints and tendons, which are difficult to measure\ndirectly. Variations in muscle strength, especially relative differences\nbetween muscles, significantly impact model outcomes. Typically, scaled generic\nMSK models use maximum isometric forces that are not adjusted for different\ndemographics, raising concerns about their accuracy. This review provides an\noverview on experimentally derived strength parameters, including physiological\ncross-sectional area (PCSA), muscle mass (Mm), and relative muscle mass (%Mm),\nwhich is the relative distribution of muscle mass across the leg. We analysed\ndifferences by age and sex, and compared open-source lower limb MSK model\nparameters with experimental data from 57 studies. Our dataset, with records\ndating back to 1884, shows that uniformly increasing all maximum isometric\nforces in MSK models does not capture key muscle ratio differences due to age\nand sex. Males have a higher proportion of muscle mass in the rectus femoris\nand semimembranosus muscles, while females have a greater relative muscle mass\nin the pelvic (gluteus maximus and medius) and ankle muscles (tibialis\nanterior, tibialis posterior, and extensor digitorum longus). Older adults have\na higher relative muscle mass in the gluteus medius, while younger individuals\nshow more in the gastrocnemius. Current MSK models do not accurately represent\nmuscle mass distribution for specific age or sex groups, and none of them\naccurately reflect female muscle mass distribution. Further research is needed\nto explore musculotendon age- and sex differences.",
      "generated_abstract": "The development of modern biotechnology has been driven by the advancement\nof sequencing technology. Sequencing enables the identification of biomolecules\nand their corresponding genetic information. The rapid advancement of\nsequencing technologies has also led to the development of advanced methods\nfor analyzing large biological datasets, such as bioinformatics. Bioinformatics\nis a field that combines computer science, statistics, and biology to\nunderstand and analyze biological data. This paper discusses the different\nmethods of analyzing biological data using bioinformatics, focusing on the\nadvantages and disadvantages of each method. Additionally, the paper reviews\nthe current state of bioinformatics and discusses future trends in the field.\nThe main focus of this paper is to provide a comprehensive overview of the\ndifferent methods of analyzing biological data using bioinformatics and to\nhighlight the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10897435897435898,
          "p": 0.2236842105263158,
          "f": 0.1465517197324615
        },
        "rouge-2": {
          "r": 0.009389671361502348,
          "p": 0.017391304347826087,
          "f": 0.012195117397570112
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.21052631578947367,
          "f": 0.1379310300772891
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/RM/2503.01886v1",
      "true_abstract": "This study presents a comparative analysis of deep learning methodologies\nsuch as BERT, FinBERT and ULMFiT for sentiment analysis of earnings call\ntranscripts. The objective is to investigate how Natural Language Processing\n(NLP) can be leveraged to extract sentiment from large-scale financial\ntranscripts, thereby aiding in more informed investment decisions and risk\nmanagement strategies. We examine the strengths and limitations of each model\nin the context of financial sentiment analysis, focusing on data preprocessing\nrequirements, computational efficiency, and model optimization. Through\nrigorous experimentation, we evaluate their performance using key metrics,\nincluding accuracy, precision, recall, and F1-score. Furthermore, we discuss\npotential enhancements to improve the effectiveness of these models in\nfinancial text analysis, providing insights into their applicability for\nreal-world financial decision-making.",
      "generated_abstract": "Reinforcement Learning (RL) has emerged as a promising approach for\nexpert recommendation in financial markets, leveraging historical data to\nidentify optimal trade strategies. However, existing RL methods often fail to\ncapture complex, multi-dimensional features of stocks, limiting their ability\nto effectively optimize trading strategies. To address this limitation, we\nintroduce Multi-dimensional Reinforcement Learning for Stock Trading (MRL4ST),\na deep reinforcement learning (DRL) framework that leverages multi-dimensional\nfeatures to enhance trading performance. Our model integrates a multi-layer\nperceptron (MLP) architecture with a multi-dimensional feature extractor,\nimproving the representation of stocks and enhancing RL performance. By\nfusing multi-dimensional features with MLP-based representations, MRL4ST\neffectively captures complex trading patterns and generates",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17708333333333334,
          "p": 0.2,
          "f": 0.18784529888587057
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.009615384615384616,
          "f": 0.008968604888096228
        },
        "rouge-l": {
          "r": 0.17708333333333334,
          "p": 0.2,
          "f": 0.18784529888587057
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02327v2",
      "true_abstract": "Nonlinear frequency hopping has emerged as a promising approach for\nmitigating interference and enhancing range resolution in automotive FMCW radar\nsystems. Achieving an optimal balance between high range-resolution and\neffective interference mitigation remains challenging, especially without\ncentralized frequency scheduling. This paper presents a game-theoretic\nframework for interference avoidance, in which each radar operates as an\nindependent player, optimizing its performance through decentralized\ndecision-making. We examine two equilibrium concepts--Nash Equilibrium (NE) and\nCoarse Correlated Equilibrium (CCE)--as strategies for frequency band\nallocation, with CCE demonstrating particular effectiveness through regret\nminimization algorithms. We propose two interference avoidance algorithms: Nash\nHopping, a model-based approach, and No-Regret Hopping, a model-free adaptive\nmethod. Simulation results indicate that both methods effectively reduce\ninterference and enhance the signal-to-interference-plus-noise ratio (SINR).\nNotably, No-regret Hopping further optimizes frequency spectrum utilization,\nachieving improved range resolution compared to Nash Hopping.",
      "generated_abstract": "The development of 5G and 6G networks requires the integration of wireless\nnetworks with digital and physical domains. In this context, the integration\nof radar and wireless networks is necessary to provide a complete and holistic\ninformation solution for the whole network. The integration of radar and wireless\nnetworks can be achieved through different approaches such as joint sensing\nand communication (JSC) and joint sensing and radar (JSR). In this paper, we\npropose a novel JSC/JSR hybrid approach, which combines the advantages of both\napproaches. The proposed methodology is based on the use of a multi-antenna\ntransmitter with multiple radar sensors, in order to improve the sensing\nperformance. The proposed approach is demonstrated through simulations, where\nthe sensing and communication performance are evaluated under different\nscenarios. The results show that the proposed methodology achieves significant\nperformance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1651376146788991,
          "p": 0.2222222222222222,
          "f": 0.1894736793191137
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14678899082568808,
          "p": 0.19753086419753085,
          "f": 0.16842104774016636
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/SC/2401.04786v1",
      "true_abstract": "The method developed by Michaelis and Menten was foundational in the\ndevelopment of our understanding of biochemical reaction kinetics. Extended\nmodels of metabolism encapsulated by reaction rate theory, stochastic reaction\nmodels, and dynamic flux estimation, amongst others, address aspects of this\nfundamental idea. The limitations of these approaches are well understood, and\nefforts to overcome those issues so far have been plentiful but with limited\nsuccess. The known issues can be summarised as the sole dependent relation with\nsubstrate concentration, the encapsulation of rate in a single relevant scalar,\nand the subsequent lack of functional control that results from this\nassumption. The Rate Control of Chaos (RCC) is a nonlinear control method that\nhas been shown to be effective in controlling the dynamic state of biological\noscillators based on the concept of rate limitation of the exponential growth\nin chaotic systems. Extending RCC with allosteric properties allows robust\ncontrol of the enzymatic process, and replicates the Michaelis-Menten kinetics.\nThe emergent dynamics is robust to perturbations and noise but susceptible to\nregulatory adjustments. This control method adapts the control parameters\ndynamically in the presence of a ligand, and permits introduction of energy\nrelations into the control function. The dynamic nature of the control\neliminates the steady-state requirements and allows the modelling of\nlarge-scale dynamic behaviour, potentially addressing issues in metabolic\ndisorder and failure of metabolic control.",
      "generated_abstract": "The human genome encodes over 30,000 genes, many of which are expressed\nin a regulated manner, which is crucial for cellular function. The transcription\nof genes is controlled by hundreds of noncoding RNAs, known as microRNAs (miRNAs).\nThe regulation of miRNAs is complex and depends on both genomic and non-genomic\nfactors. However, miRNA regulation is often studied using only genome-wide\ndata, which are not necessarily representative of the entire human genome. In\nthis study, we analyzed a dataset of 530,000 human genes, which are expressed\nin a regulated manner. Using this dataset, we found that miRNA expression\nchanges are more strongly correlated with gene expression changes than with\ngenome-wide expression changes. Furthermore, we found that miRNA expression",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11347517730496454,
          "p": 0.21052631578947367,
          "f": 0.14746543323663713
        },
        "rouge-2": {
          "r": 0.013824884792626729,
          "p": 0.02912621359223301,
          "f": 0.018749995634571328
        },
        "rouge-l": {
          "r": 0.09929078014184398,
          "p": 0.18421052631578946,
          "f": 0.12903225351313488
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2404.14137v2",
      "true_abstract": "Providing a measure of market risk is an important issue for investors and\nfinancial institutions. However, the existing models for this purpose are per\ndefinition symmetric. The current paper introduces an asymmetric capital asset\npricing model for measurement of the market risk. It explicitly accounts for\nthe fact that falling prices determine the risk for a long position in the\nrisky asset and the rising prices govern the risk for a short position. Thus, a\nposition dependent market risk measure that is provided accords better with\nreality. The empirical application reveals that Apple stock is more volatile\nthan the market only for the short seller. Surprisingly, the investor that has\na long position in this stock is facing a lower volatility than the market.\nThis property is not captured by the standard asset pricing model, which has\nimportant implications for the expected returns and hedging designs.",
      "generated_abstract": "This paper explores the integration of artificial intelligence (AI) into\nfinancial markets by analyzing the impact of AI-driven trading strategies on\nstock prices. Our analysis considers three key areas: (1) the integration of\nAI-driven trading strategies into traditional financial markets; (2) the\nimpact of AI-driven trading strategies on stock prices; and (3) the effect of\nAI-driven trading strategies on stock prices, asset returns, and market\nindicators. By integrating AI with traditional financial markets, we aim to\nimprove trading efficiency and reduce trading costs. Additionally, AI-driven\nstrategies have the potential to improve market efficiency, reduce market\nnoise, and enhance trading accuracy. By analyzing the impact of AI-driven\ntrading strategies on stock prices, we gain",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.2,
          "f": 0.1643835568024021
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12790697674418605,
          "p": 0.18333333333333332,
          "f": 0.15068492666541577
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06329v1",
      "true_abstract": "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
      "generated_abstract": "We consider the set of $n$-dimensional rational homology $3$-spheres which\nare not homeomorphic to $S^3$. It is known that the set of such spheres is\nhomeomorphic to the space of $n$-tuples of integer numbers modulo $1$.\nMoreover, we show that the set of $n$-dimensional rational homology $3$-spheres\nwhich are not homeomorphic to $S^3$ is homeomorphic to the space of\n$n$-tuples of integers modulo $1$ together with a certain equivalence relation\nwhich is induced by the action of the symmetric group on the set of\n$n$-tuples of integers modulo $1$. This result has an analogy in the study of\nthe set of $n$-dimensional rational homology $3$-spheres which are not\nhomeomorphic to $S^3$ and the set of $n$-dimensional rational homology",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.23529411764705882,
          "f": 0.22429906043147885
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.19642857142857142,
          "p": 0.21568627450980393,
          "f": 0.20560747164643214
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.04648v1",
      "true_abstract": "The time-varying reproduction number ($R_t$) gives an indication of the\ntrajectory of an infectious disease outbreak. Commonly used frameworks for\ninferring $R_t$ from epidemiological time series include those based on\ncompartmental models (such as the SEIR model) and renewal equation models.\nThese inference methods are usually validated using synthetic data generated\nfrom a simple model, often from the same class of model as the inference\nframework. However, in a real outbreak the transmission processes, and thus the\ninfection data collected, are much more complex. The performance of common\n$R_t$ inference methods on data with similar complexity to real world scenarios\nhas been subject to less comprehensive validation. We therefore propose\nevaluating these inference methods on outbreak data generated from a\nsophisticated, geographically accurate agent-based model. We illustrate this\nproposed method by generating synthetic data for two outbreaks in Northern\nIreland: one with minimal spatial heterogeneity, and one with additional\nheterogeneity. We find that the simple SEIR model struggles with the greater\nheterogeneity, while the renewal equation model demonstrates greater robustness\nto spatial heterogeneity, though is sensitive to the accuracy of the generation\ntime distribution used in inference. Our approach represents a principled way\nto benchmark epidemiological inference tools and is built upon an open-source\nsoftware platform for reproducible epidemic simulation and inference.",
      "generated_abstract": "This paper explores the potential of a novel, high-throughput method of\nmeasuring the microbiome in a mouse model of Crohn's disease, as an\nalternative to traditional bacterial culture methods, which are expensive and\ntime-consuming. The proposed approach, known as the Microbiome-to-Ingredient\n(Mi-2I) method, uses a mixture of microbial DNA extracted from fecal samples\nand proteins extracted from the colon. This approach circumvents the need for\nculturing microbes in a petri dish, and allows for the simultaneous analysis of\ndifferent bacterial species, including both commensals and pathogens. The\nMi-2I method has been shown to provide a more accurate and comprehensive\ndescription of the microbiome than traditional culture-based methods. In this\npaper, we detail the procedure for applying the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17293233082706766,
          "p": 0.27710843373493976,
          "f": 0.21296295823088146
        },
        "rouge-2": {
          "r": 0.02512562814070352,
          "p": 0.04424778761061947,
          "f": 0.032051277431172594
        },
        "rouge-l": {
          "r": 0.15037593984962405,
          "p": 0.24096385542168675,
          "f": 0.18518518045310367
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2502.18833v1",
      "true_abstract": "A topological space is domain-representable (or, has a domain model) if it is\nhomeomorphic to the maximal point space $\\mbox{Max}(P)$ of a domain $P$ (with\nthe relative Scott topology). We first construct an example to show that the\nset of maximal points of an ideal domain $P$ need not be a $G_{\\delta}$-set in\nthe Scott space $\\Sigma P$, thereby answering an open problem from Martin\n(2003). In addition, Bennett and Lutzer (2009) asked whether $X$ and $Y$ are\ndomain-representable if their product space $X \\times Y$ is\ndomain-representable. This problem was first solved by \\\"{O}nal and Vural\n(2015). In this paper, we provide a new approach to Bennett and Lutzer's\nproblem.",
      "generated_abstract": "We show that the category of finite-dimensional $A_\\infty$-categories can be\nequivalent to the category of non-degenerate quadratic forms. This result is\nestablished for the case of algebras over a field of characteristic 0, but\ngeneralizes to arbitrary algebras. We then give an application to the\nintegrability of quantum affine algebras.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14102564102564102,
          "p": 0.2972972972972973,
          "f": 0.19130434346162584
        },
        "rouge-2": {
          "r": 0.02830188679245283,
          "p": 0.06521739130434782,
          "f": 0.03947367998961264
        },
        "rouge-l": {
          "r": 0.14102564102564102,
          "p": 0.2972972972972973,
          "f": 0.19130434346162584
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06755v1",
      "true_abstract": "In this paper, we study a transfer learning framework for Linear\n  Quadratic Regulator (LQR) control, where (i) the dynamics of the\n  system of interest (target system) are unknown and only a short\n  trajectory of impulse responses from the target system is provided,\n  and (ii) impulse responses are available from $N$ source systems\n  with different dynamics. We show that the LQR controller can be\n  learned from a sufficiently long trajectory of impulse\n  responses. Further, a transferable mode set can be identified using\n  the available data from source systems and the target system,\n  enabling the reconstruction of the target system's impulse responses\n  for controller design. By leveraging data from the source systems we\n  demonstrate that only n+1 (n being the system dimension) samples\n  of data from the target system are needed to learn the LQR\n  controller, this yields a significant reduction of the required\n  data.",
      "generated_abstract": "This paper proposes a novel method for distributed parameter identification\nof a nonlinear system. The method is based on the use of a time-delayed\ncoupled Kalman filter (TDCKF), which is implemented by a distributed controller\n(DC) in a distributed parameter identification (DPI) algorithm. The DC is\nconstructed in such a way that it ensures that the estimation error is\ndecoupled from the system dynamics. This approach allows the identification of\nthe parameter of the nonlinear system, along with the parameters of the DC,\nwithout any knowledge of the parameters of the system. The proposed method is\nvalidated using a simulation example, which illustrates the effectiveness of\nthe proposed method in a real-world application.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16049382716049382,
          "p": 0.20634920634920634,
          "f": 0.18055555063368067
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.042105263157894736,
          "f": 0.03686635452441188
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.19047619047619047,
          "f": 0.1666666617447918
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2409.05333v1",
      "true_abstract": "We examine the difference in motion ordering between cellular systems with\nand without information transfer to evaluate the effect of the polar--polar\ninteraction through mutual guiding, which enables cells to inform other cells\nof their moving directions. We compare this interaction with the\npolar--nonpolar interaction through cell motion triggered by cellular contact,\nwhich cannot provide information on the moving directions. We model these\ninteractions on the basis of the cellular Potts model. We calculate the order\nparameter of the polar direction in the interactions and examine the cell\nconcentration and surface tension conditions of ordering. The results suggest\nthat the polar--polar interaction through mutual guiding efficiently induces\nthe motion ordering in comparison with the polar-nonpolar interaction for\ncontact triggering, except in cases of weak driving. The results also show that\nthe polar--polar interaction efficiently accelerates the collective motion\ncompared with the polar--nonpolar interaction.",
      "generated_abstract": "A central challenge in biological systems is to understand how cellular\ninformation is transmitted and integrated across spatial scales, from the\nmolecular to the organismal. In this paper, we propose a novel framework for\nthe analysis of spatial information transmission in biological systems that\nintegrates concepts from biological and mathematical statistics. The proposed\nframework is based on a novel model of spatial information transmission that\ncouples the transport of information through the cell membrane with the\npropagation of information through the cell. The proposed framework provides\na unified framework for understanding how spatial information is transmitted\nand integrated across spatial scales. This framework is particularly useful in\nbiological systems, where the spatial organization of information is typically\nextremely complex. Additionally, the proposed framework is highly flexible,\nallowing for a wide range of information transport mechanisms, including\ndiffusion, convection, and permeability barriers. The proposed framework is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22077922077922077,
          "p": 0.2236842105263158,
          "f": 0.2222222172224359
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.017543859649122806,
          "f": 0.016949147548119825
        },
        "rouge-l": {
          "r": 0.2077922077922078,
          "p": 0.21052631578947367,
          "f": 0.20915032179759935
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2406.11940v1",
      "true_abstract": "The stable unit treatment value assumption states that the outcome of an\nindividual is not affected by the treatment statuses of others, however in many\nreal world applications, treatments can have an effect on many others beyond\nthe immediately treated. Interference can generically be thought of as mediated\nthrough some network structure. In many empirically relevant situations\nhowever, complete network data (required to adjust for these spillover effects)\nare too costly or logistically infeasible to collect. Partially or indirectly\nobserved network data (e.g., subsamples, aggregated relational data (ARD),\negocentric sampling, or respondent-driven sampling) reduce the logistical and\nfinancial burden of collecting network data, but the statistical properties of\ntreatment effect adjustments from these design strategies are only beginning to\nbe explored. In this paper, we present a framework for the estimation and\ninference of treatment effect adjustments using partial network data through\nthe lens of structural causal models. We also illustrate procedures to assign\ntreatments using only partial network data, with the goal of either minimizing\nestimator variance or optimally seeding. We derive single network asymptotic\nresults applicable to a variety of choices for an underlying graph model. We\nvalidate our approach using simulated experiments on observed graphs with\napplications to information diffusion in India and Malawi.",
      "generated_abstract": "In this paper, we introduce a novel method for assessing the significance of\nparticular coefficients in regression models, by leveraging the concept of\npartial effect. The method is based on a novel statistical test that quantifies\nthe effect of a particular coefficient on the estimated parameters. This test\nis designed to provide a more precise interpretation of the coefficients and\ncan be used for both parametric and non-parametric regression models. The\nmethod is valid and has a finite sample size guarantee. We also demonstrate\nthat the proposed method is more interpretable than existing methods,\nparticularly when the number of covariates is large. The proposed method can\nbe used to test the significance of individual coefficients in models with\nmultiple covariates. We apply the method to test the significance of the\ncoefficients for the age and education of the respondents in the 2019 United\nStates Census. The results show that the proposed method provides a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1875,
          "p": 0.32926829268292684,
          "f": 0.23893804847364714
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.045112781954887216,
          "f": 0.03592813891982566
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2926829268292683,
          "f": 0.21238937590727552
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2409.17488v1",
      "true_abstract": "Controlling the stochastic dynamics of biological populations is a challenge\nthat arises across various biological contexts. However, these dynamics are\ninherently nonlinear and involve a discrete state space, i.e., the number of\nmolecules, cells, or organisms. Additionally, the possibility of extinction has\na significant impact on both the dynamics and control strategies, particularly\nwhen the population size is small. These factors hamper the direct application\nof conventional control theories to biological systems. To address these\nchallenges, we formulate the optimal control problem for stochastic population\ndynamics by utilizing a control cost function based on the Kullback-Leibler\ndivergence. This approach naturally accounts for population-specific factors\nand simplifies the complex nonlinear Hamilton-Jacobi-Bellman equation into a\nlinear form, facilitating efficient computation of optimal solutions. We\ndemonstrate the effectiveness of our approach by applying it to the control of\ninteracting random walkers, Moran processes, and SIR models, and observe the\nmode-switching phenomena in the control strategies. Our approach provides new\nopportunities for applying control theory to a wide range of biological\nproblems.",
      "generated_abstract": "The study of the molecular basis of the interaction between protein\ndocking and ligand binding sites is crucial for the development of novel\ndrug candidates. The study of the structure-activity relationship (SAR) of\ndrugs is a crucial step for drug design. The study of the SAR of drugs is a\ncrucial step in drug design. To study the SAR of drugs, it is necessary to\nunderstand the binding site of the drug. The docking is a key step in drug\ndesign. Docking is a critical step in drug design. Docking is a critical step\nin drug design. Docking is a critical step in drug design. Docking is a critical\nstep in drug design. Docking is a critical step in drug design. Docking is a\ncritical step in drug design. Docking is a critical step in drug design. Docking\nis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08771929824561403,
          "p": 0.25,
          "f": 0.12987012602462483
        },
        "rouge-2": {
          "r": 0.005988023952095809,
          "p": 0.015625,
          "f": 0.008658004652088583
        },
        "rouge-l": {
          "r": 0.08771929824561403,
          "p": 0.25,
          "f": 0.12987012602462483
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ME/2503.03567v1",
      "true_abstract": "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
      "generated_abstract": "We study the problem of optimal transportation between two probability\ndistributions over a finite set of variables. To this end, we introduce the\nnon-convex, non-smooth setting of optimal transportation in the setting of\ngraphs, and then we introduce the convex and smooth setting of optimal\ntransportation. We define the relative entropy distance and the relative\nKullback-Leibler divergence between probability distributions, and we show\nthat they are convex, respectively sub-additive. We prove that the relative\nentropy distance is a concave, convex, and sub-additive functional on the set of\nprobability distributions over a finite set of variables. We then define the\nrelative mutual information, which is a convex functional on the set of\nprobability distributions over a finite set of variables. We prove that the\nrelative mutual information is a convex, respectively sub-additive functional\non the set of probability distributions over a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.2641509433962264,
          "f": 0.20143884420268116
        },
        "rouge-2": {
          "r": 0.007575757575757576,
          "p": 0.011627906976744186,
          "f": 0.00917430714923235
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.2641509433962264,
          "f": 0.20143884420268116
        }
      }
    },
    {
      "paper_id": "cs.CL.cs/IR/2503.08398v1",
      "true_abstract": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems.",
      "generated_abstract": "This paper introduces a novel method for training large language models (LLMs)\nwith limited data. The proposed method combines an LLM with a pre-trained\nencoder-decoder architecture, which is trained to generate textual descriptions\nof the LLM's state. The LLM's state is then used to generate textual\ndescriptions of the encoder-decoder architecture, which are then used to\ngenerate textual descriptions of the LLM's state. This process is repeated\nuntil the LLM's state is fully described. The resulting description is then\nused to train the encoder-decoder architecture using reinforcement learning\ntechniques. This approach enables the encoder-decoder architecture to\nself-generate training data, thereby reducing the amount of data required for\nLLM training. The results of this study demonstrate that this approach\nsignificantly enhances the performance of LLM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.22727272727272727,
          "f": 0.18072288677601986
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.030927835051546393,
          "f": 0.02654866766661536
        },
        "rouge-l": {
          "r": 0.14,
          "p": 0.21212121212121213,
          "f": 0.16867469400493554
        }
      }
    },
    {
      "paper_id": "cs.OS.cs/OS/2502.07118v1",
      "true_abstract": "File systems play an essential role in modern society for managing precious\ndata. To meet diverse needs, they often support many configuration parameters.\nSuch flexibility comes at the price of additional complexity which can lead to\nsubtle configuration-related issues. To address this challenge, we study the\nconfiguration-related issues of two major file systems (i.e., Ext4 and XFS) in\ndepth, and identify a prevalent pattern called multilevel configuration\ndependencies. Based on the study, we build an extensible tool called ConfD to\nextract the dependencies automatically, and create a set of plugins to address\ndifferent configuration-related issues. Our experiments on Ext4, XFS and a\nmodern copy-on-write file system (i.e., ZFS) show that ConfD was able to\nextract 160 configuration dependencies for the file systems with a low false\npositive rate. Moreover, the dependency-guided plugins can identify various\nconfiguration issues (e.g., mishandling of configurations, regression test\nfailures induced by valid configurations). In addition, we also explore the\napplicability of ConfD on a popular storage engine (i.e., WiredTiger). We hope\nthat this comprehensive analysis of configuration dependencies of storage\nsystems can shed light on addressing configuration-related challenges for the\nsystem community in general.",
      "generated_abstract": "The growing complexity of modern cloud applications, coupled with the\ndiversification of microservice architectures, requires a new approach to\nmanaging the lifecycle of containerized services. This paper presents a\ndecentralized, lightweight orchestrator designed to manage service\nreconfiguration and service lifecycle management across multiple containers and\nmicroservices. The system is designed to support a wide range of containerized\napplications, from traditional applications to microservices that support\nmulti-tenancy and heterogeneous environments. The proposed system utilizes\nlightweight containers, eliminating the need for additional process management\nand resource allocation. Additionally, the system offers a simple and\nscalable approach to reconfiguring services, with the ability to scale to\ninfinity. The system is designed to work within a container runtime environment\nand can be used as a drop-in replacement for existing orchestrators. The\nsystem's architecture is based on the concept of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.1839080459770115,
          "f": 0.15165876292625968
        },
        "rouge-2": {
          "r": 0.010810810810810811,
          "p": 0.015873015873015872,
          "f": 0.01286173151435753
        },
        "rouge-l": {
          "r": 0.11290322580645161,
          "p": 0.16091954022988506,
          "f": 0.1327014169546957
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/chem-ph/2503.09855v1",
      "true_abstract": "Spatially varying electric fields are prevalent throughout nature and\ntechnology, arising from heterogeneity inherent to all physical systems.\nInhomogeneous electric fields can originate naturally, such as in nanoporous\nmaterials and biological membranes, or be engineered, e.g., with patterned\nelectrodes or layered van der Waals heterostructures. While uniform fields\ncause free ions to migrate, for polar fluids they simply act to reorient the\nconstituent molecules. In contrast, electric field gradients (EFGs) induce a\ndielectrophoretic force, offering exquisite electrokinetic control of a fluid,\neven in the absence of free charge carriers. EFGs, therefore, offer vast\npotential for optimizing fluid behavior under confinement, such as in\nnanoporous electrodes, nanofluidic devices, and chemical separation materials.\nYet, EFGs remain largely unexplored at the microscopic level owing to the\nabsence of a rigorous, first principles theoretical treatment of\nelectrostrictive effects. By integrating state-of-the-art advances in liquid\nstate theory and deep learning, we reveal how EFGs modulate fluid structure and\ncapillary phenomena. We demonstrate, from first principles, that\ndielectrophoretic coupling enables tunable control over the liquid-gas phase\ntransition, capillary condensation, and fluid uptake into porous media. Our\nfindings establish \"dielectrocapillarity'' -- the use of EFGs to control\nconfined fluids -- as a powerful tool for controlling volumetric capacity in\nnanopores, which holds immense potential for optimizing energy storage in\nsupercapacitors, selective gas separation, and tunable hysteresis in\nneuromorphic nanofluidic devices.",
      "generated_abstract": "The study of self-organization in soft matter systems is of fundamental\ninterest for understanding the origin of complex, hierarchical structures in\nbiological systems and the development of novel materials for applications in\nbiomaterials and biomedical sciences. In this study, we present a novel\nnanostructured model of a polyelectrolyte colloidal sphere in aqueous solution\nthat allows for the investigation of self-assembly processes and the\ndevelopment of self-organized nanostructures. We characterize the sphere by\nanalyzing the hydrodynamic interactions of the colloidal particle with the\nsolvent, the effect of the surface charge on the self-assembly process, and the\nformation of nanostructures. We find that the polyelectrolyte sphere self-assembles\ninto a cylindrical shape with a uniform height. We also demonstrate that the\nhydrophobic nature of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10843373493975904,
          "p": 0.2465753424657534,
          "f": 0.15062761081983872
        },
        "rouge-2": {
          "r": 0.004651162790697674,
          "p": 0.008849557522123894,
          "f": 0.006097556459142251
        },
        "rouge-l": {
          "r": 0.0963855421686747,
          "p": 0.2191780821917808,
          "f": 0.13389120914619856
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/RM/2502.11706v1",
      "true_abstract": "A deep BSDE approach is presented for the pricing and delta-gamma hedging of\nhigh-dimensional Bermudan options, with applications in portfolio risk\nmanagement. Large portfolios of a mixture of multi-asset European and Bermudan\nderivatives are cast into the framework of discretely reflected BSDEs. This\nsystem is discretized by the One Step Malliavin scheme (Negyesi et al. [2024,\n2025]) of discretely reflected Markovian BSDEs, which involves a $\\Gamma$\nprocess, corresponding to second-order sensitivities of the associated option\nprices. The discretized system is solved by a neural network regression Monte\nCarlo method, efficiently for a large number of underlyings. The resulting\noption Deltas and Gammas are used to discretely rebalance the corresponding\nreplicating strategies. Numerical experiments are presented on both\nhigh-dimensional basket options and large portfolios consisting of multiple\noptions with varying early exercise rights, moneyness and volatility. These\nexamples demonstrate the robustness and accuracy of the method up to $100$ risk\nfactors. The resulting hedging strategies significantly outperform benchmark\nmethods both in the case of standard delta- and delta-gamma hedging.",
      "generated_abstract": "We propose a novel model for the pricing of financial derivatives based on\nthe mean-variance framework. The model is based on the extended mean-variance\n(EMV) framework, which incorporates the concept of mean-variance portfolio\nselection into the mean-variance framework. The EMV framework has been used in\nfinance for more than 30 years, and it has been widely applied in practice. In\nthis paper, we propose an EMV-based mean-variance pricing model for financial\nderivatives, including options, futures, and swaps. The model is based on the\nprinciples of the EMV framework and is a generalization of the EMV model. It\nprovides a framework for pricing financial derivatives and offers a more\ncomprehensive approach than traditional mean-variance models. The EMV\npricing model has a simple and intuitive structure, making it easier to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17117117117117117,
          "p": 0.2835820895522388,
          "f": 0.2134831413729328
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.03669724770642202,
          "f": 0.029739772131397526
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.26865671641791045,
          "f": 0.20224718631675304
        }
      }
    },
    {
      "paper_id": "physics.optics.nlin/SI/2503.08513v1",
      "true_abstract": "We report the results of experimental studies of recurrent spectral dynamics\nof the two component Akhmediev breathers (ABs) in a single mode optical fibre.\nWe also provide the theoretical analysis and numerical simulations of the ABs\nbased on the two component Manakov equations that confirm the experimental\ndata. In particular, we observed spectral asymmetry of fundamental ABs and\ncomplex spectral evolution of second-order nondegenerate ABs.",
      "generated_abstract": "We consider the problem of determining the dispersion of the wavevector\ndistribution of an optically excited electron gas in a periodic potential. We\npropose a scheme for the calculation of the dispersion relation in terms of\nanalytical functions of the electron-electron interaction. The method is\nbased on the use of the eigenvalue problem for the self-consistent equations\nof motion of the electron distribution. Our approach is different from the\nwell-known method based on the eigenvalue problem for the equations of\nmotion of the electron distribution, which is applicable to a broad class of\nsystems. The latter method is used in the literature, but it is often\ndifficult to apply it for the calculation of the dispersion of the wavevector\ndistribution of an electron gas. We also show that the method we propose\nprovides a simpler way to calculate the dispersion relation of the wavevector\ndistribution of an electron gas than the method based on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2391304347826087,
          "p": 0.16417910447761194,
          "f": 0.19469026065940967
        },
        "rouge-2": {
          "r": 0.08196721311475409,
          "p": 0.04504504504504504,
          "f": 0.05813953030624698
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.14925373134328357,
          "f": 0.17699114561516183
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2402.15828v4",
      "true_abstract": "Geometric Asian options are a type of options where the payoff depends on the\ngeometric mean of the underlying asset over a certain period of time. This\npaper is concerned with the pricing of such options for the class of\nVolterra-Heston models, covering the rough Heston model. We are able to derive\nsemi-closed formulas for the prices of geometric Asian options with fixed and\nfloating strikes for this class of stochastic volatility models. These formulas\nrequire the explicit calculation of the conditional joint Fourier transform of\nthe logarithm of the stock price and the logarithm of the geometric mean of the\nstock price over time. Linking our problem to the theory of affine Volterra\nprocesses, we find a representation of this Fourier transform as a suitably\nconstructed stochastic exponential, which depends on the solution of a\nRiccati-Volterra equation. Finally we provide a numerical study for our results\nin the rough Heston model.",
      "generated_abstract": "This paper addresses the issue of modeling and analyzing the effects of\nmarket microstructure on financial markets. We introduce a new conceptual\nframework for analyzing market microstructure effects, based on a three-level\nframework of the financial market and its microstructure. The first level\nrepresents the market-maker and the trading activity. The second level\nrepresents the price-takers, which are the traders who are not market makers.\nThe third level represents the price-takers' market makers, which are the\nfinancial institutions that act as market makers. We propose a set of\nmechanisms that can be applied to different market microstructures to\ncharacterize and analyze their effects on financial markets. Our framework\nprovides a new perspective on the analysis of financial markets and\nconsequently the analysis of financial markets is transformed into the analysis\nof financial markets' microstructure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16470588235294117,
          "p": 0.2028985507246377,
          "f": 0.18181817687215399
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.035398230088495575,
          "f": 0.032921805724060435
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.15942028985507245,
          "f": 0.14285713791111504
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2409.10938v2",
      "true_abstract": "Modern macroeconomic models, particularly those grounded in Rational\nExpectation Dynamic Stochastic General Equilibrium (DSGE), operate under the\nassumption of fully rational decision-making. This paper examines the impact of\nbehavioral factors on the communication index/sentiment index of the US Federal\nReserve. [Upon receiving the review comments, I found some technical errors in\nthe paper. I shall update it accordingly. Please do not cite this paper without\nauthor's permission.]",
      "generated_abstract": "This paper explores the role of uncertainty in economic models. We argue\nthat uncertainty is often neglected in economic models, and that it can be\nuseful for understanding the behavior of economic systems. We show that\nuncertainty can be incorporated into the modeling framework by including a\nfunctional form of uncertainty in the model parameters. This form is often\nreferred to as a risk factor or a risk parameter. We show that the model can\nbe formulated in terms of a stochastic differential equation, and derive the\nequilibrium solution in terms of the risk factor. We also show how the risk\nfactor can be used to generate random outcomes from the model. We provide\nexamples of how uncertainty can be incorporated in economic models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.11290322580645161,
          "f": 0.11764705383235667
        },
        "rouge-2": {
          "r": 0.04477611940298507,
          "p": 0.03,
          "f": 0.03592813890781377
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.0967741935483871,
          "f": 0.10084033114328109
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.03012v1",
      "true_abstract": "We develop a framework to holistically test for and monitor the impact of\ndifferent types of events affecting a country's housing market, yet originating\nfrom housing-external sources. We classify events along three dimensions\nleading to testable hypotheses: prices versus quantities, supply versus demand,\nand immediate versus gradually evolving. These dimensions translate into\nguidance about which data type, statistical measure and testing strategy should\nbe used. To perform such test suitable statistical models are needed which we\nimplement as a hierarchical hedonic price model and a complementary count\nmodel. These models are amended by regime and contextual variables as suggested\nby our classification strategy. We apply this framework to the Austrian real\nestate market together with three disruptive events triggered by the COVID-19\npandemic, a policy tightening mortgage lending standards, as well as the\ncost-of-living crisis that came along with increased financing costs. The tests\nyield the expected results and, by that, some housing market puzzles are\nresolved. Deviating from the prior classification exercise means that some\ndevelopments would have been undetected. Further, adopting our framework\nconsistently when performing empirical research on residential real estate\nwould lead to better comparable research results and, by that, would allow\nresearchers to draw meta-conclusions from the bulk of studies available across\ntime and space.",
      "generated_abstract": "The current global energy system is characterized by high variability,\nhigh intermittency, and low efficiency. These characteristics can be explained\nby the lack of a robust mechanism for balancing energy supply and demand.\nUnbalanced supply can be caused by various factors, including environmental\nconstraints, political factors, and technological limitations. A balanced\nenergy system requires a dynamic mechanism that adjusts supply to demand,\nenhancing energy security and reliability. This paper investigates the\npossibility of integrating wind energy with the existing energy system,\nassessing the system's efficiency and viability through a dynamic balancing\nmechanism. The system consists of wind energy sources, storage batteries, and\na balancing mechanism. The balancing mechanism balances the supply and demand\nof energy through various balancing mechanisms, such as energy storage and\ninterconnecting renewable energy sources. The system has a high potential for",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11724137931034483,
          "p": 0.2236842105263158,
          "f": 0.15384614933355187
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.016129032258064516,
          "f": 0.012269933936544396
        },
        "rouge-l": {
          "r": 0.11724137931034483,
          "p": 0.2236842105263158,
          "f": 0.15384614933355187
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.11743v1",
      "true_abstract": "We consider the constrained sampling problem where the goal is to sample from\na target distribution on a constrained domain. We propose skew-reflected\nnon-reversible Langevin dynamics (SRNLD), a continuous-time stochastic\ndifferential equation with skew-reflected boundary. We obtain non-asymptotic\nconvergence rate of SRNLD to the target distribution in both total variation\nand 1-Wasserstein distances. By breaking reversibility, we show that the\nconvergence is faster than the special case of the reversible dynamics. Based\non the discretization of SRNLD, we propose skew-reflected non-reversible\nLangevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error\nfrom SRNLD, and convergence guarantees to the target distribution in\n1-Wasserstein distance. We show better performance guarantees than the\nprojected Langevin Monte Carlo in the literature that is based on the\nreversible dynamics. Numerical experiments are provided for both synthetic and\nreal datasets to show efficiency of the proposed algorithms.",
      "generated_abstract": "This paper presents a novel method for estimating the population-level\nparameters of a Bayesian nonparametric regression model. The approach is based\non a novel approach for constructing a posterior distribution, which we call the\n{\\it posterior mean-field estimator}. We show that the posterior mean-field\nestimator is consistent and asymptotically normal, and we demonstrate its\napplicability in several Bayesian nonparametric regression scenarios. We also\nshow that the posterior mean-field estimator is consistent for the\npopulation-level parameters when the model is linear, and it is consistent for\nthe population-level parameters when the model is nonlinear.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2653061224489796,
          "f": 0.20472440470952952
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.08571428571428572,
          "f": 0.06217616118016625
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2653061224489796,
          "f": 0.20472440470952952
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.07889v1",
      "true_abstract": "We present a simple method to enable processing of Spotlight Synthetic\nAperture Radar (SAR) imagery distributed in Polar Format (PFA) using standard\nRange-Doppler (RDA) geometry algorithms. Our approach is applicable to PFA SAR\nimages characterized by a constant value of the Center of Aperture (COA) time.\nWe present simplified expressions for forward (image-to-ground) and inverse\n(ground-to-image) geometry mapping using Sensor Independent Complex Data (SICD)\nconventions. We discuss simple changes needed to current open source SAR\nsoftware that implement Range-Doppler algorithms, to enable support within them\nfor Spotlight data distributed in SICD format. We include a proof-of-concept\nscript that utilizes the Python packages sarpy and isce3 to demonstrate the\ncorrectness of the proposed approach.",
      "generated_abstract": "Recent advancements in 3D ultrasound have enabled the development of high-\nmechanical contrast techniques, which improve contrast and image quality\nsignificantly. However, these methods typically rely on the use of mechanical\ncontrast agents, which are often associated with additional risks. To address\nthese issues, we propose a novel methodology, Deep-Mechanical, that leverages\nthe power of machine learning to learn the dynamics of mechanical contrast\nagents in 3D ultrasound images. This approach enables the model to learn the\neffects of mechanical contrast agents on ultrasound images, which can be used to\nenhance image quality without the need for physical contrast agents. Additionally,\nthis approach can be used to create virtual models of human organs for 3D\nultrasound imaging, which can be used for training or assessing surgical\ntechniques.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12048192771084337,
          "p": 0.125,
          "f": 0.1226993815047614
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10843373493975904,
          "p": 0.1125,
          "f": 0.11042944285445466
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2411.06076v1",
      "true_abstract": "This paper introduces BreakGPT, a novel large language model (LLM)\narchitecture adapted specifically for time series forecasting and the\nprediction of sharp upward movements in asset prices. By leveraging both the\ncapabilities of LLMs and Transformer-based models, this study evaluates\nBreakGPT and other Transformer-based models for their ability to address the\nunique challenges posed by highly volatile financial markets. The primary\ncontribution of this work lies in demonstrating the effectiveness of combining\ntime series representation learning with LLM prediction frameworks. We showcase\nBreakGPT as a promising solution for financial forecasting with minimal\ntraining and as a strong competitor for capturing both local and global\ntemporal dependencies.",
      "generated_abstract": "The emergence of large financial institutions, the evolution of risk\nmanagement, and the increasing complexity of financial markets have led to a\nhigh degree of interconnectedness and complexities, posing challenges to\nmanagers and investors in managing and analyzing financial risks. Traditional\nrisk management approaches, such as the use of risk models, are limited in\ntheir ability to capture the complexities of financial markets and the\nnonlinearity of risks. Traditional approaches, such as the use of risk models,\nare limited in their ability to capture the complexities of financial markets\nand the nonlinearity of risks. In this paper, we propose a novel model for\nquantifying and managing the risks associated with financial products in a\nnonlinear, non-Markovian, and stochastic environment. Our model leverages the\nnonlinearity and stochasticity of financial markets to incorporate uncertainty",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25316455696202533,
          "p": 0.3076923076923077,
          "f": 0.2777777728250386
        },
        "rouge-2": {
          "r": 0.04854368932038835,
          "p": 0.050505050505050504,
          "f": 0.0495049454970106
        },
        "rouge-l": {
          "r": 0.24050632911392406,
          "p": 0.2923076923076923,
          "f": 0.2638888839361498
        }
      }
    },
    {
      "paper_id": "physics.plasm-ph.physics/plasm-ph/2503.10067v1",
      "true_abstract": "The significance of laser-driven polarized beam acceleration has been\nincreasingly recognized in recent years. We propose an efficient method for\ngenerating polarized proton beams from a pre-polarized hydrogen halide gas jet,\nutilizing magnetic vortex acceleration enhanced by a laser-driven plasma\nbubble. When a petawatt laser pulse passes through a pre-polarized gas jet, a\nbubble-like ultra-nonlinear plasma wave is formed. As part of the wave\nparticles, background protons are swept by the acceleration field of the bubble\nand oscillate significantly along the laser propagation axis. Some of the\npre-accelerated protons in the plasma wave are trapped by the acceleration\nfield at the rear side of the target. This acceleration field is intensified by\nthe transverse expansion of the laser-driven magnetic vortex, resulting in\nenergetic polarized proton beams. The spin of energetic protons is determined\nby their precession within the electromagnetic field, as described by the\nThomas-Bargmann-Michel-Telegdi equation in analytical models and\nparticle-in-cell simulations. Multidimensional simulations reveal that\nmonoenergetic proton beams with hundreds of MeV in energy, a beam charge of\nhundreds of pC, and a beam polarization of tens of percent can be produced at\nlaser powers of several petawatts. Laser-driven polarized proton beams offer\npromising potential for application in polarized beam colliders, where they can\nbe utilized to investigate particle interactions and to explore the properties\nof matter under unique conditions.",
      "generated_abstract": "We propose a new approach to the study of electron transport in\nelectron-rich plasmas, based on a novel treatment of the electron-phonon\ninteraction, which accounts for electron-phonon coupling in the framework of\nthe two-body scattering approach. We show that the resulting electron\ntransport equation, which is a linear second-order partial differential\nequation, is solvable by the standard eigenvalue method. We discuss the\napplicability of this approach to electron-rich plasmas, focusing on the\ninfluence of electron-phonon coupling on the electron temperature. We\nillustrate our approach with a numerical simulation of electron-rich plasmas in\na laser-plasma torus.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1076923076923077,
          "p": 0.2545454545454545,
          "f": 0.15135134717311918
        },
        "rouge-2": {
          "r": 0.020202020202020204,
          "p": 0.045454545454545456,
          "f": 0.027972023711673594
        },
        "rouge-l": {
          "r": 0.09230769230769231,
          "p": 0.21818181818181817,
          "f": 0.1297297255514976
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/MF/2502.05839v1",
      "true_abstract": "In this paper, we examine a modified version of de Finetti's optimal dividend\nproblem, incorporating fixed transaction costs and altering the surplus process\nby introducing two-valued drift and two-valued volatility coefficients. This\nmodification aims to capture the transitions or adjustments in the company's\nfinancial status. We identify the optimal dividend strategy, which maximizes\nthe expected total net dividend payments (after accounting for transaction\ncosts) until ruin, as a two-barrier impulsive dividend strategy. Notably, the\noptimal strategy can be explicitly determined for almost all scenarios\ninvolving different drifts and volatility coefficients. Our primary focus is on\nexploring how changes in drift and volatility coefficients influence the\noptimal dividend strategy.",
      "generated_abstract": "This paper studies the portfolio optimization problem under market risk. We\nunderstand the dynamic effects of the market risk premium and the liquidity\nrisk premium on portfolio performance. The market risk premium and the liquidity\nrisk premium are both non-zero, and their effect on portfolio performance is\nhighly non-monotonic. The market risk premium, which is positive and\ndecreases with increasing market risk, has a positive impact on the\nportfolio's expected return. The liquidity risk premium, which is negative and\nincreases with increasing liquidity risk, has a negative impact on the\nportfolio's expected return. The market risk premium has a positive and\nincreasing impact on the portfolio's volatility. The liquidity risk premium\nhas a negative and decreasing impact on the portfolio's volatility. Our\nfindings provide valuable insights",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15,
          "p": 0.23529411764705882,
          "f": 0.18320610211526148
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1375,
          "p": 0.21568627450980393,
          "f": 0.1679389265427424
        }
      }
    },
    {
      "paper_id": "math.ST.math/ST/2503.06889v1",
      "true_abstract": "Community detection, which focuses on recovering the group structure within\nnetworks, is a crucial and fundamental task in network analysis. However, the\ndetection process can be quite challenging and unstable when community signals\nare weak. Motivated by a newly collected large-scale academic network dataset\nfrom the Web of Science, which includes multi-layer network information, we\npropose a Bipartite Assisted Spectral-clustering approach for Identifying\nCommunities (BASIC), which incorporates the bipartite network information into\nthe community structure learning of the primary network. The accuracy and\nstability enhancement of BASIC is validated theoretically on the basis of the\ndegree-corrected stochastic block model framework, as well as numerically\nthrough extensive simulation studies. We rigorously study the convergence rate\nof BASIC even under weak signal scenarios and prove that BASIC yields a tighter\nupper error bound than that based on the primary network information alone. We\nutilize the proposed BASIC method to analyze the newly collected large-scale\nacademic network dataset from statistical papers. During the author\ncollaboration network structure learning, we incorporate the bipartite network\ninformation from author-paper, author-institution, and author-region\nrelationships. From both statistical and interpretative perspectives, these\nbipartite networks greatly aid in identifying communities within the primary\ncollaboration network.",
      "generated_abstract": "We investigate the geometry of the moduli spaces of stable maps between\ncomplex manifolds. The most important results are the characterization of the\nstability of the moduli spaces for certain classes of maps and the proof that\nthe moduli spaces of stable maps are smooth. In particular, we show that the\nmoduli space of stable maps between a smooth projective curve and a smooth\nprojective surface is smooth. Furthermore, we show that the moduli space of\nstable maps between a smooth projective curve and a smooth projective surface\nwith a fixed singular fiber has a unique geometric structure that is\nisomorphic to the moduli space of stable maps between the singular fiber and\nthe normalization of the singular fiber. Our results generalize those of\nViehweg and Zhang.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.109375,
          "p": 0.2641509433962264,
          "f": 0.15469612845517547
        },
        "rouge-2": {
          "r": 0.0056179775280898875,
          "p": 0.012345679012345678,
          "f": 0.007722003423326433
        },
        "rouge-l": {
          "r": 0.1015625,
          "p": 0.24528301886792453,
          "f": 0.1436464046982694
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/MF/2501.17577v1",
      "true_abstract": "We study a class of singular stochastic control problems for a\none-dimensional diffusion $X$ in which the performance criterion to be\noptimised depends explicitly on the running infimum $I$ (or supremum $S$) of\nthe controlled process. We introduce two novel integral operators that are\nconsistent with the Hamilton-Jacobi-Bellman equation for the resulting\ntwo-dimensional singular control problems. The first operator involves\nintegrals where the integrator is the control process of the two-dimensional\nprocess $(X,I)$ or $(X,S)$; the second operator concerns integrals where the\nintegrator is the running infimum or supremum process itself. Using these\ndefinitions, we prove a general verification theorem for problems involving\ntwo-dimensional state-dependent running costs, costs of controlling the\nprocess, costs of increasing the running infimum (or supremum) and exit times.\nFinally, we apply our results to explicitly solve an optimal dividend problem\nin which the manager's time-preferences depend on the company's historical\nworst performance.",
      "generated_abstract": "In this paper, we investigate the existence of a unique solution to the\nsystem of stochastic differential equations\n\\begin{equation}\n\\begin{cases}\ndX_t = f_t(X_t)dt + g_t(X_t)dW_t, & t \\in (0,T),\\\\\nX_T = x_T, & t = T.\n\\end{cases}\n\\end{equation}\nThe coefficients $f_t(x)$ and $g_t(x)$ are bounded and uniformly continuous,\nwhich implies the existence of a unique solution $X$ to this system. Moreover,\nif $x_T$ is an arbitrary point, then $X$ is the unique solution of this system\nto the initial condition $X_0 = x_0$.\n  We prove that the function $x \\mapsto f_t(x) + g_t(x",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.25,
          "f": 0.2038216512215507
        },
        "rouge-2": {
          "r": 0.007633587786259542,
          "p": 0.012048192771084338,
          "f": 0.009345789644076131
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.21875,
          "f": 0.17834394421518127
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10791v1",
      "true_abstract": "We investigate methods for forecasting multivariate realized covariances\nmatrices applied to a set of 30 assets that were included in the DJ30 index at\nsome point, including two novel methods that use existing (univariate) log of\nrealized variance models that account for attenuation bias and time-varying\nparameters. We consider the implications of some modeling choices within the\nclass of heterogeneous autoregressive models. The following are our key\nfindings. First, modeling the logs of the marginal volatilities is strongly\npreferred over direct modeling of marginal volatility. Thus, our proposed model\nthat accounts for attenuation bias (for the log-response) provides superior\none-step-ahead forecasts over existing multivariate realized covariance\napproaches. Second, accounting for measurement errors in marginal realized\nvariances generally improves multivariate forecasting performance, but to a\nlesser degree than previously found in the literature. Third, time-varying\nparameter models based on state-space models perform almost equally well.\nFourth, statistical and economic criteria for comparing the forecasting\nperformance lead to some differences in the models' rankings, which can\npartially be explained by the turbulent post-pandemic data in our out-of-sample\nvalidation dataset using sub-sample analyses.",
      "generated_abstract": "We develop a method for estimating the mean function of a time-inhomogeneous\ndistribution in a large-scale model with latent state variables. The method\nrelies on a new multivariate autoregressive (MAR) model that incorporates\nadditional structural information about the distribution, including the\ndistributional shape of the mean function. We derive the asymptotic\ndistribution of the estimator and illustrate its use through simulation\nexperiments and a real data application.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.32653061224489793,
          "f": 0.18285713882514293
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.031746031746031744,
          "f": 0.016949148628627734
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.2857142857142857,
          "f": 0.1599999959680001
        }
      }
    },
    {
      "paper_id": "cs.LG.math/ST/2503.07453v1",
      "true_abstract": "Language model alignment (or, reinforcement learning) techniques that\nleverage active exploration -- deliberately encouraging the model to produce\ndiverse, informative responses -- offer the promise of super-human\ncapabilities. However, current understanding of algorithm design primitives for\ncomputationally efficient exploration with language models is limited. To\nbetter understand how to leverage access to powerful pre-trained generative\nmodels to improve the efficiency of exploration, we introduce a new\ncomputational framework for RL with language models, in which the learner\ninteracts with the model through a sampling oracle. Focusing on the linear\nsoftmax model parameterization, we provide new results that reveal the\ncomputational-statistical tradeoffs of efficient exploration:\n  1. Necessity of coverage: Coverage refers to the extent to which the\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\nWe show that coverage, while not necessary for data efficiency, lower bounds\nthe runtime of any algorithm in our framework.\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\nwhich obtains optimal data efficiency and is computationally efficient whenever\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\nSpannerSampling leverages inference-time computation with the pre-trained model\nto reduce the effective search space for exploration.\n  3. Insufficiency of training-time interventions: We contrast the result above\nby showing that training-time interventions that produce proper policies cannot\nachieve similar guarantees in polynomial time.\n  4. Computational benefits of multi-turn exploration: Finally, we show that\nunder additional representational assumptions, one can achieve improved runtime\n(replacing sequence-level coverage with token-level coverage) through\nmulti-turn exploration.",
      "generated_abstract": "We study a class of non-negative matrix-valued functions $H:\\mathbb{R}^d\\to\nH_\\infty$, where $H_\\infty$ is the class of all functions $H:\\mathbb{R}\\to\n\\mathbb{R}$ that are uniformly bounded and $H(0)=0$. $H$ is called a\n\\emph{Hilbert-Schmidt kernel} if $H(X)=H(Y)=K(X,Y)$ for some square\nintegrable $K:\\mathbb{R}^d\\to\\mathbb{R}$ and all $X,Y\\in\\mathbb{R}^d$. We show\nthat, for a large class of $H$, the $H$-Laplacian on the unit ball of $\\mathbb{R}^d$\nis Hilbert-Schmidt. We also show that, for some classes of $H$, the $H$-Laplacian\non",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06451612903225806,
          "p": 0.2127659574468085,
          "f": 0.09900989741937077
        },
        "rouge-2": {
          "r": 0.008658008658008658,
          "p": 0.031746031746031744,
          "f": 0.013605438809524644
        },
        "rouge-l": {
          "r": 0.05806451612903226,
          "p": 0.19148936170212766,
          "f": 0.08910890732036089
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15090v1",
      "true_abstract": "The R package bsvars provides a wide range of tools for empirical\nmacroeconomic and financial analyses using Bayesian Structural Vector\nAutoregressions. It uses frontier econometric techniques and C++ code to ensure\nfast and efficient estimation of these multivariate dynamic structural models,\npossibly with many variables, complex identification strategies, and non-linear\ncharacteristics. The models can be identified using adjustable exclusion\nrestrictions and heteroskedastic or non-normal shocks. They feature a flexible\nthree-level equation-specific local-global hierarchical prior distribution for\nthe estimated level of shrinkage for autoregressive and structural parameters.\nAdditionally, the package facilitates predictive and structural analyses such\nas impulse responses, forecast error variance and historical decompositions,\nforecasting, statistical verification of identification and hypotheses on\nautoregressive parameters, and analyses of structural shocks, volatilities, and\nfitted values. These features differentiate bsvars from existing R packages\nthat either focus on a specific structural model, do not consider\nheteroskedastic shocks, or lack the implementation using compiled code.",
      "generated_abstract": "This paper introduces a novel method for modeling and predicting\nperformance indicators in a variety of settings, including education, health,\nand science. The method, referred to as the Causal Matrix of Performance\nIndicators (CMPI), combines the causal graph of a model with the matrix of\nperformance indicators to produce a causal matrix of performance indicators.\nThis enables the identification of causal effects from performance indicators,\neven when the direct effect of a variable on a performance indicator is\nnonlinear or non-existent. The CMPI can be estimated using a variety of\nmethods, including a generalized method of moments (GMM) approach, the\ninformation matrix approach, and the Bayesian information criterion (BIC). The\nCMPI is also suitable for analyzing multivariate data. The method is applied to\ndata on the effects of education on earnings and health in the United States,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.24050632911392406,
          "f": 0.19895287473040774
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.015625,
          "f": 0.014440428241735604
        },
        "rouge-l": {
          "r": 0.16964285714285715,
          "p": 0.24050632911392406,
          "f": 0.19895287473040774
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2502.13431v1",
      "true_abstract": "This study proposes a novel functional vector autoregressive framework for\nanalyzing network interactions of functional outcomes in panel data settings.\nIn this framework, an individual's outcome function is influenced by the\noutcomes of others through a simultaneous equation system. To estimate the\nfunctional parameters of interest, we need to address the endogeneity issue\narising from these simultaneous interactions among outcome functions. This\nissue is carefully handled by developing a novel functional moment-based\nestimator. We establish the consistency, convergence rate, and pointwise\nasymptotic normality of the proposed estimator. Additionally, we discuss the\nestimation of marginal effects and impulse response analysis. As an empirical\nillustration, we analyze the demand for a bike-sharing service in the U.S. The\nresults reveal statistically significant spatial interactions in bike\navailability across stations, with interaction patterns varying over the time\nof day.",
      "generated_abstract": "This paper proposes a novel approach for inference in multivariate Gaussian\nmultilevel models. We develop a novel multilevel regression estimator for the\nmultivariate Gaussian distribution in the multilevel model, which we call the\nmultilevel generalized linear Gaussian multilevel (MGLGM). We demonstrate the\nuse of this estimator in a simulation study and in an application to the\nstudy of the impact of school-based mental health services on student outcomes\nin a school-based mental health service study in the Netherlands. We show that\nthe multilevel generalized linear Gaussian multilevel model outperforms the\nmultilevel model, and that the MGLGM is particularly well-suited for\nestimating the average treatment effect in a study with multiple treatments.\nAdditionally, we demonstrate the use of the multilevel generalized linear Gaussian\nmultilevel model for predictive modeling and the estimation of the average",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.3384615384615385,
          "f": 0.26829267814173113
        },
        "rouge-2": {
          "r": 0.06015037593984962,
          "p": 0.07766990291262135,
          "f": 0.06779660525028763
        },
        "rouge-l": {
          "r": 0.20202020202020202,
          "p": 0.3076923076923077,
          "f": 0.24390243423929217
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2503.04677v1",
      "true_abstract": "We present a minimal model to analyze the capacitive response of a biological\nmembrane subjected to a step voltage via blocking electrodes. Through a\nperturbative analysis of the underlying electrolyte transport equations, we\nshow that the leading-order relaxation of the transmembrane potential is\ngoverned by a capacitive timescale, ${\\tau_{\\rm C} =\\dfrac{\\lambda_{\\rm\nD}L}{D}\\left(\\dfrac{2+\\Gamma\\delta^{\\rm M}/L}{4+\\Gamma\\delta^{\\rm\nM}/\\lambda_{\\rm D}}\\right)}$, where $\\lambda_{\\rm D}$ is the Debye screening\nlength, $L$ is the electrolyte width, $\\Gamma$ is the ratio of the dielectric\npermittivity of the electrolyte to the membrane, $\\delta^{\\rm M}$ is the\nmembrane thickness, and $D$ is the ionic diffusivity. This timescale is\nconsiderably shorter than the traditional RC timescale ${\\lambda_{\\rm D} L /\nD}$ for a bare electrolyte due to the membrane's low dielectric permittivity\nand finite thickness. Beyond the linear regime, however, salt diffusion in the\nbulk electrolyte drives a secondary, nonlinear relaxation process of the\ntransmembrane potential over a longer timescale ${\\tau_{\\rm L} =L^2/4\\pi^2 D}$.\nA simple equivalent-circuit model accurately captures the linear behavior, and\nthe perturbation expansion remains applicable across the entire range of\nobserved physiological transmembrane potentials. Together, these findings\nunderscore the importance of the faster capacitive timescale and nonlinear\neffects on the bulk diffusion timescale in determining transmembrane potential\ndynamics for a range of biological systems.",
      "generated_abstract": "The concept of a single cell, which was developed in the 1950s, has evolved\ninto a concept that encompasses many different types of cells, including\ntissues and organs. However, the concept of a cell, as it is defined today,\nremains relatively vague. In this paper, we review the history of the concept of\na cell and describe its evolution. We also discuss the concept of a tissue and\nits relation to the concept of a cell, and we propose a definition of a tissue\nthat we believe is more general and more applicable to all types of cells. We\nthen discuss the concept of an organ, and we propose a definition of an organ\nthat is more general and more applicable to all types of cells.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.096,
          "p": 0.1935483870967742,
          "f": 0.1283422415568076
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.04395604395604396,
          "f": 0.028880862014102254
        },
        "rouge-l": {
          "r": 0.08,
          "p": 0.16129032258064516,
          "f": 0.10695186722525685
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-th/2503.09204v1",
      "true_abstract": "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode.",
      "generated_abstract": "We study the dynamics of hadronic matter in the framework of the\nLattice QCD. We investigate the effect of the hadronic interactions on the\nstructure of the hadronic matter. We introduce the hadronic phase transition\nparameters to determine the critical temperature, $T_c$, and the hadronic\nphase transition density, $\\rho_c$, and calculate their dependence on the\nhadronic interactions. We also calculate the critical baryon chemical potential\nand the critical temperature, $T_c$, and the hadronic phase transition density,\n$\\rho_c$, for different hadronic interactions and find the critical baryon\nchemical potential and the critical temperature, $T_c$, and the hadronic phase\ntransition density, $\\rho_c$, as a function of the quark mass $m_q$ for the\nquark-diquark model with the potential $\\Phi(r)=(\\frac",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14084507042253522,
          "p": 0.20833333333333334,
          "f": 0.16806722207753705
        },
        "rouge-2": {
          "r": 0.031914893617021274,
          "p": 0.04054054054054054,
          "f": 0.03571428078514808
        },
        "rouge-l": {
          "r": 0.1267605633802817,
          "p": 0.1875,
          "f": 0.15126049938846142
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/ST/2411.06080v1",
      "true_abstract": "Portfolio diversification, traditionally measured through asset correlations\nand volatilitybased metrics, is fundamental to managing financial risk.\nHowever, existing diversification metrics often overlook non-numerical\nrelationships between assets that can impact portfolio stability, particularly\nduring market stresses. This paper introduces the lexical ratio (LR), a novel\nmetric that leverages textual data to capture diversification dimensions absent\nin standard approaches. By treating each asset as a unique document composed of\nsectorspecific and financial keywords, the LR evaluates portfolio\ndiversification by distributing these terms across assets, incorporating\nentropy-based insights from information theory. We thoroughly analyze LR's\nproperties, including scale invariance, concavity, and maximality,\ndemonstrating its theoretical robustness and ability to enhance risk-adjusted\nportfolio returns. Using empirical tests on S&P 500 portfolios, we compare LR's\nperformance to established metrics such as Markowitz's volatility-based\nmeasures and diversification ratios. Our tests reveal LR's superiority in\noptimizing portfolio returns, especially under varied market conditions. Our\nfindings show that LR aligns with conventional metrics and captures unique\ndiversification aspects, suggesting it is a viable tool for portfolio managers.",
      "generated_abstract": "This paper introduces a novel methodology to enhance the efficiency of\nthe risk-neutral valuation of financial derivatives, using a continuous\ntime stochastic process to approximate the underlying processes. This approach\noffers several advantages over traditional Monte Carlo simulation-based methods\ninvolving discretization. The methodology is based on the introduction of a\nparameterized stochastic process that captures the complexities of the\nunderlying processes, which enables the simulation of the derivatives at\ninfinite time horizons. By incorporating a neural network model to approximate\nthe stochastic process, the methodology provides a fast, accurate and\nefficient approach to the risk-neutral valuation of financial derivatives.\nUsing the methodology, we demonstrate its effectiveness by simulating the\nderivatives of the FX-linked options of the S&P 500 index.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17164179104477612,
          "p": 0.3026315789473684,
          "f": 0.21904761442902504
        },
        "rouge-2": {
          "r": 0.02976190476190476,
          "p": 0.047619047619047616,
          "f": 0.03663003189630942
        },
        "rouge-l": {
          "r": 0.14925373134328357,
          "p": 0.2631578947368421,
          "f": 0.19047618585759646
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.10285v1",
      "true_abstract": "Accurate prediction of expected concentrations is essential for effective\ncatchment management, requiring both extensive monitoring and advanced modeling\ntechniques. However, due to limitations in the equation solving capacity, the\nintegration of monitoring and modeling has been suffering suboptimal\nstatistical approaches. This limitation results in models that can only\npartially leverage monitoring data, thus being an obstacle for realistic\nuncertainty assessments by overlooking critical correlations between both\nmeasurements and model parameters. This study presents a novel solution that\nintegrates catchment monitoring and a unified hieratical statistical catchment\nmodeling that employs a log-normal distribution for residuals within a\nleft-censored likelihood function to address measurements below detection\nlimits. This enables the estimation of concentrations within sub-catchments in\nconjunction with a source/fate sub-catchment model and monitoring data. This\napproach is possible due to a model builder R package denoted RTMB. The\nproposed approach introduces a statistical paradigm based on a hierarchical\nstructure, capable of accommodating heterogeneous sampling across various\nsampling locations and the authors suggest that this also will encourage\nfurther refinement of other existing modeling platforms within the scientific\ncommunity to improve synergy with monitoring programs. The application of the\nmethod is demonstrated through an analysis of nickel concentrations in Danish\nsurface waters.",
      "generated_abstract": "The growing complexity of modern systems has led to an increase in the\nrequirements for robust and reliable systems, and as a consequence, the\ndevelopment of robust control methods. In this paper, we propose a novel\nRobust Control Strategy for a Nonlinear System with State and Parameter\nUncertainty, which is based on the Kalman Filter and the Expected Value\nFilter. The proposed control strategy is evaluated through a simulation study\nwith realistic parameters, such as the state and parameter uncertainties.\nSimulation results demonstrate that the proposed control strategy achieves\nrobustness and stability. This study provides a new approach for robust control\nof nonlinear systems with state and parameter uncertainty.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18518518518518517,
          "p": 0.3424657534246575,
          "f": 0.2403846108288647
        },
        "rouge-2": {
          "r": 0.030612244897959183,
          "p": 0.058823529411764705,
          "f": 0.04026845187333955
        },
        "rouge-l": {
          "r": 0.14074074074074075,
          "p": 0.2602739726027397,
          "f": 0.18269230313655704
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09577v1",
      "true_abstract": "Chip-firing is a combinatorial game played on a graph in which we place and\ndisperse chips on vertices until a stable configuration is reached. We study a\nchip-firing variant played on an infinite rooted directed $k$-ary tree, where\nwe place $k^n$ chips labeled $0,1,\\dots, k^n-1$ on the root for some\nnonnegative integer $n$, and we say a vertex $v$ can fire if it has at least\n$k$ chips. A vertex fires by dispersing one chip to each out-neighbor. Once\nevery vertex has less than $k$ chips, we reach a stable configuration since no\nvertex can fire. In this paper, we focus on stable configurations resulting\nfrom applying a strategy $F_w$ corresponding to a permutation $w = w_1w_2\\dots\nw_n\\in S_n$: for each vertex $v$ on level $i$ of the $k$-ary tree, the chip\nwith $j$ as its $w_i$th most significant digit in the $k$-ary expansion gets\nsent to the $(j+1)$st child of $v$. We express the stable configuration as a\npermutation, and we explore the properties of these permutations, such as the\nnumber of inversions, descents, and the descent set.",
      "generated_abstract": "We establish a one-to-one correspondence between the set of all non-zero\ncorresponding matrices of polynomials in two variables and the set of all\nnon-zero correspondence matrices of the polynomials of the same degree in one\nvariable. We prove that the set of all non-zero correspondence matrices of\npolynomials in two variables is equal to the set of all non-zero correspondence\nmatrices of polynomials of the same degree in one variable. We also prove that\nthe set of all non-zero correspondence matrices of polynomials of the same\ndegree in one variable is equal to the set of all non-zero correspondence\nmatrices of polynomials of the same degree in two variables.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09565217391304348,
          "p": 0.39285714285714285,
          "f": 0.15384615069685567
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.075,
          "f": 0.0288461507396453
        },
        "rouge-l": {
          "r": 0.09565217391304348,
          "p": 0.39285714285714285,
          "f": 0.15384615069685567
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/SC/2311.13203v2",
      "true_abstract": "Filamentous viruses like influenza and torovirus often display systematic\nbends and arcs of mysterious physical origin. We propose that such viruses\nundergo an instability from a cylindrically symmetric to a toroidally curved\nstate. This ``toro-elastic'' state emerges via spontaneous symmetry breaking\nunder prestress due to short range spike protein interactions magnified by %the\nfilament's surface topography. Once surface stresses are sufficiently large,\nthe filament buckles and the curved state constitutes a soft mode that can\npotentially propagate through the filament's material frame around a\nmexican-hat-type potential. In the mucus of our airways, which constitutes a\nsoft, porous 3D network, glycan chains are omnipresent and influenza's spike\nproteins are known to efficiently bind and cut them. We next show that such a\nnon-equilibrium enzymatic reaction can induce spontaneous rotation of the\ncurved state, leading to a whole body reshaping propulsion similar to -- but\ndifferent from -- eukaryotic flagella and spirochetes.",
      "generated_abstract": "We investigate the role of non-equilibrium fluctuations in the dynamics of\nmechanical response of a solid. We consider a solid-state system in contact with\nan external mechanical environment and apply the fluctuation-dissipation\ntheorem to obtain the stochastic equations of motion. We derive the\nfluctuation-dissipation relation for the response of the system to a\ncontinuous-time stochastic forcing. By solving this equation, we obtain the\nstochastic equation of motion for the response of the system to a\ntime-varying stochastic forcing. The dynamics of the system are then obtained\nfrom the solution of this equation of motion. We apply our method to the case\nof a system of coupled oscillators subject to a time-varying sinusoidal\nexternal forcing.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.2,
          "f": 0.13333332888888905
        },
        "rouge-2": {
          "r": 0.013888888888888888,
          "p": 0.02197802197802198,
          "f": 0.017021271850069228
        },
        "rouge-l": {
          "r": 0.07272727272727272,
          "p": 0.14545454545454545,
          "f": 0.09696969252525273
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.08938v1",
      "true_abstract": "Alzheimer's disease (AD) is driven by the accumulation of amyloid-beta\n(Abeta) proteins in the brain, leading to memory loss and cognitive decline.\nWhile monoclonal antibodies targeting Abetahave been approved, optimizing their\nuse to maximize benefits while minimizing side effects remains a challenge.\nThis study develops a mathematical model to describe Abeta aggregation,\ncapturing its progression from monomers to toxic oligomers, protofibrils, and\nfibrils using mass-action kinetics and coarse-grained modeling. The model is\ncalibrated with experimental data, incorporating parameter estimation and\nsensitivity analysis to ensure accuracy. An optimal control framework is\nintroduced to determine the best drug dosing strategy that reduces toxic Abeta\naggregates while minimizing adverse effects, such as amyloid-related imaging\nabnormalities (ARIA). Results indicate that Donanemab achieves the greatest\nreduction in fibrils. This work provides a quantitative framework for\noptimizing AD treatment strategies, offering insights into balancing\ntherapeutic efficacy and safety.",
      "generated_abstract": "Recent advances in machine learning (ML) have enabled the development of\ntranslation-based generative models (TGMs) that can produce high-quality\nrepresentations of biological sequences. However, existing methods are\nrestricted to single-gene sequences, which limits their applicability to\nmulti-gene and multi-gene-gene interactions. To address this limitation, we\nintroduce the BioTGM, a novel TGM that generates high-quality gene\ninteraction representations. The BioTGM is based on a novel generative model\nthat learns to generate gene-gene interaction representations from gene\nsequences. Our model employs a sequence-to-sequence model to generate the\ninteraction representations, which are then processed through a\nmulti-headed attention mechanism to generate the final interaction representations.\nThe BioTGM is evaluated on three benchmark datasets, including a\nmulti-gene-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10434782608695652,
          "p": 0.15384615384615385,
          "f": 0.12435232678998111
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.009708737864077669,
          "f": 0.008230447790820692
        },
        "rouge-l": {
          "r": 0.09565217391304348,
          "p": 0.14102564102564102,
          "f": 0.11398963248946302
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/space-ph/2503.08520v1",
      "true_abstract": "Solitons are predominantly observed in near-earth plasmas as well as\nplanetary magnetospheres; however, their existence in the solar corona remains\nlargely unexplored, despite theoretical investigations. This study aims to\naddress this gap by examining the presence and dynamics of solitons in the\nsolar corona, particularly in the context of coronal heating. Utilizing\nobservational data from the Parker Solar Probe (PSP) and Solar and Heliospheric\nObservatory (SOHO) during the onset of a strong Coronal Mass Ejection (CME)\nevent, the analyses reveal a train of aperiodic solitons with increasing\namplitude preceding the eruption. A key finding of this study is that the\nobserved aperiodic soliton train serves as a potential candidate in\nfacilitating energy transfer through dissipation within the coronal plasma,\nhereby, influencing the initiation of solar eruptive events such as a CME. A\ndefining characteristic of this solitary train is its hypersonic and\nsuper-Alfvenic nature, evident from the presence of high Mach numbers that\nreinforces its role in plasma energy equilibration in the solar corona, thereby\ncontributing to plasma heating.",
      "generated_abstract": "The formation of circumstellar discs in planetary systems is a key process\nin stellar evolution, providing insights into the formation and evolution of\nstars and planets. The gravitational instability of the discs leads to the\nformation of planetesimals, which are subsequently accreted by the star. In\naddition, the angular momentum of the planetesimals can be transferred to the\nstar, leading to the dynamical formation of the disc. The discs can be\nself-consistently formed during the dynamical evolution of the system, or they\ncan be formed after the system has evolved into a more stable state. In the\nlatter case, a dynamical disc can be artificially created by injecting a\nsmall fraction of the angular momentum of the star into the system,\nsimultaneously injecting a small fraction of the angular momentum of the\nplanetesimals into the system. This paper",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11711711711711711,
          "p": 0.19117647058823528,
          "f": 0.1452513919365814
        },
        "rouge-2": {
          "r": 0.006329113924050633,
          "p": 0.009615384615384616,
          "f": 0.007633582998662754
        },
        "rouge-l": {
          "r": 0.0990990990990991,
          "p": 0.16176470588235295,
          "f": 0.12290502322149764
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.01514v1",
      "true_abstract": "Advancements in data collection have led to increasingly common repeated\nobservations with complex structures in biomedical studies. Treating these\nobservations as random objects, rather than summarizing features as vectors,\navoids feature extraction and better reflects the data's nature. Examples\ninclude repeatedly measured activity intensity distributions in physical\nactivity analysis and brain networks in neuroimaging. Testing whether these\nrepeated random objects differ across groups is fundamentally important;\nhowever, traditional statistical tests often face challenges due to the\nnon-Euclidean nature of metric spaces, dependencies from repeated measurements,\nand the unequal number of repeated measures. By defining within-subject\nvariability using pairwise distances between repeated measures and extending\nFr\\'echet analysis of variance, we develop a generalized Fr\\'echet test for\nexchangeable repeated random objects, applicable to general metric space-valued\ndata with unequal numbers of repeated measures. The proposed test can\nsimultaneously detect differences in location, scale, and within-subject\nvariability. We derive the asymptotic distribution of the test statistic, which\nfollows a weighted chi-squared distribution. Simulations demonstrate that the\nproposed test performs well across different types of random objects. We\nillustrate its effectiveness through applications to physical activity data and\nresting-state functional magnetic resonance imaging data.",
      "generated_abstract": "This paper presents a Bayesian method for modeling longitudinal data with\nintermittent events. The method is based on a hierarchical modeling framework\nin which the main model consists of a general parametric distribution\ndescribing the conditional probability of an event occurring, while the\nintermittent model consists of a Markov chain with state space representation.\nIn the proposed approach, the Markov chain is modeled using a Markov chain\nmonitoring model (MCMM) to capture the dependence between consecutive events.\nThe proposed MCMM is based on the assumption that the conditional probability\nof an intermittent event occurring depends only on the conditional probability\nof the previous intermittent event. The resulting model is a mixture of a\ngeneral parametric distribution and a Markov chain with state space\nrepresentation. The proposed MCMM is evaluated using simulation studies and\nreal data analysis. The proposed methodology is applied to analyze the\npro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.2777777777777778,
          "f": 0.1980197973924126
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.044642857142857144,
          "f": 0.034246570613624225
        },
        "rouge-l": {
          "r": 0.13076923076923078,
          "p": 0.2361111111111111,
          "f": 0.16831682709538293
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07102v1",
      "true_abstract": "The paper proposes a novel Economic Model Predictive Control (EMPC) scheme\nfor Autonomous Surface Vehicles (ASVs) to simultaneously address path following\naccuracy and energy constraints under environmental disturbances. By\nformulating lateral deviations as energy-equivalent penalties in the cost\nfunction, our method enables explicit trade-offs between tracking precision and\nenergy consumption. Furthermore, a motion-dependent decomposition technique is\nproposed to estimate terminal energy costs based on vehicle dynamics. Compared\nwith the existing EMPC method, simulations with real-world ocean disturbance\ndata demonstrate the controller's energy consumption with a 0.06 energy\nincrease while reducing cross-track errors by up to 18.61. Field experiments\nconducted on an ASV equipped with an Intel N100 CPU in natural lake\nenvironments validate practical feasibility, achieving 0.22 m average\ncross-track error at nearly 1 m/s and 10 Hz control frequency. The proposed\nscheme provides a computationally tractable solution for ASVs operating under\nresource constraints.",
      "generated_abstract": "This paper presents a novel approach to the control of an unmanned aerial\nsystem (UAS) with dynamic obstacles, by proposing a trajectory optimization\nframework that allows the control of the UAS's trajectory while taking into\naccount the dynamics of the UAS and the obstacles. The proposed approach is\nbased on a reinforcement learning (RL) approach, which is complemented by a\ngeometric approach to the control problem. The RL algorithm is used to train\na deep neural network that maps the state of the UAS to its optimal control\ntrajectory, while the geometric approach is used to design the optimal control\npolicy. The proposed approach is applied to a UAS tasked with performing\napproach and landing maneuvers to a fixed-wing UAV in a dynamic obstacle\nenvironment. The results of the experiments demonstrate that the proposed\napproach is capable of achieving trajectory optimization that is highly",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.2727272727272727,
          "f": 0.2142857095153062
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.02459016393442623,
          "f": 0.022556386011646747
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.2727272727272727,
          "f": 0.2142857095153062
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.04067v1",
      "true_abstract": "As whole genomes become widely available, maximum likelihood and Bayesian\nphylogenetic methods are demonstrating their limits in meeting the escalating\ncomputational demands. Conversely, distance-based phylogenetic methods are\nefficient, but are rarely favoured due to their inferior performance. Here, we\nextend distance-based phylogenetics using an entropy-based likelihood of the\nevolution among pairs of taxa, allowing for fast Bayesian inference in\ngenome-scale datasets. We provide evidence of a close link between the\ninference criteria used in distance methods and Felsenstein's likelihood, such\nthat the methods are expected to have comparable performance in practice. Using\nthe entropic likelihood, we perform Bayesian inference on three phylogenetic\nbenchmark datasets and find that estimates closely correspond with previous\ninferences. We also apply this rapid inference approach to a 60-million-site\nalignment from 363 avian taxa, covering most avian families. The method has\noutstanding performance and reveals substantial uncertainty in the avian\ndiversification events immediately after the K-Pg transition event. The\nentropic likelihood allows for efficient Bayesian phylogenetic inference,\naccommodating the analysis demands of the genomic era.",
      "generated_abstract": "The biological sciences have experienced a revolution in computational\nresearch, driven by advances in high-performance computing, machine learning,\nand data science. This evolution has enabled scientists to explore the\nstructural and functional properties of biological systems at unprecedented\nscales. However, the development of these methods has been limited by the\ninexperience of scientists in the design and implementation of data-driven\nresearch. This paper provides an introduction to the computational biology\nliterature, describing the methods used to model and analyze biological data. We\ndiscuss key concepts and techniques, including data collection, data\npreprocessing, data analysis, and data visualization. We emphasize the\nimportance of collaboration and training, both within and outside of the\nbiological sciences, to ensure that the research produced is of high quality and\naccurate. This guide is intended to serve as a reference for scientists who",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14782608695652175,
          "p": 0.19101123595505617,
          "f": 0.16666666174788558
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.02962962962962963,
          "f": 0.026936021977350164
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.16853932584269662,
          "f": 0.1470588186106307
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15634v2",
      "true_abstract": "Instrumental variables (IV) estimation is a fundamental method in\neconometrics and statistics for estimating causal effects in the presence of\nunobserved confounding. However, challenges such as untestable model\nassumptions and poor finite sample properties have undermined its reliability\nin practice. Viewing common issues in IV estimation as distributional\nuncertainties, we propose DRIVE, a distributionally robust IV estimation\nmethod. We show that DRIVE minimizes a square root variant of ridge regularized\ntwo stage least squares (TSLS) objective when the ambiguity set is based on a\nWasserstein distance. In addition, we develop a novel asymptotic theory for\nthis estimator, showing that it achieves consistency without requiring the\nregularization parameter to vanish. This novel property ensures that the\nestimator is robust to distributional uncertainties that persist in large\nsamples. We further derive the asymptotic distribution of Wasserstein DRIVE and\npropose data-driven procedures to select the regularization parameter based on\ntheoretical results. Simulation studies demonstrate the superior finite sample\nperformance of Wasserstein DRIVE in terms of estimation error and out-of-sample\nprediction. Due to its regularization and robustness properties, Wasserstein\nDRIVE presents an appealing option when the practitioner is uncertain about\nmodel assumptions or distributional shifts in data.",
      "generated_abstract": "This paper develops a novel method for the estimation of the dynamic\nregression discontinuity (DRD) model, and tests the validity of the DRD\nframework by comparing the estimated effects with those obtained using the\nordinary least squares (OLS) method. The proposed approach is applicable to\nmultiple treatment regimes, which is of particular interest in applications\nsuch as health care, education, and public policy. The paper examines the\napplicability of the DRD framework to different types of treatment effects,\nincluding the logit, linear, and quadratic forms, and compares the resulting\nestimates with those obtained using the DRD model with a single treatment\nregime. Additionally, we examine the sensitivity of the estimated effects to\nthe specification of the DRD model and the treatment effect function. The\nresults indicate that the DRD model performs well in most cases, with the\nexception of the linear form of treatment effects and the log",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16535433070866143,
          "p": 0.2413793103448276,
          "f": 0.19626167741767855
        },
        "rouge-2": {
          "r": 0.02197802197802198,
          "p": 0.031496062992125984,
          "f": 0.025889962795949836
        },
        "rouge-l": {
          "r": 0.15748031496062992,
          "p": 0.22988505747126436,
          "f": 0.18691588302515516
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07509v1",
      "true_abstract": "Non-orthogonal multiple access (NOMA) has gained significant attention as a\npotential next-generation multiple access technique. However, its\nimplementation with finite-alphabet inputs faces challenges. Particularly, due\nto inter-user interference, superimposed constellations may have overlapping\nsymbols leading to high bit error rates when successive interference\ncancellation (SIC) is applied. To tackle the issue, this paper employs\nautoencoders to design interference-aware super-constellations. Unlike\nconventional methods where superimposed constellation may have overlapping\nsymbols, the proposed autoencoder-based NOMA (AE-NOMA) is trained to design\nsuper-constellations with distinguishable symbols at receivers, regardless of\nchannel gains. The proposed architecture removes the need for SIC, allowing\nmaximum likelihood-based approaches to be used instead. The paper presents the\nconceptual architecture, loss functions, and training strategies for AE-NOMA.\nVarious test results are provided to demonstrate the effectiveness of\ninterference-aware constellations in improving the bit error rate, indicating\nthe adaptability of AE-NOMA to different channel scenarios and its promising\npotential for implementing NOMA systems",
      "generated_abstract": "This paper explores the potential of deep learning in the area of\ncompressed sensing (CS), particularly for non-stationary signals. The\napproach is based on a deep learning model that learns to distinguish between\nnon-stationary and stationary signals, where the stationary signal is assumed\nto be the ideal case. The proposed method consists of a convolutional neural\nnetwork (CNN) that is trained on a dataset of known non-stationary signals.\nThis dataset is augmented by samples of known stationary signals. The CNN\npredicts the classification of the unknown non-stationary signal based on the\nknown stationary signals. The approach is validated on the MUSHRA dataset,\nwhich consists of 25 signals with different characteristics. The proposed\nmethod achieves a classification accuracy of 97.7%, which is superior to the\nresults of the baseline model, which achieves an accuracy of 93",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.2465753424657534,
          "f": 0.1945945898168007
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.025423728813559324,
          "f": 0.022727267783518065
        },
        "rouge-l": {
          "r": 0.15178571428571427,
          "p": 0.2328767123287671,
          "f": 0.18378377900598988
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.10600v1",
      "true_abstract": "This study examines the impact of monetary factors on the conversion of\nfarmland to renewable energy generation, specifically solar and wind, in the\ncontext of expanding U.S. energy production. We propose a new econometric\nmethod that accounts for the diverse circumstances of landowners, including\ntheir unordered alternative land use options, non-monetary benefits from\nfarming, and the influence of local regulations. We demonstrate that\nidentifying the cross elasticity of landowners' farming income in relation to\nthe conversion of farmland to renewable energy requires an understanding of\ntheir preferences. By utilizing county legislation that we assume to be shaped\nby land-use preferences, we estimate the cross-elasticities of farming income.\nOur findings indicate that monetary incentives may only influence landowners'\ndecisions in areas with potential for future residential development,\nunderscoring the importance of considering both preferences and regulatory\ncontexts.",
      "generated_abstract": "This paper investigates the identification of an endogenous binary variable in\ngeneralized linear models with binary outcomes and nonlinear fixed effects. We\nfocus on the identification of a treatment effect in the presence of\nmeasurement errors. Our main result is that the average treatment effect is\nidentified if and only if the conditional mean is identified. This result is\napplied to the estimation of treatment effects in a context where the conditional\nmean is available in a linear model. We also derive a test statistic for the\nidentification of the conditional mean. We illustrate our findings through\nsimulation studies and empirical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17894736842105263,
          "p": 0.2833333333333333,
          "f": 0.2193548339646203
        },
        "rouge-2": {
          "r": 0.031007751937984496,
          "p": 0.045454545454545456,
          "f": 0.03686635462549703
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.25,
          "f": 0.19354838235171706
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.16313v1",
      "true_abstract": "Player tracking data have provided great opportunities to generate novel\ninsights into understudied areas of American football, such as pre-snap motion.\nUsing a Bayesian multilevel model with heterogeneous variances, we provide an\nassessment of NFL quarterbacks and their ability to synchronize the timing of\nthe ball snap with pre-snap movement from their teammates. We focus on passing\nplays with receivers in motion at the snap and running a route, and define the\nsnap timing as the time between the moment a receiver begins motioning and the\nball snap event. We assume a Gamma distribution for the play-level snap timing\nand model the mean parameter with player and team random effects, along with\nrelevant fixed effects such as the motion type identified via a Gaussian\nmixture model. Most importantly, we model the shape parameter with quarterback\nrandom effects, which enables us to estimate the differences in snap timing\nvariability among NFL quarterbacks. We demonstrate that higher variability in\nsnap timing is beneficial for the passing game, as it relates to facing less\nhavoc created by the opposing defense. We also obtain a quarterback leaderboard\nbased on our snap timing variability measure, and Patrick Mahomes stands out as\nthe top player.",
      "generated_abstract": "The study examines the impact of economic factors on the construction industry\nin the United States. Data from 2018-2024 are used to examine the impact of\nmacroeconomic variables on construction costs. The results show that the\ninfluence of macroeconomic variables on construction costs is significant.\nThe study also examined the influence of the supply chain on the cost of\nconstruction. The results show that the influence of the supply chain on the\ncost of construction is significant. The study also examined the effect of the\ncost of materials on the cost of construction. The results show that the\ninfluence of cost of materials on the cost of construction is significant. The\nstudy also examined the effect of the cost of equipment on the cost of\nconstruction. The results show that the influence of cost of equipment on the\ncost of construction is significant. The study also examined the effect of the\ncost",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0743801652892562,
          "p": 0.23684210526315788,
          "f": 0.11320754353229709
        },
        "rouge-2": {
          "r": 0.0055248618784530384,
          "p": 0.017543859649122806,
          "f": 0.008403357701788175
        },
        "rouge-l": {
          "r": 0.05785123966942149,
          "p": 0.18421052631578946,
          "f": 0.08805031082789462
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.11602v1",
      "true_abstract": "Multivariate Distributions are needed to capture the correlation structure of\ncomplex systems. In previous works, we developed a Random Matrix Model for such\ncorrelated multivariate joint probability density functions that accounts for\nthe non-stationarity typically found in complex systems. Here, we apply these\nresults to the returns measured in correlated stock markets. Only the knowledge\nof the multivariate return distributions allows for a full-fledged risk\nassessment. We analyze intraday data of 479 US stocks included in the S&P500\nindex during the trading year of 2014. We focus particularly on the tails which\nare algebraic and heavy. The non-stationary fluctuations of the correlations\nmake the tails heavier. With the few-parameter formulae of our Random Matrix\nModel we can describe and quantify how the empirical distributions change for\nvarying time resolution and in the presence of non-stationarity.",
      "generated_abstract": "The study of the performance of the Black-Scholes model is a fundamental\ntask in the financial markets. The Black-Scholes model is widely used in the\npricing of financial instruments, including options, and is known to exhibit\nhigh volatility and wide price spreads. This study focuses on the estimation\nof the Black-Scholes model, with the goal of improving the accuracy of the\npricing of options. To address this issue, a novel approach has been proposed\nthat uses the Gaussian process regression to estimate the Black-Scholes model.\nThis approach has been validated through a comparison with the Black-Scholes\nmodel and other models, such as the Monte Carlo method and the Gaussian\nprocess method. The results show that the Gaussian process method offers a\nmore accurate and reliable option pricing model, with the potential to improve\nthe accuracy of pricing in the financial markets. The Gaussian process method\nis a prom",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12903225806451613,
          "p": 0.16216216216216217,
          "f": 0.14371256991502043
        },
        "rouge-2": {
          "r": 0.0234375,
          "p": 0.02608695652173913,
          "f": 0.02469135303900251
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.16216216216216217,
          "f": 0.14371256991502043
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.08411v1",
      "true_abstract": "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
      "generated_abstract": "In this paper we prove that if $A$ is a finite dimensional simple algebra\nof dimension $d$ over a field $k$ then the number of positive roots is at\nleast $d+1$ if and only if $A$ is a simple Lie algebra. This generalizes a\nresult of Matsumoto from 1984.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15217391304347827,
          "p": 0.3783783783783784,
          "f": 0.21705425947479123
        },
        "rouge-2": {
          "r": 0.022058823529411766,
          "p": 0.06666666666666667,
          "f": 0.033149167534568966
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.32432432432432434,
          "f": 0.18604650753680677
        }
      }
    },
    {
      "paper_id": "cs.CR.cs/AR/2503.08968v1",
      "true_abstract": "Homomorphic encryption (HE) allows secure computation on encrypted data\nwithout revealing the original data, providing significant benefits for\nprivacy-sensitive applications. Many cloud computing applications (e.g., DNA\nread mapping, biometric matching, web search) use exact string matching as a\nkey operation. However, prior string matching algorithms that use homomorphic\nencryption are limited by high computational latency caused by the use of\ncomplex operations and data movement bottlenecks due to the large encrypted\ndata size. In this work, we provide an efficient algorithm-hardware codesign to\naccelerate HE-based secure exact string matching. We propose CIPHERMATCH, which\n(i) reduces the increase in memory footprint after encryption using an\noptimized software-based data packing scheme, (ii) eliminates the use of costly\nhomomorphic operations (e.g., multiplication and rotation), and (iii) reduces\ndata movement by designing a new in-flash processing (IFP) architecture. We\ndemonstrate the benefits of CIPHERMATCH using two case studies: (1) Exact DNA\nstring matching and (2) encrypted database search. Our pure software-based\nCIPHERMATCH implementation that uses our memory-efficient data packing scheme\nimproves performance and reduces energy consumption by 42.9X and 17.6X,\nrespectively, compared to the state-of-the-art software baseline. Integrating\nCIPHERMATCH with IFP improves performance and reduces energy consumption by\n136.9X and 256.4X, respectively, compared to the software-based CIPHERMATCH\nimplementation.",
      "generated_abstract": "This paper presents a novel approach to generate a high-quality synthetic\ndata set for the task of object classification. By integrating an end-to-end\napproach to generating synthetic data, we achieve a significant improvement in\nthe performance of the object detection model on the popular COCO dataset.\nIn particular, we demonstrate that our method improves the model's performance\nby 1.77 points on the COCO-stuff evaluation metric, as well as 2.5 points on\nthe COCO-text evaluation metric. This performance gain is achieved without\nmodifying the object detection model itself, which allows us to easily apply\nour approach to other popular object detection datasets. Additionally, we\ndemonstrate that our approach is capable of generating high-quality synthetic\ndata even when the original dataset is relatively small. Finally, we provide\nadditional insights into the generalization capabilities of our approach,\nhighlighting the potential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18115942028985507,
          "p": 0.28735632183908044,
          "f": 0.2222222174791112
        },
        "rouge-2": {
          "r": 0.0106951871657754,
          "p": 0.016260162601626018,
          "f": 0.012903221019564731
        },
        "rouge-l": {
          "r": 0.15942028985507245,
          "p": 0.25287356321839083,
          "f": 0.19555555081244458
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2502.14498v1",
      "true_abstract": "This paper deals with a nonparametric Nadaraya-Watson (NW) estimator of the\ntransition density function computed from independent continuous observations\nof a diffusion process. A risk bound is established on this estimator. The\npaper also deals with an extension of the penalized comparison to overfitting\nbandwidths selection method for our NW estimator. Finally, numerical\nexperiments are provided.",
      "generated_abstract": "We introduce a new and general approach to the analysis of high-dimensional\n(large) linear regression models. This is a generalization of the well-known\nSinkhorn-Kukavica-Wasserstein estimator. We prove that the Sinkhorn-Kukavica-\nWasserstein estimator is consistent and asymptotically normal when the data\ngenerating process is a Markov chain. We also provide a new, simple and\ncomputationally efficient algorithm to compute the Sinkhorn-Kukavica-Wasserstein\nestimator. Finally, we demonstrate the effectiveness of our estimator in\napplications to the analysis of gene expression data and to the analysis of\nthe covariates used in the treatment of patients in clinical trials.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23404255319148937,
          "p": 0.1896551724137931,
          "f": 0.20952380457868494
        },
        "rouge-2": {
          "r": 0.03773584905660377,
          "p": 0.023809523809523808,
          "f": 0.029197075547978818
        },
        "rouge-l": {
          "r": 0.23404255319148937,
          "p": 0.1896551724137931,
          "f": 0.20952380457868494
        }
      }
    },
    {
      "paper_id": "math.AP.math/AP/2503.10495v1",
      "true_abstract": "In the present work, we develop a comprehensive and rigorous analytical\nframework for a non-local phase-field model that describes tumour growth\ndynamics. The model is derived by coupling a non-local Cahn-Hilliard equation\nwith a parabolic reaction-diffusion equation, which accounts for both phase\nsegregation and nutrient diffusion. Previous studies have only considered\nsymmetric potentials for similar models. However, in the biological context of\ncell-to-cell adhesion, single-well potentials, like the so-called Lennard-Jones\npotential, seem physically more appropriate. The Cahn-Hilliard equation with\nthis kind of potential has already been analysed. Here, we take a step forward\nand consider a more refined model. First, we analyse the model with a viscous\nrelaxation term in the chemical potential and subject to suitable initial and\nboundary conditions. We prove the existence of solutions, a separation property\nfor the phase variable, and a continuous dependence estimate with respect to\nthe initial data. Finally, via an asymptotic analysis, we recover the existence\nof a weak solution to the initial and boundary value problem without viscosity,\nprovided that the chemotactic sensitivity is small enough.",
      "generated_abstract": "We introduce the notion of a generalised Hilbert--Schmidt norm on a\nclas\n\\[\n\\mathscr{H} = \\bigl\\{ f \\in L^2(\\mathbb{R}^d): \\Vert f \\Vert_{\\mathscr{H}} = \\sup_{\n\\lambda > 0} \\frac{\\Vert f - \\lambda f \\Vert_2}{\\lambda} < \\infty \\bigr\\}\n\\",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.03361344537815126,
          "p": 0.12903225806451613,
          "f": 0.05333333005422242
        },
        "rouge-2": {
          "r": 0.006134969325153374,
          "p": 0.027777777777777776,
          "f": 0.010050248292720755
        },
        "rouge-l": {
          "r": 0.03361344537815126,
          "p": 0.12903225806451613,
          "f": 0.05333333005422242
        }
      }
    },
    {
      "paper_id": "cs.CE.q-bio/QM/2503.05113v2",
      "true_abstract": "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
      "generated_abstract": "The field of neural control has emerged as a promising approach to\nmodern computing, allowing researchers to directly manipulate the neural\nactivity of a given neuron in a brain. This ability to directly manipulate\nneural activity has opened the door to the creation of brain-inspired\ncomputing devices. However, the development of brain-inspired computing devices\nrequires a deep understanding of how neural activity is generated in the brain.\nIn this work, we introduce a new mathematical model of the neural activity of\nthe human brain, the so-called Waveform Model. This model captures the\nspatio-temporal dynamics of neural activity in the brain and is derived from\nthe neural activity of the human brain. We also introduce a mathematical\nmodel of the neural activity of the human brain inspired by the Waveform Model,\ncalled the Waveform-Based Model. This model captures the spatio-tempor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13392857142857142,
          "p": 0.22388059701492538,
          "f": 0.16759776067913
        },
        "rouge-2": {
          "r": 0.03184713375796178,
          "p": 0.04807692307692308,
          "f": 0.03831417145138857
        },
        "rouge-l": {
          "r": 0.09821428571428571,
          "p": 0.16417910447761194,
          "f": 0.12290502324896245
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/CR/2503.09726v1",
      "true_abstract": "Graph Neural Networks (GNNs) are widely used and deployed for graph-based\nprediction tasks. However, as good as GNNs are for learning graph data, they\nalso come with the risk of privacy leakage. For instance, an attacker can run\ncarefully crafted queries on the GNNs and, from the responses, can infer the\nexistence of an edge between a pair of nodes. This attack, dubbed as a\n\"link-stealing\" attack, can jeopardize the user's privacy by leaking\npotentially sensitive information. To protect against this attack, we propose\nan approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs\nfrom $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility.\n$NARGIS$ is focused on reshaping the graph embedding space so that the\nposterior from the GNN model will still provide utility for the prediction task\nbut will introduce ambiguity for the link-stealing attackers. To this end,\n$NARGIS$ applies spectral clustering on the given graph to facilitate it being\naugmented with new nodes -- that have learned features instead of fixed ones.\nIt utilizes tri-level optimization for learning parameters for the GNN model,\nsurrogate attacker model, and our defense model (i.e. learnable node features).\nWe extensively evaluate $NARGIS$ on three benchmark citation datasets over\neight knowledge availability settings for the attackers. We also evaluate the\nmodel fidelity and defense performance on influence-based link inference\nattacks. Through our studies, we have figured out the best feature of $NARGIS$\n-- its superior fidelity-privacy performance trade-off in a significant number\nof cases. We also have discovered in which cases the model needs to be\nimproved, and proposed ways to integrate different schemes to make the model\nmore robust against link stealing attacks.",
      "generated_abstract": "Recent advancements in large language models (LLMs) have shown promise\nin text-based tasks. However, their application to discrete domain tasks, such\nas natural language inference (NLI), remains limited due to the high\ncomplexity and low generalization of these tasks. In this paper, we present\nTiKT, a novel text-to-text framework that transforms textual evidence into\ndiscrete evidence for NLI. TiKT first generates a set of candidate NLI\nstatements by leveraging a large-scale multi-hop reasoning dataset. Then, it\nimproves these statements by iteratively refining the evidence to better\nsupport the NLI task. We introduce two novel components that further enhance\nTiKT's performance. First, we propose a novel reasoning mechanism that enables\nTiKT to integrate multiple statements into a unified NLI statement, enabling\nmore flexible and accurate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14124293785310735,
          "p": 0.2717391304347826,
          "f": 0.18587360144718848
        },
        "rouge-2": {
          "r": 0.011583011583011582,
          "p": 0.024793388429752067,
          "f": 0.015789469343630003
        },
        "rouge-l": {
          "r": 0.12994350282485875,
          "p": 0.25,
          "f": 0.17100371297135208
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09539v1",
      "true_abstract": "Quenched disorder in a solid state system can result in Anderson localization\nwhere electrons are exponentially localized and the system behaves like an\ninsulator. In this study, we investigate the effect of a DC electric field on\nAnderson localization. The study highlights the case of a one-dimensional\ninsulator chain with on-site disorder when a DC electric field is applied\nthroughout the chain. We study spectral properties of an Anderson localized\nsystem in equilibrium and out-of-equilibrium using a full lattice\nnonequilibrium Green's function method in the steady-state limit. Tuning the\ndisorder and the electric field strength results in the creation of exponential\nLifshitz tails near the band edge by strongly localized levels. These Lifshtiz\ntails create effects like insulator-to-metal transitions and contribute to\nnon-local hopping. The electric field causes gradual delocalization of the\nsystem and Anderson localization crossing over to Wannier Stark ladders at very\nstrong fields. Our study makes a comparison with the coherent potential\napproximation (CPA) highlighting some major differences and similarities in the\nphysics of disorder.",
      "generated_abstract": "Recent advancements in machine learning and large language models (LLMs) have\nopened the door to powerful approaches for understanding the interactions of\nmolecules and atoms in chemical systems. In this work, we introduce the first\nLLM-based model designed to capture the molecular interactions in\nconformational space. The model is built on the B4LYP basis set, incorporating\nthe molecular orbital basis, and is trained using the molecular dynamics\nsimulation code MolDS. MolDS enables the simulation of molecular dynamics and\nmolecular mechanics at the B4LYP/6-31G(d,p) level of theory. We demonstrate the\neffectiveness of the LLM-based model in predicting molecular conformations and\nmolecular energy landscapes. Our findings reveal that the LLM-based model\noutperforms traditional machine learning approaches in molecular energetics",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14018691588785046,
          "p": 0.20270270270270271,
          "f": 0.165745851519795
        },
        "rouge-2": {
          "r": 0.012903225806451613,
          "p": 0.018518518518518517,
          "f": 0.01520912063496807
        },
        "rouge-l": {
          "r": 0.14018691588785046,
          "p": 0.20270270270270271,
          "f": 0.165745851519795
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/atm-clus/2502.16979v1",
      "true_abstract": "Technological advancements in generation of ultrafast and intense laser\npulses have enabled the real-time observation and control of charge migration\nin molecules on their natural timescale, which ranges from few femtoseconds to\nseveral hundreds of attoseconds. Present thesis discusses the effect of\nsymmetry on the adiabatic attosecond charge migration in different molecular\nsystems. The spatial representation of the charge migration is documented by\ntime-dependent electronic charge and flux densities. Furthermore, the induced\ncharge migration is imaged via time resolved x-ray diffraction (TRXD) with\natomic-scale spatiotemporal resolution in few cases.",
      "generated_abstract": "The development of quantum-enabled technologies requires high-quality,\nhigh-quality single-mode fibers (SMFs). SMFs must be highly transparent to\nultraviolet (UV) light, yet exhibit high-quality optical characteristics in\nthe visible and infrared (IR) regions. This is challenging due to the large\nbandgap of UV light, which can cause significant damage to SMFs. We present\nthe first experimental demonstration of single-mode SMFs exhibiting both UV\ntransparency and high-quality optical properties. We utilized a new technique\nthat enables the fabrication of ultra-thin (0.20 \u03bcm) single-mode SMFs with\nhigh-quality properties. The UV transparency of the SMFs was confirmed by\nmeasuring the transmission of UV light through the SMFs. The UV transpar",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15151515151515152,
          "p": 0.1388888888888889,
          "f": 0.144927531241336
        },
        "rouge-2": {
          "r": 0.012048192771084338,
          "p": 0.01,
          "f": 0.010928956791784624
        },
        "rouge-l": {
          "r": 0.15151515151515152,
          "p": 0.1388888888888889,
          "f": 0.144927531241336
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atom-ph/2503.08855v1",
      "true_abstract": "Assembly of ultracold polar molecules containing silver (Ag) from\nlaser-cooled atoms requires knowledge of the dynamic polarizabilities of Ag at\nconvenient laser wavelengths. We present calculations and analysis of the\nenergies and electric-dipole dc and ac polarizabilities of the low-lying states\nof neutral Ag. Calculations of the properties of the 4d^{10}x states, where\nx=5s,6s,7s,5p,6p,7p,5d,6d, and 4f, are performed using the linearized coupled\ncluster single-double method. The properties of the 4d^9 5s^2 ^2D_{5/2,3/2}\nstates are obtained within the framework of configuration interaction with 11\nand 17 electrons in the valence field. We analyze the different contributions\nto the polarizabilities and estimate the uncertainties of our predictions.",
      "generated_abstract": "In this study, we present a theoretical model to study the spin dynamics of\nnuclear spin-1/2 particles in the presence of a static magnetic field. The\nspin dynamics is analyzed in the framework of a non-equilibrium Green's\nfunction (NEGF) formalism. We find that the spin dynamics in the presence of a\nstatic magnetic field can be described using a spin-dependent effective\nLangevin equation that accounts for the effects of the magnetic field on the\nspin dynamics. The model is validated using a series of analytical and\nnumerical results for a variety of magnetic fields. The results are compared\nwith available experimental data for the spin dynamics of magnetic nanoparticles\nin a magnetic field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18055555555555555,
          "p": 0.21311475409836064,
          "f": 0.19548871683871347
        },
        "rouge-2": {
          "r": 0.041237113402061855,
          "p": 0.044444444444444446,
          "f": 0.042780743670108395
        },
        "rouge-l": {
          "r": 0.1527777777777778,
          "p": 0.18032786885245902,
          "f": 0.16541352886878868
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.05691v1",
      "true_abstract": "Prevailing methods of course allocation at undergraduate institutions involve\nreserving seats to give priority to designated groups of students. We introduce\na competitive equilibrium-based mechanism that assigns course seats using\nstudent preferences and course priorities. This mechanism satisfies approximate\nnotions of stability, efficiency, envy-freeness, and strategy-proofness. We\nevaluate its performance relative to a mechanism widely used in practice using\npreferences estimated from university data. Our empirical findings demonstrate\nan improvement in student satisfaction and allocation fairness. The number of\nstudents who envy another student of weakly lower priority declines by 8\npercent, or roughly 500 students.",
      "generated_abstract": "We investigate the role of the mean-variance optimization problem in the\nselection of investment portfolios for an agent with utility. In the mean-variance\nframework, the agent aims to maximize expected utility given the current\nuncertainty of the future. We study the optimal portfolio problem, which\nrequires the agent to select the portfolio that minimizes the sum of the\npresent-time expected utility and the present-time variance. We derive the\nstochastic optimal portfolio under the mean-variance framework. Moreover, we\nextend our results to the more general mean-risk framework, which assumes that\nthe uncertainty of the future is modelled by a risk factor, and the agent\ninvestment strategy is modeled by an expectation. We derive the stochastic\noptimal portfolio for the mean-risk framework and show that it is similar to\nthe optimal portfolio in the mean-variance framework.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.13846153846153847,
          "f": 0.13138685632692224
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.12307692307692308,
          "f": 0.11678831618093687
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/CB/2411.00948v1",
      "true_abstract": "Conventional histopathology has long been essential for disease diagnosis,\nrelying on visual inspection of tissue sections. Immunohistochemistry aids in\ndetecting specific biomarkers but is limited by its single-marker approach,\nrestricting its ability to capture the full tissue environment. The advent of\nmultiplexed imaging technologies, like multiplexed immunofluorescence and\nspatial transcriptomics, allows for simultaneous visualization of multiple\nbiomarkers in a single section, enhancing morphological data with molecular and\nspatial information. This provides a more comprehensive view of the tissue\nmicroenvironment, cellular interactions, and disease mechanisms - crucial for\nunderstanding disease progression, prognosis, and treatment response. However,\nthe extensive data from multiplexed imaging necessitates sophisticated\ncomputational methods for preprocessing, segmentation, feature extraction, and\nspatial analysis. These tools are vital for managing large, multidimensional\ndatasets, converting raw imaging data into actionable insights. By automating\nlabor-intensive tasks and enhancing reproducibility and accuracy, computational\ntools are pivotal in diagnostics and research. This review explores the current\nlandscape of multiplexed imaging in pathology, detailing workflows and key\ntechnologies like PathML, an AI-powered platform that streamlines image\nanalysis, making complex dataset interpretation accessible for clinical and\nresearch settings.",
      "generated_abstract": "We propose a novel framework for integrating large-scale neural data\nwith the general Bayesian framework for Bayesian nonparametric modeling.\nSpecifically, we introduce a new model class called the Neural Gaussian\nProcess (NGP), which can be viewed as a Gaussian Process (GP) with Gaussian\nnoise and a neural output layer. We derive the posterior distribution of the\nNGP and apply it to the analysis of several data sets involving the\nspiking-based neural networks. Our results demonstrate that the NGP model can\nprovide a more reliable and accurate representation of neural data compared to\nthe traditional GP model, which is more commonly used in the analysis of\nspiking-based neural data.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09774436090225563,
          "p": 0.19117647058823528,
          "f": 0.12935322935372903
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.030612244897959183,
          "f": 0.022140216785175457
        },
        "rouge-l": {
          "r": 0.08270676691729323,
          "p": 0.16176470588235295,
          "f": 0.10945273184129124
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/NC/2502.13729v2",
      "true_abstract": "Human and animal memory for sequentially presented items is well-documented\nto be more accurate for those at the beginning and end of the sequence,\nphenomena known as the primacy and recency effects, respectively. By contrast,\nartificial neural network (ANN) models are typically designed with a memory\nthat decays monotonically over time. Accordingly, ANNs are expected to show the\nrecency effect but not the primacy effect. Contrary to this theoretical\nexpectation, however, the present study reveals a counterintuitive finding: a\nrecently developed ANN architecture, called structured state-space models,\nexhibits the primacy effect when trained and evaluated on a synthetic task that\nmirrors psychological memory experiments. Given that this model was originally\ndesigned for recovering neuronal activity patterns observed in biological\nbrains, this result provides a novel perspective on the psychological primacy\neffect while also posing a non-trivial puzzle for the current theories in\nmachine learning.",
      "generated_abstract": "In this paper, we introduce a new methodology for the prediction of\nfuture gene expression levels using machine learning models. This methodology\nis based on the use of gene expression patterns in the literature to predict\nfuture gene expression levels in a given sample. The methodology involves the\ncollection of gene expression data from a large number of samples, and the\ntraining of machine learning models using gene expression patterns to\nestablish a predictive relationship between gene expression levels and the\noutcome of the sample. The methodology has been validated using gene\nexpression data from 271 samples collected from different tissues and cell\ntypes, with 104 samples collected from 14 tissues and 67 samples collected\nfrom 13 cell types. The results show that the methodology achieves an accuracy\nof 91.8% for the prediction of future gene expression levels, with a\nprecision of 90.5",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1619047619047619,
          "p": 0.22666666666666666,
          "f": 0.1888888840277779
        },
        "rouge-2": {
          "r": 0.03597122302158273,
          "p": 0.043859649122807015,
          "f": 0.039525686748426624
        },
        "rouge-l": {
          "r": 0.1523809523809524,
          "p": 0.21333333333333335,
          "f": 0.1777777729166668
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10251v1",
      "true_abstract": "Large language models based on transformer architectures have become integral\nto state-of-the-art natural language processing applications. However, their\ntraining remains computationally expensive and exhibits instabilities, some of\nwhich are expected to be caused by finite-precision computations. We provide a\ntheoretical analysis of the impact of round-off errors within the forward pass\nof a transformer architecture which yields fundamental bounds for these\neffects. In addition, we conduct a series of numerical experiments which\ndemonstrate the practical relevance of our bounds. Our results yield concrete\nguidelines for choosing hyperparameters that mitigate round-off errors, leading\nto more robust and stable inference.",
      "generated_abstract": "We study the problem of finding a solution to a linear system of\nequations in a given Hilbert space by using the Newton-Raphson method. We\ndemonstrate that the Newton-Raphson method can be applied to systems of\nequations with general coefficients in the Hilbert space and that it can be\nextended to solve systems of linear differential equations in general. We\nprovide examples in which the Newton-Raphson method is applicable and give\nexamples of systems of linear differential equations that cannot be solved\ndirectly by the Newton-Raphson method. Our results can be applied to solve\nproblems in quantum mechanics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17721518987341772,
          "p": 0.2916666666666667,
          "f": 0.22047243624279259
        },
        "rouge-2": {
          "r": 0.020618556701030927,
          "p": 0.02564102564102564,
          "f": 0.0228571379160827
        },
        "rouge-l": {
          "r": 0.17721518987341772,
          "p": 0.2916666666666667,
          "f": 0.22047243624279259
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/SP/2503.07435v1",
      "true_abstract": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels.",
      "generated_abstract": "This paper presents a novel end-to-end framework that integrates\nconvolutional neural networks (CNNs) and generative adversarial networks\n(GANs) for end-to-end speech enhancement. The proposed framework incorporates\nboth an autoencoder (AE) and a conditional generative adversarial network\n(CGAN) to improve the speech enhancement performance. The AE is used to\nreconstruct the clean speech signal, while the CGAN is employed to generate\nsynthetic speech signals that mimic the original speech. The enhanced speech\nsignals are then combined with the original clean speech signals to produce\nthe final enhanced speech. The proposed framework is trained and evaluated\nusing the Speech Quality Indicator (SQI) and the Speech Quality Evaluation\n(SQE) datasets. The experimental results demonstrate that the proposed\nframework outperforms the baseline method and other state",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10734463276836158,
          "p": 0.2602739726027397,
          "f": 0.1519999958652801
        },
        "rouge-2": {
          "r": 0.008658008658008658,
          "p": 0.018518518518518517,
          "f": 0.01179940568773491
        },
        "rouge-l": {
          "r": 0.096045197740113,
          "p": 0.2328767123287671,
          "f": 0.13599999586528014
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.physics/app-ph/2503.08941v1",
      "true_abstract": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
      "generated_abstract": "We present a theoretical study of the thermodynamics of a disordered\nnetwork of two-dimensional (2D) ferromagnetic (FM) spins. The system is\ncharacterized by the mean-field exchange-coupling $J$ and the FM anisotropy\nparameter $\\gamma$. We investigate the behavior of the system's critical\ntemperature and the magnetization for various values of the anisotropy\nparameter. We find that for $\\gamma<1$ the system exhibits a first-order\nphase transition in the mean-field regime, and for $\\gamma>1$ the system\nundergoes a continuous transition. We also study the behavior of the\nmagnetization for $\\gamma>1$. We find that for $\\gamma>1$ the system\nexhibits a first-order phase transition in the mean-field regime, while for\n$\\gamma<1$ the system exhibits a continuous transition. We discuss",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12048192771084337,
          "p": 0.19230769230769232,
          "f": 0.14814814341179713
        },
        "rouge-2": {
          "r": 0.009615384615384616,
          "p": 0.01282051282051282,
          "f": 0.010989006091053988
        },
        "rouge-l": {
          "r": 0.10843373493975904,
          "p": 0.17307692307692307,
          "f": 0.13333332859698233
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.08885v1",
      "true_abstract": "Quasilinear systems with piecewise constant arguments of generalized type are\nunder investigation from the asymptotic point of view. The systems have\ndiscontinuous right-hand sides which are identified via a discrete-time map. It\nis rigorously proved that homoclinic and heteroclinic solutions are generated,\nand they are taken into account in the functional sense. The Banach fixed point\ntheorem is used for the verification. The hyperbolic set of solutions is also\ndiscussed, and an example supporting the theoretical findings is provided.",
      "generated_abstract": "We provide an overview of the state of the art for the study of the\nfinite-volume solution of the $n$-body problem, using the recently proposed\n$\\mathrm{BV}$ space-time framework. The analysis of this space-time framework\nallows to overcome some of the limitations of the existing $\\mathrm{L^2}$\nspace-time framework, in particular by providing a rigorous mathematical\njustification for the use of the standard finite-volume scheme in the\ncomputation of the finite-volume solution of the $n$-body problem. In\nparticular, we prove the convergence of the finite-volume scheme in the\n$n$-body problem, and we show that the convergence rate of the finite-volume\nscheme in the $n$-body problem depends on the smoothness of the density\nfunction. The results of this paper are a direct consequence of the\n$\\mathrm{BV}$ space-time framework, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16393442622950818,
          "p": 0.15873015873015872,
          "f": 0.16129031758194606
        },
        "rouge-2": {
          "r": 0.02564102564102564,
          "p": 0.02197802197802198,
          "f": 0.023668634082841283
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.14285714285714285,
          "f": 0.14516128532388153
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.14041v1",
      "true_abstract": "This study investigates the effectiveness of fiscal policies on household\nconsumption, disposable income, and the propensity to consume during the\nCOVID-19 pandemic across Croatia, Slovakia, and Poland. The purpose is to\nassess how variations in government debt, expenditures, revenue, and subsidies\ninfluenced household financial behaviors in response to economic shocks. Using\na Markov Switching VAR model across three regimes: initial impact, peak crisis,\nand recovery.This analysis captures changes in household consumption,\ndisposable income, and consumption propensities under different fiscal policy\nmeasures.\n  The findings reveal that the Slovak Republic exhibited the highest fiscal\neffectiveness, demonstrating effective government policies that stimulated\nconsumer spending and supported household income during the pandemic. Croatia\nalso showed positive outcomes, particularly in terms of income, although rising\ngovernment debt posed challenges to overall effectiveness. Conversely, Poland\nfaced significant obstacles, with its fiscal measures leading to lower\nconsumption and income outcomes, indicating limited policy efficacy.\n  Conclusions emphasize the importance of tailored fiscal measures, as their\neffectiveness varied across countries and economic contexts. Recommendations\ninclude reinforcing consumption-supportive policies, particularly during crisis\nperiods, to stabilize income and consumption expectations. This study\nunderscores the significance of targeted fiscal actions in promoting household\nresilience and economic stability, as exemplified by the successful approach\ntaken by the Slovak Republic.",
      "generated_abstract": "The growth of artificial intelligence (AI) is transforming human interactions,\neconomies, and societies. Yet, the potential benefits of AI are often\nunderestimated, and the risks are rarely addressed. In this article, we introduce\nthe concept of AI risk governance (ARG), which provides a framework for\naddressing both risks and governance of AI. ARG emphasizes the need for\nsystematic risk assessment, and a comprehensive understanding of the\nsocietal and economic implications of AI. We introduce a framework for\nquantifying AI risk, and identify key factors that influence the risk\ncharacteristics of AI. We then apply ARG to the case of AI-driven automation in\nthe U.S. economy, focusing on the impact of automation on employment, wages,\nand the labor share. We find that AI-driven automation pos",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08208955223880597,
          "p": 0.14102564102564102,
          "f": 0.10377358025453919
        },
        "rouge-2": {
          "r": 0.010256410256410256,
          "p": 0.017391304347826087,
          "f": 0.012903221139439772
        },
        "rouge-l": {
          "r": 0.08208955223880597,
          "p": 0.14102564102564102,
          "f": 0.10377358025453919
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.10435v1",
      "true_abstract": "When detecting anomalous sounds in complex environments, one of the main\ndifficulties is that trained models must be sensitive to subtle differences in\nmonitored target signals, while many practical applications also require them\nto be insensitive to changes in acoustic domains. Examples of such domain\nshifts include changing the type of microphone or the location of acoustic\nsensors, which can have a much stronger impact on the acoustic signal than\nsubtle anomalies themselves. Moreover, users typically aim to train a model\nonly on source domain data, which they may have a relatively large collection\nof, and they hope that such a trained model will be able to generalize well to\nan unseen target domain by providing only a minimal number of samples to\ncharacterize the acoustic signals in that domain. In this work, we review and\ndiscuss recent publications focusing on this domain generalization problem for\nanomalous sound detection in the context of the DCASE challenges on acoustic\nmachine condition monitoring.",
      "generated_abstract": "This paper proposes a novel framework for joint speech recognition and\nlanguage modeling in wireless networks, leveraging the power of Graph Neural\nNetworks (GNNs) to enhance speech recognition and modeling. The proposed\nframework integrates a Graph Convolutional Network (GCN) with an LSTM network\nfor speech recognition, and a Graph Attention Network (GAT) with a Long-Short\nTerm Memory (LSTM) network for language modeling. The GNN-based speech\nrecognition module employs a graph convolutional network (GCN) to model the\nstructural dependencies between speech tokens and their context, while the\nLSTM-based language modeling module leverages a graph attention network to\ncapture the long-term dependencies between speech tokens and their context.\nExperimental results on the LibriSpeech dataset demonstrate that the proposed\nframework outperforms existing state-of-the-art speech",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11926605504587157,
          "p": 0.18055555555555555,
          "f": 0.14364640404871665
        },
        "rouge-2": {
          "r": 0.006369426751592357,
          "p": 0.009523809523809525,
          "f": 0.007633582983220784
        },
        "rouge-l": {
          "r": 0.10091743119266056,
          "p": 0.1527777777777778,
          "f": 0.12154695653490451
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06178v1",
      "true_abstract": "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
      "generated_abstract": "We propose a novel algorithm to generate synthetic cell populations with\nproliferation, differentiation, and apoptosis dynamics using a Markov chain\nMonte Carlo method. The method is general and can be applied to different types\nof cell populations, including neural cells, cancer cells, and vascular\ncells. The proposed algorithm is based on a generalized Markov chain model for\nthe evolution of the population. The model includes the effects of proliferation\nand differentiation on the population dynamics and is based on a stochastic\ndiffusion process. The population dynamics are described by a set of\nfirst-order differential equations. A novel approach is used to solve these\nequations using a Markov chain Monte Carlo method. We apply the method to\nsimulate cell populations with different types of cellular states (proliferative,\ndifferentiated, and apoptotic) and differentiation rates. We compare the\nproposed method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1791044776119403,
          "p": 0.32,
          "f": 0.229665067168792
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.05042016806722689,
          "f": 0.0368098113152552
        },
        "rouge-l": {
          "r": 0.15671641791044777,
          "p": 0.28,
          "f": 0.2009569331975002
        }
      }
    },
    {
      "paper_id": "cond-mat.mtrl-sci.cond-mat/mes-hall/2503.10059v1",
      "true_abstract": "Surface structure affects the growth, shape and properties of nanoparticles.\nIn wet chemical syntheses, metal additives and surfactants are used to modify\nsurfaces and guide nanocrystal growth. To understand this process, it is\ncritical to understand how the surface structure is modified. However,\nmeasuring the type and arrangement of atoms at hard-soft interfaces on\nnanoscale surfaces, especially in the presence of surfactants, is extremely\nchallenging. Here, we determine the atomic structure of the hard-soft interface\nin a metallic nanoparticle by developing low-dose imaging conditions in\nfour-dimensional scanning transmission electron microscopy that are\npreferentially sensitive to surface adatoms. By revealing experimentally the\ncopper additives and bromide surfactant counterion at the surface of a gold\nnanocuboid and quantifying their interatomic distances, our direct, low-dose\nimaging method provides atomic-level understanding of chemically sophisticated\nnanomaterial surface structures. These measurements of the atomic structure of\nthe hard-soft interface provide the information necessary to understand and\nquantify surface chemistries and energies and their pivotal role in nanocrystal\ngrowth.",
      "generated_abstract": "The experimental observation of a novel phase transition of the spin Hall\neffect (SHE) in the magnetic tunnel junction (MTJ) leads to the development of\nthe spin Hall angle (SHA), which can be used to characterize the SHE in\nmagnetic devices. In this paper, we explore the experimental method for\ndetermining the SHA. Based on the principle of magnetic field-driven current\nmeasurement, we propose an experiment to determine the SHA from the\nmagnetic-field-dependent current response. We construct a magnetic field-based\nmagnetic-field-dependent current response (MF-MDIR) measurement scheme to\nmeasure the SHA. By using the MF-MDIR method, we can obtain the SHA from the\nexperimental measurements of the magnetic field-dependent current response and\nthe magnetic field dependence of the SHE in a MTJ. We present the\nconceptual",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14018691588785046,
          "p": 0.20833333333333334,
          "f": 0.16759776055428996
        },
        "rouge-2": {
          "r": 0.04697986577181208,
          "p": 0.0660377358490566,
          "f": 0.05490195592649024
        },
        "rouge-l": {
          "r": 0.14018691588785046,
          "p": 0.20833333333333334,
          "f": 0.16759776055428996
        }
      }
    },
    {
      "paper_id": "cs.AR.cs/AR/2503.09975v1",
      "true_abstract": "Low-precision data types are essential in modern neural networks during both\ntraining and inference as they enhance throughput and computational capacity by\nbetter exploiting available hardware resources. Despite the incorporation of\nFP8 in commercially available neural network accelerators, a comprehensive\nexposition of its underlying mechanisms, along with rigorous performance and\naccuracy evaluations, is still lacking. In this work, we contribute in three\nsignificant ways. First, we analyze the implementation details and quantization\noptions associated with FP8 for inference on the Intel Gaudi AI accelerator.\nSecond, we empirically quantify the throughput improvements afforded by the use\nof FP8 at both the operator level and in end-to-end scenarios. Third, we assess\nthe accuracy impact of various FP8 quantization methods. Our experimental\nresults indicate that the Intel Gaudi 2 accelerator consistently achieves high\ncomputational unit utilization, frequently exceeding 90\\% MFU, while incurring\nan accuracy degradation of less than 1\\%.",
      "generated_abstract": "This paper presents a novel approach to the problem of estimating the\neffective sample size (ESI) of a finite sample distribution. The approach is\nbased on the concept of the mean square error (MSE) of the estimated\ndistribution. We introduce a new MSE measure that is based on the mean squared\nerror of the sample mean. We also show that the proposed MSE measure is\nindependent of the estimator used to estimate the distribution. Our approach is\nbased on the asymptotic properties of the sample mean. We provide a\ncomputational procedure for estimating the MSE of the mean of a finite sample\ndistribution. We propose to use the MSE of the mean of the sample mean as an\nestimator of the ESI of the finite sample distribution. We illustrate the\nproposed approach with two examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12037037037037036,
          "p": 0.22807017543859648,
          "f": 0.15757575305344365
        },
        "rouge-2": {
          "r": 0.014084507042253521,
          "p": 0.020833333333333332,
          "f": 0.016806717875857604
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.21052631578947367,
          "f": 0.14545454093223154
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2502.20177v1",
      "true_abstract": "The paper extends in two directions the work of \\cite{Plackett77} who studied\nhow, in a $2\\times 2$ table, the likelihood of the column totals depends on the\nodds ratio. First, we study the marginal likelihood of a single $R\\times C$\nfrequency table when only the marginal frequencies are observed and then\nconsider a collection of, say, $s$ $R\\times C$ tables, where only the row and\ncolumn totals can be observed, which is the basic framework which in\napplications of Ecological Inference. In the simpler context, we derive the\nlikelihood equations and show that the likelihood has a collection of local\nmaxima which, after a suitable rearrangement of the row and column categories,\nexhibit the strongest positive association compatible with the marginals, a\nkind of paradox, considering that the available data are so poor. Next, we\nderive the likelihood equations for the marginal likelihood of a collection of\ntow-way tables, under the assumption that they share the same row conditional\ndistributions and derive a necessary condition for the information matrix to be\nwell defined. We also describe a Fisher-scoring algorithm for maximizing the\nmarginal likelihood which, however, can be used only if the number of available\nreplications reaches a given threshold.",
      "generated_abstract": "The aim of this work is to develop a novel and more intuitive Bayesian\nmodel selection method for the logistic regression model based on the Bayes\nfactor. We demonstrate the method through a simulation study and an application\nto the prediction of blood transfusion needs in the USA. The proposed method\nprovides a more flexible and intuitive way to select between competing\nhypotheses and, therefore, can be applied to complex Bayesian models. The\nmethodology is easily implementable in R.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1271186440677966,
          "p": 0.2631578947368421,
          "f": 0.17142856703608172
        },
        "rouge-2": {
          "r": 0.01744186046511628,
          "p": 0.038461538461538464,
          "f": 0.023999995706880765
        },
        "rouge-l": {
          "r": 0.11016949152542373,
          "p": 0.22807017543859648,
          "f": 0.1485714241789389
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.05435v1",
      "true_abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias\ndue to training and inference mismatch. Prior works propose the contrastive\nmethod to deal with caption degeneration. However, the contrastive method\nignores the temporal information when measuring similarity across acoustic and\nlinguistic modalities, leading to inferior performance. In this work, we\ndevelop the temporal-similarity score by introducing the unbiased sliced\nWasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to\naccount for temporal information across modalities. In contrast to the\nconventional sliced Wasserstein RBF kernel, we can form an unbiased estimation\nof USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to\nstochastic gradient optimization algorithms, and its approximation error\ndecreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo\nsamples. Additionally, we introduce an audio captioning framework based on the\nunbiased sliced Wasserstein kernel, incorporating stochastic decoding methods\nto mitigate caption degeneration during the generation process. We conduct\nextensive quantitative and qualitative experiments on two datasets, AudioCaps\nand Clotho, to illustrate the capability of generating high-quality audio\ncaptions. Experimental results show that our framework is able to increase\ncaption length, lexical diversity, and text-to-audio self-retrieval accuracy.",
      "generated_abstract": "We present a novel algorithm for the continuous-time optimal control of a\npneumatic system with variable pressure and a linear dynamic model. The control\nproblem is formulated as a nonlinear optimal control problem in the time domain,\nand is solved by an iterative least squares (ILS) method. The ILS algorithm is\napplied to the control of the pressure of a pneumatic system with variable\npressure. By analyzing the system dynamics, the ILS algorithm is formulated as\nan optimization problem with variable coefficients. This optimization problem\nis solved by a generalized eigenvalue problem, which is solved using a\nmethod based on the decomposition of the eigenproblem into a linear system.\nThe proposed algorithm is validated through numerical experiments with\nsimulated and experimental data. The experimental data are obtained from the\nsystem of a gas turbine compressor with variable pressure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.22857142857142856,
          "f": 0.15999999545000015
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.02727272727272727,
          "f": 0.020905918617442096
        },
        "rouge-l": {
          "r": 0.12307692307692308,
          "p": 0.22857142857142856,
          "f": 0.15999999545000015
        }
      }
    },
    {
      "paper_id": "cs.SI.q-bio/PE/2502.12847v1",
      "true_abstract": "Understanding how cognitive and social mechanisms shape the evolution of\ncomplex artifacts such as songs is central to cultural evolution research.\nSocial network topology (what artifacts are available?), selection (which are\nchosen?), and reproduction (how are they copied?) have all been proposed as key\ninfluencing factors. However, prior research has rarely studied them together\ndue to methodological challenges. We address this gap through a controlled\nnaturalistic paradigm whereby participants (N=2,404) are placed in networks and\nare asked to iteratively choose and sing back melodies from their neighbors. We\nshow that this setting yields melodies that are more complex and more pleasant\nthan those found in the more-studied linear transmission setting, and exhibits\nrobust differences across topologies. Crucially, these differences are\ndiminished when selection or reproduction bias are eliminated, suggesting an\ninteraction between mechanisms. These findings shed light on the interplay of\nmechanisms underlying the evolution of cultural artifacts.",
      "generated_abstract": "The development of deep learning methods has revolutionized the\ncomputational analysis of single-cell data. While recent advances have\nimproved the performance of these methods, there is still a lack of scalable\napproaches that can handle large-scale datasets. In this work, we introduce\nCosyNet, a framework that integrates several key components to improve the\nperformance of deep learning models for single-cell data analysis. Our\napproach consists of a deep neural network that performs the data pre-processing\nand segmentation. The network is then coupled with an attention mechanism to\nimprove the performance of the model. The attention mechanism allows for\nsegmentation of cells by their spatial position in the image, allowing for\nsegmentation of cells based on their location in the image. CosyNet also\nincorporates a graph-based attention mechanism that allows for a more\naccurate representation of the spatial information of the cells in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16363636363636364,
          "p": 0.21176470588235294,
          "f": 0.18461537969756753
        },
        "rouge-2": {
          "r": 0.006896551724137931,
          "p": 0.008130081300813009,
          "f": 0.007462681600860958
        },
        "rouge-l": {
          "r": 0.15454545454545454,
          "p": 0.2,
          "f": 0.17435896944115728
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.09678v1",
      "true_abstract": "For the first time, quality distribution of trees is introduced in a tree\ngrowth model. Consequently, the effects of quality thinning on stand\ndevelopment can be investigated. Quality thinning improves the financial return\nin all cases studied, but the effect is small. Rotation ages, timber stocks and\nmaturity diameters are not much affected by quality thinning. Bare land\nvaluation neither changes the contribution of the quality thinning. The reason\nfor the small effect apparently lies in the value development of individual\ntrees. The relative value development of small pulpwood trunks is large, since\nthe harvesting expense per volume unit is reduced along with size increment.\nSuch trees are not feasible objects for quality thinning, unless quality\ncorrelates with growth rate. Another enhanced stage of value development is\nwhen pulpwood trunks turn to sawlog trunks. For large pulpwood trunks, quality\nthinning is feasible. Existing sawlog content in trees dilutes the effect of\nquality thinning on the financial return. The results change if the growth rate\nis positively correlated with quality, quality thinning becoming feasible in\nall commercial diameter classes.",
      "generated_abstract": "This paper explores the implications of the U.S. Supreme Court's decision in\nWayne Lines LLC v. United States, a landmark case that invalidated the\nUnited States' 2020 tax on carbon emissions, for the global carbon tax debate.\nWe find that the ruling significantly reduces the incentives for developing\nclean energy technologies, leading to a decline in global carbon emissions by\n$570 billion between 2025 and 2030. This decline is driven by a decrease in\ninvestments in clean energy technologies, with a 25 percent drop in the\nnumber of new renewable energy projects being built. Our findings also show\nthat the carbon tax ruling has a substantially larger impact on developed\ncountries, where carbon emissions decline by $1,200 billion in 2030 compared to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10679611650485436,
          "p": 0.1375,
          "f": 0.12021857431395404
        },
        "rouge-2": {
          "r": 0.01875,
          "p": 0.02586206896551724,
          "f": 0.021739125561857843
        },
        "rouge-l": {
          "r": 0.0970873786407767,
          "p": 0.125,
          "f": 0.10928961256532017
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.06009v1",
      "true_abstract": "In this paper, we investigate one of the most fundamental nonconvex learning\nproblems, ReLU regression, in the Differential Privacy (DP) model. Previous\nstudies on private ReLU regression heavily rely on stringent assumptions, such\nas constant bounded norms for feature vectors and labels. We relax these\nassumptions to a more standard setting, where data can be i.i.d. sampled from\n$O(1)$-sub-Gaussian distributions. We first show that when $\\varepsilon =\n\\tilde{O}(\\sqrt{\\frac{1}{N}})$ and there is some public data, it is possible to\nachieve an upper bound of $\\Tilde{O}(\\frac{d^2}{N^2 \\varepsilon^2})$ for the\nexcess population risk in $(\\epsilon, \\delta)$-DP, where $d$ is the dimension\nand $N$ is the number of data samples. Moreover, we relax the requirement of\n$\\epsilon$ and public data by proposing and analyzing a one-pass mini-batch\nGeneralized Linear Model Perceptron algorithm (DP-MBGLMtron). Additionally,\nusing the tracing attack argument technique, we demonstrate that the minimax\nrate of the estimation error for $(\\varepsilon, \\delta)$-DP algorithms is lower\nbounded by $\\Omega(\\frac{d^2}{N^2 \\varepsilon^2})$. This shows that\nDP-MBGLMtron achieves the optimal utility bound up to logarithmic factors.\nExperiments further support our theoretical results.",
      "generated_abstract": "We study the problem of learning a deep neural network for the task of\nprompting, that is, predicting the next token in a sequence of tokens based on\nthe current token. This problem arises in natural language processing tasks\nsuch as summarization and question answering, where prompting a sequence of\ntokens is essential for generating high-quality answers. However, the task of\nprompting is challenging due to the non-linearity of the data generation process\nand the complexity of the model architecture. In this paper, we propose a\nneural network architecture that learns a function that maps the current token\nto the next token in the sequence. This function is parameterized by a\nparameterized neural network, and we show that it can be learned with the\nvanilla RL algorithms. We then propose a two-stage training procedure for\nlearning this function, and we show that this procedure converges to a\nparametrized neural network",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21052631578947367,
          "p": 0.345679012345679,
          "f": 0.26168223828587656
        },
        "rouge-2": {
          "r": 0.05747126436781609,
          "p": 0.07633587786259542,
          "f": 0.06557376559118554
        },
        "rouge-l": {
          "r": 0.20300751879699247,
          "p": 0.3333333333333333,
          "f": 0.2523364438933532
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2408.07497v1",
      "true_abstract": "This paper presents a method for accurately predicting the full distribution\nof stock returns, given a comprehensive set of 194 stock characteristics and\nmarket variables. Such distributions, learned from rich data using a machine\nlearning algorithm, are not constrained by restrictive model assumptions and\nallow the exploration of non-Gaussian, heavy-tailed data and their non-linear\ninteractions. The method uses a two-stage quantile neural network combined with\nspline interpolation. The results show that the proposed approach outperforms\nalternative models in terms of out-of-sample losses. Furthermore, we show that\nthe moments derived from such distributions can be useful as alternative\nempirical estimates in many cases, including mean estimation and forecasting.\nFinally, we examine the relationship between cross-sectional returns and\nseveral distributional characteristics. The results are robust to a wide range\nof US and international data.",
      "generated_abstract": "The analysis of time series of financial markets is an essential tool for\nresearchers and practitioners. The classical approach to time series analysis\nrelies on the construction of a time series model, which is then tested for\nconsistency using statistical tests. However, this approach faces several\nchallenges. First, it is often difficult to find a suitable time series model\nthat fits the data. Second, it is often difficult to determine whether a\ntime series model is consistent. In this paper, we propose a novel approach to\ntime series analysis based on the identification of the minimal set of\nparameters. We show that, under certain conditions, the identification of\nminimal set of parameters allows us to obtain consistent time series models.\nWe further show that this approach yields more interpretable results than\nclassical approaches. We apply our approach to the analysis of the stock market\nand the Euro Stoxx 50 index.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.23529411764705882,
          "f": 0.21621621124908705
        },
        "rouge-2": {
          "r": 0.015625,
          "p": 0.016129032258064516,
          "f": 0.01587301087427721
        },
        "rouge-l": {
          "r": 0.17,
          "p": 0.2,
          "f": 0.18378377881665464
        }
      }
    },
    {
      "paper_id": "cs.IR.cs/IR/2503.09492v1",
      "true_abstract": "Cascade Ranking is a prevalent architecture in large-scale top-k selection\nsystems like recommendation and advertising platforms. Traditional training\nmethods focus on single-stage optimization, neglecting interactions between\nstages. Recent advances such as RankFlow and FS-LTR have introduced\ninteraction-aware training paradigms but still struggle to 1) align training\nobjectives with the goal of the entire cascade ranking (i.e., end-to-end\nrecall) and 2) learn effective collaboration patterns for different stages. To\naddress these challenges, we propose LCRON, which introduces a novel surrogate\nloss function derived from the lower bound probability that ground truth items\nare selected by cascade ranking, ensuring alignment with the overall objective\nof the system. According to the properties of the derived bound, we further\ndesign an auxiliary loss for each stage to drive the reduction of this bound,\nleading to a more robust and effective top-k selection. LCRON enables\nend-to-end training of the entire cascade ranking system as a unified network.\nExperimental results demonstrate that LCRON achieves significant improvement\nover existing methods on public benchmarks and industrial applications,\naddressing key limitations in cascade ranking training and significantly\nenhancing system performance.",
      "generated_abstract": "The popularity of open-source machine learning (ML) models, such as\ntranslation models and question-answering systems, has driven the need for\nsystems that can process large quantities of textual data efficiently. While\nstate-of-the-art deep learning models have demonstrated strong performance in\nsuch tasks, their large size and computational requirements have limited their\napplicability in real-world environments. In this paper, we propose a\nlightweight, efficient, and effective framework for model-free translation\nthrough the use of neural message-passing algorithms. Our approach, called\nNeural Message-Passing Translation (NMP-Trans), is based on the NMP (Neural\nMessage-Passing) architecture, which has shown promise for tasks such as\nquestion-answering. By introducing a novel message-passing module that\nintegrates translation and language modeling, NMP-Trans enhances translation\nperformance without",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15267175572519084,
          "p": 0.2222222222222222,
          "f": 0.18099547028521132
        },
        "rouge-2": {
          "r": 0.023121387283236993,
          "p": 0.034782608695652174,
          "f": 0.02777777298056603
        },
        "rouge-l": {
          "r": 0.13740458015267176,
          "p": 0.2,
          "f": 0.16289592277389914
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2405.10920v1",
      "true_abstract": "We study the data-generating processes for factors expressed in return\ndifferences, which the literature on time-series asset pricing seems to have\noverlooked. For the factors' data-generating processes or long-short zero-cost\nportfolios, a meaningful definition of returns is impossible; further, the\ncompounded market factor (MF) significantly underestimates the return\ndifference between the market and the risk-free rate compounded separately.\nSurprisingly, if MF were treated coercively as periodic-rebalancing long-short\n(i.e., the same as size and value), Fama-French three-factor (FF3) would be\neconomically unattractive for lacking compounding and irrelevant for suffering\nfrom the small \"size of an effect.\" Otherwise, FF3 might be misspecified if MF\nwere buy-and-hold long-short. Finally, we show that OLS with net returns for\nsingle-index models leads to inflated alphas, exaggerated t-values, and\noverestimated Sharpe ratios (SR); worse, net returns may lead to pathological\nalphas and SRs. We propose defining factors (and SRs) with non-difference\ncompound returns.",
      "generated_abstract": "We study the problem of portfolio selection in a heterogeneous market with\nportfolios of different risk levels. Our goal is to find a portfolio that\nmaximizes expected utility subject to a constraint on the total risk of the\nportfolio. We develop a novel approach that employs a stochastic control\nformulation of the portfolio selection problem with the goal of optimizing\nrisk-adjusted expected utility. The control is determined by the optimal\nportfolio, and the corresponding expected utility is obtained as a function of\nthe optimal portfolio. We derive the solution for a class of Markovian\ncontinuous-time stochastic control problems and prove that the resulting\ncontrol is a solution to the stochastic control problem with the\nexpectation-adjusted expected utility. We then use this control to derive\nclosed-form expressions for the optimal portfolio and the optimal expected\nutility, and we prove that these expressions are also solutions to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1415929203539823,
          "p": 0.2318840579710145,
          "f": 0.1758241711164112
        },
        "rouge-2": {
          "r": 0.020689655172413793,
          "p": 0.024,
          "f": 0.02222221724965818
        },
        "rouge-l": {
          "r": 0.12389380530973451,
          "p": 0.2028985507246377,
          "f": 0.15384614913838923
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/GR/2503.08061v2",
      "true_abstract": "Realistic hand manipulation is a key component of immersive virtual reality\n(VR), yet existing methods often rely on a kinematic approach or motion-capture\ndatasets that omit crucial physical attributes such as contact forces and\nfinger torques. Consequently, these approaches prioritize tight,\none-size-fits-all grips rather than reflecting users' intended force levels. We\npresent ForceGrip, a deep learning agent that synthesizes realistic hand\nmanipulation motions, faithfully reflecting the user's grip force intention.\nInstead of mimicking predefined motion datasets, ForceGrip uses generated\ntraining scenarios-randomizing object shapes, wrist movements, and trigger\ninput flows-to challenge the agent with a broad spectrum of physical\ninteractions. To effectively learn from these complex tasks, we employ a\nthree-phase curriculum learning framework comprising Finger Positioning,\nIntention Adaptation, and Dynamic Stabilization. This progressive strategy\nensures stable hand-object contact, adaptive force control based on user\ninputs, and robust handling under dynamic conditions. Additionally, a proximity\nreward function enhances natural finger motions and accelerates training\nconvergence. Quantitative and qualitative evaluations reveal ForceGrip's\nsuperior force controllability and plausibility compared to state-of-the-art\nmethods. The video presentation of our paper is accessible at\nhttps://youtu.be/lR-YAfninJw.",
      "generated_abstract": "We present a novel method for performing robust and high-fidelity\nmanipulation in open-loop. The method is based on a 2D-3D mapping of the robot's\nhand and is optimized to achieve high-fidelity 3D manipulation in an open-loop\nmanner. This methodology provides a novel perspective on robotic manipulation\nand its integration with the open-loop manipulation paradigm, with implications\nfor both hardware and software development. The methodology is applicable to\nrobots operating in dynamic environments and is designed to be robust and\nfuture-proof. To demonstrate the methodology, we introduce a novel robot\narchitecture, the Open-Loop Robot, which achieves high-fidelity 3D manipulation\nin an open-loop manner.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12751677852348994,
          "p": 0.3064516129032258,
          "f": 0.18009478257990624
        },
        "rouge-2": {
          "r": 0.016853932584269662,
          "p": 0.03333333333333333,
          "f": 0.02238805524058899
        },
        "rouge-l": {
          "r": 0.12080536912751678,
          "p": 0.2903225806451613,
          "f": 0.17061610959412427
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2503.08638v1",
      "true_abstract": "We tackle the task of long-form music generation--particularly the\nchallenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)\ntrack-decoupled next-token prediction to overcome dense mixture signals, (2)\nstructural progressive conditioning for long-context lyrical alignment, and (3)\na multitask, multiphase pre-training recipe to converge and generalize. In\naddition, we redesign the in-context learning technique for music generation,\nenabling versatile style transfer (e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perform well on music understanding tasks, where the\nresults of YuE match or exceed state-of-the-art methods on the MARBLE\nbenchmark. Keywords: lyrics2song, song generation, long-form, foundation model,\nmusic generation",
      "generated_abstract": "The rapid expansion of Internet of Things (IoT) applications has created\nmany challenges in data quality and privacy, including the lack of\nconsensus on data access and use, data ownership and control, data sharing,\nand data protection. While there is a growing body of research on privacy\nenhancing technologies (PETs) and data protection, there remains a lack of\nconsensus on the role of data sharing in enhancing privacy and data\nprotection. This paper addresses this gap by proposing a novel framework for\nenhancing data privacy and data protection through data sharing. The framework\naddresses the challenges of data ownership and control, data sharing, and data\nprotection by proposing three key principles: data sharing as a right,\ndata-sharing agreements as contracts, and data sharing as a layer of privacy.\nThis framework provides a framework for data privacy and data protection,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06164383561643835,
          "p": 0.140625,
          "f": 0.0857142814766442
        },
        "rouge-2": {
          "r": 0.005291005291005291,
          "p": 0.009259259259259259,
          "f": 0.0067340021059107405
        },
        "rouge-l": {
          "r": 0.0547945205479452,
          "p": 0.125,
          "f": 0.0761904719528347
        }
      }
    },
    {
      "paper_id": "cs.AI.stat/OT/2412.14222v1",
      "true_abstract": "In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.",
      "generated_abstract": "This paper proposes a new method for estimating the average number of\nrandom walks that a particular agent may take, under various environmental\nconditions. Our approach utilizes the fact that the average number of random\nwalks that an agent takes under a given environment is related to the\nprobability that the agent takes a particular number of random walks. We\ndevelop a new estimator that is based on this relationship, and we show that\nthis estimator has high asymptotic accuracy, as long as the environment is\nwell-behaved. We then apply our estimator to the case of random walks on\n$d$-dimensional regular lattices, where the environment is the Laplacian matrix\nof the lattice. We demonstrate that our estimator can accurately estimate the\nprobability that an agent takes $n$ random walks, even if the Laplacian\nmatrix is non-positive definite. We also show that our estimator",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.1891891891891892,
          "f": 0.15384614902064983
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.008928571428571428,
          "f": 0.008130076340805457
        },
        "rouge-l": {
          "r": 0.10185185185185185,
          "p": 0.14864864864864866,
          "f": 0.12087911605361691
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.02899v1",
      "true_abstract": "Accurately discriminating progressive stages of Alzheimer's Disease (AD) is\ncrucial for early diagnosis and prevention. It often involves multiple imaging\nmodalities to understand the complex pathology of AD, however, acquiring a\ncomplete set of images is challenging due to high cost and burden for subjects.\nIn the end, missing data become inevitable which lead to limited sample-size\nand decrease in precision in downstream analyses. To tackle this challenge, we\nintroduce a holistic imaging feature imputation method that enables to leverage\ndiverse imaging features while retaining all subjects. The proposed method\ncomprises two networks: 1) An encoder to extract modality-independent\nembeddings and 2) A decoder to reconstruct the original measures conditioned on\ntheir imaging modalities. The encoder includes a novel {\\em ordinal contrastive\nloss}, which aligns samples in the embedding space according to the progression\nof AD. We also maximize modality-wise coherence of embeddings within each\nsubject, in conjunction with domain adversarial training algorithms, to further\nenhance alignment between different imaging modalities. The proposed method\npromotes our holistic imaging feature imputation across various modalities in\nthe shared embedding space. In the experiments, we show that our networks\ndeliver favorable results for statistical analysis and classification against\nimputation baselines with Alzheimer's Disease Neuroimaging Initiative (ADNI)\nstudy.",
      "generated_abstract": "High-dimensional images are often described by a multi-scale sparse\ncode, which can be learned using an unsupervised model such as diffusion\nproposal networks (DPNs). However, the learned code often fails to capture\nhigh-frequency features, which can be crucial for medical imaging. In this\npaper, we propose a new approach to enhance the learned sparse codes by\nutilizing the high-frequency information. Specifically, we propose a DPN-based\nmodel with a sparse code augmentation module. This module leverages the\nsparse codes generated by DPNs to enhance the learning of the high-frequency\ncomponents of the input images. The proposed model is trained end-to-end with a\nreinforcement learning framework, and it achieves state-of-the-art performance\non the PET-MRI dataset. Our code is available at\nhttps://github.com/Yi",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14583333333333334,
          "p": 0.2625,
          "f": 0.1874999954081634
        },
        "rouge-2": {
          "r": 0.010471204188481676,
          "p": 0.018518518518518517,
          "f": 0.013377921806692792
        },
        "rouge-l": {
          "r": 0.14583333333333334,
          "p": 0.2625,
          "f": 0.1874999954081634
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.02683v1",
      "true_abstract": "Streaming multi-talker speech translation is a task that involves not only\ngenerating accurate and fluent translations with low latency but also\nrecognizing when a speaker change occurs and what the speaker's gender is.\nSpeaker change information can be used to create audio prompts for a zero-shot\ntext-to-speech system, and gender can help to select speaker profiles in a\nconventional text-to-speech model. We propose to tackle streaming speaker\nchange detection and gender classification by incorporating speaker embeddings\ninto a transducer-based streaming end-to-end speech translation model. Our\nexperiments demonstrate that the proposed methods can achieve high accuracy for\nboth speaker change detection and gender classification.",
      "generated_abstract": "This paper introduces a novel audio-driven approach for automatic speaker\ndiscrimination and speech enhancement in the context of conversational assistants.\nWe propose a multi-task learning framework that integrates a multilingual\nconversational assistant, a large language model (LLM), and a dedicated speaker\ndiscrimination network. The assistant uses the multilingual LLM as a speech\ngenerator, while the speaker discrimination network utilizes a multilingual\nmultitask LLM for automatic speaker verification and enhancement. Our\napproach enables the assistant to generate speech in a diverse range of\nlanguages, while the multitask LLM is trained to recognize and enhance speech\nfrom a single language. This approach allows the assistant to seamlessly\nhandle multiple languages, and the multitask LLM is trained to handle the\ndiverse range of languages. We evaluate our approach on the open-source\nB",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.20588235294117646,
          "f": 0.202898545725688
        },
        "rouge-2": {
          "r": 0.02127659574468085,
          "p": 0.018018018018018018,
          "f": 0.019512190156336776
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.20588235294117646,
          "f": 0.202898545725688
        }
      }
    },
    {
      "paper_id": "cs.HC.q-bio/OT/2410.10513v1",
      "true_abstract": "Structuring data analysis projects, that is, defining the layout of files and\nfolders needed to analyze data using existing tools and novel code, largely\nfollows personal preferences. In this work, we look at the structure of several\ndata analysis project templates and find little structural overlap. We\nhighlight the parts that are similar between them, and propose guiding\nprinciples to keep in mind when one wishes to create a new data analysis\nproject. Finally, we present Kerblam!, a project management tool that can\nexpedite project data management, execution of workflow managers, and sharing\nof the resulting workflow and analysis outputs. We hope that, by following\nthese principles and using Kerblam!, the landscape of data analysis projects\ncan become more transparent, understandable, and ultimately useful to the wider\ncommunity.",
      "generated_abstract": "Recent advances in large language models (LLMs) have demonstrated their\ncapability to generate human-like language. However, these models often struggle\nto generalize to unseen tasks, even when their training data is balanced. We\npropose a novel method for improving LLMs' generalizability, called\n\"balanced-generation\". In our approach, we first train an LLM to generate\nbalanced samples from a balanced dataset, and then fine-tune this LLM on a\nsingle task. This method ensures that the LLM's performance on unseen tasks\ndoes not decrease over time, regardless of the balance of training data. We\nevaluate our approach on three tasks: medical-related, scientific, and\ngeneral-purpose language generation. Our results show that our method significantly\nimproves LLMs' generalizability, with a 42% increase in test accuracy on the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1839080459770115,
          "p": 0.1797752808988764,
          "f": 0.18181817681882761
        },
        "rouge-2": {
          "r": 0.008130081300813009,
          "p": 0.00847457627118644,
          "f": 0.008298750188877168
        },
        "rouge-l": {
          "r": 0.16091954022988506,
          "p": 0.15730337078651685,
          "f": 0.1590909040915549
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14873v1",
      "true_abstract": "We examine the well-posedness of inverse eigenstrain problems for residual\nstress analysis from the perspective of the non-uniqueness of solutions,\nstructure of the corresponding null space and associated orthogonal range-null\ndecompositions. Through this process we highlight the existence of a trivial\nsolution to all inverse eigenstrain problems, with all other solutions\ndiffering from this trivial version by an unobservable null component. From one\nperspective, this implies that no new information can be gained though\neigenstrain analysis, however we also highlight the utility of the eigenstrain\nframework for enforcing equilibrium while estimating residual stress from\nincomplete experimental data. Two examples based on measured experimental data\nare given; one axisymmetric system involving ancient Roman medical tools, and\none more-general system involving an additively manufactured Inconel sample. We\nconclude by drawing a link between eigenstrain and reconstruction formulas\nrelated to strain tomography based on the Longitudinal Ray Transform (LRT).\nThrough this link, we establish a potential means for tomographic\nreconstruction of residual stress from LRT measurements.",
      "generated_abstract": "In this paper, we develop a framework to study the quantum Hamiltonian\nsystems with a quantum Hamiltonian in the form of a generalized Pauli-Villars\nsymbol. The generalized Pauli-Villars symbol is defined in the\n$(\\mathbb{C}^n\\otimes\\mathbb{C}^n)$-module, where the tensor product is\nover the algebra of polynomials in the Pauli-Villars variables. We prove that\nthe quantum Hamiltonian system is equivalent to the linear Dirac equation\nwith a generalized Pauli-Villars symbol. We also study the quantum Hamiltonian\nsystem with a generalized Pauli-Villars symbol of the form of a polynomial in\nthe Pauli-Villars variables, and derive the equation of motion for the\npolynomial in the Pauli-Villars variables.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.30434782608695654,
          "f": 0.18064515711633727
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.0273972602739726,
          "f": 0.01785713846340988
        },
        "rouge-l": {
          "r": 0.11926605504587157,
          "p": 0.2826086956521739,
          "f": 0.16774193130988563
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PM/2502.19213v1",
      "true_abstract": "We consider an optimal investment-consumption problem for a\nutility-maximizing investor who has access to assets with different liquidity\nand whose consumption rate as well as terminal wealth are subject to\nlower-bound constraints. Assuming utility functions that satisfy standard\nconditions, we develop a methodology for deriving the optimal strategies in\nsemi-closed form. Our methodology is based on the generalized martingale\napproach and the decomposition of the problem into subproblems. We illustrate\nour approach by deriving explicit formulas for agents with power-utility\nfunctions and discuss potential extensions of the proposed framework.",
      "generated_abstract": "In this paper, we propose a novel approach for the portfolio selection problem\nin the presence of time-varying risk premiums. We consider a portfolio\nselection model where the expected return of the investment portfolio depends on\nthe risk premium, which is estimated over a certain period of time. The\nperiodicity of the risk premium leads to a non-linear dependency between the\nexpected return and the risk premium. To address this non-linear dependency, we\npropose a method that employs a non-linear regression model to obtain the\nrisk premium at the beginning of the period. We then apply a thresholding\ntechnique to obtain the risk premium at the end of the period. The proposed\nmethod is applied to the CBOE Volatility Index (VIX) and the S\\&P 500 Index\n(SPY). We then compare our proposed method with the standard portfolio",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2753623188405797,
          "p": 0.2602739726027397,
          "f": 0.2676056288067844
        },
        "rouge-2": {
          "r": 0.04597701149425287,
          "p": 0.03571428571428571,
          "f": 0.04020100010403838
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.2054794520547945,
          "f": 0.2112676006377704
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.10008v1",
      "true_abstract": "We study whether ChatGPT and DeepSeek can extract information from the Wall\nStreet Journal to predict the stock market and the macroeconomy. We find that\nChatGPT has predictive power. DeepSeek underperforms ChatGPT, which is trained\nmore extensively in English. Other large language models also underperform.\nConsistent with financial theories, the predictability is driven by investors'\nunderreaction to positive news, especially during periods of economic downturn\nand high information uncertainty. Negative news correlates with returns but\nlacks predictive value. At present, ChatGPT appears to be the only model\ncapable of capturing economic news that links to the market risk premium.",
      "generated_abstract": "This paper introduces a novel approach to modeling dynamic network\nrepresentations. We use a Bayesian framework to analyze the probability of\nnetworks evolving from a given initial condition. We focus on the evolution of\nthe number of nodes and edges, as well as the degree distribution of the\nnetwork. We show that the network structure is a Markov process, with the\nprobability of observing a given network at a later time determined by the\ncurrent network, as well as the state of the network at the start of the\nprocess. The model is flexible enough to accommodate a wide range of\ninterpretations. We illustrate the model with examples from social and economic\nnetworks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15789473684210525,
          "p": 0.18181818181818182,
          "f": 0.16901407953183908
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.16666666666666666,
          "f": 0.15492957248958558
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/TH/2502.09525v1",
      "true_abstract": "We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.",
      "generated_abstract": "We consider the problem of learning a latent-variable Gaussian process\ndistribution over a finite set of points from a noisy observation of the\nprocess. We propose to learn this distribution using the empirical process\ngradients (EPGs), which are smooth and bounded. We derive a generalization of\nthe Gaussian process regularization method, which we call EPG regularization.\nOur method is based on the idea of using EPGs to approximate the gradient of\nthe Gaussian process distribution. We then propose a new algorithm,\nEPG-RegRD, which is a variant of the regularized regression method. We prove\nthat EPG-RegRD is a consistent estimator of the GP mean and covariance.\nMoreover, we prove that EPG-RegRD is asymptotically normal. Finally, we\nanalyze the performance of EPG-RegRD in terms of both the sample size and the\nnoise level",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14838709677419354,
          "p": 0.30666666666666664,
          "f": 0.19999999560491502
        },
        "rouge-2": {
          "r": 0.02608695652173913,
          "p": 0.05084745762711865,
          "f": 0.034482754138592205
        },
        "rouge-l": {
          "r": 0.14193548387096774,
          "p": 0.29333333333333333,
          "f": 0.19130434343100197
        }
      }
    },
    {
      "paper_id": "physics.data-an.nucl-th/2503.09415v1",
      "true_abstract": "SPARKX is an open-source Python package developed to analyze simulation data\nfrom heavy-ion collision experiments. By offering a comprehensive suite of\ntools, SPARKX simplifies data analysis workflows, supports multiple formats\nsuch as OSCAR2013, and integrates seamlessly with SMASH and JETSCAPE/X-SCAPE.\nThis paper describes SPARKX's architecture, features, and applications and\ndemonstrates its effectiveness through detailed examples and performance\nbenchmarks. SPARKX enhances productivity and precision in relativistic\nkinematics studies.",
      "generated_abstract": "In this work, we present the first complete analysis of the decay of the\n$\\Lambda(1405)$ meson, using data from the BESIII collaboration at the\nBeijing Electron-Positron Collider (BEPC-II). The decay of the $\\Lambda(1405)$\nis observed to have a branching ratio of 5.74$\\pm$0.10\\%. This is the first\nexclusive observation of this decay. The branching ratio is consistent with\nexisting decay modes of the $\\Lambda(1405)$, and is in agreement with\nprevious predictions from theoretical models. The decay rate of the $\\Lambda(1405)$\nis found to be 1.20$\\pm$0.06$\\times$10$^{-22}$ cm$^3$ s$^{-1}$, which is\ncompatible with the theoretical prediction of 1.2",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1864406779661017,
          "p": 0.18032786885245902,
          "f": 0.18333332833472235
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.011363636363636364,
          "f": 0.01298700808905565
        },
        "rouge-l": {
          "r": 0.1694915254237288,
          "p": 0.16393442622950818,
          "f": 0.16666666166805572
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.06432v3",
      "true_abstract": "We prove that the Lusztig's $a$-function is bounded for any Coxeter group of\nfinite rank.",
      "generated_abstract": "We consider the problem of finding a minimal set of generators of the\nalgebra $H_1(\\mathbb{C} P^2,\\mathbb{Z})$ for the homology theory of the\ncompact complex torus $\\mathbb{C} P^2$. The group $H_1(\\mathbb{C} P^2,\\mathbb{Z})$\nis a free group on four generators. The torus is the fundamental group of the\ncomplete intersection surface $\\mathbb{C} P^2\\setminus D^2$. In this paper, we\ngive a combinatorial proof that $H_1(\\mathbb{C} P^2,\\mathbb{Z})$ has four\ngenerators, which was first proved by Bott in 1964. In addition, we\nconstruct a minimal set of generators of $H_1(\\mathbb{C} P^2,\\mathbb{Z})$ that\nis not a subalgebra of any",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.5333333333333333,
          "p": 0.14285714285714285,
          "f": 0.22535210934338426
        },
        "rouge-2": {
          "r": 0.07142857142857142,
          "p": 0.012195121951219513,
          "f": 0.020833330842014188
        },
        "rouge-l": {
          "r": 0.4666666666666667,
          "p": 0.125,
          "f": 0.19718309525887726
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2412.10199v1",
      "true_abstract": "This document presents an in-depth examination of stock market sentiment\nthrough the integration of Convolutional Neural Networks (CNN) and Gated\nRecurrent Units (GRU), enabling precise risk alerts. The robust feature\nextraction capability of CNN is utilized to preprocess and analyze extensive\nnetwork text data, identifying local features and patterns. The extracted\nfeature sequences are then input into the GRU model to understand the\nprogression of emotional states over time and their potential impact on future\nmarket sentiment and risk. This approach addresses the order dependence and\nlong-term dependencies inherent in time series data, resulting in a detailed\nanalysis of stock market sentiment and effective early warnings of future\nrisks.",
      "generated_abstract": "We investigate the problem of optimizing the investment of an investor who\nmakes a series of sequential trades in a continuous-time stochastic market.\nTrades are made at fixed prices, and the investor's goal is to minimize the\ntotal expected loss. We focus on a class of linear stochastic markets that\ngeneralize the one studied in [Krishnamurthy (2017)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.26666666666666666,
          "f": 0.19047618588435386
        },
        "rouge-2": {
          "r": 0.009708737864077669,
          "p": 0.017857142857142856,
          "f": 0.012578611789092276
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.24444444444444444,
          "f": 0.17460317001133802
        }
      }
    },
    {
      "paper_id": "cs.AI.q-bio/NC/2502.21142v1",
      "true_abstract": "Humans leverage rich internal models of the world to reason about the future,\nimagine counterfactuals, and adapt flexibly to new situations. In Reinforcement\nLearning (RL), world models aim to capture how the environment evolves in\nresponse to the agent's actions, facilitating planning and generalization.\nHowever, typical world models directly operate on the environment variables\n(e.g. pixels, physical attributes), which can make their training slow and\ncumbersome; instead, it may be advantageous to rely on high-level latent\ndimensions that capture relevant multimodal variables. Global Workspace (GW)\nTheory offers a cognitive framework for multimodal integration and information\nbroadcasting in the brain, and recent studies have begun to introduce efficient\ndeep learning implementations of GW. Here, we evaluate the capabilities of an\nRL system combining GW with a world model. We compare our GW-Dreamer with\nvarious versions of the standard PPO and the original Dreamer algorithms. We\nshow that performing the dreaming process (i.e., mental simulation) inside the\nGW latent space allows for training with fewer environment steps. As an\nadditional emergent property, the resulting model (but not its comparison\nbaselines) displays strong robustness to the absence of one of its observation\nmodalities (images or simulation attributes). We conclude that the combination\nof GW with World Models holds great potential for improving decision-making in\nRL agents.",
      "generated_abstract": "Deep learning models have shown promise in protein structure prediction,\nchallenging the long-held belief that structure prediction is a hard problem.\nHowever, despite recent advances in pre-trained models, the task of structure\nprediction remains challenging, and it remains unclear how to optimize pre-trained\nmodels to achieve high-quality structure predictions. Here, we propose a\nframework for structure prediction using pre-trained models, which we call\nPretrained-Style Prediction (PSP). PSP leverages pre-trained models to guide\nstructure prediction, ensuring that the model is trained to focus on specific\nfeatures of protein structure. Our experiments show that PSP outperforms\nstate-of-the-art methods on benchmark datasets, and our approach demonstrates\nthat pre-trained models can be used to enhance structural prediction. We\nconclude that pre-trained models are a promising approach for structure",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17532467532467533,
          "p": 0.3375,
          "f": 0.23076922626926735
        },
        "rouge-2": {
          "r": 0.028708133971291867,
          "p": 0.05357142857142857,
          "f": 0.03738317302665985
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.2625,
          "f": 0.17948717498721614
        }
      }
    },
    {
      "paper_id": "math.LO.math/LO/2503.04471v1",
      "true_abstract": "We give a survey of cardinal charcteristics of the higher Cicho\\'n diagram\ndefined on the higher Baire space ${}^\\kappa\\kappa$ for $\\kappa$ regular with\n$2^{<\\kappa}=\\kappa$. Specifically, we will compare consistency proofs from the\nclassical Cicho\\'n diagram with various well-known forcing notions to similar\nconstructions generalised to the higher Cicho\\'n diagram. We are especially\ninterested in separation in a horizontal direction, that is, the consistency of\n$\\mathrm{add}(\\mathcal M_\\kappa)<\\mathrm{non}(\\mathcal M_\\kappa)$ and of\n$\\mathrm{cov}(\\mathcal M_\\kappa)<\\mathrm{cof}(\\mathcal M_\\kappa)$.\n  We will have a look at (higher analogues of) Cohen, Hechler, localisation,\neventually different, Sacks, random, Laver, Mathias and Miller forcing, and\ntheir effect on the cardinal characteristics of the higher Cicho\\'n diagram.",
      "generated_abstract": "In this article, we study the local stability of the solution of the\nHamilton-Jacobi-Bellman equation for a general linear diffusion model on\n$(\\mathbb{R}^{n}\\times \\mathbb{R}^{d})\\times[0,T",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08333333333333333,
          "p": 0.2857142857142857,
          "f": 0.12903225456815828
        },
        "rouge-2": {
          "r": 0.010638297872340425,
          "p": 0.045454545454545456,
          "f": 0.017241376236623616
        },
        "rouge-l": {
          "r": 0.05555555555555555,
          "p": 0.19047619047619047,
          "f": 0.08602150187998626
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/CP/2502.01495v1",
      "true_abstract": "We investigate the application of quantum cognition machine learning (QCML),\na novel paradigm for both supervised and unsupervised learning tasks rooted in\nthe mathematical formalism of quantum theory, to distance metric learning in\ncorporate bond markets. Compared to equities, corporate bonds are relatively\nilliquid and both trade and quote data in these securities are relatively\nsparse. Thus, a measure of distance/similarity among corporate bonds is\nparticularly useful for a variety of practical applications in the trading of\nilliquid bonds, including the identification of similar tradable alternatives,\npricing securities with relatively few recent quotes or trades, and explaining\nthe predictions and performance of ML models based on their training data.\nPrevious research has explored supervised similarity learning based on\nclassical tree-based models in this context; here, we explore the application\nof the QCML paradigm for supervised distance metric learning in the same\ncontext, showing that it outperforms classical tree-based models in high-yield\n(HY) markets, while giving comparable or better performance (depending on the\nevaluation metric) in investment grade (IG) markets.",
      "generated_abstract": "We study the problem of trading in an infinite-dimensional space of\nstocks, where the assets' market prices are noisy and inaccurate. The assets\nare represented by a stochastic process that is generated by a diffusion\nequation and is correlated with the market price of stocks. Our aim is to\nfind the best portfolio of assets in terms of their expected returns, given the\nnoisy prices of the assets. We propose a two-step methodology for solving the\nproblem. In the first step, we solve a linear optimization problem to estimate\nthe optimal portfolio weights. In the second step, we solve an\noptimization problem to find the optimal portfolio. We show that our\napproach is computationally efficient and that the two-step methodology\noutperforms the one-step methodology in terms of the expected value of the\nportfolio's returns. Our numerical results show that the two-step methodology",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14953271028037382,
          "p": 0.21621621621621623,
          "f": 0.17679557527670106
        },
        "rouge-2": {
          "r": 0.006535947712418301,
          "p": 0.008264462809917356,
          "f": 0.0072992651411935925
        },
        "rouge-l": {
          "r": 0.102803738317757,
          "p": 0.14864864864864866,
          "f": 0.12154695649217077
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2503.07837v1",
      "true_abstract": "To double the cellular population of ribosomes, a fraction of the active\nribosomes is allocated to synthesize ribosomal proteins. Subsequently, these\nribosomal proteins enter the ribosome self-assembly process, synthesizing new\nribosomes and forming the well-known ribosome autocatalytic subcycle.\nNeglecting ribosome lifetime and the duration of the self-assembly process, the\ndoubling rate of all cellular biomass can be equated with the fraction of\nribosomes allocated to synthesize an essential ribosomal protein times its\nsynthesis rate. However, ribosomes have a finite lifetime, and the assembly\nprocess has a finite duration. Furthermore, the number of ribosomes is known to\ndecrease with slow growth rates. The finite lifetime of ribosomes and the\ndecline in their numbers present a challenge in sustaining slow growth solely\nthrough controlling the allocation of ribosomes to synthesize more ribosomal\nproteins. When the number of ribosomes allocated per mRNA of an essential\nribosomal protein is approximately one, the resulting fluctuations in the\nproduction rate of new ribosomes increase, causing a potential risk that the\nactual production rate will fall below the ribosome death rate. Thus, in this\nregime, a significant risk of extinction of the ribosome population emerges. To\nmitigate this risk, we suggest that the ribosome translation speed is used as\nan alternative control parameter, which facilitates the maintenance of slow\ngrowth rates with a larger ribosome pool. We clarify the observed reduction in\ntranslation speed at harsh environments in E. coli and C. Glutamicum, explore\nother mitigation strategies, and suggest additional falsifiable predictions of\nour model.",
      "generated_abstract": "We present a novel framework for evaluating the fitness of proteins in\nbiological systems, based on the integration of the energetics of protein\nfolding, the thermodynamic constraints of the folding pathway, and the\nrelationship between protein function and protein structure. This framework\nreinforces the importance of proteins as functional systems, and demonstrates\nhow proteins can be fitness landscapes, where fitness is quantified by\nrelative abundance. We use this framework to analyze the fitness of the\nprotein-folding pathway and of the protein-folding step, demonstrating that\nthe latter is more important. We also discuss the implications of this\nframework for proteins in nature and in engineering.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12686567164179105,
          "p": 0.27419354838709675,
          "f": 0.173469383429821
        },
        "rouge-2": {
          "r": 0.02358490566037736,
          "p": 0.053763440860215055,
          "f": 0.03278688100704167
        },
        "rouge-l": {
          "r": 0.1044776119402985,
          "p": 0.22580645161290322,
          "f": 0.14285713853186185
        }
      }
    },
    {
      "paper_id": "math.NT.math/IT/2503.10201v1",
      "true_abstract": "The theories of automorphic forms and self-dual linear codes share many\nremarkable analogies. In both worlds there are functions invariant under an\naction of a group, notions of cusp forms and Hecke operators, also projections\nand lifts between different geni. It is then natural to ask if other important\nautomorphic objects or techniques could be introduced into coding theory. In\nthis article we propose a way to introduce the doubling method, an efficient\ntechnique used to construct and study $L$-functions. As a result, we prove the\nso-called doubling identity, which usually forms a base of many applications.\nHere we use it to solve an analogue of the \"basis problem\". Namely, we express\na cusp form as an explicit linear combination of complete weight enumerators of\nthe same type.",
      "generated_abstract": "In this paper, we give a new proof of the result of G. F. Piatetski-Shapiro\nand S. J. Weinstein [J. Symb. Comput. 19 (1995) 61-70",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07368421052631578,
          "p": 0.2916666666666667,
          "f": 0.11764705560341793
        },
        "rouge-2": {
          "r": 0.016,
          "p": 0.08333333333333333,
          "f": 0.026845634881311926
        },
        "rouge-l": {
          "r": 0.07368421052631578,
          "p": 0.2916666666666667,
          "f": 0.11764705560341793
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.00209v1",
      "true_abstract": "We explore the influence of framing on decision-making, where some products\nare framed (e.g., displayed, recommended, endorsed, or labeled). We introduce a\nnovel choice function that captures observed variations in framed alternatives.\nBuilding on this, we conduct a comprehensive revealed preference analysis,\nemploying the concept of frame-dependent utility using both deterministic and\nprobabilistic data. We demonstrate that simple and intuitive behavioral\nprinciples characterize our frame-dependent random utility model (FRUM), which\noffers testable conditions even with limited data. Finally, we introduce a\nparametric model to increase the tractability of FRUM. We also discuss how to\nrecover the choice types in our framework.",
      "generated_abstract": "We consider the problem of minimizing the total cost of a system of\nnetworked agents. We first study the case where the agents are simple\ndiscrete-time Markov chains, and show that the cost can be approximated by\nexpectations over the empirical process of agent states. Next, we extend this\nresult to the case of a continuous-time Markov chain, where the cost can be\napproximated by integrals over the empirical process of agent states. We show\nthat this approximation is asymptotically optimal. Finally, we consider the\ncase where the agents are continuous-time Markov processes, and show that\napproximating the cost by integrals over the empirical process of agent\nstates is optimal in the sense that the cost cannot be further reduced.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16455696202531644,
          "p": 0.23636363636363636,
          "f": 0.19402984590666086
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.011904761904761904,
          "f": 0.010810805853033682
        },
        "rouge-l": {
          "r": 0.16455696202531644,
          "p": 0.23636363636363636,
          "f": 0.19402984590666086
        }
      }
    },
    {
      "paper_id": "math.CO.math/GR/2503.07299v1",
      "true_abstract": "Let $\\varphi:V\\times V\\to W$ be a bilinear map of finite vector spaces $V$\nand $W$ over a finite field $\\mathbb{F}_q$. We present asymptotic bounds on the\nnumber of isomorphism classes of bilinear maps under the natural action of\n$\\mathrm{GL}(V)$ and $\\mathrm{GL}(W)$, when $\\dim(V)$ and $\\dim(W)$ are\nlinearly related.\n  As motivations and applications of the results, we present almost tight upper\nbounds on the number of $p$-groups of Frattini class $2$ as first studied by\nHigman (Proc. Lond. Math. Soc., 1960). Such bounds lead to answers for some\nopen questions by Blackburn, Neumann, and Venkataraman (Cambridge Tracts in\nMathematics, 2007). Further applications include sampling matrix spaces with\nthe trivial automorphism group, and asymptotic bounds on the number of\nisomorphism classes of finite cube-zero commutative algebras.",
      "generated_abstract": "We study the regularity of the boundary trace of a linear Neumann problem\nfor the wave equation in a domain with a non-smooth boundary.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07865168539325842,
          "p": 0.3888888888888889,
          "f": 0.13084111869682946
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.043478260869565216,
          "f": 0.014925370290711167
        },
        "rouge-l": {
          "r": 0.06741573033707865,
          "p": 0.3333333333333333,
          "f": 0.11214952991178276
        }
      }
    },
    {
      "paper_id": "quant-ph.cs/IT/2503.09012v1",
      "true_abstract": "The thought experiment of Maxwell's demon highlights the effect of side\ninformation in thermodynamics. In this paper, we present an axiomatic treatment\nof a quantum Maxwell's demon, by introducing a resource-theoretic framework of\nquantum thermodynamics in the presence of quantum side information. Under\nminimal operational assumptions that capture the demon's behaviour, we derive\nthe one-shot work costs of preparing, as well as erasing, a thermodynamic\nsystem whose coupling with the demon's mind is described by a bipartite quantum\nstate. With trivial Hamiltonians, these work costs are precisely captured by\nthe smoothed conditional min- and max-entropies, respectively, thus providing\noperational interpretations for these one-shot information-theoretic quantities\nin microscopic thermodynamics. An immediate, information-theoretic implication\nof our results is an affirmative proof of the conjectured maximality of the\nconditional max-entropy among all axiomatically plausible conditional\nentropies, complementing the recently established minimality of the conditional\nmin-entropy. We then generalize our main results to the setting with nontrivial\nHamiltonians, wherein the work costs of preparation and erasure are captured by\na generalized type of mutual information. Finally, we present a macroscopic\nsecond law of thermodynamics in the presence of quantum side information, in\nterms of a conditional version of the Helmholtz free energy. Our results extend\nthe conceptual connection between thermodynamics and quantum information theory\nby refining the axiomatic common ground between the two theories and revealing\nfundamental insights of each theory in light of the other.",
      "generated_abstract": "The recent progress in quantum error-correcting codes has led to the\ndevelopment of quantum error-correction codes (QECCs) based on the\nAdams-Chakrabarti-Grover-Cabano-Bakkers (ACGB) construction. In this paper, we\npropose a QECC based on the ACGB construction that is more efficient than the\nACGB-based QECCs proposed in Refs. [19, 20",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0962962962962963,
          "p": 0.3333333333333333,
          "f": 0.14942528387831955
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.1111111111111111,
          "f": 0.03921568336793563
        },
        "rouge-l": {
          "r": 0.08888888888888889,
          "p": 0.3076923076923077,
          "f": 0.13793103100475634
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.02108v1",
      "true_abstract": "Generalized Bayesian Inference (GBI) provides a flexible framework for\nupdating prior distributions using various loss functions instead of the\ntraditional likelihoods, thereby enhancing the model robustness to model\nmisspecification. However, GBI often suffers the problem associated with\nintractable likelihoods. Kernelized Stein Discrepancy (KSD), as utilized in a\nrecent study, addresses this challenge by relying only on the gradient of the\nlog-likelihood. Despite this innovation, KSD-Bayes suffers from critical\npathologies, including insensitivity to well-separated modes in multimodal\nposteriors. To address this limitation, we propose a weighted KSD method that\nretains computational efficiency while effectively capturing multimodal\nstructures. Our method improves the GBI framework for handling intractable\nmultimodal posteriors while maintaining key theoretical properties such as\nposterior consistency and asymptotic normality. Experimental results\ndemonstrate that our method substantially improves mode sensitivity compared to\nstandard KSD-Bayes, while retaining robust performance in unimodal settings and\nin the presence of outliers.",
      "generated_abstract": "In this paper, we develop a novel statistical framework for analyzing the\nnon-convex and non-smooth regularized least squares (LS) problem. The proposed\nframework is based on a novel concentration inequality, which is proven to hold\nunder mild conditions. Moreover, we provide a theoretical guarantee on the\nasymptotic minimax optimal convergence rate of the proposed estimator. To\nexploit the theoretical results, we propose a novel algorithm for solving the\nnon-convex and non-smooth regularized LS problem. Numerical experiments are\nconducted to verify the effectiveness of the proposed algorithm in both\nsmooth and non-smooth cases.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.25806451612903225,
          "f": 0.1839080413898799
        },
        "rouge-2": {
          "r": 0.035211267605633804,
          "p": 0.06097560975609756,
          "f": 0.044642852501594874
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.25806451612903225,
          "f": 0.1839080413898799
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/LO/2503.06284v1",
      "true_abstract": "Isolation bugs, stemming especially from design-level defects, have been\nrepeatedly found in carefully designed and extensively tested production\ndatabases over decades. In parallel, various frameworks for modeling database\ntransactions and reasoning about their isolation guarantees have been\ndeveloped. What is missing however is a mathematically rigorous and systematic\nframework with tool support for formally verifying a wide range of such\nguarantees for all possible system behaviors. We present the first such\nframework, VerIso, developed within the theorem prover Isabelle/HOL. To\nshowcase its use in verification, we model the strict two-phase locking\nconcurrency control protocol and verify that it provides strict serializability\nisolation guarantee. Moreover, we show how VerIso helps identify isolation bugs\nduring protocol design. We derive new counterexamples for the TAPIR protocol\nfrom failed attempts to prove its claimed strict serializability. In\nparticular, we show that it violates a much weaker isolation level, namely,\natomic visibility.",
      "generated_abstract": "This paper introduces a new paradigm for query-level optimization (QLO) in\nNetezza. We present a novel optimization technique that leverages\ndeclarative-expressive queries to directly optimize query execution plans,\ninstead of relying on the traditional query-level optimization (QLO) process\nthat employs heuristics. Our technique is designed to optimize execution plans\nbased on the results of the query, rather than relying on heuristics that\nsuggest plausible plans. This approach enables QLO to adapt to dynamically\nchanging execution plans, improving the accuracy and efficiency of QLO.\n  We evaluate our approach in a real-world dataset of 35 million rows,\ndemonstrating that our approach significantly outperforms traditional QLO\napproaches in terms of query efficiency and cost reduction. We also demonstrate\nthat our approach outperforms other query-level optimization techniques in terms",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11607142857142858,
          "p": 0.17567567567567569,
          "f": 0.13978494144525394
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.008771929824561403,
          "f": 0.007812495059817576
        },
        "rouge-l": {
          "r": 0.11607142857142858,
          "p": 0.17567567567567569,
          "f": 0.13978494144525394
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.11183v3",
      "true_abstract": "An agent engages in sequential search. He does not directly observe the\nquality of the goods he samples, but he can purchase signals designed by profit\nmaximizing principal(s). We formulate the principal-agent relationship as a\nrepeated contracting problem within a stopping game and characterize the set of\nequilibrium payoffs. We show that when the agent's search cost falls below a\ngiven threshold, competition does not impact how much surplus is generated in\nequilibrium nor how the surplus is divided. In contrast, competition benefits\nthe agent at the expense of total surplus when the search cost exceeds that\nthreshold. Our results challenge the view that monopoly decreases market\nefficiency, and moreover, suggest that it generates the highest value of\ninformation for the agent.",
      "generated_abstract": "This paper studies the design of a simple auction that maximizes the\nequilibrium outcome and minimizes the expected auction cost. The equilibrium\nprice is the optimal price of the auction and is obtained by maximizing the\nexpected auction cost. This cost is a function of the number of bids, the\nprice and the auction length. We show that this cost is convex in the number\nof bids, and that the optimal auction length is proportional to the number of\nbids. We further analyze the optimal number of bids and the optimal auction\nlength. Finally, we study the effect of the auction length on the equilibrium\nprice and the expected auction cost. We show that for moderate values of the\nauction length, the equilibrium price is close to the optimal price, and for\nlarge values of the auction length, the equilibrium price converges to the\noptimal price, as expected.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.28846153846153844,
          "f": 0.21897809747988714
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.030612244897959183,
          "f": 0.02803737821294524
        },
        "rouge-l": {
          "r": 0.16470588235294117,
          "p": 0.2692307692307692,
          "f": 0.20437955733390173
        }
      }
    },
    {
      "paper_id": "math.ST.q-fin/MF/2412.06343v1",
      "true_abstract": "We propose analytically tractable SDE models for correlation in financial\nmarkets. We study diffusions on the circle, namely the Brownian motion on the\ncircle and the von Mises process, and consider these as models for correlation.\nThe von Mises process was proposed in Kent (1975) as a probabilistic\njustification for the von Mises distribution which is widely used in Circular\nstatistics. The transition density of the von Mises process has been unknown,\nwe identify an approximate analytic transition density for the von Mises\nprocess. We discuss the estimation of these diffusion models and a stochastic\ncorrelation model in finance. We illustrate the application of the proposed\nmodel on real-data of equity-currency pairs.",
      "generated_abstract": "We study the stochastic differential equation (SDE) driven by a multivariate\n$t$-dependent Brownian motion (BM) with continuous-time jumps. We consider\ntwo different scenarios: (i) the jump-diffusion component is independent of\nthe drift component and (ii) the jump-diffusion component is correlated with\nthe drift component. Under mild regularity assumptions on the jump-diffusion\ncomponent, we show that the solution to the SDE can be written as the solution\nto a stochastic differential equation (SDE) driven by a multivariate $t$-dependent\nBM with correlated jumps, which is a generalization of the well-known\ncorrelated-jumps SDE of Girsanov. We also study the case of correlated jump\ndiffusion with correlated jump density and derive the corresponding\ncorrelated-jumps SDE. Our results provide a unified",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.27692307692307694,
          "p": 0.2727272727272727,
          "f": 0.27480915530563493
        },
        "rouge-2": {
          "r": 0.0625,
          "p": 0.06382978723404255,
          "f": 0.06315788973739653
        },
        "rouge-l": {
          "r": 0.26153846153846155,
          "p": 0.25757575757575757,
          "f": 0.2595419797331159
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2503.09604v1",
      "true_abstract": "The article studies the extension of the internal spaces of fermion and boson\nsecond quantized fields, described by the superposition of odd (for fermions)\nand even (for bosons) products of the operators $\\gamma^ {a}$, to strings and\nodd dimensional spaces.\\\\ For any symmetry $SO(d-1,1)$ of the internal spaces,\nit is the number of fermion fields (they appear in families and have their\nHermitian conjugated partners in a separate group) equal to the number of boson\nfields (they appear in two orthogonal groups), manifesting a kind of\nsupersymmetry, which differs from the usual supersymmetry.\\\\ The article\nsearches for the supersymmetry arising from extending the ``basis vectors'' of\nsecond quantized fermion and boson fields described in $d=2(2n+1)$ (in\nparticular $d=(13+1)$) either to strings or to odd-dimensional spaces\n($d=2(2n+1)+1$).",
      "generated_abstract": "We present a novel method to study the evolution of a black hole interior\nbased on a semi-classical approach. We apply the method to a static, spherically\nsymmetric spacetime and consider the evolution of a black hole as the spacetime\npasses through a thin shell. The spacetime is perturbed by a small perturbation\nof the shell's location, and we study the effects of this perturbation on the\nblack hole interior. We find that the perturbed spacetime can be described by\na modified Kerr black hole, and that the effects of the perturbation depend on\nthe parameter $\\alpha$. We also find that the black hole interior evolves\naccording to the general relativistic formalism, with the gravitational\npotential decreasing in the interior. However, we also show that, for\nsufficiently large values of $\\alpha$, the gravitational potential can be\napproximated by a simple exponential function, which can",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15584415584415584,
          "p": 0.16,
          "f": 0.1578947318429711
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.025210084033613446,
          "f": 0.0259740209786183
        },
        "rouge-l": {
          "r": 0.15584415584415584,
          "p": 0.16,
          "f": 0.1578947318429711
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.17883v1",
      "true_abstract": "The expected utility theorem of von Neumann and Morgenstern (1947) has been a\nmilestone in economics, describing rational behavior by two axioms on a weak\npreference on lotteries on a finite set of outcomes: the Independence Axiom and\nthe Continuity Axiom. For a weak preference fulfilling the Independence Axiom,\nI prove that continuity is equivalent to the existence of a set indifferent\nlotteries spanning a hyperplane.",
      "generated_abstract": "We introduce a novel method for computing the expected utility of an agent\nin an uncertain environment. Our approach is based on the concept of\n\\emph{infinite-horizon regret} and employs a recursive Bayesian updating rule\nto derive a closed-form expression for the expected utility of the agent.\nSpecifically, the agent's utility function can be expressed as a weighted sum\nof the agent's expected payoffs from each possible action, each weighted by a\nBayes-optimal probability distribution. In our numerical experiments, we show\nthat our methodology can produce accurate estimates of the expected utility\neven when the agent's payoff functions are non-convex.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2553191489361702,
          "p": 0.17391304347826086,
          "f": 0.20689654690398349
        },
        "rouge-2": {
          "r": 0.01639344262295082,
          "p": 0.011235955056179775,
          "f": 0.013333328507557302
        },
        "rouge-l": {
          "r": 0.19148936170212766,
          "p": 0.13043478260869565,
          "f": 0.15517240897294904
        }
      }
    },
    {
      "paper_id": "cs.CL.eess/AS/2502.17284v1",
      "true_abstract": "We test and study the variation in speech recognition of fine-tuned versions\nof the Whisper model on child, elderly and non-native Dutch speech from the\nJASMIN-CGN corpus. Our primary goal is to evaluate how speakers' age and\nlinguistic background influence Whisper's performance. Whisper achieves varying\nWord Error Rates (WER) when fine-tuned on subpopulations of specific ages and\nlinguistic backgrounds. Fine-tuned performance is remarkably better than\nzero-shot performance, achieving a relative reduction in WER of 81% for native\nchildren, 72% for non-native children, 67% for non-native adults, and 65% for\nnative elderly people. Our findings underscore the importance of training\nspeech recognition models like Whisper on underrepresented subpopulations such\nas children, the elderly, and non-native speakers.",
      "generated_abstract": "In recent years, the use of large language models (LLMs) in speech recognition\nhas exploded. However, LLMs struggle with challenging tasks such as\nnatural-language understanding (NLU), speech-to-text (S2T), and timed-response\n(TR). Existing methods either focus on specific LLMs or are limited to a\nspecific task. This paper proposes a unified framework for NLU, S2T, and TR\nwithin the framework of multi-modal dialogue systems. We introduce a\nmulti-modal dialogue system (MDS) framework, which consists of a LLM-based NLU\nsubsystem, a S2T subsystem, and a TR subsystem. The MDS framework integrates\nmultiple LLMs, each specialized in a specific task, into a unified system. This\napproach allows us to efficiently leverage the capabilities of multiple LLM",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.17857142857142858,
          "f": 0.18518518019204405
        },
        "rouge-2": {
          "r": 0.027522935779816515,
          "p": 0.027522935779816515,
          "f": 0.027522930779817423
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.16666666666666666,
          "f": 0.17283950117969835
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/OT/2404.08738v2",
      "true_abstract": "Air quality is a critical component of environmental health. Monitoring and\nanalysis of particulate matter with a diameter of 2.5 micrometers or smaller\n(PM2.5) plays a pivotal role in understanding air quality changes. This study\nfocuses on the application of a new bandpass bootstrap approach, termed the\nVariable Bandpass Periodic Block Bootstrap (VBPBB), for analyzing time series\ndata which provides modeled predictions of daily mean PM2.5 concentrations over\n16 years in Manhattan, New York, the United States. The VBPBB can be used to\nexplore periodically correlated (PC) principal components for this daily mean\nPM2.5 dataset. This method uses bandpass filters to isolate distinct PC\ncomponents from datasets, removing unwanted interference including noise, and\nbootstraps the PC components. This preserves the PC structure and permits a\nbetter understanding of the periodic characteristics of time series data. The\nresults of the VBPBB are compared against outcomes from alternative block\nbootstrapping techniques. The findings of this research indicate potential\ntrends of elevated PM2.5 levels, providing evidence of significant semi-annual\nand weekly patterns missed by other methods.",
      "generated_abstract": "We examine the use of a simple, nonparametric method for testing whether a\nnon-stationary, non-Gaussian time series has a mean-reverting trend. We show\nthat the method can be applied in a wide variety of settings. The main\nchallenge is the need to identify the trend direction. We propose two\nalternative tests, both based on the central limit theorem, and show that they\nare equivalent. We provide a simulation study to illustrate the methods'\nperformance. We illustrate their use in real data from the energy sector and\nfrom the stock market.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.3125,
          "f": 0.20833332888888897
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.04597701149425287,
          "f": 0.03137254452410675
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.25,
          "f": 0.16666666222222234
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09062v1",
      "true_abstract": "Knowledge dissemination in educational settings is profoundly influenced by\nthe curse of knowledge, a cognitive bias that causes experts to underestimate\nthe challenges faced by learners due to their own in-depth understanding of the\nsubject. This bias can hinder effective knowledge transfer and pedagogical\neffectiveness, and may be exacerbated by inadequate instructor-student\ncommunication. To encourage more effective feedback and promote empathy, we\nintroduce TSConnect, a bias-aware, adaptable interactive MOOC (Massive Open\nOnline Course) learning system, informed by a need-finding survey involving 129\nstudents and 6 instructors. TSConnect integrates instructors, students, and\nArtificial Intelligence (AI) into a cohesive platform, facilitating diverse and\ntargeted communication channels while addressing previously overlooked\ninformation needs. A notable feature is its dynamic knowledge graph, which\nenhances learning support and fosters a more interconnected educational\nexperience. We conducted a between-subjects user study with 30 students\ncomparing TSConnect to a baseline system. Results indicate that TSConnect\nsignificantly encourages students to provide more feedback to instructors.\nAdditionally, interviews with 4 instructors reveal insights into how they\ninterpret and respond to this feedback, potentially leading to improvements in\nteaching strategies and the development of broader pedagogical skills.",
      "generated_abstract": "The emergence of large language models (LLMs) has led to a growing\nvalue in understanding their internal mechanisms and how they are trained.\nHowever, these models can also be leveraged for malicious purposes, such as\nmalware generation. Existing malware detection methods often rely on a few\nrelevant features, which can lead to over-fitting and poor generalization. In\nthis paper, we propose a novel LLM-based malware detection model that integrates\nthe advantages of large language models with the strengths of deep learning.\nOur approach leverages the latent code representation of LLMs, which captures\nthe internal structure of the model. This enables the model to effectively\nlearn complex relationships among malicious patterns and the underlying\nrepresentations of malicious code. We also propose a novel feature\nextraction technique that leverages the latent code representation to extract\nrelevant features from the input malicious code",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15942028985507245,
          "p": 0.23655913978494625,
          "f": 0.1904761856659359
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.031496062992125984,
          "f": 0.025559100608969054
        },
        "rouge-l": {
          "r": 0.12318840579710146,
          "p": 0.1827956989247312,
          "f": 0.14718614237589267
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2409.04772v1",
      "true_abstract": "Cell motility is fundamental to many biological processes, and cells exhibit\na variety of migration patterns. Many motile cell types follow a universal law\nthat connects their speed and persistency, a property that can originate from\nthe intracellular transport of polarity cues due to the global actin retrograde\nflow. This mechanism was termed the ``Universal Coupling between cell Speed and\nPersistency\"(UCSP). Here we implemented a simplified version of the UCSP\nmechanism in a coarse-grained ``minimal-cell\" model, which is composed of a\nthree-dimensional vesicle that contains curved active proteins. This model\nspontaneously forms a lamellipodia-like motile cell shape, which is however\nsensitive and can depolarize into a non-motile form due to random fluctuations\nor when interacting with external obstacles. The UCSP implementation introduces\nlong-range inhibition, which stabilizes the motile phenotype. This allows our\nmodel to describe the robust polarity observed in cells and explain a large\nvariety of cellular dynamics, such as the relation between cell speed and\naspect ratio, cell-barrier scattering, and cellular oscillations in different\ntypes of geometric confinements.",
      "generated_abstract": "We study the evolution of two populations that compete for a common resource,\nand that have distinct genetic architectures. The populations are described by\ntheir respective genotypes, $G_1$ and $G_2$, and their fitnesses, $f_1$ and\n$f_2$. We assume that the fitnesses of the two populations are positively\ncorrelated, so that they increase as $G_1$ and $G_2$ increase. We also assume\nthat the populations exhibit different levels of genetic variation, as indicated\nby their allele frequencies. We show that, under this scenario, the evolution\nof the fitness of one of the populations is determined by the evolution of the\nfitness of the other, and that the fitness of both populations increases\nsimultaneously as the fitnesses of the two populations increase. In particular,\nthe evolution of the fitness of one population",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09565217391304348,
          "p": 0.1746031746031746,
          "f": 0.12359550104469148
        },
        "rouge-2": {
          "r": 0.012269938650306749,
          "p": 0.020833333333333332,
          "f": 0.015444010778612339
        },
        "rouge-l": {
          "r": 0.09565217391304348,
          "p": 0.1746031746031746,
          "f": 0.12359550104469148
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.00934v2",
      "true_abstract": "Background: Global viral threats underscore the need for effective genomic\nsurveillance, but high costs and uneven resource distribution hamper its\nimplementation. Targeting surveillance to international travelers in major\ntravel hubs may offer a more efficient strategy for the early detection of\nSARS-CoV-2 variants.\n  Methods: We developed and calibrated a multiple-strain metapopulation model\nof global SARS-CoV-2 transmission using extensive epidemiological,\nphylogenetic, and high-resolution air travel data. We then compared baseline\nsurveillance with various resource-allocation approaches that prioritize\ntravelers, focusing on Omicron BA.1/BA.2 retrospectively and on hypothetical\nfuture variants under different emergence, transmission and vaccine\neffectiveness scenarios.\n  Findings: Focusing existing surveillance resources on travelers at key global\nhubs significantly shortened detection delays without increasing total\nsurveillance efforts. In retrospective analyses of Omicron BA.1/BA.2,\ntraveler-targeted approaches consistently outperformed baseline strategies,\neven when overall resources were reduced. Simulations indicate that focusing\nsurveillance on key travel hubs outperform baseline practices in detecting\nfuture variants, across different possible origins, even with reduced\nresources. This approach also remains effective in future pandemic scenarios\nwith varying reproductive numbers and vaccine effectiveness.\n  Interpretation: These findings provide a quantitative, cost-effective\nframework for strengthening global genomic surveillance. By reallocating\nresources toward international travelers in select travel hubs, early detection\nof emerging variants can be enhanced, informing rapid public health\ninterventions and bolstering preparedness for future pandemics.",
      "generated_abstract": "The advent of next-generation sequencing (NGS) has revolutionized our\nunderstanding of the human genome, allowing for the identification of variants\nof unknown significance (VUS) and other variants that are not expected to have\nany clinical impact. In order to provide a complete and accurate view of the\ngenetic background of the population, it is crucial to have a reference genome\nfor the study of VUS. In the case of the European continent, the European\nGenome-phenome Archive (EGA) is the reference genome for all variants of\nunknown significance detected in the European Union. However, it is not\nsufficient to simply provide a reference genome. To understand the genetic\nbackground of a population, a comprehensive analysis of the genetic variation\nin the population is necessary. In this paper, we propose a novel methodology\nto analyze the genetic variation of a population in the context of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07333333333333333,
          "p": 0.14473684210526316,
          "f": 0.09734512827942694
        },
        "rouge-2": {
          "r": 0.009569377990430622,
          "p": 0.017241379310344827,
          "f": 0.012307687717114137
        },
        "rouge-l": {
          "r": 0.07333333333333333,
          "p": 0.14473684210526316,
          "f": 0.09734512827942694
        }
      }
    },
    {
      "paper_id": "cs.CL.q-bio/NC/2503.04848v2",
      "true_abstract": "Human language and logic abilities are computationally quantified within the\nwell-studied grammar-automata hierarchy. We identify three hierarchical tiers\nand two corresponding transitions and show their correspondence to specific\nabilities in transformer-based language models (LMs). These emergent abilities\nhave often been described in terms of scaling; we show that it is the\ntransition between tiers, rather than scaled size itself, that determines a\nsystem's capabilities. Specifically, humans effortlessly process language yet\nrequire critical training to perform arithmetic or logical reasoning tasks; and\nLMs possess language abilities absent from predecessor systems, yet still\nstruggle with logical processing. We submit a novel benchmark of computational\npower, provide empirical evaluations of humans and fifteen LMs, and, most\nsignificantly, provide a theoretically grounded framework to promote careful\nthinking about these crucial topics. The resulting principled analyses provide\nexplanatory accounts of the abilities and shortfalls of LMs, and suggest\nactionable insights into the expansion of their logic abilities.",
      "generated_abstract": "The emergence of deep learning in natural language processing (NLP) has\nproved instrumental in advancing knowledge in a wide range of fields. In\nspeech recognition (SR), advancements in architectures and large language models\n(LLMs) have enabled the recognition of complex speech in noisy environments.\nHowever, the reliance on LLMs has raised concerns about their potential to\nexpose vulnerabilities in data collection and processing. To address this\nissue, we introduce the Speech Data Privacy (SDP) framework, which utilizes a\npre-trained LLM as a robust data augmentation mechanism for speech data\nprivacy protection. This framework introduces a privacy loss function that\noptimizes the augmented speech data. Additionally, the framework incorporates a\ndata distribution shifting strategy to ensure that the augmented speech data\nare indistinguishable from the original data. We demonstrate the effectiveness\nof",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.20652173913043478,
          "f": 0.18446601447450292
        },
        "rouge-2": {
          "r": 0.006666666666666667,
          "p": 0.007936507936507936,
          "f": 0.007246371849404784
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.1956521739130435,
          "f": 0.17475727661042526
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.05905v1",
      "true_abstract": "Recent developments in sequential experimental design look to construct a\npolicy that can efficiently navigate the design space, in a way that maximises\nthe expected information gain. Whilst there is work on achieving tractable\npolicies for experimental design problems, there is significantly less work on\nobtaining policies that are able to generalise well - i.e. able to give good\nperformance despite a change in the underlying statistical properties of the\nexperiments. Conducting experiments sequentially has recently brought about the\nuse of reinforcement learning, where an agent is trained to navigate the design\nspace to select the most informative designs for experimentation. However,\nthere is still a lack of understanding about the benefits and drawbacks of\nusing certain reinforcement learning algorithms to train these agents. In our\nwork, we investigate several reinforcement learning algorithms and their\nefficacy in producing agents that take maximally informative design decisions\nin sequential experimental design scenarios. We find that agent performance is\nimpacted depending on the algorithm used for training, and that particular\nalgorithms, using dropout or ensemble approaches, empirically showcase\nattractive generalisation properties.",
      "generated_abstract": "Recent advances in the field of generative AI have inspired a surge of\nresearch on generating adversarial samples with high quality, which can be\nconsidered as a form of adversarial attack. Generative adversarial networks\n(GANs) have been widely used for generating adversarial samples. However, existing\nGAN models have difficulty in generating samples with high quality. To address\nthis issue, we propose an adversarial GAN model with a self-supervised\nrepresentation learning module. Our method can generate high-quality adversarial\nsamples while preserving the original semantics. The proposed method has\nsignificant advantages over the existing GAN models in generating high-quality\nadversarial samples. Our method can generate high-quality adversarial samples\nwith different levels of perturbation while preserving the original semantics.\nFurthermore, we design a self-supervised representation learning module to\ncapture the semantic information of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14782608695652175,
          "p": 0.22666666666666666,
          "f": 0.1789473636426594
        },
        "rouge-2": {
          "r": 0.012121212121212121,
          "p": 0.019230769230769232,
          "f": 0.01486988373295158
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.22666666666666666,
          "f": 0.1789473636426594
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.physics/geo-ph/2503.04227v1",
      "true_abstract": "We present the first extensive analysis of K/Ka-band ranging post-fit\nresiduals of an official Level-2 product, characterised as Line-of-Sight\nGravity Differences (LGD), which exhibit and showcase interesting sub-monthly\ngeophysical signals. These residuals, provided by CSR, were derived from the\ndifference between spherical harmonic coefficient least-squares fits and\nreduced Level-1B range-rate observations. We classified the geophysical signals\ninto four distinct categories: oceanic, meteorological, hydrological, and solid\nEarth, focusing primarily on the first three categories in this study. In our\nexamination of oceanic processes, we identified notable mass anomalies in the\nArgentine basin, specifically within the Zapiola Rise, where persistent\nremnants of the rotating dipole-like modes are evident in the LGD post-fit\nresiduals. Our analysis extended to the Gulf of Carpentaria and Australia\nduring the 2013 Oswald cyclone, revealing significant LGD residual anomalies\nthat correlate with cyclone tracking and precipitation data. Additionally, we\ninvestigated the monsoon seasons in Bangladesh, particularly from June to\nSeptember 2007, where we observed peaks in sub-monthly variability. These\nfindings were further validated by demonstrating high spatial and temporal\ncorrelations between gridded LGD residuals and ITSG-Grace2018 daily solutions.\nGiven that these anomalies are associated with significant mass change\nphenomena, it is essential to integrate the post-fit residuals into a\nhigh-frequency mass change framework, with the purpose of providing enhanced\nspatial resolution compared to conventional Kalman-filtered methods.",
      "generated_abstract": "The present study proposes a new method for the analysis of the groundwater\nground-water-water interactions in the geological subsurface using the\nhigh-resolution magnetotelluric (HRMT) data. The method is based on the\napplication of the wavelet transform to the HRMT data to reduce the spatial\nvariability and enhance the sensitivity to the subsurface structures. The\nanalysis of the HRMT data using the proposed method was carried out on the\nNorth-South Tectonic Zone of Iran, which is characterized by complex\nsubsurface structures, including large-scale fractures, faults, and\ninversion-related structures. The proposed method was validated using\nsimulation data, and the results were compared with the results obtained using\nthe conventional approach. The results of the proposed method showed a\nsignificant reduction in the uncertainty of the water-water contact depth\nwith",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14193548387096774,
          "p": 0.3055555555555556,
          "f": 0.19383259478740136
        },
        "rouge-2": {
          "r": 0.02843601895734597,
          "p": 0.05555555555555555,
          "f": 0.037617550380205124
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.2777777777777778,
          "f": 0.17621144941295203
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/hist-ph/2503.00966v1",
      "true_abstract": "The Frauchiger-Renner argument purports to show that the standard framework\nof quantum mechanics yields a contradiction when used to reason about systems\ncontaining agents who are themselves using quantum mechanics to perform\ndeductions. This has been framed as an obstacle to taking quantum mechanics to\nbe a complete theory. I formalize the argument in two closely related ways and\nelucidate the source of the paradox, clarifying the flaw in the original\nargument.",
      "generated_abstract": "We present a novel approach to the study of quantum many-body systems with\neffective interactions derived from a non-relativistic effective field theory.\nThis approach provides a general, unified framework for the study of quantum\nmany-body systems with non-relativistic interactions in the framework of\nHamiltonian formulations, and we apply it to the study of the interactions\nbetween qubits in a quantum bit error-correcting code. We demonstrate that this\napproach can be applied to various quantum many-body systems, including\nquantum circuits, quantum simulators, and quantum machines, and we present\ncomputational results for quantum codes based on a quantum bit error-correcting\ncode.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24074074074074073,
          "p": 0.23636363636363636,
          "f": 0.23853210509216405
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.04819277108433735,
          "f": 0.052980127499671535
        },
        "rouge-l": {
          "r": 0.2222222222222222,
          "p": 0.21818181818181817,
          "f": 0.22018348123895307
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/RO/2503.10630v1",
      "true_abstract": "In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.",
      "generated_abstract": "Deep neural networks (DNNs) are ubiquitous in modern robotics, enabling\nefficient and robust navigation and manipulation. However, their inherent\nlinearity poses challenges in high-dimensional, nonlinear environments,\nespecially when the robot is in a complex configuration. To address this, we\npropose a novel approach, Robust Diffusion-based Neural Networks (RobDNNs),\nwhich leverages diffusion models for robust and scalable learning in\nnonlinear environments. RobDNNs consists of two key components: (1) a\nhigh-capacity neural network, and (2) a diffusion model that synthesizes\nnovel trajectories by blending sampled latent variables. To address the\ndifficulty of training large neural networks, we introduce a novel method,\nDiffusion-Guided Inverse Dynamics, which employs an inverse dynamics network\nto generate novel trajectories",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1292517006802721,
          "p": 0.21839080459770116,
          "f": 0.16239315772189367
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.027777777777777776,
          "f": 0.018867920042720893
        },
        "rouge-l": {
          "r": 0.10884353741496598,
          "p": 0.1839080459770115,
          "f": 0.136752132080868
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/CB/2407.07797v2",
      "true_abstract": "The mechanics of animal cells is strongly determined by stress fibers, which\nare contractile filament bundles that form dynamically in response to\nextracellular cues. Stress fibers allow the cell to adapt its mechanics to\nenvironmental conditions and to protect it from structural damage. While the\nphysical description of single stress fibers is well-developed, much less is\nknown about their spatial distribution on the level of whole cells. Here, we\ncombine a finite element method for one-dimensional fibers embedded in an\nelastic bulk medium with dynamical rules for stress fiber formation based on\ngenetic algorithms. We postulate that their main goal is to achieve minimal\nmechanical stress in the bulk material with as few fibers as possible. The\nfiber positions and configurations resulting from this optimization task alone\nare in good agreement with those found in experiments where cells in\n3D-scaffolds were mechanically strained at one attachment point. For optimized\nconfigurations, we find that stress fibers typically run through the cell in a\ndiagonal fashion, similar to reinforcement strategies used for composite\nmaterial.",
      "generated_abstract": "The genetic code is the chemical language used by cells to encode\ntheir proteins. A fundamental question is how the genetic code is specified.\nSeveral theoretical models have been proposed to explain how the genetic code\nis encoded in the genome. In this paper, we present a simple model that can\nexplain how the genetic code is specified. The model is based on the\nprobabilistic theory of Markov processes and is the first to combine the\nco-evolution of the genetic code and the protein code. Our model predicts the\nemergence of codon-specific mutations, which are common in natural populations\nand have been observed in several genes. The model also predicts that the\ncodon-to-protein mapping is not a simple mapping between the codon and the\nprotein, but rather a continuous mapping. We also show that the probability of\na codon-to-protein mapping",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18032786885245902,
          "p": 0.2857142857142857,
          "f": 0.22110552289386645
        },
        "rouge-2": {
          "r": 0.023668639053254437,
          "p": 0.03389830508474576,
          "f": 0.02787455961781822
        },
        "rouge-l": {
          "r": 0.16393442622950818,
          "p": 0.2597402597402597,
          "f": 0.2010050203813036
        }
      }
    },
    {
      "paper_id": "quant-ph.eess/IV/2503.01916v1",
      "true_abstract": "In transportation cyber-physical systems (CPS), ensuring safety and\nreliability in real-time decision-making is essential for successfully\ndeploying autonomous vehicles and intelligent transportation networks. However,\nthese systems face significant challenges, such as computational complexity and\nthe ability to handle ambiguous inputs like shadows in complex environments.\nThis paper introduces a Quantum Deep Convolutional Neural Network (QDCNN)\ndesigned to enhance the safety and reliability of CPS in transportation by\nleveraging quantum algorithms. At the core of QDCNN is the UU{\\dag} method,\nwhich is utilized to improve shadow detection through a propagation algorithm\nthat trains the centroid value with preprocessing and postprocessing operations\nto classify shadow regions in images accurately. The proposed QDCNN is\nevaluated on three datasets on normal conditions and one road affected by rain\nto test its robustness. It outperforms existing methods in terms of\ncomputational efficiency, achieving a shadow detection time of just 0.0049352\nseconds, faster than classical algorithms like intensity-based thresholding\n(0.03 seconds), chromaticity-based shadow detection (1.47 seconds), and local\nbinary pattern techniques (2.05 seconds). This remarkable speed, superior\naccuracy, and noise resilience demonstrate the key factors for safe navigation\nin autonomous transportation in real-time. This research demonstrates the\npotential of quantum-enhanced models in addressing critical limitations of\nclassical methods, contributing to more dependable and robust autonomous\ntransportation systems within the CPS framework.",
      "generated_abstract": "We present a novel approach to the problem of quantum communication with\nhigher-dimensional entanglement, where a quantum communication channel is\nassumed to be a bipartite entanglement-assisted channel. This framework\nenables the encoding of entanglement with arbitrary number of parties,\nparticularly for communication tasks where a single-party channel is not\napplicable. We show that the quantum capacity of such a channel is upper\nbounded by the capacity of its entanglement-assisted counterpart. Furthermore,\nwe demonstrate that this bound is tight when the entanglement-assisted channel\nis a single-particle channel with a large Hilbert space, such as a\n$n$-qubit entanglement-assisted quantum error-correcting code. We also show\nthat the capacity of such a channel is always at least as large as the\ncapacity of a $2^{n}-1$ party entanglement",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1038961038961039,
          "p": 0.24615384615384617,
          "f": 0.14611871728696244
        },
        "rouge-2": {
          "r": 0.004739336492890996,
          "p": 0.009523809523809525,
          "f": 0.006329109486663903
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.2153846153846154,
          "f": 0.12785387710431406
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18328v1",
      "true_abstract": "Recent advances in Visual Anomaly Detection (VAD) have introduced\nsophisticated algorithms leveraging embeddings generated by pre-trained feature\nextractors. Inspired by these developments, we investigate the adaptation of\nsuch algorithms to the audio domain to address the problem of Audio Anomaly\nDetection (AAD). Unlike most existing AAD methods, which primarily classify\nanomalous samples, our approach introduces fine-grained temporal-frequency\nlocalization of anomalies within the spectrogram, significantly improving\nexplainability. This capability enables a more precise understanding of where\nand when anomalies occur, making the results more actionable for end users. We\nevaluate our approach on industrial and environmental benchmarks, demonstrating\nthe effectiveness of VAD techniques in detecting anomalies in audio signals.\nMoreover, they improve explainability by enabling localized anomaly\nidentification, making audio anomaly detection systems more interpretable and\npractical.",
      "generated_abstract": "This paper investigates the impact of training with a large-scale dataset on\nLow-Power Wireless Networks (LPWANs), particularly the Internet of Things (IoT).\nThe authors analyze the impact of training with different datasets on the\nperformance of the Layer-2 Transceiver Integrated Circuit (LTIC) in LPWANs. The\nperformance metrics analyzed include the received signal strength indicator\n(RSSI), the Signal-to-Interference-plus-Noise Ratio (SINR), the Signal to\nAir-Interference Ratio (SINAR), and the Signal to Noise Ratio (SNR). The\nperformance of the LTIC is evaluated using a 10-year dataset from 2008 to\n2018, with 200,000 samples per year, while the other metrics are evaluated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08333333333333333,
          "p": 0.12121212121212122,
          "f": 0.09876542727023344
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07291666666666667,
          "p": 0.10606060606060606,
          "f": 0.08641974825788779
        }
      }
    },
    {
      "paper_id": "physics.optics.eess/SP/2503.04402v1",
      "true_abstract": "Chaos lidars detect targets through the cross-correlation between the\nback-scattered chaos signal from the target and the local reference one. Chaos\nlidars have excellent anti-jamming and anti-interference capabilities, owing to\nthe random nature of chaotic oscillations. However, most chaos lidars operate\nin the near-infrared spectral regime, where the atmospheric attenuation is\nsignificant. Here we show a mid-infrared chaos lidar, which is suitable for\nlong-reach ranging and imaging applications within the low-loss transmission\nwindow of the atmosphere. The proof-of-concept mid-infrared chaos lidar\nutilizes an interband cascade laser with optical feedback as the laser chaos\nsource. Experimental results reveal that the chaos lidar achieves an accuracy\nbetter than 0.9 cm and a precision better than 0.3 cm for ranging distances up\nto 300 cm. In addition, it is found that a minimum signal-to-noise ratio of\nonly 1 dB is required to sustain both sub-cm accuracy and sub-cm precision.\nThis work paves the way for developing remote chaos lidar systems in the\nmid-infrared spectral regime.",
      "generated_abstract": "The development of new technologies and applications that require high\nspeed and low power consumption is increasingly challenging the need for\nquantum devices, especially single photon emitters. In this work, we propose a\nquantum dot (QD) based photonic integrated circuit (PIC) architecture for\nquantum computing and quantum sensing. The QDs are connected to an\nelectro-optic modulator to produce the necessary electrical excitation signals.\nWe show that a QD-based PIC can be designed for quantum computing and quantum\nsensing in a single chip, providing a highly integrated architecture that\noptimizes power consumption and signal transmission. We demonstrate the\ncapabilities of the proposed architecture by implementing a quantum circuit\nbased on the Pauli spin model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15454545454545454,
          "p": 0.22077922077922077,
          "f": 0.1818181769738913
        },
        "rouge-2": {
          "r": 0.01282051282051282,
          "p": 0.019230769230769232,
          "f": 0.015384610584616884
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.2077922077922078,
          "f": 0.1711229898081159
        }
      }
    },
    {
      "paper_id": "econ.EM.stat/CO/2502.04945v1",
      "true_abstract": "We study an alternative use of machine learning. We train neural nets to\nprovide the parameter estimate of a given (structural) econometric model, for\nexample, discrete choice or consumer search. Training examples consist of\ndatasets generated by the econometric model under a range of parameter values.\nThe neural net takes the moments of a dataset as input and tries to recognize\nthe parameter value underlying that dataset. Besides the point estimate, the\nneural net can also output statistical accuracy. This neural net estimator\n(NNE) tends to limited-information Bayesian posterior as the number of training\ndatasets increases. We apply NNE to a consumer search model. It gives more\naccurate estimates at lighter computational costs than the prevailing approach.\nNNE is also robust to redundant moment inputs. In general, NNE offers the most\nbenefits in applications where other estimation approaches require very heavy\nsimulation costs. We provide code at: https://nnehome.github.io.",
      "generated_abstract": "This paper investigates the optimal sampling strategy for a two-stage\nlongitudinal data analysis. Specifically, we consider a two-stage longitudinal\nstudy in which we observe the outcome of a treatment effect at the\npre-treatment stage and the treatment effect of the treatment at the post-\ntreatment stage. The study is repeated over time and the treatment is\nintroduced at each stage. We first consider the case where the treatment\nintroduced at the post-treatment stage is a binary treatment. We propose a\npost-treatment stage-dependent sampling strategy based on the median of the\noutcome at the post-treatment stage. Then, we extend our proposed strategy to\nthe case where the treatment is a continuous variable and propose a\npost-treatment stage-dependent sampling strategy based on the mean of the\noutcome at the post-treatment stage. Finally, we discuss the finite",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.2413793103448276,
          "f": 0.16867469424880255
        },
        "rouge-2": {
          "r": 0.006944444444444444,
          "p": 0.010309278350515464,
          "f": 0.008298750376890235
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.2413793103448276,
          "f": 0.16867469424880255
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2311.03292v4",
      "true_abstract": "Consensus on the definition of data science remains low despite the\nwidespread establishment of academic programs in the field and continued demand\nfor data scientists in industry. Definitions range from rebranded statistics to\ndata-driven science to the science of data to simply the application of machine\nlearning to so-called big data to solve real-world problems. Current efforts to\ntrace the history of the field in order to clarify its definition, such as\nDonoho's \"50 Years of Data Science\" (Donoho 2017), tend to focus on a short\nperiod when a small group of statisticians adopted the term in an unsuccessful\nattempt to rebrand their field in the face of the overshadowing effects of\ncomputational statistics and data mining. Using textual evidence from primary\nsources, this essay traces the history of the term to the 1960s, when it was\nfirst used by the US Air Force in a surprisingly similar way to its current\nusage, to 2012, the year that Harvard Business Review published the enormously\ninfluential article \"Data Scientist: The Sexiest Job of the 21st Century\"\n(Davenport and Patil 2012) and the American Statistical Association\nacknowledged a profound disconnect between statistics and data science\n(Rodriguez 2012). Among the themes that emerge from this review are (1) the\nlong-standing opposition between data analysts and data miners that continues\nto animate the field, (2) an established definition of the term as the practice\nof managing and processing scientific data that has been occluded by recent\nusage, and (3) the phenomenon of data impedance -- the disproportion between\nsurplus data, indexed by phrases like data deluge and big data, and the\nlimitations of computational machinery and methods to process them. This\npersistent condition appears to have motivated the use of the term and the\nfield itself since its beginnings.",
      "generated_abstract": "We present an unsupervised method for identifying and localizing\nsignatures of fractal structures in high-dimensional data. We propose a\ngeneralized fractal dimension method that operates on a graphical representation\nof the data, enabling the identification of fractal structures in the graph.\nThis representation is based on the number of cycles in a graph and the\naverage degree of the graph. Our approach is motivated by the idea of\nidentifying fractal structures in random data. To perform the identification,\nwe develop a method for computing the generalized fractal dimension on the\ngraph. We show that the generalized fractal dimension is a monotonic\nfunction of the graph Laplacian. We demonstrate the effectiveness of our\nmethod through simulations and an application to the problem of identifying\nfractal structures in the time series data of the Indian Monsoon. We use a\nhigh-dimensional time series data set",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09714285714285714,
          "p": 0.2537313432835821,
          "f": 0.14049586376442877
        },
        "rouge-2": {
          "r": 0.029850746268656716,
          "p": 0.06896551724137931,
          "f": 0.04166666245008723
        },
        "rouge-l": {
          "r": 0.08571428571428572,
          "p": 0.22388059701492538,
          "f": 0.12396693814459409
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.05504v2",
      "true_abstract": "This paper explores whether and to what extent ambiguous communication can be\nbeneficial to the sender in a persuasion problem, when the receiver (and\npossibly the sender) is ambiguity averse. We provide a concavification-like\ncharacterization of the sender's optimal ambiguous communication. The\ncharacterization highlights the necessity of using a collection of experiments\nthat form a splitting of an obedient (i.e., incentive compatible) experiment.\nSome experiments in the collection must be Pareto-ranked in the sense that both\nplayers agree on their payoff ranking. The existence of a binary such\nPareto-ranked splitting is necessary for ambiguous communication to benefit the\nsender, and, if an optimal Bayesian persuasion experiment can be split in this\nway, this is sufficient for an ambiguity-neutral sender as well as the receiver\nto benefit. Such gains are impossible when the receiver has only two actions.\nThe possibility of gains is substantially robust to (non-extreme) sender\nambiguity aversion.",
      "generated_abstract": "This paper proposes a novel mechanism for distributing a fixed amount of\nresources to agents who can choose to trade resources with each other,\nrepresenting a resource allocation mechanism. The resource allocation\nmechanism is shown to be optimal for agents who are risk-averse. The\nrepresentative agent is risk-neutral. The resource allocation mechanism is\nderived by minimizing a linear program that considers the expected social\nutility of agents. The optimal mechanism is a weighted average of the\naggregate-utility and the resource-utilitarian utilitarian mechanisms. The\nmechanism is shown to be optimal in terms of expected social utility. The\nmechanism can be implemented in a market economy through a mechanism that\nallows agents to trade resources with each other. The mechanism is also shown\nto be optimal in terms of expected social utility in a model with a constant\nprice of scarcity and where there is no",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.23529411764705882,
          "f": 0.1999999951125001
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.036036036036036036,
          "f": 0.03162054843475213
        },
        "rouge-l": {
          "r": 0.16304347826086957,
          "p": 0.22058823529411764,
          "f": 0.18749999511250015
        }
      }
    },
    {
      "paper_id": "quant-ph.econ/TH/2501.17189v3",
      "true_abstract": "Quantum games, like quantum algorithms, exploit quantum entanglement to\nestablish strong correlations between strategic player actions. This paper\nintroduces quantum game-theoretic models applied to trading and demonstrates\ntheir implementation on an ion-trap quantum computer. The results showcase a\nquantum advantage, previously known only theoretically, realized as\nhigher-paying market Nash equilibria. This advantage could help uncover alpha\nin trading strategies, defined as excess returns compared to established\nbenchmarks. These findings suggest that quantum computing could significantly\ninfluence the development of financial strategies.",
      "generated_abstract": "We propose a general framework for computing the optimal (or near-optimal)\nfundamental solution of a class of stochastic linear systems. This framework\nenables us to establish connections between the classical and quantum\ninformation-theoretic foundations of stochastic control. Our approach is\nmotivated by the study of Markov chains in continuous-time and the theory of\nstochastic optimal control in discrete-time.\n  We first provide a general description of a linear stochastic control problem\nin continuous-time. The optimal solution is then described as the unique\nsolution to a stochastic optimal control problem in discrete-time. We then\npropose a general framework for computing the optimal solution to the linear\nstochastic control problem in continuous-time. This framework is based on a\ngeneralization of the Lagrangian method. We derive the necessary and sufficient\nconditions for the existence and uniqueness of the optimal solution. We also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.21875,
          "f": 0.20895521889062166
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.009345794392523364,
          "f": 0.01069518227001282
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.21875,
          "f": 0.20895521889062166
        }
      }
    },
    {
      "paper_id": "cs.GL.cs/GL/2304.12898v1",
      "true_abstract": "The development of advanced generative chat models, such as ChatGPT, has\nraised questions about the potential consciousness of these tools and the\nextent of their general artificial intelligence. ChatGPT consistent avoidance\nof passing the test is here overcome by asking ChatGPT to apply the Turing test\nto itself. This explores the possibility of the model recognizing its own\nsentience. In its own eyes, it passes this test. ChatGPT's self-assessment\nmakes serious implications about our understanding of the Turing test and the\nnature of consciousness. This investigation concludes by considering the\nexistence of distinct types of consciousness and the possibility that the\nTuring test is only effective when applied between consciousnesses of the same\nkind. This study also raises intriguing questions about the nature of AI\nconsciousness and the validity of the Turing test as a means of verifying such\nconsciousness.",
      "generated_abstract": "The field of computational geometry has made significant progress in\ncomputing the geometric properties of large geometric objects. However,\ncomputing the geometric properties of smaller objects, like polygons and\npolyhedra, remains challenging. This paper addresses this gap by proposing a\nmethod for computing the geometric properties of a polygon with a single\ncomputation. We show that this method can compute the distance between a\npolygon and a line in $O(n^3)$ time, where $n$ is the number of vertices of the\npolygon. We also show that this method can compute the distance between a\npolygon and a plane in $O(n^3)$ time, where $n$ is the number of vertices of the\npolygon. Our methods are general and can be used for computing geometric\nproperties of any type of geometric object, including polyhedra and\npolytopes. We have implemented our methods in C++ and demonstrate their",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19047619047619047,
          "p": 0.21621621621621623,
          "f": 0.2025316405896492
        },
        "rouge-2": {
          "r": 0.00847457627118644,
          "p": 0.009523809523809525,
          "f": 0.008968604882465718
        },
        "rouge-l": {
          "r": 0.17857142857142858,
          "p": 0.20270270270270271,
          "f": 0.18987341274154798
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10019v1",
      "true_abstract": "Millinovae are a new class of transient supersoft X-ray sources with no clear\nsignature of mass ejection. They show similar triangle shapes of $V/I$ band\nlight curves with thousand times fainter peaks than typical classical novae.\nMaccarone et al. regarded the prototype millinova, ASASSN-16oh, as a dwarf nova\nand interpreted the supersoft X-rays to originate from an accretion belt on a\nwhite dwarf (WD). Kato et al. proposed a nova model induced by a high-rate\nmass-accretion during a dwarf nova outburst; the X-rays originate from the\nphotosphere of a hydrogen-burning hot WD whereas the $V/I$ band photons are\nfrom the irradiated accretion disk. Because each peak brightness differs\nlargely from millinova to millinova, we suspect that not all the millinova\ncandidates host a hydrogen burning WD. Based on the light curve analysis of the\nclassical nova KT Eri that has a bright disk, we find that the disk is more\nthan two magnitudes brighter when the disk is irradiated by the hydrogen\nburning WD than when not irradiated. We present the demarcation criterion for\nhydrogen burning to be $I_{\\rm q} - I_{\\rm max} > 2.2$, where $I_q$ and $I_{\\rm\nmax}$ are the $I$ magnitudes in quiescence and at maximum light, respectively.\nAmong many candidates, this requirement is satisfied with the two millinovae in\nwhich soft X-rays were detected.",
      "generated_abstract": "We present a 3D model of the protoplanetary disk around the young star\nHAT-P-12b using the SPH-DYNAMICS code, which accounts for non-ideal gas\ndynamics. We present a global view of the disk and its inner regions, as well as\na detailed view of the outer regions. We include a number of key physical\nprocesses that are not accounted for in previous 3D models of the disk around\nHAT-P-12b: (1) magnetic fields, (2) radiative cooling and heating, (3)\nmagnetohydrodynamic (MHD) turbulence, (4) the coupling between the MHD turbulence\nand the viscous disk, and (5) the effects of the disk on the star. Our\n3D-model shows that the disk is in an edge-on configuration",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1366906474820144,
          "p": 0.25675675675675674,
          "f": 0.17840375133417102
        },
        "rouge-2": {
          "r": 0.033816425120772944,
          "p": 0.07,
          "f": 0.04560260147057305
        },
        "rouge-l": {
          "r": 0.12949640287769784,
          "p": 0.24324324324324326,
          "f": 0.16901407997266868
        }
      }
    },
    {
      "paper_id": "math.OC.q-fin/MF/2412.11383v1",
      "true_abstract": "In this paper, we explore a new class of stochastic control problems\ncharacterized by specific control constraints. Specifically, the admissible\ncontrols are subject to the ratcheting constraint, meaning they must be\nnon-decreasing over time and are thus self-path-dependent. This type of\nproblems is common in various practical applications, such as optimal\nconsumption problems in financial engineering and optimal dividend payout\nproblems in actuarial science. Traditional stochastic control theory does not\nreadily apply to these problems due to their unique self-path-dependent control\nfeature. To tackle this challenge, we introduce a new class of\nHamilton-Jacobi-Bellman (HJB) equations, which are variational inequalities\nconcerning the derivative of a new spatial argument that represents the\nhistorical maximum control value. Under the standard Lipschitz continuity\ncondition, we demonstrate that the value functions for these\nself-path-dependent control problems are the unique solutions to their\ncorresponding HJB equations in the viscosity sense.",
      "generated_abstract": "This paper presents a framework for calibrating stochastic volatility models\nwith continuous-time diffusion processes, based on a dynamic programming\nalgorithm. The methodology is developed within a Markovian framework, and the\nresults are extended to an equivalent continuous-time setting. In the\nMarkovian setting, the calibration problem is reduced to a single-equation\nstochastic differential equation, which is solved using a numerical method. In\nthe continuous-time setting, a discrete-time Markovian model is derived, and\nthis is used to construct a numerical algorithm for solving the calibration\nproblem. The numerical results are provided for a variety of stochastic\nvolatility models, including the Black-Scholes model, the implied volatility\nmodel, and the Black-Scholes-Heston model. The results indicate that the\ncontinuous-time approach yields more accurate calibrated volatility\nestimates than the Markov",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14432989690721648,
          "p": 0.1891891891891892,
          "f": 0.16374268514893486
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.017391304347826087,
          "f": 0.01606425205722642
        },
        "rouge-l": {
          "r": 0.14432989690721648,
          "p": 0.1891891891891892,
          "f": 0.16374268514893486
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2411.10079v1",
      "true_abstract": "Abstract This paper proposes a novel approach to Bermudan swaption hedging by\napplying the deep hedging framework to address limitations of traditional\narbitrage-free methods. Conventional methods assume ideal conditions, such as\nzero transaction costs, perfect liquidity, and continuous-time hedging, which\noften differ from real market environments. This discrepancy can lead to\nresidual profit and loss (P&L), resulting in two primary issues. First,\nresidual P&L may prevent achieving the initial model price, especially with\nimproper parameter settings, potentially causing a negative P&L trend and\nsignificant financial impacts. Second, controlling the distribution of residual\nP&L to mitigate downside risk is challenging, as hedged positions may become\ncurve gamma-short, making them vulnerable to large interest rate movements. The\ndeep hedging approach enables flexible selection of convex risk measures and\nhedge strategies, allowing for improved residual P&L management. This study\nalso addresses challenges in applying the deep hedging approach to Bermudan\nswaptions, such as efficient arbitrage-free market scenario generation and\nmanaging early exercise conditions. Additionally, we introduce a unique \"Option\nSpread Hedge\" strategy, which allows for robust hedging and provides intuitive\ninterpretability. Numerical analysis results demonstrate the effectiveness of\nour approach.",
      "generated_abstract": "The paper examines the impact of credit risk on the returns and risk\nportfolios of two popular alternatives to traditional mutual funds:\ninvestment-grade credit funds and balanced funds. The research examines\ndiversification, correlations, and the effect of credit risk on performance\nmeasures, including Sharpe ratio, excess return, and risk-adjusted return. The\nfindings reveal that credit risk significantly affects portfolio performance,\nparticularly in balanced funds. Credit risk also reduces diversification and\ninflates excess return and risk-adjusted returns. The study also finds that\ncredit risk has a positive impact on excess return and risk-adjusted return,\nwhile it reduces diversification and increases the risk of the portfolio. The\nresults underscore the importance of managing credit risk in alternative\ninvestment portfolios, particularly for investors seeking diversification and\nhigher",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11594202898550725,
          "p": 0.23529411764705882,
          "f": 0.15533980140258283
        },
        "rouge-2": {
          "r": 0.005681818181818182,
          "p": 0.009259259259259259,
          "f": 0.0070422488077793895
        },
        "rouge-l": {
          "r": 0.11594202898550725,
          "p": 0.23529411764705882,
          "f": 0.15533980140258283
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/SC/2404.19158v1",
      "true_abstract": "The three-dimensional organization of chromatin is thought to play an\nimportant role in controlling gene expression. Specificity in expression is\nachieved through the interaction of transcription factors and other nuclear\nproteins with particular sequences of DNA. At unphysiological concentrations\nmany of these nuclear proteins can phase-separate in the absence of DNA, and it\nhas been hypothesized that, in vivo, the thermodynamic forces driving these\nphases help determine chromosomal organization. However it is unclear how DNA,\nitself a long polymer subject to configurational transitions, interacts with\nthree-dimensional protein phases. Here we show that a long compressible polymer\ncan be coupled to interacting protein mixtures, leading to a generalized\nprewetting transition where polymer collapse is coincident with a locally\nstabilized liquid droplet. We use lattice Monte-Carlo simulations and a\nmean-field theory to show that these phases can be stable even in regimes where\nboth polymer collapse and coexisting liquid phases are unstable in isolation,\nand that these new transitions can be either abrupt or continuous. For polymers\nwith internal linear structure we further show that changes in the\nconcentration of bulk components can lead to changes in three-dimensional\npolymer structure. In the nucleus there are many distinct proteins that\ninteract with many different regions of chromatin, potentially giving rise to\nmany different Prewet phases. The simple systems we consider here highlight\nchromatin's role as a lower-dimensional surface whose interactions with\nproteins are required for these novel phases.",
      "generated_abstract": "The dynamics of a single-cell organism is characterized by the stochastic\nCell-free synthesis of proteins from DNA is a promising approach for\ncell-free production of proteins and cell signaling molecules. However, the\ncurrent literature lacks a comprehensive discussion of the inherent stochasticity\nin cell-free protein synthesis. Here, we provide a quantitative analysis of the\nstochasticity in cell-free protein synthesis using a Bayesian framework. The\nanalysis is based on a detailed experimental study of the stochastic\ndynamics of cell-free protein synthesis from a single-cell organism, i.e., the\nE. coli bacteria. The results show that the synthesis of proteins from a\nsingle-cell organism is inherently stochastic, with the number of amino acid\nsequences produced per time unit following a Poisson process. Furthermore, we\ndemon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11188811188811189,
          "p": 0.21621621621621623,
          "f": 0.14746543329355066
        },
        "rouge-2": {
          "r": 0.0045045045045045045,
          "p": 0.009615384615384616,
          "f": 0.006134964980243207
        },
        "rouge-l": {
          "r": 0.1048951048951049,
          "p": 0.20270270270270271,
          "f": 0.1382488434317995
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.06846v1",
      "true_abstract": "Chytridiomycosis is a fungal disease, primarily caused by Batrachochytrium\ndendrobatidis, that poses a major threat to frog populations worldwide, driving\nat least 90 amphibian species to extinction, and severely affecting hundreds of\nothers. Difficulties in management of this disease have shown a need for novel\nconservation approaches.\n  In this paper, we present a novel dynamic mathematical model for\nchytridiomycosis transmission in frogs that includes the natural history of\ninfection, to test the hypothesis that sunlight-heated refugia reduce\ntransmission in frog populations. This model was fit using approximate Bayesian\ncomputation to experimental data where frogs were separated into cohorts based\nthe amount of heat the refugia received.\n  Our results show that the effect of sunlight-heating the refugia reduced\ninfection in frogs by 40%. Further, frogs that were infected and recovered had\nsignificant protection, with a reduction in susceptibility of approximately 97%\ncompared to naive frogs. The mathematical model can be used to gain further\ninsight into using sunlight-heated refugia to reduce chytridiomycosis\nprevalence amongst amphibians.",
      "generated_abstract": "The evolutionary dynamics of an ecological community is a complex\nfunction of the interactions between individuals and their environment.\nNevertheless, these interactions are generally described by simple linear\nmodels, which are often overlooked when analyzing data from real-world\necological systems. We present a Bayesian framework for modeling the\nevolutionary dynamics of a species-population system using non-linear\ninteractions. The framework extends the Linear Stable State (LSS) model, a\nwell-established mathematical tool for studying evolutionary dynamics, to\ninclude non-linear interactions. This framework enables us to incorporate\nboth neutral and selection effects into the evolutionary dynamics of a species,\nproviding a more realistic representation of the evolutionary history of a\npopulation. The framework also allows us to analyze the effects of non-linear\ninteractions on the long-term evolutionary trajectories of a species. By\nmodifying",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16071428571428573,
          "p": 0.21428571428571427,
          "f": 0.18367346448979607
        },
        "rouge-2": {
          "r": 0.019230769230769232,
          "p": 0.02608695652173913,
          "f": 0.022140216516660705
        },
        "rouge-l": {
          "r": 0.15178571428571427,
          "p": 0.20238095238095238,
          "f": 0.173469382857143
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/QM/2502.15848v1",
      "true_abstract": "This paper introduces a non-parametric estimation algorithm designed to\neffectively estimate the joint distribution of model parameters with\napplication to population pharmacokinetics. Our research group has previously\ndeveloped the non-parametric adaptive grid (NPAG) algorithm, which while\naccurate, explores parameter space using an ad-hoc method to suggest new\nsupport points. In contrast, the non-parametric optimal design (NPOD) algorithm\nuses a gradient approach to suggest new support points, which reduces the\namount of time spent evaluating non-relevant points and by this the overall\nnumber of cycles required to reach convergence. In this paper, we demonstrate\nthat the NPOD algorithm achieves similar solutions to NPAG across two datasets,\nwhile being significantly more efficient in both the number of cycles required\nand overall runtime. Given the importance of developing robust and efficient\nalgorithms for determining drug doses quickly in pharmacokinetics, the NPOD\nalgorithm represents a valuable advancement in non-parametric modeling. Further\nanalysis is needed to determine which algorithm performs better under specific\nconditions.",
      "generated_abstract": "The ability to accurately predict the outcome of a clinical trial relies on\ncrucial information such as the efficacy of the intervention. This includes\nthe baseline characteristics of the population, the covariates used, the\ntreatment allocation, and the outcome. In addition, the clinical trial itself\nmay influence the outcome by introducing bias. This paper introduces a\nnovel framework for predicting the outcome of clinical trials using a\nhigh-dimensional data set that includes covariates, treatment allocation,\noutcome, and clinical trial information. The framework is based on a\ncorrespondence analysis (CA) approach that identifies the covariates that\ninfluence the outcome. A novel approach is proposed to quantify the uncertainty\nin the predicted outcome using the maximum likelihood estimation (MLE) of the\ndistribution of the outcome. We also propose an adaptive MLE approach that\nincorpor",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18584070796460178,
          "p": 0.27631578947368424,
          "f": 0.22222221741384632
        },
        "rouge-2": {
          "r": 0.026845637583892617,
          "p": 0.034482758620689655,
          "f": 0.030188674322820307
        },
        "rouge-l": {
          "r": 0.1592920353982301,
          "p": 0.23684210526315788,
          "f": 0.19047618566781457
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04518v1",
      "true_abstract": "We introduce Dirichlet Process Posterior Sampling (DPPS), a Bayesian\nnon-parametric algorithm for multi-arm bandits based on Dirichlet Process (DP)\npriors. Like Thompson-sampling, DPPS is a probability-matching algorithm, i.e.,\nit plays an arm based on its posterior-probability of being optimal. Instead of\nassuming a parametric class for the reward generating distribution of each arm,\nand then putting a prior on the parameters, in DPPS the reward generating\ndistribution is directly modeled using DP priors. DPPS provides a principled\napproach to incorporate prior belief about the bandit environment, and in the\nnoninformative limit of the DP posteriors (i.e. Bayesian Bootstrap), we recover\nNon Parametric Thompson Sampling (NPTS), a popular non-parametric bandit\nalgorithm, as a special case of DPPS. We employ stick-breaking representation\nof the DP priors, and show excellent empirical performance of DPPS in\nchallenging synthetic and real world bandit environments. Finally, using an\ninformation-theoretic analysis, we show non-asymptotic optimality of DPPS in\nthe Bayesian regret setup.",
      "generated_abstract": "The statistical significance of a randomized trial's effect is determined by\nthe randomization distribution. The null distribution is the distribution of\nthe randomization distribution when all treatment groups are identical.\nEvaluating the difference in the observed and expected treatment effect for a\ngiven treatment is called the treatment effect difference. The null distribution\nis a crucial component of statistical analysis, and it is often derived using\nthe central limit theorem. This article explains the central limit theorem and\nthe null distribution using two examples. The first example demonstrates how\nthe central limit theorem is derived using the Laplace approximation, and the\nsecond example illustrates how the central limit theorem is derived using the\nmethod of moments. These two examples are used to explain the differences\nbetween the central limit theorem and the method of moments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13,
          "p": 0.21311475409836064,
          "f": 0.16149067852320523
        },
        "rouge-2": {
          "r": 0.03424657534246575,
          "p": 0.04950495049504951,
          "f": 0.04048582512547387
        },
        "rouge-l": {
          "r": 0.11,
          "p": 0.18032786885245902,
          "f": 0.13664595802631088
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.physics/bio-ph/2503.05401v1",
      "true_abstract": "We study the first-passage-time (FPT) properties of active Brownian particles\nto reach an absorbing wall in two dimensions. Employing a perturbation approach\nwe obtain exact analytical predictions for the survival and FPT distributions\nfor small P\\'eclet numbers, measuring the importance of self-propulsion\nrelative to diffusion. While randomly oriented active agents reach the wall\nfaster than their passive counterpart, their initial orientation plays a\ncrucial role in the FPT statistics. Using the median as a metric, we quantify\nthis anisotropy and find that it becomes more pronounced at distances where\npersistent active motion starts to dominate diffusion.",
      "generated_abstract": "We study the self-assembly of a class of amphiphilic molecules, the so-called\nstick-and-roll polymers. These are amphiphilic molecules that can adopt a\nconfigurational space spanned by two different configurations, each corresponding\nto an extended, two-dimensional, conformation. We show that the self-assembly\nof these polymers can be described by a system of coupled Oseen-Ferroelastic\nequations. We provide a classification of the possible types of self-assembled\nstructures, depending on the symmetry properties of the Oseen-Ferroelastic\nequations. We show that, for certain types of stick-and-roll polymers, the\nself-assembly of the polymer into a certain type of self-assembled structure\ncan be described by a system of coupled equations that are similar to those\ndescribing the assembly of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.1774193548387097,
          "f": 0.16058393665086063
        },
        "rouge-2": {
          "r": 0.031578947368421054,
          "p": 0.031914893617021274,
          "f": 0.03174602674617251
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.16129032258064516,
          "f": 0.14598539650487521
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/LO/2503.09730v1",
      "true_abstract": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the\nmodel, even for the step-wise rewards, or large quantities of human annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
      "generated_abstract": "The goal of this paper is to provide a survey of recent advances in the\nstudy of learning-based robotics, focusing on learning-based robot control,\nlearning-based robot planning, and learning-based robotics applications. The\nsurvey starts with a general overview of the field, followed by a detailed\naccount of the recent advances in the field of robot learning. The survey\nfocuses on recent advances in robot control, robot planning, and robotics\napplications. It also provides an overview of recent advances in the study of\nlearning-based robotics. It is organized into five sections: 1) Overview,\n2) Learning-based Robot Control, 3) Learning-based Robot Planning, 4)\nLearning-based Robotics Applications, and 5) Conclusion. The survey is\nstructured into the following sections: Section 1 provides a general over",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10714285714285714,
          "p": 0.19047619047619047,
          "f": 0.13714285253485728
        },
        "rouge-2": {
          "r": 0.013513513513513514,
          "p": 0.020833333333333332,
          "f": 0.016393437850041704
        },
        "rouge-l": {
          "r": 0.08928571428571429,
          "p": 0.15873015873015872,
          "f": 0.11428570967771447
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2502.16336v1",
      "true_abstract": "We present a new method for generating confidence sets within the split\nconformal prediction framework. Our method performs a trainable transformation\nof any given conformity score to improve conditional coverage while ensuring\nexact marginal coverage. The transformation is based on an estimate of the\nconditional quantile of conformity scores. The resulting method is particularly\nbeneficial for constructing adaptive confidence sets in multi-output problems\nwhere standard conformal quantile regression approaches have limited\napplicability. We develop a theoretical bound that captures the influence of\nthe accuracy of the quantile estimate on the approximate conditional validity,\nunlike classical bounds for conformal prediction methods that only offer\nmarginal coverage. We experimentally show that our method is highly adaptive to\nthe local data structure and outperforms existing methods in terms of\nconditional coverage, improving the reliability of statistical inference in\nvarious applications.",
      "generated_abstract": "In this paper, we address the problem of estimating a function of functions\nin the context of Gaussian process regression. This problem arises in a variety\nof contexts, including Gaussian process classification, Bayesian nonparametric\nregression, and Bayesian nonparametric density estimation. The focus of this\npaper is on Gaussian process regression, where we consider the problem of\nestimating a function of functions. In this setting, we propose a novel\napproach for the estimation of a function of functions using Gaussian processes\nwith a Gaussian process prior. We show that the proposed approach is a\nspecial case of a more general approach, which we call the Gaussian process\nprincipal component regression, that is capable of estimating a function of\nfunctions using a Gaussian process prior. The proposed approach is shown to\noutperform existing approaches for Gaussian process regression, such as\napproximate Bayesian computation, in terms of computational efficiency,\neffectiveness, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2159090909090909,
          "p": 0.2638888888888889,
          "f": 0.23749999505000013
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.02564102564102564,
          "f": 0.02429149298955994
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.2222222222222222,
          "f": 0.1999999950500001
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.09212v1",
      "true_abstract": "We study how Generative AI (GenAI) adoption is reshaping work. While prior\nstudies show that GenAI enhances role-level productivity and task composition,\nits influence on skills - the fundamental enablers of task execution, and the\nultimate basis for employability - is less understood. Using job postings from\n378 US public firms that recruited explicitly for GenAI skills (2021-2023), we\nanalyze how GenAI adoption shifts the demand for workers' skills. Our findings\nreveal that the advertised roles which explicitly rely on GenAI tools such as\nChatGPT, Copilot, etc., have 36.7 percent higher requirements for cognitive\nskills. Further, a difference-in-differences analysis shows that the demand for\nsocial skills within GenAI roles increases by 5.2 percent post-ChatGPT launch.\nThese emerging findings indicate the presence of a hierarchy of skills in\norganizations with GenAI adoption associated with roles that rely on cognitive\nskills and social skills.",
      "generated_abstract": "In this paper, we study the impact of the COVID-19 pandemic on the\ndistribution of income and wealth in the United States. Using data from the\nAmerican Time Use Survey, we find that the pandemic disproportionately impacted\nhouseholds with lower incomes, with lower-income households spending more time\non caregiving and childcare activities, and spending more time with elderly\nrelatives. We also find that the pandemic had a large impact on the\ndistribution of income, with the wealthiest 1% of households gaining an average\nof $40,000 in wealth, while the bottom 10% saw their wealth decline by an\naverage of $20,000. Finally, we find that the pandemic had a large impact on\nthe distribution of wealth among the poor, with the poorest 10% losing an\naverage of $20",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14432989690721648,
          "p": 0.2028985507246377,
          "f": 0.16867469393743664
        },
        "rouge-2": {
          "r": 0.0072992700729927005,
          "p": 0.009900990099009901,
          "f": 0.008403356458939354
        },
        "rouge-l": {
          "r": 0.13402061855670103,
          "p": 0.18840579710144928,
          "f": 0.1566265011663523
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "The economic crisis of 2008 has had a significant impact on the global\ndynamics of the financial sector. The emergence of the shadow banking sector\nincreased its importance, as it was not subject to the same regulatory\nconstraints as traditional banks. The analysis of the evolution of the shadow\nbanking sector in the US and the EU shows that it grew significantly,\nparticularly in the EU, where it became the third largest financial sector\nafter banks and insurance companies. The study also highlights the importance\nof the growth of the shadow banking sector in the US and EU, and its\ninterdependence with the main financial system, especially in the case of the\nEU, where the sector's share in total credit reached 12.5%. The study also\ndiscusses the main factors that led to the growth of the shadow banking sector\nin the US and EU. It emphasizes",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.21333333333333335,
          "f": 0.17877094485190864
        },
        "rouge-2": {
          "r": 0.02112676056338028,
          "p": 0.02702702702702703,
          "f": 0.023715410094831437
        },
        "rouge-l": {
          "r": 0.1346153846153846,
          "p": 0.18666666666666668,
          "f": 0.15642457613682484
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.03232v1",
      "true_abstract": "Prior approaches to lead instrument detection primarily analyze mixture\naudio, limited to coarse classifications and lacking generalization ability.\nThis paper presents a novel approach to lead instrument detection in multitrack\nmusic audio by crafting expertly annotated datasets and designing a novel\nframework that integrates a self-supervised learning model with a track-wise,\nframe-level attention-based classifier. This attention mechanism dynamically\nextracts and aggregates track-specific features based on their auditory\nimportance, enabling precise detection across varied instrument types and\ncombinations. Enhanced by track classification and permutation augmentation,\nour model substantially outperforms existing SVM and CRNN models, showing\nrobustness on unseen instruments and out-of-domain testing. We believe our\nexploration provides valuable insights for future research on audio content\nanalysis in multitrack music settings.",
      "generated_abstract": "The increasing availability of large-scale audio-visual datasets is\npromising for the development of multimedia-focused audio-visual speech\nrecognition (AVSR) systems. However, the pre-training of AVSR models often\nrequires large-scale datasets, which can be expensive to obtain. To address\nthis challenge, we introduce PALM, a novel pre-training approach for AVSR.\nPALM first extracts multimodal features using audio and visual information from\naudio-visual data and then performs multi-task learning to train a model for\nboth speech recognition and visual-language modeling. This approach enables\nPALM to train a unified model with improved performance on both speech and\nvisual tasks. We evaluate PALM on the Audio-Visual Speech Recognition (AVSR)\nbenchmark and show that our approach achieves state-of-the-art performance on\nboth speech and visual",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1956521739130435,
          "p": 0.225,
          "f": 0.20930232060573295
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.018691588785046728,
          "f": 0.018264835185256054
        },
        "rouge-l": {
          "r": 0.1956521739130435,
          "p": 0.225,
          "f": 0.20930232060573295
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.08986v1",
      "true_abstract": "This paper considers communication between a base station (BS) to two users,\neach from one side of a simultaneously transmitting-reflecting reconfigurable\nintelligent surface (STAR-RIS) in the absence of a direct link. Rate-splitting\nmultiple access (RSMA) strategy is employed and the STAR-RIS is subjected to\nphase errors. The users are equipped with a planar fluid antenna system (FAS)\nwith position reconfigurability for spatial diversity. First, we derive the\ndistribution of the equivalent channel gain at the FAS-equipped users,\ncharacterized by a t-distribution. We then obtain analytical expressions for\nthe outage probability (OP) and average capacity (AC), with the latter obtained\nvia a heuristic approach. Our findings highlight the potential of FAS to\nmitigate phase imperfections in STAR-RIS-assisted communications, significantly\nenhancing system performance compared to traditional antenna systems (TAS).\nAlso, we quantify the impact of practical phase errors on system efficiency,\nemphasizing the importance of robust strategies for next-generation wireless\nnetworks.",
      "generated_abstract": "In the era of digital transformation, the automation of manufacturing and\nproduction processes is becoming increasingly important. However, the\ncomplexity of the underlying models has limited the scalability of existing\nautomation systems. This study proposes a novel multi-agent architecture\ncombining the principles of the evolutionary computing paradigm and the\nintelligence of deep learning for the automation of production processes. The\nproposed approach is based on the use of a multi-agent system composed of\nrobots and controllers that interact to perform tasks, using reinforcement\nlearning (RL) algorithms to solve tasks, and evolutionary computation (EC) to\nselect the best solution for each task. This approach allows the agents to\nunderstand the environment, adapt to changes, and collaborate effectively to\ncomplete tasks. The approach is evaluated through simulations and real\nindustrial cases. The results show that the proposed approach can effectively\nhandle real-world tasks, impro",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.15730337078651685,
          "f": 0.13930347765253354
        },
        "rouge-2": {
          "r": 0.0273972602739726,
          "p": 0.02962962962962963,
          "f": 0.0284697458973426
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.1348314606741573,
          "f": 0.11940298014009573
        }
      }
    },
    {
      "paper_id": "math.GR.math/KT/2503.09264v1",
      "true_abstract": "Let $p$ be a prime, we say that a Kummerian oriented pro-$p$ group\n$(G,\\theta)$ has the Bogomolov-Positselski property if $I_\\theta(G)$ is a free\npro-$p$ group. We give a new criterion for an oriented pro-$p$ group to have\nthe Bogomolov-Positselski property based on previous work by Positselski\n(arXiv:1405.0965) and Quadrelli and Weigel (arXiv:2103.12438) linking their\nseemingly unrelated approaches and thereby answering a question posed by\nQuadrelli and Weigel.\n  Under further assumptions, we derive two additional criteria. The first of\nwhich strongly resembles an analogue of the Merkujev-Suslin theorem. The second\nallows to relax the conditions given by Positselski in Theorem 2 of\narXiv:1405.0965. In addition, we show how to make those weaker assumptions\ncomputationally effective in some special cases.",
      "generated_abstract": "In this paper we prove that for any unital algebra $A$ of finite type over a\nfinitely generated field $k$, the Gelfand-Kirillov-Rozansky invariant is\nnon-zero only if $A$ is a finite-dimensional $k$-algebra, or more generally if\n$A$ is a finite-dimensional unital $k$-algebra with finite-dimensional\nHilbert-Schmidt $k$-tensor product. We also prove that if $A$ is a finite-dimensional\nunital $k$-algebra with finite-dimensional Hilbert-Schmidt $k$-tensor product,\nthen the Gelfand-Kirillov-Rozansky invariant vanishes.\n  We apply our results to study the properties of the $k$-tensor product of\nfinite-dimensional $k$-algebras with finite-dimensional Hilbert-Schmidt\n$k$-tensor product. For example, we",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12222222222222222,
          "p": 0.21568627450980393,
          "f": 0.1560283641768524
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.029411764705882353,
          "f": 0.022222217520988647
        },
        "rouge-l": {
          "r": 0.12222222222222222,
          "p": 0.21568627450980393,
          "f": 0.1560283641768524
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.03214v1",
      "true_abstract": "The rice grain quality can be determined from its size and chalkiness. The\ntraditional approach to measure the rice grain size involves manual inspection,\nwhich is inefficient and leads to inconsistent results. To address this issue,\nan image processing based approach is proposed and developed in this research.\nThe approach takes image of rice grains as input and outputs the number of rice\ngrains and size of each rice grain. The different steps, such as extraction of\nregion of interest, segmentation of rice grains, and sub-contours removal,\ninvolved in the proposed approach are discussed. The approach was tested on\nrice grain images captured from different height using mobile phone camera. The\nobtained results show that the proposed approach successfully detected 95\\% of\nthe rice grains and achieved 90\\% accuracy for length and width measurement.",
      "generated_abstract": "Accurate segmentation of gastrointestinal (GI) lesions is essential for\nmedical image analysis. Despite advances in deep learning, the segmentation of\nlesions remains a challenging task. This paper proposes a novel convolutional\nneural network (CNN) based approach for segmenting GI lesions. The proposed\nmethod utilizes a U-Net-like architecture with a stacked convolutional\nblock, which enhances the generalization capabilities of the model. The\nexperimental results demonstrate that the proposed method achieves a\nperformance of 98.28% in terms of the Dice coefficient and 89.32% in terms of\nthe classification accuracy, highlighting its effectiveness in GI lesion\nsegmentation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.23529411764705882,
          "f": 0.213333328376889
        },
        "rouge-2": {
          "r": 0.04132231404958678,
          "p": 0.056179775280898875,
          "f": 0.047619042735147894
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.22058823529411764,
          "f": 0.1999999950435557
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.physics/flu-dyn/2503.09461v1",
      "true_abstract": "We investigate convection in a thin cylindrical gas layer with an imposed\nflux at the bottom and a fixed temperature along the side, using a combination\nof direct numerical simulations and laboratory experiments. The experimental\napproach allows us to extend by two orders of magnitude the explored range in\nterms of flux Rayleigh number. We identify a scaling law governing the\nroot-mean-square horizontal velocity and explain it through a dimensional\nanalysis based on heat transport in the turbulent regime. Using particle image\nvelocimetry, we experimentally confirm, for the most turbulent regimes, the\npresence of a drifting persistent pattern consisting of radial branches, as\nidentified by Rein et al. (2023, J. Fluid Mech. 977, A26). We characterise the\nangular drift frequency and azimuthal wavenumber of this pattern as functions\nof the Rayleigh number. The system exhibits a wide distribution of heat flux\nacross various time scales, with the longest fluctuations attributed to the\nbranch pattern and the shortest to turbulent fluctuations. Consequently, the\nbranch pattern must be considered to better forecast important wall heat flux\nfluctuations, a result of great relevance in the context of nuclear safety, the\ninitial motivation for our study.",
      "generated_abstract": "This work presents a novel approach to model the dynamics of a liquid-air\nbubble in a viscous liquid. We propose a stochastic formulation of the Navier-Stokes\nequations coupled to an incompressible Navier-Stokes equation to account for the\nfluid's viscosity. We then derive an analytical solution for the motion of the\nbubble, based on the linear Stokes equation. To validate the model, we compare\nthe results of our numerical simulations to the analytical solution for a\nnon-viscous liquid, and to the numerical simulations of a viscous liquid. We\nfind that our model captures the main features of the non-viscous liquid, but\nunderestimates the bubble's size. We also find that the model underestimates the\nbubble's radius when the viscosity is non-zero, and overestimates the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12213740458015267,
          "p": 0.23880597014925373,
          "f": 0.1616161571385574
        },
        "rouge-2": {
          "r": 0.03763440860215054,
          "p": 0.0660377358490566,
          "f": 0.04794520085475743
        },
        "rouge-l": {
          "r": 0.10687022900763359,
          "p": 0.208955223880597,
          "f": 0.14141413693653723
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/MN/2409.07812v1",
      "true_abstract": "Emergence during mammalian evolution of concordant and divergent traits of\ngenomic regulatory networks encompassing ubiquitous, qualitatively nearly\nidentical yet quantitatively distinct arrays of sequences of transcription\nfactor binding sites (TFBS) for 716 proteins is reported. A vast majority of\nTFs (770 of 716; 98%) comprising protein constituents of these networks appear\nto share common Gene Ontology (GO) features of sequence-specific\ndouble-stranded DNA binding (GO: 1990837). Genome-wide and individual\nchromosome-level analyses of 17,935 ATAC-seq-defined brain development\nregulatory regions (BDRRs) revealed nearly universal representations of TFBS\nfor TF-constituents of these networks, TFBS densities of which appear\nconsistently higher within thousands BDRRs of Modern Humans compare to\nChimpanzee. Transposable elements (TE), including LTR/HERV, SINE/Alu, SVA, and\nLINE families, appear to harbor and spread genome-wide consensus regulatory\nnodes of identified herein highly conserved sequence-specific double-stranded\nDNA binding networks, selections of TFBS panels of which manifest individual\nchromosome-specific profiles and species-specific divergence patterns.\nCollectively, observations reported in this contribution highlight a previously\nunrecognized essential function of human genomic DNA sequences encoded by TE in\nproviding genome-wide regulatory seed templates of highly conserved\nsequence-specific double-stranded DNA binding networks likely contributing to\ncontinuing divergent genomic evolution of human and chimpanzee brain\ndevelopment.",
      "generated_abstract": "We present a general framework for evaluating the accuracy of deep learning\nmodels in predicting cellular states, using a new metric that quantifies the\ndifference between predicted and true cellular states. This framework is applicable\nto both binary (e.g., ``active'', ``inactive'') and continuous (e.g.,\n``high-G1'', ``low-G1'') cellular states. We show that the general framework can\nbe applied to the majority of existing deep learning models for cellular states,\nincluding those that use deep neural networks, convolutional neural networks,\nand recurrent neural networks. We also show that the framework can be applied\nto the majority of existing cellular state prediction methods, including\nclassification methods, density-based methods, and density-independent\nmethods. We provide detailed examples of the framework's application to three\ndifferent cellular state prediction methods, demonstrating its effectiveness and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08396946564885496,
          "p": 0.14864864864864866,
          "f": 0.10731706855728754
        },
        "rouge-2": {
          "r": 0.00558659217877095,
          "p": 0.009174311926605505,
          "f": 0.006944439739827646
        },
        "rouge-l": {
          "r": 0.07633587786259542,
          "p": 0.13513513513513514,
          "f": 0.09756097099631193
        }
      }
    },
    {
      "paper_id": "math.FA.math/GN/2502.16712v1",
      "true_abstract": "Let $G$ and $H$ be locally compact groups and consider their associate spaces\nof almost periodic functions $AP(G)$ and $AP(H)$. We investigate the continuous\ngroup homomorphisms induced by isometries of $AP(G)$ into $AP(H)$. Among\nothers, the following results are proved:\n  {\\bf Theorem} Let $G$ and $H$ be $\\sigma$-compact maximally almost periodic\nlocally compact groups. Suppose that $T$ is a non-vanishing linear isometry of\n$AP(G)$ into $AP(H)$ that respects finite dimensional unitary representations.\nThen there is a closed subgroup $H_0\\subseteq H$, a continuous group\nhomomorphism $t$ of $H_0$ onto $G$ and an character $\\gamma\\in \\widehat{H}$\nsuch that $(Tf)(h)=\\gamma (h)~f(t(h))$ for all $h\\in H_0$ and for all $f\\in\nC(G)$.\n  {\\bf Theorem} Let $G$ and $H$ be $LC$ Abelian groups and $H$ is connected.\nSuppose that $T$ is a non-vanishing linear isometry of $AP(G)$ into $AP(H)$\nthat preserves trigonometric polynomials. Then there is a closed subgroup\n$H_0\\subseteq H$, a continuous group homomorphism $t$ of $H_0$ onto $G$, an\nelement $h_0\\in H_0$, a character $\\alpha \\in \\widehat{H}$ and an unimodular\ncomplex number $a$ such that $(Tf)(h)=a\\cdot \\alpha (h)~\\cdot f(t(h-h_0))\\text{\nfor\n  all }h\\in H_0\\text{ and for all }f\\in C(G)\\text{.}$",
      "generated_abstract": "In this paper, we study the existence of a global strong solution to the\nequilibrium problem of a stochastic differential equation with jump and\nstochastic volatility with continuous state space and drift. We prove that\nthis problem admits a unique strong solution, and also provide conditions on\nthe drift function that ensure the existence of the solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0707070707070707,
          "p": 0.18421052631578946,
          "f": 0.10218977701316016
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06060606060606061,
          "p": 0.15789473684210525,
          "f": 0.08759123686717478
        }
      }
    },
    {
      "paper_id": "math.PR.math/PR/2503.09732v1",
      "true_abstract": "We investigate a modified one-dimensional contact process with varying\ninfection rates. Specifically, the infection spreads at rate $\\lambda_e$ along\nthe boundaries of the infected region and at rate $\\lambda_i$ elsewhere. We\nestablish the existence of an invariant measure when $\\lambda_i = \\lambda_c$\nand $\\lambda_e > \\lambda_c$, where $\\lambda_c$ is the critical parameter of the\nstandard contact process. Moreover, we show that, when viewed from the\nrightmost infected site, the process converges weakly to this invariant\nmeasure. Finally, we prove that along the critical curve within the attractive\nregion of the phase space, the infection almost surely dies out.",
      "generated_abstract": "We construct a $2$-cocycle on the set of finite sequences of non-negative\nintegers. We prove that this $2$-cocycle is an isomorphism on the space of\nfinite sequences with a fixed length. This $2$-cocycle is related to the\n$2$-cocycle on the set of finite sequences of non-negative integers introduced\nby A. Schoen and E. Szab\\'o in 1985. We also prove that this $2$-cocycle is\ninversely isomorphic to the $2$-cocycle defined by D. S. Singer in 1963.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1791044776119403,
          "p": 0.2857142857142857,
          "f": 0.2201834815015572
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.01818181818181818,
          "f": 0.013888884167632776
        },
        "rouge-l": {
          "r": 0.14925373134328357,
          "p": 0.23809523809523808,
          "f": 0.1834862337951352
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10029v1",
      "true_abstract": "Hand interactions are increasingly used as the primary input modality in\nimmersive environments, but they are not always feasible due to situational\nimpairments, motor limitations, and environmental constraints. Speech\ninterfaces have been explored as an alternative to hand input in research and\ncommercial solutions, but are limited to initiating basic hand gestures and\nsystem controls. We introduce HandProxy, a system that expands the affordances\nof speech interfaces to support expressive hand interactions. Instead of\nrelying on predefined speech commands directly mapped to possible interactions,\nHandProxy enables users to control the movement of a virtual hand as an\ninteraction proxy, allowing them to describe the intended interactions\nnaturally while the system translates speech into a sequence of hand controls\nfor real-time execution. A user study with 20 participants demonstrated that\nHandProxy effectively enabled diverse hand interactions in virtual\nenvironments, achieving a 100% task completion rate with an average of 1.09\nattempts per speech command and 91.8% command execution accuracy, while\nsupporting flexible, natural speech input with varying levels of control and\ngranularity.",
      "generated_abstract": "This paper presents the implementation of a novel multi-objective\noptimization framework that combines the K-means clustering method with\ndecentralized and decentralized distributed optimization algorithms. The\nK-means clustering method is used to group the data points into clusters and\nestablish a mapping between the data points and the clusters. The clustering\nresults are then used as inputs to the distributed optimization algorithms to\noptimize the cluster-level parameters. This approach enables efficient and\nadaptive clustering and optimization. The framework is implemented using the\nC++ programming language and is available as an open-source package. The\nexperimental results demonstrate the effectiveness of the proposed framework\nin clustering and optimization. The framework is used to cluster breast cancer\ntumor samples and optimize the clustering parameters using a variety of\ndistributed optimization algorithms. The results show that the framework\nachieves high clustering accuracy and can be adapted to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.19444444444444445,
          "f": 0.15053762966354506
        },
        "rouge-2": {
          "r": 0.01764705882352941,
          "p": 0.024193548387096774,
          "f": 0.020408158387709992
        },
        "rouge-l": {
          "r": 0.11403508771929824,
          "p": 0.18055555555555555,
          "f": 0.13978494149150206
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2412.12025v1",
      "true_abstract": "This study is a continuation of previous work, which highlights the nutrient\nenhancement by using rice straw (RS) and pressmud (PM) on vermicomposting.\nHerein, we demonstrate the significant impact of Moringa oleifera derived\nCu/Ni/Co oxide nanoparticles (TmONs) in conjunction with these vermicompost on\nthe growth performance of Abelmoschus esculentus. Vermicompost produced under\nvarious combinations (T0, cow dung (CD) only; T1, 1CD:1RS; T2, 1CD:1PM, and T3,\n1CD:1RS:1PM) were further enriched by blending with biogenic nanoparticles.\nThis strategic combination enhances the nutritional composition of the\nvermicompost, contributing to its overall effectiveness in promoting plant\ngrowth and health. Various analytical techniques, including FTIR, XRD, XPS,\nFESEM-EDX, TEM, and ICP-OES, were employed for comprehensive characterization.\nThe synthesized TmONs with sizes ranging from 13 to 54 nm exhibited distinct\nCuO, NiO, and CoO phases. The vermicompost blended TmONs demonstrated\nsignificant improvements (P < 0.05) in seed germination (167%), coefficient\nvelocity (67%), and vigour index (95%), while reducing the mean germination\ntime by 41% for A. esculentus compared to the control group. The plant culture\ngroup nT3 (T3 + TmONs) showed the best growth performance. Furthermore, trace\nelement concentrations in both soil and plant leaves were found to be below the\nmaximum permissible limits set by WHO (1996). This investigation extends the\nunderstanding of the role played by these nanoparticles in fostering optimal\nconditions for plant growth and development as these micronutrients are\nessential components for several plant enzymes.",
      "generated_abstract": "This paper presents a novel approach to developing and validating a\nsystem for analyzing the effects of dietary supplements on the genotoxicity\nof human cells in vitro. The proposed system is based on a multiscale\nframework integrating the quantitative proteomics and metabolomics techniques\nwith the computational modeling approaches. This system is able to: (i)\nquantify the relative contribution of each metabolic pathway to the total\nmetabolite production, (ii) predict the metabolite profiles associated with\ndifferent dietary supplements, (iii) identify the metabolites that are most\nsensitive to the effect of dietary supplements, and (iv) estimate the\ndifferences in the metabolite profiles between the dietary supplements and\ncontrol cells. The proposed multiscale approach is able to detect the effect of\ndietary supplements",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08139534883720931,
          "p": 0.19718309859154928,
          "f": 0.11522633331233399
        },
        "rouge-2": {
          "r": 0.008733624454148471,
          "p": 0.018867924528301886,
          "f": 0.011940294181511926
        },
        "rouge-l": {
          "r": 0.0755813953488372,
          "p": 0.18309859154929578,
          "f": 0.10699588063743687
        }
      }
    },
    {
      "paper_id": "stat.AP.q-bio/QM/2502.16130v1",
      "true_abstract": "The COVID-19 pandemic has adversely affected US public health, resulting in\nover a hundred million cases and more than one million deaths. Vaccination is\nthe key intervention against the COVID-19 pandemic. Multiple COVID-19 vaccines\nare now available for human use. However, a number of factors, including\nsocio-demographic variables, impact the uptake of COVID-19 vaccines. In this\nstudy, we apply a Bayesian mixed-effects model to assess different\nsocio-demographic and spatial factors that influence the acceptance of COVID-19\nvaccines in the US. The fitted mixed-effects model provides the probabilistic\ninference about the vaccine acceptance determinants with uncertainty\nquantification.",
      "generated_abstract": "The field of computational biology has seen rapid advancements in the\ncomputational modeling of biological systems. This paper explores the\ndevelopment of a novel methodology to address the challenges of modeling\ncomplex systems using state-of-the-art algorithms. The methodology presented\nencompasses three main elements: (i) the development of a novel optimization\nframework, (ii) the integration of the framework with an existing\nhigh-performance computing (HPC) environment, and (iii) the application of the\nframework to a biological system. This work demonstrates the ability of the\nframework to perform efficient optimization tasks in a real-world scenario,\nenabling the exploration of a diverse set of parameters while providing\ninformative insights. The proposed framework offers the potential to provide\nmore efficient and effective solutions to challenges in biological systems\nthrough the integration of existing HPC resources.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.12987012987012986,
          "f": 0.13605441678004554
        },
        "rouge-2": {
          "r": 0.011111111111111112,
          "p": 0.008620689655172414,
          "f": 0.009708732943729518
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.12987012987012986,
          "f": 0.13605441678004554
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2503.03337v1",
      "true_abstract": "We identify a common scheme in several existing algorithms adressing\ncomputational problems on linear differential equations with polynomial\ncoefficients. These algorithms reduce to computing a linear relation between\nvectors obtained as iterates of a simple differential operator known as\npseudo-linear map.\n  We focus on establishing precise degree bounds on the output of this class of\nalgorithms. It turns out that in all known instances (least common left\nmultiple, symmetric product,. . . ), the bounds that are derived from the\nlinear algebra step using Cramer's rule are pessimistic. The gap with the\nbehaviour observed in practice is often of one order of magnitude, and better\nbounds are sometimes known and derived from ad hoc methods and independent\narguments. We propose a unified approach for proving output degree bounds for\nall instances of the class at once. The main technical tools come from the\ntheory of realisations of matrices of rational functions and their\ndeterminantal denominators.",
      "generated_abstract": "The increasing availability of large-scale datasets and advancements in\ncomputational\n  techniques have led to the development of various generative models, such as\ntransformer-based models and latent space models, for text-to-image (T2I)\ngeneration. However, these models are often trained on a small dataset and\nexperience significant performance degradation when faced with a new dataset.\nThis paper presents a novel approach for generating realistic T2I images from\nlow-quality synthetic images using the latent space model. Our method leverages\nthe latent space of the latent space model, which is trained on a large\ndataset, to generate high-quality synthetic images. We demonstrate that our\nmethod can generate realistic images from low-quality synthetic images,\nhighlighting its potential for T2I generation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1941747572815534,
          "p": 0.25316455696202533,
          "f": 0.21978021486716592
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.019417475728155338,
          "f": 0.015748026674624827
        },
        "rouge-l": {
          "r": 0.17475728155339806,
          "p": 0.22784810126582278,
          "f": 0.1978021928891439
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.22001v1",
      "true_abstract": "We examine the effect of item arrangement on choices using a novel\ndecision-making model based on the Markovian exploration of choice sets. This\nmodel is inspired by experimental evidence suggesting that the decision-making\nprocess involves sequential search through rapid stochastic pairwise\ncomparisons. Our findings show that decision-makers following a reversible\nprocess are unaffected by item rearrangements, and further demonstrate that\nthis property can be inferred from their choice behavior. Additionally, we\nprovide a characterization of the class of Markovian models in which the agent\nmakes all possible pairwise comparisons with positive probability. The\nintersection of reversible models and those allowing all pairwise comparisons\nis observationally equivalent to the well-known Luce model. Finally, we\ncharacterize the class of Markovian models for which the initial fixation does\nnot impact the final choice and show that choice data reveals the existence and\ncomposition of consideration sets.",
      "generated_abstract": "We study the problem of a heterogeneous agent with incomplete information\nplaying a continuous-time game with an unknown payoff function. The agent's\ninformation about the payoff is partially observable and the agent's belief\nabout the payoff is unknown to the agent. We show that, under mild assumptions\non the payoff function, the agent's optimal policy is to play the game until\nthe agent learns the payoff, which can be done by a single step of observation\nand information transmission. This result implies that, in the absence of\ninformation, the agent does not benefit from the game. Our result provides an\nexplicit characterization of the information structure in which the agent's\noptimal policy is to play the game until learning the payoff.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24468085106382978,
          "p": 0.35384615384615387,
          "f": 0.28930817126695946
        },
        "rouge-2": {
          "r": 0.06716417910447761,
          "p": 0.08823529411764706,
          "f": 0.0762711815326059
        },
        "rouge-l": {
          "r": 0.23404255319148937,
          "p": 0.3384615384615385,
          "f": 0.2767295549147582
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/AP/2503.10481v1",
      "true_abstract": "In clinical trials involving both mortality and morbidity, an active\ntreatment can influence the observed risk of the first non-fatal event either\ndirectly, through its effect on the non-fatal event process, or indirectly,\nthrough its effect on the death process, or both. Discerning the direct effect\nof treatment on the first non-fatal event holds clinical interest. However,\nwith the competing risk of death, the Cox proportional hazards model that\ntreats death as non-informative censoring and evaluates treatment effects on\ntime to the first non-fatal event provides an estimate of the cause-specific\nhazard ratio, which may not correspond to the direct effect. To obtain the\ndirect effect on the first non-fatal event, within the principal stratification\nframework, we define the principal stratum hazard and introduce the\nProportional Principal Stratum Hazards model. This model estimates the\nprincipal stratum hazard ratio, which reflects the direct effect on the first\nnon-fatal event in the presence of death and simplifies to the hazard ratio in\nthe absence of death. The principal stratum membership is identified using the\nshared frailty model, which assumes independence between the first non-fatal\nevent process and the potential death process from the counterfactual arm,\nconditional on per-subject random frailty. Simulation studies are conducted to\nverify the reliability of our estimators. We illustrate the method using the\nCarvedilol Prospective Randomized Cumulative Survival trial which involves\nheart-failure events.",
      "generated_abstract": "This paper provides a systematic analysis of the Bayesian inference for\nthe two-sample t-test. First, we show that the proposed Bayesian test can be\nformulated as a generalized linear model with a linear predictor, and\nidentification of the Bayes factors is equivalent to the likelihood ratio\ntest. Next, we investigate the asymptotic properties of the proposed Bayesian\ntest. We establish that the proposed test is consistent and asymptotically\nnormal when the null hypothesis is true. The asymptotic normality is shown to\nhold under mild conditions, and the asymptotic normality of the log-ratio of\nthe proposed test is established. Moreover, we derive the asymptotic\ndistribution of the Bayes factor of the proposed test under the null\nhypothesis, and the asymptotic distribution of the proposed test is also\nestablished. The proposed Bayesian test is compared with the existing Bayesian\ntests for the two-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.2318840579710145,
          "f": 0.16753926240070185
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.037383177570093455,
          "f": 0.02797202328891467
        },
        "rouge-l": {
          "r": 0.11475409836065574,
          "p": 0.2028985507246377,
          "f": 0.14659685402373854
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2409.18443v1",
      "true_abstract": "The primary objective of this study was to examine the impact of the US\nsovereign credit rating downgrade on its equity market. Utilizing the event\nstudy methodology, a sample of three most capitalized listed companies --\nMicrosoft, Apple, and Amazon -- and the equity market index -- S&P500 -- were\nused as the proxy for the overall equity market. Three market models were\nconstructed within the estimation window to determine the expected daily\nreturns on the selected companies stocks. The result showed that the sovereign\ncredit rating downgrade of the US government debt did not have any significant\neffects on the US equity market.",
      "generated_abstract": "This paper proposes an innovative risk-based strategy for the hedging of\nsignificant interest rate swaptions. We present a novel pricing approach based\non the use of the Lagrangian approach to derive the derivatives of the\nstochastic volatility, which is based on the Black-Scholes model. This approach\nis used to derive the derivative of the option's payoff, which is then used to\nderive the option's payoff and the associated hedging strategy. The risk\nprofiles of the options are estimated using a stochastic differential\nequation (SDE) model and are then used to calculate the hedging portfolio\nreturn. We test the proposed strategy on the Swiss 300 Index and the German\n300 Index and demonstrate that the strategy is effective in reducing the\nvolatility of the option's payoff.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.16417910447761194,
          "f": 0.1654135288348693
        },
        "rouge-2": {
          "r": 0.053763440860215055,
          "p": 0.049019607843137254,
          "f": 0.05128204629270266
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.16417910447761194,
          "f": 0.1654135288348693
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.09979v1",
      "true_abstract": "Silicon has striking similarity with carbon and is found in plant cells.\nHowever, there is no specific role that has been assigned to silicon in the\nlife cycle of plants. The amount of silicon in plant cells is species specific\nand can reach levels comparable to macronutrients. Silicon is the central\nelement for artificial intelligence, nanotechnology and digital revolution thus\ncan act as an informational molecule like nucleic acids while the diverse\nbonding potential of silicon with different chemical species is analogous to\ncarbon and thus can serve as a structural candidate such as proteins. The\ndiscovery of large amounts of silicon on Mars and the moon along with the\nrecent developments of enzyme that can incorporate silicon into organic\nmolecules has propelled the theory of creating silicon-based life. More\nrecently, bacterial cytochrome has been modified through directed evolution\nsuch that it could cleave silicon-carbon bonds in organo-silicon compounds thus\nconsolidating on the idea of utilizing silicon in biomolecules. In this article\nthe potential of silicon-based life forms has been hypothesized along with the\nreasoning that autotrophic virus-like particles can be a lucrative candidate to\ninvestigate such potential. Such investigations in the field of synthetic\nbiology and astrobiology will have corollary benefit on Earth in the areas of\nmedicine, sustainable agriculture and environmental sustainability.\nBibliometric analysis indicates an increasing interest in synthetic biology.\nGermany leads in research related to plant synthetic biology, while\nBiotechnology and Biological Sciences Research Council (BBSRC) at UK has\nhighest financial commitments and Chinese Academy of Sciences generates the\nhighest number of publications in the field.",
      "generated_abstract": "In this work, we propose a novel approach for the analysis of gene expression\ndata. By utilizing a novel multivariate representation of the data, we\nrepresent the gene expression matrix as a graph. Based on the representation,\nwe propose a novel method for the analysis of gene expression data. The\nmethodology is based on the use of spectral clustering, which is a graph\nclustering technique, to obtain a clustered representation of the gene expression\nmatrix. We then perform spectral clustering on the clustered representation to\nobtain the spectral clustering of the gene expression matrix. We perform\nclustering on the spectral clustering of the gene expression matrix to obtain\nthe clusters of the gene expression matrix. We then use the spectral clustering\nof the clustered representation of the gene expression matrix to obtain the\nclustering of the gene expression matrix. We then obtain the spectral\nclustering of the spectral clustering of the gene expression",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08280254777070063,
          "p": 0.28888888888888886,
          "f": 0.12871286782423302
        },
        "rouge-2": {
          "r": 0.0125,
          "p": 0.039473684210526314,
          "f": 0.01898733811889191
        },
        "rouge-l": {
          "r": 0.08280254777070063,
          "p": 0.28888888888888886,
          "f": 0.12871286782423302
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.10264v1",
      "true_abstract": "While peer review enhances writing and research quality, harsh feedback can\nfrustrate and demotivate authors. Hence, it is essential to explore how\ncritiques should be delivered to motivate authors and enable them to keep\niterating their work. In this study, we explored the impact of appending an\nautomatically generated positive summary to the peer reviews of a writing task,\nalongside varying levels of overall evaluations (high vs. low), on authors'\nfeedback reception, revision outcomes, and motivation to revise. Through a 2x2\nonline experiment with 137 participants, we found that adding an AI-reframed\npositive summary to otherwise harsh feedback increased authors' critique\nacceptance, whereas low overall evaluations of their work led to increased\nrevision efforts. We discuss the implications of using AI in peer feedback,\nfocusing on how AI-driven critiques can influence critique acceptance and\nsupport research communities in fostering productive and friendly peer feedback\npractices.",
      "generated_abstract": "This paper presents a new framework for the synthesis of hybrid cognitive\nnetworks (HCNs) that can be implemented on embedded and resource-constrained\ndevices. We propose a novel approach for the design of HCNs that incorporates\nthe use of an agent-based model (ABM) to model cognitive processes and an\noptimization framework to develop rules for rule-based control. This approach\nprovides a framework for the design of HCNs that is compatible with a wide range\nof hardware platforms, such as edge devices, low-power sensors, and embedded\nsystems, and enables the development of practical solutions that can be\ndeployed in real-world applications. In this paper, we present a model for the\ndevelopment of an HCN that is based on an ABM and a rule-based control\nalgorithm, demonstrating its applicability in various scenarios. The model is\nvalidated through simulations on a low",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18,
          "p": 0.21428571428571427,
          "f": 0.1956521689508508
        },
        "rouge-2": {
          "r": 0.007194244604316547,
          "p": 0.00819672131147541,
          "f": 0.0076628302702576355
        },
        "rouge-l": {
          "r": 0.17,
          "p": 0.20238095238095238,
          "f": 0.18478260373345953
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2411.12769v1",
      "true_abstract": "The Genebass dataset, released by Karczewski et al. (2022), provides a\ncomprehensive resource elucidating associations between genes and 4,529\nphenotypes based on nearly 400,000 exomes from the UK Biobank. This extensive\ndataset enables the evaluation of gene set enrichment across a wide range of\nphenotypes, facilitating the inference of associations between specified gene\nsets and phenotypic traits. Despite its potential, no established method for\napplying gene set enrichment analysis (GSEA) to Genebass data exists. To\naddress this gap, we propose utilizing fast pre-ranked gene set enrichment\nanalysis (FGSEA) as a novel approach to determine whether a specified set of\ngenes is significantly enriched in phenotypes within the UK Biobank. We\ndeveloped an R package, ukbFGSEA, to implement this analysis, completed with a\nhands-on tutorial. Our approach has been validated by analyzing gene sets\nassociated with autism spectrum disorder, developmental disorder, and\nneurodevelopmental disorders, demonstrating its capability to reveal\nestablished and novel associations.",
      "generated_abstract": "Neuronal networks are often studied using the graph Laplacian. However,\nthis approach does not provide a meaningful representation of the network's\nstructure. In this paper, we introduce a novel graph Laplacian that explicitly\ncaptures the structure of the network. This new Laplacian, called the\n\\textit{graph Laplacian with structure}, is defined on a graph $G = (V, E)$\nwith a set of \\textit{nodes} $V$ and a set of \\textit{edges} $E$. We show that\nthe graph Laplacian with structure is a positive semi-definite matrix, which\nallows us to use it as a tool for network analysis. Moreover, we show that the\ngraph Laplacian with structure is the spectral decomposition of the\n\\textit{graph Laplacian}, which is a more general matrix than the Laplacian.\nThis generality allows us to apply the graph",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.24,
          "f": 0.19672130663800066
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.037383177570093455,
          "f": 0.032128509155014376
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.21333333333333335,
          "f": 0.1748633831407329
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01533v5",
      "true_abstract": "Elite economics PhD programs aim to train graduate students for a lifetime of\nacademic research. This paper asks how advising affects graduate students'\npost-PhD research productivity. Advising is highly concentrated: at the eight\nhighly-selective schools in our study, a minority of advisors do most of the\nadvising work. We quantify advisor attributes such as an advisor's own research\noutput and aspects of the advising relationship like coauthoring and research\nfield affinity that might contribute to student research success. Students\nadvised by research-active, prolific advisors tend to publish more, while\ncoauthoring has no effect. Student-advisor research affinity also predicts\nstudent success. But a school-level aggregate production function provides much\nweaker evidence of causal effects, suggesting that successful advisors attract\nstudents likely to succeed-without necessarily boosting their students' chances\nof success. Evidence for causal effects is strongest for a measure of advisors'\nown research output. Aggregate student research output appears to scale\nlinearly with graduate student enrollment, with no evidence of negative\nclass-size effects. An analysis of gender differences in research output shows\nmale and female graduate students to be equally productive in the first few\nyears post-PhD, but female productivity peaks sooner than male productivity.",
      "generated_abstract": "We examine the effect of a carbon tax on household savings and consumption\nin a simple model with endogenous saving, substitution effects, and\nheterogeneity. Our findings are consistent with the existence of a \"savings\neffect\" due to the tax, and a \"consumption effect\" due to substitution. The\nsavings effect is largest when the tax rate is high and the substitution\neffect is largest when the tax rate is low. The model also provides insights\ninto the impact of the carbon tax on the labor supply. When the tax rate is\nhigh and substitution is large, the tax reduces labor supply by reducing the\nprice of labor. When the tax rate is low and substitution is small, the tax\nincreases labor supply by reducing the price of labor. We also show that\nsubstitution effects can be partially offset by the carbon tax if the tax\nrate is high and the substitution effect is large. Finally, we discuss",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13178294573643412,
          "p": 0.2537313432835821,
          "f": 0.17346938325541453
        },
        "rouge-2": {
          "r": 0.00546448087431694,
          "p": 0.00909090909090909,
          "f": 0.006825933876926658
        },
        "rouge-l": {
          "r": 0.13178294573643412,
          "p": 0.2537313432835821,
          "f": 0.17346938325541453
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/DC/2503.10217v1",
      "true_abstract": "Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from\ngeneral language comprehension to task-specific expertise. To preserve user\ndata privacy, federated fine-tuning is often employed and has emerged as the de\nfacto paradigm. However, federated fine-tuning is prohibitively inefficient due\nto the tension between LLM complexity and the resource constraint of end\ndevices, incurring unaffordable fine-tuning overhead. Existing literature\nprimarily utilizes parameter-efficient fine-tuning techniques to mitigate\ncommunication costs, yet computational and memory burdens continue to pose\nsignificant challenges for developers. This work proposes DropPEFT, an\ninnovative federated PEFT framework that employs a novel stochastic transformer\nlayer dropout method, enabling devices to deactivate a considerable fraction of\nLLMs layers during training, thereby eliminating the associated computational\nload and memory footprint. In DropPEFT, a key challenge is the proper\nconfiguration of dropout ratios for layers, as overhead and training\nperformance are highly sensitive to this setting. To address this challenge, we\nadaptively assign optimal dropout-ratio configurations to devices through an\nexploration-exploitation strategy, achieving efficient and effective\nfine-tuning. Extensive experiments show that DropPEFT can achieve a\n1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory\nfootprint compared to state-of-the-art methods.",
      "generated_abstract": "We introduce the concept of a \\emph{priority queue}, a novel data structure\nthat is both efficient and scalable, while preserving a natural ordering\nstructure. We show how to construct a priority queue in polynomial time from a\ngraph and its associated edge weights. Additionally, we show that if a graph is\nedge-weighted, then its associated priority queue is acyclic. We present two\nimplementations of priority queues, both of which are in polynomial time and\nscale well with graph sizes. We also present a number of algorithms for\nconstructing priority queues from graphs. Our algorithms are efficient,\nscalable, and preserve the natural ordering structure. We show how to compute\nthe minimum weight path in a graph with a priority queue, as well as how to find\nthe minimum weight path in a graph with a priority queue. We also show how to\nfind a minimum weight path in a graph with a priority queue, and a minimum",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13286713286713286,
          "p": 0.2753623188405797,
          "f": 0.17924527862807058
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.03418803418803419,
          "f": 0.026143786126490664
        },
        "rouge-l": {
          "r": 0.11888111888111888,
          "p": 0.2463768115942029,
          "f": 0.16037735409976872
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16677v1",
      "true_abstract": "Understanding cooperation in social systems is challenging because the\never-changing rules that govern societies interact with individual actions,\nresulting in intricate collective outcomes. In virtual-world experiments, we\nallowed people to make changes in the systems that they are making decisions\nwithin and investigated how they weigh the influence of different rules in\ndecision-making. When choosing between worlds differing in more than one rule,\na naive heuristics model predicted participants decisions as well, and in some\ncases better, than game earnings (utility) or by the subjective quality of\nsingle rules. In contrast, when a subset of engaged participants made\ninstantaneous (within-world) decisions, their behavior aligned very closely\nwith objective utility and not with the heuristics model. Findings suggest\nthat, whereas choices between rules may deviate from rational benchmarks, the\nfrequency of real time cooperation decisions to provide feedback can be a\nreliable indicator of the objective utility of these rules.",
      "generated_abstract": "Membrane fusion is a critical step in many cellular processes, including\ncell division, migration, and endocytosis. However, the molecular mechanisms\nunderpinning this process remain poorly understood. In this review, we\nintroduce a conceptual framework for membrane fusion that integrates\nmechanisms of trans-Golgi network (TGN) remodelling with the dynamics of\nmembrane-protein-liposome complexes. We begin by discussing the\ninteraction between TGN remodelling and membrane fusion, emphasising how\nmolecular dynamics can be exploited to identify key molecular players. We\nthen present a theoretical framework for membrane fusion based on the\ninteraction between liposome-membrane complexes and the dynamics of TGN\nremodelling. This framework provides a unifying approach to membrane fusion and\nreveals the critical role of TGN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.21621621621621623,
          "f": 0.17582417099867179
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.0196078431372549,
          "f": 0.01612902741545411
        },
        "rouge-l": {
          "r": 0.1388888888888889,
          "p": 0.20270270270270271,
          "f": 0.16483516000966084
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/CP/2502.07518v1",
      "true_abstract": "Implied volatility IV is a key metric in financial markets, reflecting market\nexpectations of future price fluctuations. Research has explored IV's\nrelationship with moneyness, focusing on its connection to the implied Hurst\nexponent H. Our study reveals that H approaches 1/2 when moneyness equals 1,\nmarking a critical point in market efficiency expectations. We developed an IV\nmodel that integrates H to capture these dynamics more effectively. This model\nconsiders the interaction between H and the underlying-to-strike price ratio\nS/K, crucial for capturing IV variations based on moneyness. Using Optuna\noptimization across multiple indexes, the model outperformed SABR and fSABR in\naccuracy. This approach provides a more detailed representation of market\nexpectations and IV-H dynamics, improving options pricing and volatility\nforecasting while enhancing theoretical and pratcical financial analysis.",
      "generated_abstract": "This study investigates the pricing of options with binary payoffs using\nnon-standard stochastic volatility models. Our focus is on the case where the\nvolatility process is stationary and ergodic, and the volatility is\nassumed to be Gaussian. Our analysis is based on the framework of\nsemimartingale modelling, which allows us to derive the exact expression of the\nstochastic volatility. Our results are then applied to the pricing of options\nwith binary payoffs, for which the Black-Scholes formula is not valid. The\napproach we adopt is based on the assumption that the log-returns of the stock\nhave a Gaussian distribution. The method we propose is based on the use of\nGaussian copulas to model the log-returns. The paper is structured as follows.\nFirst, we provide an introduction to the Black-Scholes model and its\nsemimartingale mod",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22916666666666666,
          "p": 0.2894736842105263,
          "f": 0.2558139485559763
        },
        "rouge-2": {
          "r": 0.023809523809523808,
          "p": 0.02608695652173913,
          "f": 0.02489626057058346
        },
        "rouge-l": {
          "r": 0.1875,
          "p": 0.23684210526315788,
          "f": 0.2093023206489996
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/RO/2503.10349v1",
      "true_abstract": "This study proposes a new Gaussian Mixture Filter (GMF) to improve the\nestimation performance for the autonomous robotic radio signal source search\nand localization problem in unknown environments. The proposed filter is first\ntested with a benchmark numerical problem to validate the performance with\nother state-of-practice approaches such as Particle Gaussian Mixture (PGM)\nfilters and Particle Filter (PF). Then the proposed approach is tested and\ncompared against PF and PGM filters in real-world robotic field experiments to\nvalidate its impact for real-world robotic applications. The considered\nreal-world scenarios have partial observability with the range-only measurement\nand uncertainty with the measurement model. The results show that the proposed\nfilter can handle this partial observability effectively whilst showing\nimproved performance compared to PF, reducing the computation requirements\nwhile demonstrating improved robustness over compared techniques.",
      "generated_abstract": "Reinforcement Learning (RL) is becoming an increasingly important tool for\nmotion planning in robotic systems. However, many existing methods focus on\nsingle-goal scenarios, which often require a large number of planning steps to\nachieve the desired goal. To address this limitation, we introduce a\nmulti-goal RL method that can execute multiple goal-driven motions in a\nsingle-step planning process. This method introduces a novel reward function\nthat takes into account the effect of the other goals on the current goal.\nThis reward function is derived from a multi-objective optimization problem,\nwhich is solved in an offline manner. The proposed method is tested on a\nvariety of robotic manipulators. Experimental results demonstrate that the\nproposed method can effectively handle multiple goals while maintaining the\nsame motion planning efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22093023255813954,
          "p": 0.21839080459770116,
          "f": 0.21965317419091865
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.03278688524590164,
          "f": 0.032520320203583294
        },
        "rouge-l": {
          "r": 0.19767441860465115,
          "p": 0.19540229885057472,
          "f": 0.19653178690768164
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2411.05851v1",
      "true_abstract": "This paper explores a GIS-based application of the conditional p-median\nproblem (where p = 1) in last-mile delivery logistics. The rapid growth of\ne-commerce in Pakistan has primarily benefited logistics companies, which face\nthe challenge of resolving inefficiencies in the existing infrastructure and\nscaling effectively to meet increasing demand. Addressing these challenges\nwould not only reduce operational costs but also lower carbon footprints. We\npresent an algorithm that utilizes road-network-based distances to determine\nthe optimal location for a new hub facility, a problem known in operations\nresearch as the conditional p-median problem. The algorithm optimizes the\nplacement of a new facility, given q existing facilities. The past delivery\ndata for this research was provided by Muller and Phipps Logistics Pakistan.\nOur method involves constructing a distance matrix between candidate hub\nlocations and past delivery points, followed by a grid search to identify the\noptimal hub location. To simulate the absence of past delivery data, we\nrepeated the process using the population distribution of Lahore. Our results\ndemonstrate a 16% reduction in average delivery distance with the addition of a\nnew hub.",
      "generated_abstract": "This study examines the impact of gender norms on the well-being of women in\nthe workplace. We focus on the impact of gender norms on workplace performance\nand productivity, and on the influence of gender norms on women's job\ndesirability. We find that gender norms influence women's performance,\ninfluencing their well-being, and the well-being of women is positively\ninfluenced by gender norms. Our findings suggest that the impact of gender\nnorms on workplace performance and productivity is less pronounced when\ngender norms are not explicit, and that the influence of gender norms on women's\nwell-being can be enhanced through the application of gender norms in the\nworkplace. Our findings also suggest that the influence of gender norms on\nwomen's well-being depends on the specific gender norms that are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09917355371900827,
          "p": 0.24,
          "f": 0.1403508730549572
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.025,
          "f": 0.015999995648001183
        },
        "rouge-l": {
          "r": 0.08264462809917356,
          "p": 0.2,
          "f": 0.11695906018946015
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10006v1",
      "true_abstract": "We study a novel class of algorithms for solving model-free feedback\noptimization problems in dynamical systems. The key novelty is the introduction\nof \\emph{persistent resetting learning integrators} (PRLI), which are\nintegrators that are reset at the same frequency at which the plant is dithered\nusing exploratory signals for model-free optimization. It is shown that PRLIs\ncan serve as core mechanisms for real-time gradient estimation in online\nfeedback-optimization tasks where only cost function measurements are\navailable. In particular, unlike existing approaches based on approximation\ntheory, such as averaging or finite-differences, PRLIs can produce global\nreal-time gradient estimates of cost functions, with uniformly bounded\nperturbations of arbitrarily small magnitude. In this sense, PRLIs function as\nrobust \\emph{hybrid} \"Oracles\" suitable for interconnection with discrete-time\noptimization algorithms that optimize the performance of continuous-time\ndynamical plants in closed-loop operation. Compared to existing methods, PRLIs\nyield \\emph{global} stability properties for a broad class of cost functions,\nsurpassing the local or semi-global guarantees offered by traditional\napproaches based on perturbation and approximation theory. The proposed\nframework naturally bridges physical systems, modeled as continuous-time plants\nwhere continuous exploration is essential, with digital algorithms, represented\nas discrete-time optimization methods. The main results are illustrated using\ndifferent numerical examples.",
      "generated_abstract": "In this paper, we consider the stochastic optimal control problem in a\nexponential-type stochastic Gaussian environment with the L\\'evy measure\n${\\rm Lg}(\\cdot)$ and a finite number of jumps. We propose to solve it by the\nframework of the stochastic optimal control problem for the general L\\'evy\nmeasure. We derive the optimal control policy, which is the optimal solution of\nthe original stochastic optimal control problem with the L\\'evy measure\n${\\rm Lg}(\\cdot)$. The optimal control policy is an optimal stochastic\ncontrol policy for the general L\\'evy measure, which is the optimal solution of\nthe original stochastic optimal control problem with the L\\'evy measure\n${\\rm Lg}(\\cdot)$. The optimal control policy is the optimal solution of the\noriginal stochastic optimal control problem with the L\\'evy measure\n${\\rm Lg}(\\cdot)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.36363636363636365,
          "f": 0.17777777408395068
        },
        "rouge-2": {
          "r": 0.010526315789473684,
          "p": 0.03076923076923077,
          "f": 0.015686270711265816
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.3409090909090909,
          "f": 0.16666666297283958
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.14258v1",
      "true_abstract": "Accelerating the deep transformation and upgrading of industrial structure\nand forming new quality productive forces are essential components for China to\nachieve the great rejuvenation of the Chinese Dream. After more than 40 years\nof rapid development, China has entered the \"new normal\" of development, making\nthe advancement of new quality productive forces an urgent task. This paper\nreviews the evolution of China's industrial structure, argues the necessity for\na new round of deep industrial transformation, and explores the impact of\nindustrial structure transformation and upgrading on the level of new quality\nproductive forces using various methods. The research findings are as\nfollows:(1)The deep transformation and upgrading of the industrial structure\ncan significantly promote the development of new quality productive forces, but\nthere are obvious regional differences.(2)The core indicator of the improvement\nin the level of new quality productive forces is the enhancement of total\nfactor productivity. Furthermore, this paper summarizes past industrial\ndevelopment processes and the challenges faced, and analyzes and discusses the\npotential challenges that may arise in promoting the development of new quality\nproductive forces through deep industrial structure transformation, based on\nempirical research results.",
      "generated_abstract": "This paper addresses the issue of information asymmetry and information\ncompetition in the labor market, using a novel data set covering 6,600\nemployees in the European banking sector. We find that information asymmetry\nis common and that it significantly influences wage determination. The\nemployees with the most information asymmetry receive higher wages than their\npeers. However, the effect of information asymmetry on wage determination is\nstrongly affected by the degree of competition among employees. The lower the\ndegree of competition among employees, the greater the impact of information\nasymmetry on wage determination. Furthermore, the more information asymmetry\nthere is within a firm, the higher the wage premium for employees with more\ninformation asymmetry. These results indicate that information asymmetry is a\nkey factor driving wage inequality and the wage premium for employees with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20588235294117646,
          "p": 0.30434782608695654,
          "f": 0.24561403027393053
        },
        "rouge-2": {
          "r": 0.032679738562091505,
          "p": 0.047619047619047616,
          "f": 0.03875968509554775
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.2608695652173913,
          "f": 0.21052631097568494
        }
      }
    },
    {
      "paper_id": "hep-ex.hep-ex/2503.09742v1",
      "true_abstract": "Measurements are presented of the W and Z boson production cross sections in\nproton-proton collisions at a center-of-mass energy of 13.6 TeV. Data collected\nin 2022 and corresponding to an integrated luminosity of 5.01 fb$^{-1}$ with\none or two identified muons in the final state are analyzed. The results for\nthe products of total inclusive cross sections and branching fractions for\nmuonic decays of W and Z bosons are 11.93 $\\pm$ 0.08 (syst) $\\pm$ 0.17 (lumi)\n$^{+0.07}_{-0.07}$ (acc) nb for W$^+$ boson production, 8.86 $\\pm$ 0.06 (syst)\n$\\pm$ 0.12 (lumi) $^{+0.05}_{-0.06}$ (acc) nb for W$^-$ boson production, and\n2.021 $\\pm$ 0.009 (syst) $\\pm$ 0.028 (lumi) $^{+0.011}_{-0.013}$ (acc) nb for\nthe Z boson production in the dimuon mass range of 60-120 GeV, all with\nnegligible statistical uncertainties. Furthermore, the corresponding fiducial\ncross sections, as well as cross section ratios for both fiducial and total\nphase space, are provided. The ratios include charge-separated results for W\nboson production (W$^+$ and W$^-$) and the sum of the two contributions\n(W$^\\pm$), each relative to the measured Z boson production cross section.\nAdditionally, the ratio of the measured cross sections for W$^+$ and W$^-$\nboson production is reported. All measurements are in agreement with\ntheoretical predictions, calculated at next-to-next-to-leading order accuracy\nin quantum chromodynamics.",
      "generated_abstract": "The 2015 and 2016 Run 25 of the Large Hadron Collider (LHC) were the first\nruns where the ATLAS and CMS collaborations switched from the standard\n$\\sqrt{s}=13$ TeV LHC to a 14 TeV centre-of-mass energy. In the previous\nRuns 24 and 23, the ATLAS and CMS experiments had switched to a 13 TeV energy\nwithin a year, and this transition was made relatively smooth. In this paper,\nwe describe the details of the LHC switch-over and the transition to the\n14 TeV energy. We also discuss the impact of the transition on the LHC\nperformance, including the changes in the luminosity, the change in the beam\nbeam-loss rate, and the change in the detector performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07936507936507936,
          "p": 0.14925373134328357,
          "f": 0.1036269384724424
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.04040404040404041,
          "f": 0.02758620239976292
        },
        "rouge-l": {
          "r": 0.07936507936507936,
          "p": 0.14925373134328357,
          "f": 0.1036269384724424
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.06596v1",
      "true_abstract": "Intelligent reflecting surfaces (IRSs) are envisioned to enhance the\nperformance of mmWave wireless systems. In practice, multiple mobile operators\n(MO) coexist in an area and provide simultaneous and independent services to\nuser-equipments (UEs) on different frequency bands. Then, if each MO deploys an\nIRS to enhance its performance, the IRSs also alter the channels of UEs of\nother MOs. In this context, this paper addresses the following questions: can\nan MO still continue to control its IRS independently of other MOs and IRSs? Is\njoint optimization of IRSs deployed by different MOs and inter-MO cooperation\nneeded? To that end, by considering the mmWave bands, we first derive the\nergodic sum spectral efficiency (SE) in a $2$-MO system for the following\nschemes: 1) joint optimization of an overall phase angle of the IRSs with MO\ncooperation, 2) MO cooperation via time-sharing, and 3) no cooperation between\nthe MOs. We find that even with no cooperation between the MOs, the performance\nof a given MO is not degraded by the presence of an out-of-band (OOB) MO\ndeploying and independently controlling its own IRS. On the other hand, the SE\ngain obtained at a given MO using joint optimization and cooperation over the\nno-cooperation scheme decreases inversely with the number of elements in the\nIRS deployed by the other MO. We generalize our results to a multiple MO setup\nand show that the gain in the sum-SE over the no-cooperation case increases at\nleast linearly with the number of OOB MOs. Finally, we numerically verify our\nfindings and conclude that every MO can independently operate and tune its IRS;\ncooperation via optimizing an overall phase only brings marginal benefits in\npractice.",
      "generated_abstract": "Recent advancements in multi-modal data fusion have been widely applied in\ndifferent fields, such as smart cities, healthcare, and autonomous driving.\nHowever, the impact of heterogeneous modalities on fusing is still unclear.\nThis paper explores the impact of heterogeneous modalities on fusing in\nmulti-modal data fusion. Firstly, we introduce the concept of \"heterogeneous\nmodality\" to capture the differences between different modalities, and analyze\nthe impact of different modalities on fusing. Secondly, we propose a\nmulti-modal data fusion framework, which takes advantage of the diversity of\ndata to enhance the fusion performance. Then, we study the impact of\nheterogeneous modalities on the fusion performance. Finally, we discuss the\nimplications and challenges of the proposed framework. The paper provides a\nsystematic understanding of the impact of heterogeneous modalities on data\nfusion, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11038961038961038,
          "p": 0.2463768115942029,
          "f": 0.15246636343944187
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.04,
          "f": 0.02298850165147384
        },
        "rouge-l": {
          "r": 0.1038961038961039,
          "p": 0.2318840579710145,
          "f": 0.14349775357397107
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.07296v1",
      "true_abstract": "This paper introduces the concept of wireless-powered zero-energy\nreconfigurable intelligent surface (zeRIS), and investigates a wireless-powered\nzeRIS aided communication system in terms of security, reliability and energy\nefficiency. In particular, we propose three new wireless-powered zeRIS modes:\n1) in mode-I, N reconfigurable reflecting elements are adjusted to the optimal\nphase shift design of information user to maximize the reliability of the\nsystem; 2) in mode-II, N reconfigurable reflecting elements are adjusted to the\noptimal phase shift design of cooperative jamming user to maximize the security\nof the system; 3) in mode-III, N1 and N2 (N1+N2=N) reconfigurable reflecting\nelements are respectively adjusted to the optimal phase shift designs of\ninformation user and cooperative jamming user to balance the reliability and\nsecurity of the system. Then, we propose three new metrics, i.e., joint outage\nprobability (JOP), joint intercept probability (JIP), and secrecy energy\nefficiency (SEE), and analyze their closed-form expressions in three modes,\nrespectively. The results show that under high transmission power, all the\ndiversity gains of three modes are 1, and the JOPs of mode-I, mode-II and\nmode-III are improved by increasing the number of zeRIS elements, which are\nrelated to N2, N, and N^2_1, respectively. In addition, mode-I achieves the\nbest JOP, while mode-II achieves the best JIP among three modes. We exploit two\nsecurity-reliability trade-off (SRT) metrics, i.e., JOP versus JIP, and\nnormalized joint intercept and outage probability (JIOP), to reveal the SRT\nperformance of the proposed three modes. It is obtained that mode-II\noutperforms the other two modes in the JOP versus JIP, while mode-III and\nmode-II achieve the best performance of normalized JIOP at low and high\ntransmission power, respectively.",
      "generated_abstract": "This paper presents a novel method for extracting the spatial and spectral\nenergy profiles of multiple sources in the presence of interference. The\nmethod exploits the fact that the interference contribution can be expressed as\na sum of a continuous and discrete component. By decomposing the interference\nsignal into a continuous component and discrete component, the spatial and\nspectral energy profiles of multiple sources are separately extracted. This\nmethod can be applied to both real-world and simulated interference scenarios\nand can be applied to multiple sources with different spectral characteristics\nor interference types. The proposed method can be implemented in a variety of\nscenarios and has applications in wireless communications and signal\nprocessing.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0948905109489051,
          "p": 0.21666666666666667,
          "f": 0.13197969119534142
        },
        "rouge-2": {
          "r": 0.00904977375565611,
          "p": 0.021505376344086023,
          "f": 0.012738849334051427
        },
        "rouge-l": {
          "r": 0.08029197080291971,
          "p": 0.18333333333333332,
          "f": 0.11167512266742267
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.09640v1",
      "true_abstract": "Rendering realistic human-object interactions (HOIs) from sparse-view inputs\nis challenging due to occlusions and incomplete observations, yet crucial for\nvarious real-world applications. Existing methods always struggle with either\nlow rendering qualities (\\eg, visual fidelity and physically plausible HOIs) or\nhigh computational costs. To address these limitations, we propose HOGS\n(Human-Object Rendering via 3D Gaussian Splatting), a novel framework for\nefficient and physically plausible HOI rendering from sparse views.\nSpecifically, HOGS combines 3D Gaussian Splatting with a physics-aware\noptimization process. It incorporates a Human Pose Refinement module for\naccurate pose estimation and a Sparse-View Human-Object Contact Prediction\nmodule for efficient contact region identification. This combination enables\ncoherent joint rendering of human and object Gaussians while enforcing\nphysically plausible interactions. Extensive experiments on the HODome dataset\ndemonstrate that HOGS achieves superior rendering quality, efficiency, and\nphysical plausibility compared to existing methods. We further show its\nextensibility to hand-object grasp rendering tasks, presenting its broader\napplicability to articulated object interactions.",
      "generated_abstract": "We introduce the first work to use a neural network to predict the outcome of\na physics-informed neural network (PINN) problem. Our approach uses a\nrepresentation learning approach, which leverages the structure of a PINN to\npredict the target function. The proposed method is implemented using a\ncombination of a recurrent neural network and a linear layer, which is then\nfitted to the PINN problem. The proposed method is evaluated on several PINNs\nwith different structure, and it achieves state-of-the-art performance.\nAdditionally, we demonstrate the effectiveness of our method on the\nEinstein-Podolsky-Rosen (EPR) problem. We present a new solution and a\ncomparison of the EPR problem with the Einstein-Podolsky-Nernst-Schr\\\"odinger\n(EPNS) problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10743801652892562,
          "p": 0.19696969696969696,
          "f": 0.1390374285876063
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.02,
          "f": 0.01599999520000144
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.16666666666666666,
          "f": 0.11764705425605555
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/TH/2503.05542v2",
      "true_abstract": "We consider standard gradient descent, gradient flow and conjugate gradients\nas iterative algorithms for minimizing a penalized ridge criterion in linear\nregression. While it is well known that conjugate gradients exhibit fast\nnumerical convergence, the statistical properties of their iterates are more\ndifficult to assess due to inherent nonlinearities and dependencies. On the\nother hand, standard gradient flow is a linear method with well known\nregularizing properties when stopped early. By an explicit non-standard error\ndecomposition we are able to bound the prediction error for conjugate gradient\niterates by a corresponding prediction error of gradient flow at transformed\niteration indices. This way, the risk along the entire regularisation path of\nconjugate gradient iterations can be compared to that for regularisation paths\nof standard linear methods like gradient flow and ridge regression. In\nparticular, the oracle conjugate gradient iterate shares the optimality\nproperties of the gradient flow and ridge regression oracles up to a constant\nfactor. Numerical examples show the similarity of the regularisation paths in\npractice.",
      "generated_abstract": "In this paper, we introduce a novel approach for non-convex and non-smooth\nstatistical learning problems, which we term as non-convex regularized\n(non-concave) statistical learning problems. We propose a novel regularization\nstrategy for non-convex and non-smooth statistical learning problems. Our\nproposed regularization strategy, named as the regularized non-convex\nstatistical learning problem, is based on a novel regularized non-concave\nstatistical learning problem. We prove that our regularized non-convex\nstatistical learning problem is equivalent to the non-convex statistical\nlearning problem. Furthermore, we present a convergence analysis of the\nregularized non-convex statistical learning problem. We also provide a\nconcentration analysis of the regularized non-convex statistical learning\nproblem. Finally, we provide theoretical guarantees for the proposed regularized\nnon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13,
          "p": 0.24528301886792453,
          "f": 0.16993463599470301
        },
        "rouge-2": {
          "r": 0.006756756756756757,
          "p": 0.012345679012345678,
          "f": 0.00873361988215567
        },
        "rouge-l": {
          "r": 0.13,
          "p": 0.24528301886792453,
          "f": 0.16993463599470301
        }
      }
    },
    {
      "paper_id": "physics.ins-det.physics/ins-det/2503.10395v1",
      "true_abstract": "One of the main issues of the satellite-to-ground optical communication,\nincluding free-space satellite quantum key distribution (QKD), is an\nachievement of the reasonable accuracy of positioning, navigation and optical\nstabilization. Proportional-integral-derivative (PID) controllers can handle\nwith various control tasks in optical systems. Recent research shows the\npromising results in the area of composite control systems including classical\ncontrol via PID controllers and reinforcement learning (RL) approach. In this\nwork we apply RL agent to an experimental stand of the optical stabilization\nsystem of QKD terminal. We find via agent control history more precise PID\nparameters and also provide effective combined RL-PID dynamic control approach\nfor the optical stabilization of satellite-to-ground communication system.",
      "generated_abstract": "The development of next-generation detectors for high-precision cosmology\nrequire a precise and robust measurement of the detector's dead time. In this\narticle, we present a method to measure the dead time in a single-phase\ndiode-pumped solid-state laser (SPSSL) experiment using a single-photon counting\ndetector. The method employs a set of pre-measured laser pulses, which are\nintroduced to the SPSSL to ensure the correct timing of each laser pulse and\nmeasure the dead time. We demonstrate that the dead time can be measured to\nbetter than 0.01 ps using a single-photon counting detector in a 500 mK\ntemperature-controlled SPSSL experiment. The method is general and can be\napplied to other types of detectors, such as single-photon counting",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17105263157894737,
          "p": 0.18055555555555555,
          "f": 0.17567567067932813
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.0196078431372549,
          "f": 0.019323666498636035
        },
        "rouge-l": {
          "r": 0.15789473684210525,
          "p": 0.16666666666666666,
          "f": 0.1621621571658146
        }
      }
    },
    {
      "paper_id": "nucl-th.nucl-ex/2503.08396v1",
      "true_abstract": "This study investigates the moderation of 14.1 MeV neutrons in a natural\nberyllium moderator arranged in a spherical geometry. The neutron interactions\nand moderation efficiency were analyzed using Monte Carlo simulations with the\nGEANT4 toolkit. Various sphere radii were tested to determine the optimal\nmoderator thickness for neutron thermalization.",
      "generated_abstract": "In this paper, we calculate the time evolution of the nucleon-nucleon\nsector of the energy-momentum tensor for the isotropic and anisotropic\nscattering geometries in the framework of the G-matrix formalism. We find the\neffects of isotropy and anisotropy in the nucleon-nucleon scattering\ngeometries on the energy-momentum tensor. We also calculate the time evolution\nof the total energy and momentum of the system. Our results show that the\neffects of isotropy and anisotropy in the scattering geometries can be\nconsidered as perturbations to the energy-momentum tensor, and the effects are\nsmall in comparison to the isotropic scattering geometry.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.14285714285714285,
          "f": 0.1538461488757398
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.14285714285714285,
          "f": 0.1538461488757398
        }
      }
    },
    {
      "paper_id": "cs.DB.cs/DB/2503.08087v1",
      "true_abstract": "Entity resolution (ER) is a critical task in data management which identifies\nwhether multiple records refer to the same real-world entity. Despite its\nsignificance across domains such as healthcare, finance, and machine learning,\nimplementing effective ER systems remains challenging due to the abundance of\nmethodologies and tools, leading to a paradox of choice for practitioners. This\npaper proposes Resolvi, a reference architecture aimed at enhancing\nextensibility, interoperability, and scalability in ER systems. By analyzing\nexisting ER frameworks and literature, we establish a structured approach to\ndesigning ER solutions that address common challenges. Additionally, we explore\nbest practices for system implementation and deployment strategies to\nfacilitate largescale entity resolution. Through this work, we aim to provide a\nfoundational blueprint that assists researchers and practitioners in developing\nrobust, scalable ER systems while reducing the complexity of architectural\ndecisions.",
      "generated_abstract": "Recent advancements in deep learning have enabled a revolution in the\ncomputer vision (CV) domain, with state-of-the-art methods achieving impressive\nperformance across a variety of tasks. However, the widespread adoption of\nCV models in real-world applications has raised concerns about the\nperformance implications of over-parameterization. In this paper, we propose a\nnew framework for evaluating the performance of large-scale deep learning models\nin real-world scenarios. Our approach uses the model's architecture, training\ndetails, and data distribution to derive a parameter-to-sample-ratio\nrepresentation for the model. This representation is then used to calculate the\neffect of over-parameterization on the model's performance. We apply our\nframework to a dataset of real-world web crawl datasets, including the\nGigaword5, DBPedia-20, and TRE",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14563106796116504,
          "p": 0.18518518518518517,
          "f": 0.16304347333234892
        },
        "rouge-2": {
          "r": 0.007575757575757576,
          "p": 0.00909090909090909,
          "f": 0.008264457851242643
        },
        "rouge-l": {
          "r": 0.13592233009708737,
          "p": 0.1728395061728395,
          "f": 0.15217390811495762
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03633v1",
      "true_abstract": "Autonomous motion planning under unknown nonlinear dynamics presents\nsignificant challenges. An agent needs to continuously explore the system\ndynamics to acquire its properties, such as reachability, in order to guide\nsystem navigation adaptively. In this paper, we propose a hybrid\nplanning-control framework designed to compute a feasible trajectory toward a\ntarget. Our approach involves partitioning the state space and approximating\nthe system by a piecewise affine (PWA) system with constrained control inputs.\nBy abstracting the PWA system into a directed weighted graph, we incrementally\nupdate the existence of its edges via affine system identification and reach\ncontrol theory, introducing a predictive reachability condition by exploiting\nprior information of the unknown dynamics. Heuristic weights are assigned to\nedges based on whether their existence is certain or remains indeterminate.\nConsequently, we propose a framework that adaptively collects and analyzes data\nduring mission execution, continually updates the predictive graph, and\nsynthesizes a controller online based on the graph search outcomes. We\ndemonstrate the efficacy of our approach through simulation scenarios involving\na mobile robot operating in unknown terrains, with its unknown dynamics\nabstracted as a single integrator model.",
      "generated_abstract": "This paper investigates the design of optimal control policies for a\nnonlinear robotic system that interacts with an external object and a\nsensor. The external object interacts with the robot's dynamics by exerting\nexternal force inputs, while the robot's dynamics are affected by the\nnonlinear object's force inputs. In this paper, the object is modeled as a\nnonlinear force-constraint system, and the robot is modeled as a nonlinear\ndynamics-constraint system. The robot's dynamics are described by a state-space\nmodel, while the nonlinear object's dynamics are described by a nonlinear\ndifferential equation. To ensure the stability of the robot's dynamics, the\nobject's dynamics are incorporated as an additional constraint. The\nnonlinearity of the object's dynamics, and the nonlinearity of the robot's\ndynamics, are modelled using high-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1484375,
          "p": 0.31666666666666665,
          "f": 0.20212765522861034
        },
        "rouge-2": {
          "r": 0.027932960893854747,
          "p": 0.05263157894736842,
          "f": 0.03649634583488788
        },
        "rouge-l": {
          "r": 0.140625,
          "p": 0.3,
          "f": 0.1914893573562699
        }
      }
    },
    {
      "paper_id": "cs.CV.q-bio/TO/2409.20407v3",
      "true_abstract": "Periorbital segmentation and distance prediction using deep learning allows\nfor the objective quantification of disease state, treatment monitoring, and\nremote medicine. However, there are currently no reports of segmentation\ndatasets for the purposes of training deep learning models with sub mm accuracy\non the regions around the eyes. All images (n=2842) had the iris, sclera, lid,\ncaruncle, and brow segmented by five trained annotators. Here, we validate this\ndataset through intra and intergrader reliability tests and show the utility of\nthe data in training periorbital segmentation networks. All the annotations are\npublicly available for free download. Having access to segmentation datasets\ndesigned specifically for oculoplastic surgery will permit more rapid\ndevelopment of clinically useful segmentation networks which can be leveraged\nfor periorbital distance prediction and disease classification. In addition to\nthe annotations, we also provide an open-source toolkit for periorbital\ndistance prediction from segmentation masks. The weights of all models have\nalso been open-sourced and are publicly available for use by the community.",
      "generated_abstract": "We introduce the LumBar dataset, which captures the visual appearance of\nthe lumbar spine in 3D CT scans of 100 subjects. This dataset provides a\ncomprehensive analysis of lumbar anatomy and spine deformations, which are of\ngreat importance in clinical imaging and biomechanics. We introduce the\nLumBar dataset with 20,000 images, which are organized into 100 anatomical\ngroups, each with 200 images. Each group is a subset of the entire dataset and\nrepresents a distinct anatomical region. We also release the LumBar dataset\nannotations, which are available at\nhttps://lumbar-datasets.github.io/.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10909090909090909,
          "p": 0.2033898305084746,
          "f": 0.14201182977486798
        },
        "rouge-2": {
          "r": 0.006622516556291391,
          "p": 0.012195121951219513,
          "f": 0.008583686425613505
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.1864406779661017,
          "f": 0.13017751024824076
        }
      }
    },
    {
      "paper_id": "eess.SY.cs/NA/2503.09898v1",
      "true_abstract": "Dynamic simulation plays a crucial role in power system transient stability\nanalysis, but traditional numerical integration-based methods are\ntime-consuming due to the small time step sizes. Other semi-analytical solution\nmethods, such as the Differential Transformation method, often struggle to\nselect proper orders and steps, leading to slow performance and numerical\ninstability. To address these challenges, this paper proposes a novel adaptive\ndynamic simulation approach for power system transient\n  stability analysis. The approach adds feedback control and optimization to\nselecting the step and order, utilizing the Differential Transformation method\nand a proportional-integral control strategy to control truncation errors.\nOrder selection is formulated as an optimization problem resulting in a\nvariable-step-optimal-order method that achieves significantly larger time step\nsizes without violating numerical stability. It is applied to three systems:\nthe IEEE 9-bus, 3-generator system, IEEE 39-bus, 10-generator system, and a\nPolish 2383-bus, 327-generator system, promising computational efficiency and\nnumerical robustness for large-scale power system is demonstrated in\ncomprehensive case studies.",
      "generated_abstract": "In this paper, we propose a novel approach to the design of optimal control\nplans for multi-agent systems with nonlinear dynamics. We consider an\ninter-agent system composed of two agents interacting with a single nonlinear\ndynamics, where the two agents have different degrees of freedom. We introduce\na new notion of control-to-state mapping that allows us to represent the\ninteraction of the two agents with the nonlinear dynamics. We then derive a\nprojection-based control-to-state mapping for the two-agent system with\nnonlinear dynamics. We extend this mapping to the three-agent system with\nnonlinear dynamics and propose an optimal control plan for the multi-agent\nsystem. The plan minimizes a given objective function subject to constraints on\nthe control signals of the two agents and the dynamics of the two agents. We\nshow that the optimal control plan is a trajectory of the three-agent system\nunder the constraints",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.20588235294117646,
          "f": 0.1590909043491737
        },
        "rouge-2": {
          "r": 0.013605442176870748,
          "p": 0.01694915254237288,
          "f": 0.015094334682522087
        },
        "rouge-l": {
          "r": 0.12962962962962962,
          "p": 0.20588235294117646,
          "f": 0.1590909043491737
        }
      }
    },
    {
      "paper_id": "cs.LG.econ/EM/2502.14131v2",
      "true_abstract": "We study the problem of estimating Dynamic Discrete Choice (DDC) models, also\nknown as offline Maximum Entropy-Regularized Inverse Reinforcement Learning\n(offline MaxEnt-IRL) in machine learning. The objective is to recover reward or\n$Q^*$ functions that govern agent behavior from offline behavior data. In this\npaper, we propose a globally convergent gradient-based method for solving these\nproblems without the restrictive assumption of linearly parameterized rewards.\nThe novelty of our approach lies in introducing the Empirical Risk Minimization\n(ERM) based IRL/DDC framework, which circumvents the need for explicit state\ntransition probability estimation in the Bellman equation. Furthermore, our\nmethod is compatible with non-parametric estimation techniques such as neural\nnetworks. Therefore, the proposed method has the potential to be scaled to\nhigh-dimensional, infinite state spaces. A key theoretical insight underlying\nour approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL)\ncondition -- a property that, while weaker than strong convexity, is sufficient\nto ensure fast global convergence guarantees. Through a series of synthetic\nexperiments, we demonstrate that our approach consistently outperforms\nbenchmark methods and state-of-the-art alternatives.",
      "generated_abstract": "We propose a novel framework to model multi-round auctions with repeated\nbid submissions, where bidders submit bids at multiple times and bidding\nstrategies are sequentially learned. We formulate this problem as a zero-sum\ngame, where the objective is to maximize the expected revenue of the auction\nholders. To model the sequential bidding behavior, we leverage the\nreinforcement learning framework and introduce a bid-reinforcement learning\n(BRL) model to capture the sequential learning behavior of bidders. The BRL\nmodel is formulated as a reinforcement learning problem, and we propose a\ntwo-stage learning procedure for BRL: a pre-learning stage to construct a\nreinforcement learning agent, and a post-learning stage to refine the agent\nparameters by maximizing the expected revenue of the auction holders. We\ndemon",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14074074074074075,
          "p": 0.2638888888888889,
          "f": 0.18357487469019126
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.037037037037037035,
          "f": 0.028776973665960104
        },
        "rouge-l": {
          "r": 0.14074074074074075,
          "p": 0.2638888888888889,
          "f": 0.18357487469019126
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.04040v1",
      "true_abstract": "The fluid antenna system (FAS) has emerged as a disruptive technology for\nfuture wireless networks, offering unprecedented degrees of freedom (DoF)\nthrough the dynamic configuration of antennas in response to propagation\nenvironment variations. The integration of fluid antennas (FAs) with multiuser\nmultiple-input multiple-output (MU-MIMO) networks promises substantial weighted\nsum rate (WSR) gains via joint beamforming and FA position optimization.\nHowever, the joint design is challenging due to the strong coupling between\nbeamforming matrices and antenna positions. To address the challenge, we\npropose a novel block coordinate ascent (BCA)-based method in FA-assisted\nMU-MIMO networks. Specifically, we first employ matrix fractional programming\ntechniques to reformulate the original complex problem into a more tractable\nform. Then, we solve the reformulated problem following the BCA principle,\nwhere we develop a low-complexity majorization maximization algorithm capable\nof optimizing all FA positions simultaneously. To further reduce the\ncomputational, storage, and interconnection costs, we propose a decentralized\nimplementation for our proposed algorithm by utilizing the decentralized\nbaseband processing (DBP) architecture. Simulation results demonstrate that\nwith our proposed algorithm, the FA-assisted MU-MIMO system achieves up to a\n47% WSR improvement over conventional MIMO networks equipped with\nfixed-position antennas. Moreover, the decentralized implementation reduces\ncomputation time by approximately 70% and has similar performance compared with\nthe centralized implementation.",
      "generated_abstract": "The advent of the Internet of Things (IoT) has enabled the development of\nnetworks capable of handling massive amounts of data. In this context, the\ndevelopment of data analytics algorithms is essential to extract information\nfrom this vast data, enabling the analysis of the collected data. However,\ncomputing large data sets, especially for unsupervised learning tasks, is\nchallenging. In this paper, we propose a novel framework that uses the\nspectral properties of the Laplacian matrix to accelerate the execution of\nunsupervised learning algorithms. We introduce a novel spectral clustering\napproach that leverages the eigenvectors of the Laplacian matrix to cluster\nunsupervised data. We evaluate the performance of this approach through\nexperiments on real-world datasets. The results show that the proposed\napproach can improve the speed of the spectral clustering algorithm, making it\nsuitable for large-scale data processing.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15436241610738255,
          "p": 0.2804878048780488,
          "f": 0.19913419455482478
        },
        "rouge-2": {
          "r": 0.024752475247524754,
          "p": 0.04132231404958678,
          "f": 0.03095974763642012
        },
        "rouge-l": {
          "r": 0.14093959731543623,
          "p": 0.25609756097560976,
          "f": 0.18181817723880747
        }
      }
    },
    {
      "paper_id": "nlin.CD.physics/data-an/2503.07582v1",
      "true_abstract": "Small, forested catchments are prototypes of terrestrial ecosystems and have\nbeen studied in several disciplines of environmental sciences since several\ndecades. Time series of water and matter fluxes and nutrient concentrations\nfrom these systems exhibit a bewildering diversity of spatio-temporal patterns,\nindicating the intricate nature of processes acting on a large range of time\nscales. Nonlinear dynamics is an obvious framework to investigate catchment\ntime series. We analyze selected long-term data from three headwater catchments\nin the Bramke valley, Harz mountains, Lower Saxony in Germany at common\nbiweekly resolution for the period 1991 to 2023. For every time series, we\nperform gap filling, detrending and removal of the annual cycle using Singular\nSystem Analysis (SSA), and then calculate metrics based on ordinal pattern\nstatistics: the permutation entropy, permutation complexity and Fisher\ninformation, as well as their generalized versions (q-entropy and\n{\\alpha}-entropy). Further, the position of each variable in Tarnopolski\ndiagrams is displayed and compared to reference stochastic processes, like\nfractional Brownian motion, fractional Gaussian noise, and \\b{eta} noise. Still\nanother way of distinguishing deterministic chaos and structured noise, and\nquantifying the latter, is provided by the complexity from ordinal pattern\npositioned slopes (COPPS). We also construct Horizontal Visibility Graphs and\nestimate the exponent of the decay of the degree distribution. Taken together,\nthe analyses create a characterization of the dynamics of these systems which\ncan be scrutinized for universality, either across variables or between the\nthree geographically very close catchments.",
      "generated_abstract": "We present a novel and comprehensive set of time-dependent simulations of\nthe linear and nonlinear propagation of solitary waves in the presence of a\nnonlinear dissipative term. The simulations are performed in a two-dimensional\nunstable oscillator with a two-dimensional periodic potential, which leads to\ntwo distinct solitary states. We employ a numerical method based on the\nhigh-order time-dependent Fourier method and the time-dependent Hirota\nmethod. In the linear regime, we obtain the exact analytical solutions of the\nwave equations and the exact analytical solutions of the time-dependent\nSchr\\\"odinger equation. In the nonlinear regime, we simulate the propagation\nof the solitary wave and the nonlinear evolution of the solitary state. We\ncalculate the time-dependent nonlinear self-focusing and self-defocusing\nsoliton solutions, and we analyze the effect of the dissipative",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08092485549132948,
          "p": 0.21875,
          "f": 0.1181434559732238
        },
        "rouge-2": {
          "r": 0.012875536480686695,
          "p": 0.02830188679245283,
          "f": 0.017699110745991765
        },
        "rouge-l": {
          "r": 0.08092485549132948,
          "p": 0.21875,
          "f": 0.1181434559732238
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.05481v1",
      "true_abstract": "Large language models (LLMs) often generate inaccurate yet credible-sounding\ncontent, known as hallucinations. This inherent feature of LLMs poses\nsignificant risks, especially in critical domains. I analyze LLMs as a new\nclass of engineering products, treating hallucinations as a product attribute.\nI demonstrate that, in the presence of imperfect awareness of LLM\nhallucinations and misinformation externalities, net welfare improves when the\nmaximum acceptable level of LLM hallucinations is designed to vary with two\ndomain-specific factors: the willingness to pay for reduced LLM hallucinations\nand the marginal damage associated with misinformation.",
      "generated_abstract": "This paper investigates the impact of the COVID-19 pandemic on women's labor\nforce\n  participation (LFP) in Japan. Using a difference-in-differences (DID)\napproach, the study identifies the impact of the pandemic on LFP by\nintroducing a robustness check using a counterfactual scenario. The results\nshow that the pandemic had a significant negative impact on LFP. Moreover, the\nfindings indicate that the impact of the pandemic on LFP is more pronounced\namong low-skilled women than among high-skilled women, suggesting that the\npandemic had a more significant impact on women with lower skills than on\nhigh-skilled women. The findings highlight the need for targeted interventions\nto address the unique labor market challenges faced by women during the\npandemic.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14925373134328357,
          "p": 0.15151515151515152,
          "f": 0.15037593484990688
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11940298507462686,
          "p": 0.12121212121212122,
          "f": 0.12030074687998212
        }
      }
    },
    {
      "paper_id": "math.DS.q-bio/MN/2409.12487v2",
      "true_abstract": "We present a systematic procedure for testing whether reaction networks\nexhibit non-expansivity or monotonicity. This procedure identifies explicit\nnorms under which a network is non-expansive or cones for which the system is\nmonotone-or provides proof that no such structures exist. Our approach\nreproduces known results, generates novel findings, and demonstrates that\ncertain reaction networks cannot exhibit monotonicity or non-expansivity with\nrespect to any cone or norm. Additionally, we establish a duality relationship\nwhich states that if a network is monotone, so is its dual network.",
      "generated_abstract": "The mathematical study of bifurcation in systems of ordinary differential\nequations (ODEs) has developed rapidly in recent years, with an emphasis on\nsystems with a high number of parameters. This is due to the fact that these\nsystems are easier to model than those with a small number of parameters. In\nthis paper, we focus on the study of bifurcations in systems of ODEs with a\nvariable number of parameters, where the number of parameters is not necessarily\nfixed, and we analyze the existence of limit cycles and saddle-center bifurcations.\n  We show that when the system has a finite number of parameters, the\nbifurcation theory is similar to that of systems with a small number of\nparameters, and the bifurcation types and the existence of limit cycles and\nsaddle-center bifurcations are the same. However, when the number of parameters\nin",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1746031746031746,
          "p": 0.16417910447761194,
          "f": 0.16923076423550312
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.009259259259259259,
          "f": 0.010582005684053663
        },
        "rouge-l": {
          "r": 0.15873015873015872,
          "p": 0.14925373134328357,
          "f": 0.15384614885088774
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.07809v1",
      "true_abstract": "For a permutation $w$ in the symmetric group $\\mathfrak{S}_{n}$, let $L(w)$\ndenote the simple highest weight module in the principal block of the BGG\ncategory $\\mathcal{O}$ for the Lie algebra $\\mathfrak{sl}_{n}(\\mathbb{C})$. We\nfirst prove that $L(w)$ is Kostant negative whenever $w$ consecutively contains\ncertain patterns. We then provide a complete answer to Kostant's problem in\ntype $A_{6}$ and show that the indecomposability conjecture also holds in type\n$A_{6}$, that is, applying an indecomposable projective functor to a simple\nmodule outputs either an indecomposable module or zero.",
      "generated_abstract": "We introduce a new notion of $K$-theory for a compact connected\nmanifold $M$ and prove that it coincides with the usual $K$-theory for the\nclassifying space of the principal $G$-bundle on $M$. The basic idea is that\nthe group $\\pi_1(M)$ acts on the homotopy category of $C^\\infty$\n$K$-theory sheaves on $M$ by conjugation, and thus the latter is equivalent to\nthe usual $K$-theory for the classifying space of the principal $G$-bundle on\n$M$ by the group-theoretic arguments.\n  We apply our result to the fundamental group of a compact connected\npseudotoroidal affine variety $Y$ and prove that the $K$-theory of its\nhomotopy category is isomorphic to the $K$-theory of its universal cover\n$\\widetilde{Y}$ by an $R$-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21875,
          "p": 0.23728813559322035,
          "f": 0.2276422714310266
        },
        "rouge-2": {
          "r": 0.06097560975609756,
          "p": 0.056818181818181816,
          "f": 0.0588235244179935
        },
        "rouge-l": {
          "r": 0.203125,
          "p": 0.22033898305084745,
          "f": 0.21138210882940062
        }
      }
    },
    {
      "paper_id": "cs.CL.stat/ME/2503.04910v1",
      "true_abstract": "The emergence of powerful LLMs has led to a paradigm shift in Natural\nLanguage Understanding and Natural Language Generation. The properties that\nmake LLMs so valuable for these tasks -- creativity, ability to produce fluent\nspeech, and ability to quickly and effectively abstract information from large\ncorpora -- also present new challenges to evaluating their outputs. The rush to\nmarket has led teams to fall back on quick, cost-effective automatic\nevaluations which offer value, but do not obviate the need for human judgments\nin model training and evaluation. This paper argues that in cases in which end\nusers need to agree with the decisions made by ML models -- e.g. in toxicity\ndetection or extraction of main points for summarization -- models should be\ntrained and evaluated on data that represent the preferences of those users. We\nsupport this argument by explicating the role of human feedback in labeling and\njudgment tasks for model training and evaluation. First, we propose methods for\ndisentangling noise from signal in labeling tasks. Then we show that noise in\nlabeling disagreement can be minimized by adhering to proven methodological\nbest practices, while signal can be maximized to play an integral role in model\ntraining and evaluation tasks. Finally, we illustrate best practices by\nproviding a case study in which two guardrails classifiers are evaluated using\nhuman judgments to align final model behavior to user preferences. We aim for\nthis paper to provide researchers and professionals with guidelines to\nintegrating human judgments into their ML and generative AI evaluation toolkit,\nparticularly when working toward achieving accurate and unbiased features that\nalign with users' needs and expectations.",
      "generated_abstract": "This paper introduces the first large-scale dataset of human-to-human\ntranslation for Arabic-English and English-Arabic. The dataset consists of 100,000\ntranslation pairs, each containing a translation and the corresponding\ntransliterated Arabic text. The dataset is constructed by automatically\nannotating transcripts for a collection of video-recorded conversations. The\ndataset is publicly available at https://github.com/Saber-Arabic-Language/ARHIT.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05421686746987952,
          "p": 0.23076923076923078,
          "f": 0.08780487496775741
        },
        "rouge-2": {
          "r": 0.00392156862745098,
          "p": 0.02040816326530612,
          "f": 0.006578944664344948
        },
        "rouge-l": {
          "r": 0.05421686746987952,
          "p": 0.23076923076923078,
          "f": 0.08780487496775741
        }
      }
    },
    {
      "paper_id": "cs.AI.q-fin/CP/2501.12399v1",
      "true_abstract": "Current financial Large Language Models (LLMs) struggle with two critical\nlimitations: a lack of depth in stock analysis, which impedes their ability to\ngenerate professional-grade insights, and the absence of objective evaluation\nmetrics to assess the quality of stock analysis reports. To address these\nchallenges, this paper introduces FinSphere, a conversational stock analysis\nagent, along with three major contributions: (1) Stocksis, a dataset curated by\nindustry experts to enhance LLMs' stock analysis capabilities, (2) AnalyScore,\na systematic evaluation framework for assessing stock analysis quality, and (3)\nFinSphere, an AI agent that can generate high-quality stock analysis reports in\nresponse to user queries. Experiments demonstrate that FinSphere achieves\nsuperior performance compared to both general and domain-specific LLMs, as well\nas existing agent-based systems, even when they are enhanced with real-time\ndata access and few-shot guidance. The integrated framework, which combines\nreal-time data feeds, quantitative tools, and an instruction-tuned LLM, yields\nsubstantial improvements in both analytical quality and practical applicability\nfor real-world stock analysis.",
      "generated_abstract": "This paper introduces a novel framework to leverage the insights of\naccelerator performance models for the development of risk models. By\nidentifying a common foundation for the acceleration of deep learning\napplications, we propose a framework for the development of risk models that\ncan be leveraged to develop more accurate risk models for the acceleration of\ndeep learning applications. We provide a framework for the development of risk\nmodels, which is based on the performance of accelerator models. In this\nframework, we propose a risk model for the acceleration of deep learning\napplications, which includes the risk model for the acceleration of deep\nlearning models, which is derived from the performance of accelerator models.\nWe demonstrate the effectiveness of the proposed framework through a case study\nof the development of a risk model for the acceleration of deep learning\napplications, which is based on the performance of accelerator models. This\nframework can be applied to the development of other risk models",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12605042016806722,
          "p": 0.2631578947368421,
          "f": 0.17045454107502594
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.031914893617021274,
          "f": 0.02419354367976158
        },
        "rouge-l": {
          "r": 0.12605042016806722,
          "p": 0.2631578947368421,
          "f": 0.17045454107502594
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.15945v1",
      "true_abstract": "A unified framework is provided for analysing check-all-that-apply (CATA)\nproduct data following the ``one citation, one vote\" principle. CATA data arise\nfrom studies where A consumers evaluate P products by describing samples by\nchecking all of the T terms that apply. Giving every citation the same weight,\nregardless of the assessor, product, or term, leads to analyses based on the L1\nnorm where the median absolute deviation is the measure of dispersion. Five\npermutation tests are proposed to answer the following questions. Do any\nproducts differ? For which terms do products differ? Within each of the terms,\nwhich products differ? Which product pairs differ? On which terms does each\nproduct pair differ? Additionally, we show how products and terms can be\nclustered following the ``one citation, one vote\" principle and how L1-norm\nprincipal component analysis (L1-norm PCA) can be applied to visualize CATA\nresults in few dimensions. Together, the permutation tests, clustering methods,\nand L1-norm PCA provide a unified approach. The proposed methods are\nillustrated using a data set in which 100 consumers evaluated 11 products using\n34 CATA terms.R code is provided to perform the analyses.",
      "generated_abstract": "The recent success of Large Language Models (LLMs) in natural language\nprocesses has inspired research in the field of Natural Language Generation\n(NLG). However, the current state of the art in LLM-based NLG often focuses on\nspeech-to-text (S2T) generation, neglecting the important role of text-to-speech\n(T2S) generation. This gap is largely due to the lack of a standardized\ninterpretation of the T2S generation task, which poses challenges for the\ndevelopment of robust evaluation protocols. To address this gap, we propose a\nnew benchmark for T2S generation, dubbed T2S-Gen, designed to measure the\nperformance of LLMs on both S2T and T2S tasks. We validate our approach on the\nOpenAI-GPT-3 model, demonstrating its effectiveness in T2S",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11666666666666667,
          "p": 0.16470588235294117,
          "f": 0.1365853609994053
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.017857142857142856,
          "f": 0.013986009220990425
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.16470588235294117,
          "f": 0.1365853609994053
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.06404v1",
      "true_abstract": "Reinsurance optimization is critical for insurers to manage risk exposure,\nensure financial stability, and maintain solvency. Traditional approaches often\nstruggle with dynamic claim distributions, high-dimensional constraints, and\nevolving market conditions. This paper introduces a novel hybrid framework that\nintegrates {Generative Models}, specifically Variational Autoencoders (VAEs),\nwith {Reinforcement Learning (RL)} using Proximal Policy Optimization (PPO).\nThe framework enables dynamic and scalable optimization of reinsurance\nstrategies by combining the generative modeling of complex claim distributions\nwith the adaptive decision-making capabilities of reinforcement learning.\n  The VAE component generates synthetic claims, including rare and catastrophic\nevents, addressing data scarcity and variability, while the PPO algorithm\ndynamically adjusts reinsurance parameters to maximize surplus and minimize\nruin probability. The framework's performance is validated through extensive\nexperiments, including out-of-sample testing, stress-testing scenarios (e.g.,\npandemic impacts, catastrophic events), and scalability analysis across\nportfolio sizes. Results demonstrate its superior adaptability, scalability,\nand robustness compared to traditional optimization techniques, achieving\nhigher final surpluses and computational efficiency.\n  Key contributions include the development of a hybrid approach for\nhigh-dimensional optimization, dynamic reinsurance parameterization, and\nvalidation against stochastic claim distributions. The proposed framework\noffers a transformative solution for modern reinsurance challenges, with\npotential applications in multi-line insurance operations, catastrophe\nmodeling, and risk-sharing strategy design.",
      "generated_abstract": "We develop a method for estimating the mean and covariance of a heteroskedastic\ndistribution with known mean and covariance. Our estimator uses a kernel\nregression approach to estimate the mean and covariance of a heteroskedastic\ndistribution, and it can handle any finite-sample estimator of the mean and\ncovariance of the original distribution. We also derive an efficient estimator\nfor the mean of the distribution and a fast estimator for the covariance. Our\nmethodology extends existing methods for estimating the mean and covariance\nof heteroskedastic distributions, particularly those that are based on kernel\nregression. Our estimator is computationally efficient and can handle any\nfinite-sample estimator of the mean and covariance of the original distribution.\n  We illustrate the use of our method by applying it to simulated data and to\ndata from the Housing Price Index.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08227848101265822,
          "p": 0.22033898305084745,
          "f": 0.11981566424345402
        },
        "rouge-2": {
          "r": 0.004975124378109453,
          "p": 0.010752688172043012,
          "f": 0.00680271676315699
        },
        "rouge-l": {
          "r": 0.056962025316455694,
          "p": 0.15254237288135594,
          "f": 0.08294930479644946
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2502.16088v1",
      "true_abstract": "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
      "generated_abstract": "We present a new approach to the estimation of the effective number of\nspecies in a single-cell population, based on the use of the so-called\n\"independent component analysis\" (ICA) method. This method allows for the\nestimation of the number of species of a given population, through the\nestimation of the number of clusters in the data. ICA is a powerful tool in\nbiological applications, as it can be used to analyze multivariate data, such as\nthe data of single cells, and can be applied to a large number of problems in\nbioinformatics, ranging from the analysis of transcriptomic data to the\ncomparative genomics of genomes. In this study, we apply ICA to the analysis of\ntranscriptomic data from a single cell. We propose a novel methodology to\nanalyze the data, combining ICA with a genome-wide association study (GWAS) to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12781954887218044,
          "p": 0.22666666666666666,
          "f": 0.16346153385031448
        },
        "rouge-2": {
          "r": 0.005494505494505495,
          "p": 0.008547008547008548,
          "f": 0.006688958447000629
        },
        "rouge-l": {
          "r": 0.09774436090225563,
          "p": 0.17333333333333334,
          "f": 0.12499999538877606
        }
      }
    },
    {
      "paper_id": "eess.SY.q-fin/PM/2412.07688v1",
      "true_abstract": "Given the vital role that smart meter data could play in handling uncertainty\nin energy markets, data markets have been proposed as a means to enable\nincreased data access. However, most extant literature considers energy markets\nand data markets separately, which ignores the interdependence between them. In\naddition, existing data market frameworks rely on a trusted entity to clear the\nmarket. This paper proposes a joint energy and data market focusing on the\nday-ahead retailer energy procurement problem with uncertain demand. The\nretailer can purchase differentially-private smart meter data from consumers to\nreduce uncertainty. The problem is modelled as an integrated forecasting and\noptimisation problem providing a means of valuing data directly rather than\nvaluing forecasts or forecast accuracy. Value is determined by the Wasserstein\ndistance, enabling privacy to be preserved during the valuation and procurement\nprocess. The value of joint energy and data clearing is highlighted through\nnumerical case studies using both synthetic and real smart meter data.",
      "generated_abstract": "This paper introduces a novel multi-agent reinforcement learning (MARL)\nmethod to solve the dynamic portfolio allocation problem with multiple agents.\nThe proposed method is based on a multi-objective reinforcement learning (MORL)\nframework. The MORL agent is designed to optimize the portfolio's expected\nreturn by jointly optimizing the allocation of risk and return. To tackle the\nhigh computational complexity in the conventional MORL framework, we propose a\ntwo-stage MORL algorithm. In the first stage, we introduce an efficient\nreinforcement learning-based algorithm for the allocation of risk. In the\nsecond stage, we develop a new MORL algorithm for the allocation of return. We\nevaluate the performance of the proposed method by simulating a risk-return\nportfolio allocation problem with three types of agents: one risk-seeking\nagent, one risk-averse agent,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16037735849056603,
          "p": 0.2236842105263158,
          "f": 0.1868131819490401
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.02727272727272727,
          "f": 0.02334629860558173
        },
        "rouge-l": {
          "r": 0.1509433962264151,
          "p": 0.21052631578947367,
          "f": 0.17582417096002914
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/GN/2502.01311v1",
      "true_abstract": "Transcription factors are proteins that regulate the expression of genes by\nbinding to specific genomic regions known as Transcription Factor Binding Sites\n(TFBSs), typically located in the promoter regions of those genes. Accurate\nprediction of these binding sites is essential for understanding the complex\ngene regulatory networks underlying various cellular functions. In this regard,\nmany deep learning models have been developed for such prediction, but there is\nstill scope of improvement. In this work, we have developed a deep learning\nmodel which uses pre-trained DNABERT, a Convolutional Neural Network (CNN)\nmodule, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale\nConvolutions with Attention (MSCA) module and an output module. The pre-trained\nDNABERT is used for sequence embedding, thereby capturing the long-term\ndependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are\nuseful in extracting higher-order local features. TFBS-Finder is trained and\ntested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies\nas well as cross-cell line validations and comparisons with other models. The\nexperimental results show the superiority of the proposed method in predicting\nTFBSs compared to the existing methodologies. The codes and the relevant\ndatasets are publicly available at\nhttps://github.com/NimishaGhosh/TFBS-Finder/.",
      "generated_abstract": "The concept of the gene as an individual unit of heredity was introduced by\nChinese biologist Linus Pauling in the 1940s. Pauling argued that a gene is a\nunit of heredity, and a gene is a unit of inheritance. In the past 70 years,\nresearch on the genetics of gene expression has made progress in understanding\nhow genes are expressed in cells. A major advance in this area is the\nidentification of the first gene expressed in the nucleus of a cell, known as\nnucleocytoplasmic transcription factor 1 (NTF1), which was identified in 2012.\nThe function of NTF1 is unknown, and its expression is not regulated by the\ngene expression regulators (TERs) that are known to regulate the expression of\nother genes. To address these issues, this paper introduces a new concept",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1736111111111111,
          "p": 0.31645569620253167,
          "f": 0.2242152420615738
        },
        "rouge-2": {
          "r": 0.031088082901554404,
          "p": 0.05,
          "f": 0.03833865341893923
        },
        "rouge-l": {
          "r": 0.1527777777777778,
          "p": 0.27848101265822783,
          "f": 0.1973094124651613
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/CC/2503.05062v1",
      "true_abstract": "Despite of tremendous research on decoding Reed-Solomon (RS) and algebraic\ngeometry (AG) codes under the random and adversary substitution error models,\nfew studies have explored these codes under the burst substitution error model.\nBurst errors are prevalent in many communication channels, such as wireless\nnetworks, magnetic recording systems, and flash memory. Compared to random and\nadversarial errors, burst errors often allow for the design of more efficient\ndecoding algorithms. However, achieving both an optimal decoding radius and\nquasi-linear time complexity for burst error correction remains a significant\nchallenge. The goal of this paper is to design (both list and probabilistic\nunique) decoding algorithms for RS and AG codes that achieve the Singleton\nbound for decoding radius while maintaining quasi-linear time complexity.\n  Our idea is to build a one-to-one correspondence between AG codes (including\nRS codes) and interleaved RS codes with shorter code lengths (or even constant\nlengths). By decoding the interleaved RS codes with burst errors, we derive\nefficient decoding algorithms for RS and AG codes. For decoding interleaved RS\ncodes with shorter code lengths, we can employ either the naive methods or\nexisting algorithms. This one-to-one correspondence is constructed using the\ngeneralized fast Fourier transform (G-FFT) proposed by Li and Xing (SODA 2024).\nThe G-FFT generalizes the divide-and-conquer technique from polynomials to\nalgebraic function fields. More precisely speaking, assume that our AG code is\ndefined over a function field $E$ which has a sequence of subfields\n$\\mathbb{F}_q(x)=E_r\\subseteq E_{r-1}\\subseteq \\cdots\\subset E_1\\subseteq\nE_0=E$ such that $E_{i-1}/E_i$ are Galois extensions for $1\\le i\\le r$. Then\nthe AG code based on $E$ can be transformed into an interleaved RS code over\nthe rational function field $\\mathbb{F}_q(x)$.",
      "generated_abstract": "Recent advancements in deep learning and large language models (LLMs) have\ndemonstrated their ability to generate high-quality textual descriptions of\nimages. However, this ability to generate descriptions also raises ethical\nconcerns, especially in domains where the generated textual descriptions may\nseriously harm individuals or infringe upon privacy. To address these issues,\nthis paper introduces a novel approach to LLM-based image captioning, where we\nintegrate a privacy-preserving framework with a generative adversarial\nnetwork (GAN) to generate captions that adhere to privacy requirements while\nretaining the ability to convey meaningful information about the images.\nSpecifically, we first encode each image into a latent vector using an\nunsupervised GAN, and then use a transformer-based GAN-based GPT-3.5 model to\ngenerate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.23595505617977527,
          "f": 0.16342411998607106
        },
        "rouge-2": {
          "r": 0.00411522633744856,
          "p": 0.00909090909090909,
          "f": 0.005665718089386841
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.23595505617977527,
          "f": 0.16342411998607106
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.09740v1",
      "true_abstract": "In this paper, we prove a KAM theorem in a-posteriori format, using the\nparameterization method to look invariant tori in non-autonomous Hamiltonian\nsystems with $n$ degrees of freedom that depend periodically or\nquasi-periodically (QP) on time, with $\\ell$ external frequencies. Such a\nsystem is described by a Hamiltonian function in the $2n$-dimensional phase\nspace, $\\mathscr{M}$, that depends also on $\\ell$ angles, $\\varphi\\in\n\\mathbb{T}^\\ell$. We take advantage of the fibbered structure of the extended\nphase space $\\mathscr{M} \\times \\mathbb{T}^\\ell$. As a result of our approach,\nthe parameterization of tori requires the last $\\ell$ variables, to be precise\n$\\varphi$, while the first $2n$ components are determined by an invariance\nequation. This reduction decreases the dimension of the problem where the\nunknown is a parameterization from $2(n+\\ell)$ to $2n$. We employ a\nquasi-Newton method, in order to prove the KAM theorem. This iterative method\nbegins with an initial parameterization of an approximately invariant torus,\nmeaning it approximately satisfies the invariance equation. The approximation\nis refined by applying corrections that reduce quadratically the invariance\nequation error. This process converges to a torus in a complex strip of size\n$\\rho_\\infty$, provided suitable Diophantine $(\\gamma,\\tau)$ conditions and a\nnon-degeneracy condition on the torsion are met. Given the nature of the proof,\nthis provides a numerical method that can be effectively implemented on a\ncomputer, the details are given in the companion paper [CHP25]. This approach\nleverages precision and efficiency to compute invariant tori.",
      "generated_abstract": "In this paper, we construct a family of hyperbolic 3-manifolds $M$ with\nsuch that $M$ is hyperbolic, $B^2$ is $3$-connected, and $M$ is not\n$2$-surgery equivalent to a Seifert fibered space. Such a $3$-manifold is\ncalled a $3$-surgery equivalent. We construct a family of $3$-surgery equivalent\n$3$-manifolds by gluing $B^2$ along some essential curves. Furthermore, we\nconstruct a family of $3$-surgery equivalent $3$-manifolds by gluing $B^2$ along\nsome essential curves, and then gluing $B^2$ along some essential curves.\nFinally, we construct a family of $3$-surgery equivalent $3$-manifolds by\ngluing $B^2$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10273972602739725,
          "p": 0.35714285714285715,
          "f": 0.15957446461521055
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.08928571428571429,
          "f": 0.035587185420650994
        },
        "rouge-l": {
          "r": 0.10273972602739725,
          "p": 0.35714285714285715,
          "f": 0.15957446461521055
        }
      }
    },
    {
      "paper_id": "math.RA.math/GR/2503.06221v1",
      "true_abstract": "Let $\\mathbf{O}(\\mathbb{F})$ be the split octonion algebra over an\nalgebraically closed field $\\mathbb{F}$. For positive integers $k_1, k_2\\geq\n2$, we study surjectivity of the map $A_1(x^{k_1}) + A_2(y^{k_2}) \\in\n\\mathbf{O}(\\mathbb{F})\\langle x, y\\rangle$ on $\\mathbf{O}(\\mathbb{F})$. For\nthis, we use the orbit representatives of the ${G}_2(\\mathbb{F})$-action on\n$\\mathbf{O}(\\mathbb{F}) \\times \\mathbf{O}(\\mathbb{F}) $ for the tuple $(A_1,\nA_2)$, and characterize the ones which give a surjective map.",
      "generated_abstract": "In this paper, we study a particular class of $n$-dimensional generalised\nhorospherical manifolds. These are generalised horospherical manifolds with\nfixed volume and fixed first fundamental form, and we show that they are\nequivalent to the $n$-dimensional generalised horospherical manifolds with\nconstant volume and constant first fundamental form. We also study the\ngeometrical properties of these manifolds. We give a complete classification of\nthe $n$-dimensional generalised horospherical manifolds with constant volume\nand constant first fundamental form. We also give a complete classification of\nthe $n$-dimensional generalised horospherical manifolds with fixed volume and\nfixed second fundamental form.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1346153846153846,
          "p": 0.1794871794871795,
          "f": 0.15384614894819482
        },
        "rouge-2": {
          "r": 0.04918032786885246,
          "p": 0.05263157894736842,
          "f": 0.05084745263286461
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.15384615384615385,
          "f": 0.13186812697017286
        }
      }
    },
    {
      "paper_id": "physics.optics.physics/app-ph/2503.09048v1",
      "true_abstract": "In recent years, twisting has emerged as a new degree of freedom that plays\nan increasingly important role in Bloch bands of various physical systems.\nHowever, there is currently a lack of reports on the non-trivial physics of\ntopological degeneracy in twisted systems. In this work, we investigated the\nintrinsic physical correlation between twisting and topological degeneracy. We\nfound that twisting not only breaks the symmetry of the system but also\nintroduces topological degeneracy that does not exist under the original\nsymmetric system without twisting. Furthermore, the topological degeneracy can\nbe easily tuned through twisting. This new twist-induced topological degeneracy\ngives rise to a unique polarization-degenerate birefringent medium, wherein the\ntwist angle acts as a novel degree of freedom for dispersion and polarization\nmanagement of interface states. Exhibiting fascinating properties and\nexperimental feasibilities, our work points to new possibilities in the\nresearch of various topological physics in twisted photonics.",
      "generated_abstract": "We present a new method for the study of the scattering properties of\ntransparent, polarizable materials with a simple, robust, and highly\ncomprehensive approach. Our method is based on the use of a laser beam, which\nis scattered by the material, and is detected by a camera. The scattered light\nis analyzed using an image processing algorithm. By varying the polarization\nof the laser beam, we obtain a comprehensive picture of the scattering\nproperties of the material. This approach provides a powerful tool for the\nstudy of the scattering properties of polarizable materials, and for the\nmeasurement of their refractive index and polarization.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2413793103448276,
          "f": 0.17948717481591073
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.04597701149425287,
          "f": 0.03539822535319979
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2413793103448276,
          "f": 0.17948717481591073
        }
      }
    },
    {
      "paper_id": "math.CO.math/CT/2503.06722v1",
      "true_abstract": "In this paper we explore the algebraic structure and combinatorial properties\nof eulerian magnitude homology. First, we analyze the diagonality conditions of\neulerian magnitude homology, providing a characterization of complete graphs.\nThen, we construct the regular magnitude-path spectral sequence as the spectral\nsequence of the (filtered) injective nerve of the reachability category, and\nexplore its consequences. Among others, we show that such spectral sequence\nconverges to the complex of injective words on a digraph, and yields\ncharacterization results for the regular path homology of diagonal directed\ngraphs.",
      "generated_abstract": "We give a characterisation of the isomorphism classes of complete,\nlinearly-graded commutative Hopf algebras over a field of characteristic\n$0$ or $p$ using the theory of $\\mathbb{F}$-graded Lie algebras and their\nHopf algebras. This extends a result of Rota and Vitoria to the case where the\nfield of fractions is a field of characteristic $p$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08928571428571429,
          "p": 0.14285714285714285,
          "f": 0.10989010515638228
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.04081632653061224,
          "f": 0.031007747226729877
        },
        "rouge-l": {
          "r": 0.08928571428571429,
          "p": 0.14285714285714285,
          "f": 0.10989010515638228
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.physics/soc-ph/2503.05380v1",
      "true_abstract": "This paper presents a quantitative comparison of power grid reinforcement\nstrategies. We evaluate three approaches: (1) doubling transmission links\n(bridges) between different communities, (2) adding bypasses around weakly\nsynchronized nodes, and (3) reinforcing edges that trigger the largest cascade\nfailures. We use two different models of the Hungarian high-voltage network.\nThese models are built from the official data provided by the transmission\nsystem operator, thus eliminating the assumptions typically used in other\nstudies. The coupling strength distribution of the Hungarian models shows good\nagreement with our previous works using the European and North American grids.\n  Additionally, we examine the occurrence of Braess' paradox, where added\ntransmission capacity unexpectedly reduces overall stability. Our results show\nthat reinforcement through community-based bridge duplication yields the most\nsignificant improvements across all parameters. A visual comparison highlights\ndifferences between this method and traditional reinforcement approaches. To\nthe authors' knowledge, this is the first attempt to quantitatively compare\nresults of oscillator-based studies with those relying on power system analysis\nsoftware.\n  Characteristic results of line-cut simulations reveal cascade size\ndistributions with fat-tailed decays for medium coupling strengths, while\nexponential behavior emerges for small and large couplings. The observed\nexponents are reminiscent of the continuously changing exponents due to\nGriffiths effects near a hybrid type of phase transition.",
      "generated_abstract": "The emergence of cooperative behavior in social systems, such as cooperation,\ncollaboration, and coordination, is ubiquitous across various domains, from\nthe biological to the technological. While these emergent behaviors have\nlong been studied from a single-agent perspective, recent advances in artificial\nintelligence have allowed for the study of cooperative behavior in multi-agent\nsystems. However, despite the potential of multi-agent systems to foster\ncooperative behavior, the mechanisms underlying such behavior remain\nunderexplored. This study explores the role of communication in shaping\ncooperative behavior in multi-agent systems. We develop a theory based on the\ncommunication-induced information leakage model, which accounts for the\neffects of communication on information leakage, information sharing, and\ncooperation. Our theory highlights the importance of communication in shaping\ncooperative behavior",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11180124223602485,
          "p": 0.23684210526315788,
          "f": 0.15189872982036368
        },
        "rouge-2": {
          "r": 0.004878048780487805,
          "p": 0.009523809523809525,
          "f": 0.006451608423520281
        },
        "rouge-l": {
          "r": 0.11180124223602485,
          "p": 0.23684210526315788,
          "f": 0.15189872982036368
        }
      }
    },
    {
      "paper_id": "math.CO.cs/CG/2503.09919v1",
      "true_abstract": "We provide a family of $5$-dimensional prismatoids whose width grows linearly\nin the number of vertices. This provides a new infinite family of\ncounter-examples to the Hirsch conjecture whose excess width grows linearly in\nthe number of vertices, and answers a question of Matschke, Santos and Weibel.",
      "generated_abstract": "We introduce a new construction of strongly connected components in\ngraphs and prove that it coincides with the classical one in finite graphs.\nWe also establish the connection between strongly connected components and\nstrongly connected components of the underlying graph and the graph of groups.\nOur construction allows to handle infinite graphs with arbitrary topological\ndimension, and it yields new algorithms for finding strongly connected\ncomponents of any graph. Our methods are applicable to both undirected and\ndirected graphs, as well as to graphs with holes and graphs with multiple\nedges. We also propose a simple way to compute strongly connected components\nof any graph with an arbitrary number of edges and without using a graph\nalgebra.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3225806451612903,
          "p": 0.16129032258064516,
          "f": 0.21505375899641585
        },
        "rouge-2": {
          "r": 0.05128205128205128,
          "p": 0.019801980198019802,
          "f": 0.02857142455204138
        },
        "rouge-l": {
          "r": 0.2903225806451613,
          "p": 0.14516129032258066,
          "f": 0.19354838265232985
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.physics/ao-ph/2503.03009v1",
      "true_abstract": "Wave breaking is a critical process in the upper ocean: an energy sink for\nthe surface wave field and a source for turbulence in the ocean surface\nboundary layer. We apply a novel multi-layer numerical solver resolving\nupper-ocean dynamics over scales from O(50cm) to O(1km), including a\nbroad-banded wave field and wave breaking. The present numerical study isolates\nthe effect of wave breaking and allows us to study the surface layer in\nwave-influenced and wave-breaking-dominated regimes. Following our previous\nwork showing wave breaking statistics in agreement with field observations, we\nextend the analysis to underwater breaking-induced turbulence and related\ndissipation (in freely decaying conditions). We observe a rich field of\nvorticity resulting from the turbulence generation by breaking waves. We\ndiscuss the vertical profiles of dissipation rate which are compared with field\nobservations, and propose an empirical universal shape function. Good agreement\nis found, further demonstrating that wave breaking can dominate turbulence\ngeneration in the near-surface layer. We examine the dissipation from different\nangles: the global dissipation of the wave field computed from the decaying\nwave field, the spectral dissipation from the fifth moment of breaking front\ndistribution, and a turbulence dissipation estimated from the underwater strain\nrate tensor. Finally, we consider how these different estimates can be\nunderstood as part of a coherent framework.",
      "generated_abstract": "The coupling between the flow of a viscous, compressible, and viscous\ndissipative fluid and the evolution of a vortex sheet is investigated using a\nsemi-analytical approach. The governing equations are derived from the\nEinstein-Smoluchowski-Prelog (ESP) theory of turbulence. The turbulent\nfluid-flow coupling is described by a Reynolds-averaged Navier-Stokes (RANS)\nequation, which is solved using a numerical scheme. The vortex-sheet evolution\nis described by the Stokes equation, and is treated using a numerical scheme.\nThe vortex sheet is a steady state vortex with no vortex lines and a constant\ndensity. The vortex sheet is assumed to be inviscid and unsteady. The\nsemi-analytical solution describes the evolution of the vortex sheet in terms\nof",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13008130081300814,
          "p": 0.26229508196721313,
          "f": 0.1739130390459595
        },
        "rouge-2": {
          "r": 0.025510204081632654,
          "p": 0.05263157894736842,
          "f": 0.03436425677070474
        },
        "rouge-l": {
          "r": 0.12195121951219512,
          "p": 0.2459016393442623,
          "f": 0.16304347382856818
        }
      }
    },
    {
      "paper_id": "stat.ML.cs/SI/2503.09660v1",
      "true_abstract": "Point signatures based on the Laplacian operators on graphs, point clouds,\nand manifolds have become popular tools in machine learning for graphs,\nclustering, and shape analysis. In this work, we propose a novel point\nsignature, the power spectrum signature, a measure on $\\mathbb{R}$ defined as\nthe squared graph Fourier transform of a graph signal. Unlike eigenvectors of\nthe Laplacian from which it is derived, the power spectrum signature is\ninvariant under graph automorphisms. We show that the power spectrum signature\nis stable under perturbations of the input graph with respect to the\nWasserstein metric. We focus on the signature applied to classes of indicator\nfunctions, and its applications to generating descriptive features for vertices\nof graphs. To demonstrate the practical value of our signature, we showcase\nseveral applications in characterizing geometry and symmetries in point cloud\ndata, and graph regression problems.",
      "generated_abstract": "This paper considers the problem of learning a classifier from a dataset of\n$n$ samples from a distribution $\\mathcal{P}$ over a high-dimensional\nspace. We study the problem of estimating the true support of $\\mathcal{P}$\nfrom the dataset and propose a class of estimators that can be interpreted as\nestimators of the support of the true classifier. We show that the class of\nestimators achieves the same statistical power as the optimal classifier,\nprovided that the number of samples is large enough. We also propose a\nconstruction of estimators for which the sample complexity is polynomial in\nthe dimension $d$ of the space and in the number of samples. The\nconstruction is based on the use of a new class of estimators that are\nparameterized by the support of the classifier. We show that the support of\nthe classifier is unknown and, therefore, the optimal classifier is unknown",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1978021978021978,
          "p": 0.27692307692307694,
          "f": 0.23076922590811977
        },
        "rouge-2": {
          "r": 0.06870229007633588,
          "p": 0.07964601769911504,
          "f": 0.07377048683048945
        },
        "rouge-l": {
          "r": 0.18681318681318682,
          "p": 0.26153846153846155,
          "f": 0.21794871308760697
        }
      }
    },
    {
      "paper_id": "math.ST.q-bio/PE/2501.16526v1",
      "true_abstract": "Ancestral inference for branching processes in random environments involves\ndetermining the ancestor distribution parameters using the population sizes of\ndescendant generations. In this paper, we introduce a new methodology for\nancestral inference utilizing the generalized method of moments. We demonstrate\nthat the estimator's behavior is critically influenced by the coefficient of\nvariation of the environment sequence. Furthermore, despite the process's\nevolution being heavily dependent on the offspring means of various\ngenerations, we show that the joint limiting distribution of the ancestor and\noffspring estimators of the mean, under appropriate centering and scaling,\ndecouple and converge to independent Gaussian random variables when the ratio\nof the number of generations to the logarithm of the number of replicates\nconverges to zero. Additionally, we provide estimators for the limiting\nvariance and illustrate our findings through numerical experiments and data\nfrom Polymerase Chain Reaction experiments and COVID-19 data.",
      "generated_abstract": "We present an algorithm to compute the maximum of a sum of $n$ independent\nrandom variables over a compact metric space $X$. Our algorithm is based on a\nrandomized version of the maximum of a sum of independent random variables. It\nis shown that the expected time to compute the maximum of a sum of $n$\nindependent random variables over a compact metric space is $O(n\\log n)$. The\nproof is based on the fact that the expected number of iterations of the\nalgorithm is $O(n)$. Our approach is inspired by the work of\nSzymanowski--Wolff--Wagner--Zamora--Zambrano--Yan (SWZZY).",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13541666666666666,
          "p": 0.28888888888888886,
          "f": 0.1843971587747096
        },
        "rouge-2": {
          "r": 0.04477611940298507,
          "p": 0.09230769230769231,
          "f": 0.06030150313880995
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.26666666666666666,
          "f": 0.17021276161158908
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2503.07203v1",
      "true_abstract": "Network pharmacology (NP) explores pharmacological mechanisms through\nbiological networks. Multi-omics data enable multi-layer network construction\nunder diverse conditions, requiring integration into NP analyses. We developed\nPOINT, a novel NP platform enhanced by multi-omics biological networks,\nadvanced algorithms, and knowledge graphs (KGs) featuring network-based and\nKG-based analytical functions. In the network-based analysis, users can perform\nNP studies flexibly using 1,158 multi-omics biological networks encompassing\nproteins, transcription factors, and non-coding RNAs across diverse cell line-,\ntissue- and disease-specific conditions. Network-based analysis-including\nrandom walk with restart (RWR), GSEA, and diffusion profile (DP) similarity\nalgorithms-supports tasks such as target prediction, functional enrichment, and\ndrug screening. We merged networks from experimental sources to generate a\npre-integrated multi-layer human network for evaluation. RWR demonstrated\nsuperior performance with a 33.1% average ranking improvement over the\nsecond-best algorithm, PageRank, in identifying known targets across 2,002\ndrugs. Additionally, multi-layer networks significantly improve the ability to\nidentify FDA-approved drug-disease pairs compared to the single-layer network.\nFor KG-based analysis, we compiled three high-quality KGs to construct POINT\nKG, which cross-references over 90% of network-based predictions. We\nillustrated the platform's capabilities through two case studies. POINT bridges\nthe gap between multi-omics networks and drug discovery; it is freely\naccessible at http://point.gene.ac/.",
      "generated_abstract": "Cellular processes are characterized by non-Markovian dynamics, which\ncan be described by the Ornstein-Uhlenbeck (OU) process. The goal of this\nwork is to investigate the influence of the OU process on the dynamics of a\nspiking neural network (SNN). The SNN is modeled as a system of coupled\ndifferential equations and the OU process is described by a Wiener process. We\nintroduce a numerical method to simulate the OU process and the SNN dynamics\nunder different conditions, including Markovian and non-Markovian models. The\nsimulations demonstrate that the OU process can influence the spiking dynamics\nof the SNN. The influence of the OU process on the SNN is investigated by\nexploring the spike-timing dependence (STD) of the SNN, which indicates the\nspiking response of the SNN to the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08860759493670886,
          "p": 0.2222222222222222,
          "f": 0.12669682850310202
        },
        "rouge-2": {
          "r": 0.005050505050505051,
          "p": 0.009708737864077669,
          "f": 0.0066445137704912345
        },
        "rouge-l": {
          "r": 0.08860759493670886,
          "p": 0.2222222222222222,
          "f": 0.12669682850310202
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/SC/2503.10416v1",
      "true_abstract": "Runtime repeated recursion unfolding was recently introduced as a\njust-in-time program transformation strategy that can achieve super-linear\nspeedup. So far, the method was restricted to single linear direct recursive\nrules in the programming language Constraint Handling Rules (CHR). In this\ncompanion paper, we generalize the technique to multiple recursion and to\nmultiple recursive rules and provide an implementation of the generalized\nmethod in the logic programming language Prolog.\n  The basic idea of the approach is as follows: When a recursive call is\nencountered at runtime, the recursive rule is unfolded with itself and this\nprocess is repeated with each resulting unfolded rule as long as it is\napplicable to the current call. In this way, more and more recursive steps are\ncombined into one recursive step. Then an interpreter applies these rules to\nthe call starting from the most unfolded rule. For recursions which have\nsufficiently simplifyable unfoldings, a super-linear can be achieved, i.e. the\ntime complexity is reduced.\n  We implement an unfolder, a generalized meta-interpreter and a novel\nround-robin rule processor for our generalization of runtime repeated recursion\nunfolding with just ten clauses in Prolog. We illustrate the feasibility of our\ntechnique with worst-case time complexity estimates and benchmarks for some\nbasic classical algorithms that achieve a super-linear speedup.",
      "generated_abstract": "This paper introduces the KATO-Bot, an agent for learning and reasoning about\nKATO, a new language for building and managing autonomous robotic systems. KATO\nallows users to describe complex tasks and build them automatically, but it\nrequires domain experts to interpret the resulting programs and understand\ntheir behavior. KATO-Bot is a dialogue agent that learns how to understand KATO\nprograms and their semantics, providing a human-like interface for interacting\nwith KATO. KATO-Bot is a collaborative agent, built using a multi-agent\nsystem, designed to learn how to communicate with humans in a natural way.\nKATO-Bot is a collaborative agent, built using a multi-agent system, designed\nto learn how to communicate with humans in a natural way. It integrates a\nnatural language processing module, a reasoning module,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.104,
          "p": 0.18309859154929578,
          "f": 0.13265305660401933
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.096,
          "p": 0.16901408450704225,
          "f": 0.1224489749713663
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2410.23275v1",
      "true_abstract": "We introduce a novel Dynamic Graph Neural Network (DGNN) architecture for\nsolving conditional $m$-steps ahead forecasting problems in temporal financial\nnetworks. The proposed DGNN is validated on simulated data from a temporal\nfinancial network model capturing stylized features of Interest Rate Swaps\n(IRSs) transaction networks, where financial entities trade swap contracts\ndynamically and the network topology evolves conditionally on a reference rate.\nThe proposed model is able to produce accurate conditional forecasts of net\nvariation margins up to a $21$-day horizon by leveraging conditional\ninformation under pre-determined stress test scenarios. Our work shows that the\nnetwork dynamics can be successfully incorporated into stress-testing\npractices, thus providing regulators and policymakers with a crucial tool for\nsystemic risk monitoring.",
      "generated_abstract": "This paper introduces a novel method for determining the optimal investment\nportfolio of a non-traded, non-listed firm based on a set of financial\nstatistics and the firm's market value. This approach avoids the need to\nestimate the firm's value or the performance of its securities and provides a\nsimple and scalable solution to the problem of finding optimal portfolio\nconstructions. The method is based on the use of a portfolio with low Sharpe\nratio, which, in the case of a non-listed firm, can be obtained by applying\nthe firm's own net worth to the firm's market capitalization. We show that the\nproposed method can be used to determine optimal portfolio constructions in\ncases where the firm's market capitalization is unknown, which is often the\ncase in practice. We demonstrate the effectiveness of the proposed method in\nsimulation",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21875,
          "p": 0.27631578947368424,
          "f": 0.24418604157923213
        },
        "rouge-2": {
          "r": 0.04424778761061947,
          "p": 0.04201680672268908,
          "f": 0.043103443279206885
        },
        "rouge-l": {
          "r": 0.20833333333333334,
          "p": 0.2631578947368421,
          "f": 0.23255813460248795
        }
      }
    },
    {
      "paper_id": "cs.GT.cs/CR/2503.10185v1",
      "true_abstract": "Following the publication of Bitcoin's arguably most famous attack, selfish\nmining, various works have introduced mechanisms to enhance blockchain systems'\ngame theoretic resilience. Some reward mechanisms, like FruitChains, have been\nshown to be equilibria in theory. However, their guarantees assume\nnon-realistic parameters and their performance degrades significantly in a\npractical deployment setting. In this work we introduce a reward allocation\nmechanism, called Proportional Splitting (PRS), which outperforms existing\nstate of the art. We show that, for large enough parameters, PRS is an\nequilibrium, offering the same theoretical guarantees as the state of the art.\nIn addition, for practical, realistically small, parameters, PRS outperforms\nall existing reward mechanisms across an array of metrics. We implement PRS on\ntop of a variant of PoEM, a Proof-of-Work (PoW) protocol that enables a more\naccurate estimation of each party's mining power compared to e.g., Bitcoin. We\nthen evaluate PRS both theoretically and in practice. On the theoretical side,\nwe show that our protocol combined with PRS is an equilibrium and guarantees\nfairness, similar to FruitChains. In practice, we compare PRS with an array of\nexisting reward mechanisms and show that, assuming an accurate estimation of\nthe mining power distribution, it outperforms them across various\nwell-established metrics. Finally, we realize this assumption by approximating\nthe power distribution via low-work objects called \"workshares\" and quantify\nthe tradeoff between the approximation's accuracy and storage overhead.",
      "generated_abstract": "In this paper, we present a novel framework for distributed estimation\nof the mean and covariance of Gaussian distributions. Our approach\ncomprises two distinct parts: (1) a novel non-linear stochastic filtering\nmethodology for the mean, and (2) a novel distributed estimation algorithm for\nthe covariance. The former relies on a novel filtering methodology that\nrepresents the Gaussian distribution as a multivariate Gaussian, and uses\niterative Kalman filtering techniques to obtain a stochastic estimate of the\nmean. The latter, based on the covariance of the mean, uses a novel\ndistributed estimator based on a linear algebra-based approach to estimate the\ncovariance. We demonstrate the effectiveness of our approach by evaluating it\non simulated and real-world datasets. The simulated datasets include both\nwell-known and uncommon distributions, and the real-world datasets include\nclinical data from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1292517006802721,
          "p": 0.2638888888888889,
          "f": 0.1735159773215739
        },
        "rouge-2": {
          "r": 0.014018691588785047,
          "p": 0.026785714285714284,
          "f": 0.018404903464941447
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.25,
          "f": 0.1643835572302497
        }
      }
    },
    {
      "paper_id": "quant-ph.cond-mat/other/2503.09292v1",
      "true_abstract": "In the pursuit of robust quantum computing, we put forth a platform based on\nphotonic qubits in a circuit-QED environment. Specifically, we propose a\nversatile two-qubit gate based on two cavities coupled via a transmon,\nconstituting a selective number-dependent phase gate operating on the in-phase\neigenmodes of the two cavities, the Eigen-SNAP gate. This gate natively\noperates in the dispersive coupling regime of the cavities and the transmon,\nand operates by driving the transmon externally, to imprint desired phases on\nthe number states. As an example for the utility of the Eigen-SNAP gate, we\nimplement a $\\sqrt{\\text{SWAP}}$ gate on a system of two logical bosonic qubits\nencoded in the cavities. Further, we use numerical optimization to determine\nthe optimal implementation of the $\\sqrt{\\text{SWAP}}$. We find that the\nfidelities of these optimal protocols are only limited by the coherence times\nof the system's components. These findings pave the way to continuous variable\nquantum computing in cavity-transmon systems.",
      "generated_abstract": "We investigate the role of disorder in the emergence of long-range order in\nthe ground states of one-dimensional quantum spin chains. We show that, for a\nfinite number of spins, disorder induces long-range order even without\ndisorder-assisted localization, and that the emergence of long-range order\nbecomes exponentially more likely as the disorder strength is increased. We\ndemonstrate that, in contrast to a previous result, the disorder-induced\nlong-range order does not necessarily persist in the thermodynamic limit,\nproviding a physical explanation for the discrepancy between previous and\npresent results. Our results suggest that the emergence of long-range order in\nspin chains is not necessarily an effect of disorder-assisted localization, but\nmay instead arise spontaneously from a competition between disorder-assisted\nlocalization and an inherent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13541666666666666,
          "p": 0.18571428571428572,
          "f": 0.15662650114675586
        },
        "rouge-2": {
          "r": 0.02054794520547945,
          "p": 0.028846153846153848,
          "f": 0.023999995141120983
        },
        "rouge-l": {
          "r": 0.10416666666666667,
          "p": 0.14285714285714285,
          "f": 0.12048192283350287
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09367v1",
      "true_abstract": "Shi, Walsh and Yu demonstrated that any dense circuit graph contains a large\nnear-triangulation. We extend the result to $2$-connected plane graphs, thereby\naddressing a question posed by them. Using the result, we prove that the planar\nTu\\'{a}n number of $2C_k$ is $\\left[3-\\Theta(k^{\\log_23})^{-1}\\right]n$ when\n$k\\geq 5$.",
      "generated_abstract": "We study the convergence of iterates of the random walk on the symmetric\n$2\\times 2$ Toeplitz matrix-quadratic Bernstein matrix-quadratic Toeplitz\nmatrix-quadratic Bernstein matrix-quadratic matrix-quadratic Bernstein matrix\nchain, and generalize the corresponding results for random walks on other\nsymmetric $2\\times 2$ Toeplitz matrices and $2\\times 2$ Toeplitz matrices with\npositive entries. We also discuss the convergence of the random walk on the\nsymmetric $2\\times 2$ Toeplitz matrix-quadratic Bernstein matrix-quadratic Toeplitz\nmatrix-quadratic Bernstein matrix-quadratic matrix-quadratic Bernstein matrix\nchain in the case that the matrices have the same sign. We prove that the\nconvergence is uniform in the step",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.17073170731707318,
          "f": 0.1686746937959067
        },
        "rouge-2": {
          "r": 0.044444444444444446,
          "p": 0.03278688524590164,
          "f": 0.037735844170523954
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.17073170731707318,
          "f": 0.1686746937959067
        }
      }
    },
    {
      "paper_id": "cs.MA.cs/MA/2503.08728v1",
      "true_abstract": "Multi-agent reinforcement learning (MARL) has shown significant potential in\ntraffic signal control (TSC). However, current MARL-based methods often suffer\nfrom insufficient generalization due to the fixed traffic patterns and road\nnetwork conditions used during training. This limitation results in poor\nadaptability to new traffic scenarios, leading to high retraining costs and\ncomplex deployment. To address this challenge, we propose two algorithms:\nPLight and PRLight. PLight employs a model-based reinforcement learning\napproach, pretraining control policies and environment models using predefined\nsource-domain traffic scenarios. The environment model predicts the state\ntransitions, which facilitates the comparison of environmental features.\nPRLight further enhances adaptability by adaptively selecting pre-trained\nPLight agents based on the similarity between the source and target domains to\naccelerate the learning process in the target domain. We evaluated the\nalgorithms through two transfer settings: (1) adaptability to different traffic\nscenarios within the same road network, and (2) generalization across different\nroad networks. The results show that PRLight significantly reduces the\nadaptation time compared to learning from scratch in new TSC scenarios,\nachieving optimal performance using similarities between available and target\nscenarios.",
      "generated_abstract": "The advent of deep learning has led to significant advancements in audio\nestimation, but it remains challenging for systems that require accurate\ntemporal information. This paper presents a new approach for audio speech\nsynthesis based on a multimodal transformer that integrates acoustic and\nspeech modeling. Our approach introduces an adaptive speech modeling module\nthat learns a high-quality speech model while preserving the acoustic features\nof the input speech. This module improves the quality of synthesized speech\nwhile preserving the acoustic information. We also introduce a temporal\nmodeling module that enables the system to capture temporal information in\naudio speech synthesis. This module enhances the quality of speech synthesis\nwhile preserving the acoustic information. Experiments on four datasets\ndemonstrate the effectiveness of the proposed model. Our code is available at\nhttps://github.com/Pra",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16,
          "p": 0.26666666666666666,
          "f": 0.19999999531250012
        },
        "rouge-2": {
          "r": 0.005714285714285714,
          "p": 0.009009009009009009,
          "f": 0.00699300224338921
        },
        "rouge-l": {
          "r": 0.16,
          "p": 0.26666666666666666,
          "f": 0.19999999531250012
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.00391v1",
      "true_abstract": "In this working paper, I developed a suite of macroeconomic models that shed\nlight on the intricate relationship between economic development, health, and\nfertility. These innovative models conceptualize health as an intermediate\ngood, paving the way for new interpretations of dynamic socio-economic\nphenomena, particularly the non-monotonic effects of health on economic and\npopulation growth. The evolving dynamic interactions among economic growth,\npopulation, and health during the early stages of human development have been\nwell interpreted in this research.",
      "generated_abstract": "We present a novel method for determining the optimal tax rate for a\ntaxpayer in a heterogeneous-population setting. We develop a dynamic\nprobabilistic taxation framework that accounts for heterogeneity in taxpayer\nbehavior and tax rates. The framework integrates the theory of dynamic\npolynomial optimization with the Bayesian inference framework to provide a\nreliable estimate of the optimal tax rate for a taxpayer. The methodology is\nvalidated through simulations and empirical applications. Our findings\nhighlight the importance of considering the heterogeneity in taxpayer behavior\nand tax rates in estimating the optimal tax rate.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14516129032258066,
          "p": 0.17307692307692307,
          "f": 0.1578947318805788
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.013157894736842105,
          "f": 0.013071890425052107
        },
        "rouge-l": {
          "r": 0.14516129032258066,
          "p": 0.17307692307692307,
          "f": 0.1578947318805788
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.astro-ph/IM/2503.10106v1",
      "true_abstract": "This work presents GalProTE, a proof-of-concept Machine Learning model\nutilizing a Transformer Encoder to determine stellar age, metallicity, and dust\nattenuation from optical spectra. Designed for large astronomical surveys,\nGalProTE significantly accelerates processing while maintaining accuracy. Using\nthe E-MILES spectral library, we construct a dataset of 111,936 diverse\ntemplates by expanding 636 simple stellar population models with varying\nextinction, spectral combinations, and noise modifications. This ensures robust\ntraining over 4750 to 7100 Angstrom at 2.5 Angstrom resolution. GalProTE\nemploys four parallel attention-based encoders with varying kernel sizes to\ncapture spectral features. On synthetic test data, it achieves a mean squared\nerror (MSE) of 0.27% between input and predicted spectra. Validation on\nPHANGS-MUSE galaxies NGC4254 and NGC5068 confirms its ability to extract\nphysical parameters efficiently, with residuals averaging -0.02% and 0.28% and\nstandard deviations of 4.3% and 5.3%, respectively. To contextualize these\nresults, we compare GalProTE's age, metallicity, and dust attenuation maps with\npPXF, a state-of-the-art spectral fitting tool. While pPXF requires\napproximately 11 seconds per spectrum, GalProTE processes one in less than 4\nmilliseconds, offering a 2750 times speedup and consuming 68 times less power\nper spectrum. The strong agreement between pPXF and GalProTE highlights the\npotential of machine learning to enhance traditional methods, paving the way\nfor faster, energy-efficient, and scalable analyses of galactic properties in\nmodern surveys.",
      "generated_abstract": "We present an updated catalog of 1,130,405 point sources in the SDSS\nR23 data release (DR23) within a 250 pc radius sphere centered on the Sun.\nCombining this with the SDSS DR11 spectroscopic sample of 1,173,786 sources,\nwe have created a combined sample of 2,314,291 sources. We have removed sources\nfrom the combined sample that are in other catalogs (within 100 pc, 1 Mpc, or\n$z>0.1$ from the combined sample) and those that have already been observed in\nthe SDSS, with a total of 2,311,949 sources remaining. We have created a\ncombined catalog of 2,311,949 sources with 2,310,45",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05263157894736842,
          "p": 0.14285714285714285,
          "f": 0.07692307298816588
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.05263157894736842,
          "p": 0.14285714285714285,
          "f": 0.07692307298816588
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.07649v1",
      "true_abstract": "We propose a method to learn the nonlinear impulse responses to structural\nshocks using neural networks, and apply it to uncover the effects of US\nfinancial shocks. The results reveal substantial asymmetries with respect to\nthe sign of the shock. Adverse financial shocks have powerful effects on the US\neconomy, while benign shocks trigger much smaller reactions. Instead, with\nrespect to the size of the shocks, we find no discernible asymmetries.",
      "generated_abstract": "This paper extends the theory of the conditional mean adjustment (CMA)\nmethod to the multivariate setting. The CMA method is a widely used method for\nadjusting for heteroskedasticity and non-stationarity in time series data,\nspecifically, when the residuals are non-stationary and the model is misspecified\nor too complex. We extend the CMA method to the multivariate setting, where we\nassume that the data are generated from a stationary, multivariate, and\nnon-linear model. We propose a CMA for the multivariate setting and show that\nour proposed CMA is consistent and asymptotically normal, and that it\nefficiently adjusts for heteroskedasticity. We also show that our proposed\nCMA has a closed-form expression for the adjusted mean vector and propose\nprocedures for computing the CMA. We illustrate the CMA by",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22,
          "p": 0.1527777777777778,
          "f": 0.18032786401504985
        },
        "rouge-2": {
          "r": 0.07692307692307693,
          "p": 0.04504504504504504,
          "f": 0.05681817715973695
        },
        "rouge-l": {
          "r": 0.22,
          "p": 0.1527777777777778,
          "f": 0.18032786401504985
        }
      }
    },
    {
      "paper_id": "quant-ph.cond-mat/stat-mech/2503.10308v1",
      "true_abstract": "We consider learnability transitions in monitored quantum systems that\nundergo noisy evolution, subject to a global strong symmetry -- i.e., in\naddition to the measuring apparatus, the system can interact with an unobserved\nenvironment, but does not exchange charge with it. As in the pure-state\nsetting, we find two information-theoretic phases -- a sharp (fuzzy) phase in\nwhich an eavesdropper can rapidly (slowly) learn the symmetry charge. However,\nbecause the dynamics is noisy, both phases can be simulated efficiently using\ntensor networks. Indeed, even when the true dynamics is unitary, introducing\nnoise by hand allows an eavesdropper to efficiently learn the symmetry charge\nfrom local measurements, as we demonstrate. We identify the fuzzy phase in this\nsetting as a mixed-state phase that exhibits spontaneous strong-to-weak\nsymmetry breaking.",
      "generated_abstract": "We derive the master equation for the density matrix of a quantum many-body\nsystem in the adiabatic limit. The system is described by a Hamiltonian with\none-dimensional local interactions, and we explicitly construct the master\nequation in the mean-field approximation. We find that the mean-field approximation\nleads to a system of equations that can be solved analytically. In the\nadiabatic limit, the master equation reduces to a master equation for the\ndensity matrix of a single-particle system. We provide a derivation of the\nsingle-particle master equation in the adiabatic limit for a general many-body\nHamiltonian, and we show how to apply the single-particle master equation to\nthe many-body problem. We also discuss the effects of disorder in the\nmean-field approximation and find that the mean-field approximation leads to a\nperturbative expansion in the strength of disorder. We",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17582417582417584,
          "p": 0.2711864406779661,
          "f": 0.21333332856088905
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.04040404040404041,
          "f": 0.03619909007678044
        },
        "rouge-l": {
          "r": 0.16483516483516483,
          "p": 0.2542372881355932,
          "f": 0.19999999522755568
        }
      }
    },
    {
      "paper_id": "cs.AI.cs/GT/2503.09858v1",
      "true_abstract": "This paper investigates the complex interplay between AI developers,\nregulators, users, and the media in fostering trustworthy AI systems. Using\nevolutionary game theory and large language models (LLMs), we model the\nstrategic interactions among these actors under different regulatory regimes.\nThe research explores two key mechanisms for achieving responsible governance,\nsafe AI development and adoption of safe AI: incentivising effective regulation\nthrough media reporting, and conditioning user trust on commentariats'\nrecommendation. The findings highlight the crucial role of the media in\nproviding information to users, potentially acting as a form of \"soft\"\nregulation by investigating developers or regulators, as a substitute to\ninstitutional AI regulation (which is still absent in many regions). Both\ngame-theoretic analysis and LLM-based simulations reveal conditions under which\neffective regulation and trustworthy AI development emerge, emphasising the\nimportance of considering the influence of different regulatory regimes from an\nevolutionary game-theoretic perspective. The study concludes that effective\ngovernance requires managing incentives and costs for high quality\ncommentaries.",
      "generated_abstract": "In this paper, we investigate the effect of the degree of sparsity on the\nconvergence of the sparse approximation to the true distribution of a\nclassification problem. In particular, we propose a novel approach based on\nsparsity-aware gradient descent (SA-Grad) that ensures convergence at a\npredefined error threshold, independent of the degree of sparsity. To further\nboost the convergence rate, we also propose an error-tolerant approach that\nintroduces a penalty term to the objective function, allowing us to effectively\ncontrol the sparsity level while maintaining a desired convergence rate.\nFinally, we show that SA-Grad is superior to the conventional sparsity-aware\ngradient descent (SAS-Grad) algorithm in terms of both the convergence rate and\nthe sparsity level required to achieve the same convergence rate.\n  We conduct extensive experiments on benchmark datasets to validate the\neffectiveness of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09649122807017543,
          "p": 0.1375,
          "f": 0.11340205700924667
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.01694915254237288,
          "f": 0.014869883551085778
        },
        "rouge-l": {
          "r": 0.08771929824561403,
          "p": 0.125,
          "f": 0.10309277865873123
        }
      }
    },
    {
      "paper_id": "cs.RO.eess/SY/2503.03973v1",
      "true_abstract": "Range-only Simultaneous Localisation and Mapping (RO-SLAM) is of interest due\nto its practical applications in ultra-wideband (UWB) and Bluetooth Low Energy\n(BLE) localisation in terrestrial and aerial applications and acoustic beacon\nlocalisation in submarine applications. In this work, we consider a mobile\nrobot equipped with an inertial measurement unit (IMU) and a range sensor that\nmeasures distances to a collection of fixed landmarks. We derive an equivariant\nfilter (EqF) for the RO-SLAM problem based on a symmetry Lie group that is\ncompatible with the range measurements. The proposed filter does not require\nbootstrapping or initialisation of landmark positions, and demonstrates\nrobustness to the no-prior situation. The filter is demonstrated on a\nreal-world dataset, and it is shown to significantly outperform a\nstate-of-the-art EKF alternative in terms of both accuracy and robustness.",
      "generated_abstract": "This paper presents a novel approach for autonomous driving by integrating\ntwo key principles: learning and reasoning. Traditional driving is based on\nexpertise, while learning-based driving focuses on data-driven learning.\nHowever, the two approaches often conflict. This paper proposes a\nlearning-based reasoning strategy for autonomous driving, integrating\nrepresentative components of expertise and learning. First, it utilizes\ndriving data to optimize the learning model to achieve a reasonable learning\nspeed. Second, it integrates the expertise-based driving model to adjust the\nlearning model according to the driving environment, thereby ensuring that the\ndriving model adapts to the driving situation. The paper provides a\ncomprehensive analysis of the advantages and disadvantages of the two driving\nmethods. Finally, the paper proposes a generalization method to adapt the\ndriving model to different driving environments. The results show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13978494623655913,
          "p": 0.17567567567567569,
          "f": 0.15568861781921206
        },
        "rouge-2": {
          "r": 0.0234375,
          "p": 0.02586206896551724,
          "f": 0.024590158946520765
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.16216216216216217,
          "f": 0.14371256991502043
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10419v1",
      "true_abstract": "In motion simulation, motion cueing algorithms are used for the trajectory\nplanning of the motion simulator platform, where workspace limitations prevent\ndirect reproduction of reference trajectories. Strategies such as motion\nwashout, which return the platform to its center, are crucial in these\nsettings. For serial robotic MSPs with highly nonlinear workspaces, it is\nessential to maximize the efficient utilization of the MSPs kinematic and\ndynamic capabilities. Traditional approaches, including classical washout\nfiltering and linear model predictive control, fail to consider\nplatform-specific, nonlinear properties, while nonlinear model predictive\ncontrol, though comprehensive, imposes high computational demands that hinder\nreal-time, pilot-in-the-loop application without further simplification. To\novercome these limitations, we introduce a novel approach using deep\nreinforcement learning for motion cueing, demonstrated here for the first time\nin a 6-degree-of-freedom setting with full consideration of the MSPs kinematic\nnonlinearities. Previous work by the authors successfully demonstrated the\napplication of DRL to a simplified 2-DOF setup, which did not consider\nkinematic or dynamic constraints. This approach has been extended to all 6 DOF\nby incorporating a complete kinematic model of the MSP into the algorithm, a\ncrucial step for enabling its application on a real motion simulator. The\ntraining of the DRL-MCA is based on Proximal Policy Optimization in an\nactor-critic implementation combined with an automated hyperparameter\noptimization. After detailing the necessary training framework and the\nalgorithm itself, we provide a comprehensive validation, demonstrating that the\nDRL MCA achieves competitive performance against established algorithms.\nMoreover, it generates feasible trajectories by respecting all system\nconstraints and meets all real-time requirements with low...",
      "generated_abstract": "This paper investigates the state estimation performance of a linear\nestimator in a system with nonlinear disturbances and feedback. The estimator\nis based on a generalized mean-field control approach, where the state of the\nsystem is modeled as a Gaussian random variable, and the control input is\ndetermined by a linear function of the mean. To enhance the estimation\nstability, a linearized model of the control input is used to estimate the\ncontrol input, and the estimation error is estimated based on a discrete\nfeedback law. The performance of the proposed estimator is analyzed through\nstochastic stability analysis. The effect of the disturbances and feedback\ncontrol on the estimation error is investigated. It is shown that the\nestimation error converges to zero as the estimation horizon increases and\nthat the estimation error converges to the true value as the number of\nmeasurements increases. Furthermore, it is shown that the proposed estimator",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13872832369942195,
          "p": 0.3380281690140845,
          "f": 0.19672130734916696
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.0782608695652174,
          "f": 0.04986149150328841
        },
        "rouge-l": {
          "r": 0.12138728323699421,
          "p": 0.29577464788732394,
          "f": 0.17213114341474076
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/TR/2411.13564v1",
      "true_abstract": "According to The Exchange Act, 1934 unlawful insider trading is the abuse of\naccess to privileged corporate information. While a blurred line between\n\"routine\" the \"opportunistic\" insider trading exists, detection of strategies\nthat insiders mold to maneuver fair market prices to their advantage is an\nuphill battle for hand-engineered approaches. In the context of detailed\nhigh-dimensional financial and trade data that are structurally built by\nmultiple covariates, in this study, we explore, implement and provide detailed\ncomparison to the existing study (Deng et al. (2019)) and independently\nimplement automated end-to-end state-of-art methods by integrating principal\ncomponent analysis to the random forest (PCA-RF) followed by a standalone\nrandom forest (RF) with 320 and 3984 randomly selected, semi-manually labeled\nand normalized transactions from multiple industry. The settings successfully\nuncover latent structures and detect unlawful insider trading. Among the\nmultiple scenarios, our best-performing model accurately classified 96.43\npercent of transactions. Among all transactions the models find 95.47 lawful as\nlawful and $98.00$ unlawful as unlawful percent. Besides, the model makes very\nfew mistakes in classifying lawful as unlawful by missing only 2.00 percent. In\naddition to the classification task, model generated Gini Impurity based\nfeatures ranking, our analysis show ownership and governance related features\nbased on permutation values play important roles. In summary, a simple yet\npowerful automated end-to-end method relieves labor-intensive activities to\nredirect resources to enhance rule-making and tracking the uncaptured unlawful\ninsider trading transactions. We emphasize that developed financial and trading\nfeatures are capable of uncovering fraudulent behaviors.",
      "generated_abstract": "The financial sector is constantly evolving. In order to remain competitive,\nassets and businesses must adapt and integrate new technologies to stay\nfuture-ready. This study presents the evolution of the financial sector,\nhighlighting the impact of technological advancements on the industry. We\nexplore the role of blockchain and AI in the sector, analyzing how these\ntechnologies are changing the way we do business. We also examine the\nevolution of regulation, focusing on the implications of new laws and\nregulations on the industry. This research provides a comprehensive\nunderstanding of the evolving landscape of the financial sector and its\nimpact on businesses and investors alike. By exploring the impact of new\ntechnologies on the sector, this study offers valuable insights for future\ndevelopments and growth.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10526315789473684,
          "p": 0.24,
          "f": 0.14634145917608576
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0935672514619883,
          "p": 0.21333333333333335,
          "f": 0.13008129657445977
        }
      }
    },
    {
      "paper_id": "cs.FL.cs/LO/2503.04525v1",
      "true_abstract": "We give an active learning algorithm for deterministic one-counter automata\n(DOCAs) where the learner can ask the teacher membership and minimal\nequivalence queries. The algorithm called OL* learns a DOCA in time polynomial\nin the size of the smallest DOCA, recognising the target language.\n  All existing algorithms for learning DOCAs, even for the subclasses of\ndeterministic real-time one-counter automata (DROCAs) and visibly one-counter\nautomata (VOCAs), in the worst case, run in exponential time with respect to\nthe size of the DOCA under learning. Furthermore, previous learning algorithms\nare ``grey-box'' algorithms relying on an additional query type - counter value\nquery - where the teacher returns the counter value reached on reading a given\nword. In contrast, our algorithm is a ``black-box'' algorithm.\n  It is known that the minimisation of VOCAs is NP-hard. However, OL* can be\nused for approximate minimisation of DOCAs. In this case, the output size is at\nmost polynomial in the size of a minimal DOCA.",
      "generated_abstract": "This paper introduces a novel method for training deep generative models\n(DGMs) that incorporates both generative and discriminative training. The\nmethod combines a generative model with a discriminative model to enhance the\ngenerative capabilities of the DGM. The generative model is trained to generate\nrepresentations that closely match the target distribution, while the discriminative\nmodel is trained to distinguish between generated and real data. This approach\nenables the DGM to generate data that are more consistent with the target\ndistribution while preserving the characteristics of the real data. The\napproach is illustrated through a case study on the development of a DGM for\ngenerating text-based images. The results demonstrate that the proposed\napproach can improve the quality of generated images compared to existing\nmethods.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16129032258064516,
          "p": 0.22388059701492538,
          "f": 0.18749999513203136
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.035398230088495575,
          "f": 0.0311283997420105
        },
        "rouge-l": {
          "r": 0.15053763440860216,
          "p": 0.208955223880597,
          "f": 0.17499999513203138
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.08100v1",
      "true_abstract": "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
      "generated_abstract": "We study the optimal allocation of capital to two firms in the presence of\ndiffering market power, where firms' rationality can be compromised by\ncollusion. The firms can coordinate their behavior through a shared\ninformation structure. We provide a necessary and sufficient condition for\ncoordination, and show that it is optimal if and only if the coordination\ncosts are sufficiently high. Additionally, we show that under the condition of\nhigh coordination costs, the optimal allocation is a concave function of\ncapital, which is the concave function of capital that maximizes the average\nutility of the firms. These results are generalizations of a result of\nVenkatesh and Tiwari (2016). We also consider a variant of the problem where\nthe firms can coordinate through a common strategy and find that the\ncoordination cost is in general a suboptimal measure of the coordination",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21951219512195122,
          "p": 0.22784810126582278,
          "f": 0.22360247947378584
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.024,
          "f": 0.024896260567139995
        },
        "rouge-l": {
          "r": 0.2073170731707317,
          "p": 0.21518987341772153,
          "f": 0.21118011922533864
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/CP/2412.01062v1",
      "true_abstract": "High-frequency trading (HFT) represents a pivotal and intensely competitive\ndomain within the financial markets. The velocity and accuracy of data\nprocessing exert a direct influence on profitability, underscoring the\nsignificance of this field. The objective of this work is to optimise the\nreal-time processing of data in high-frequency trading algorithms. The dynamic\nfeature selection mechanism is responsible for monitoring and analysing market\ndata in real time through clustering and feature weight analysis, with the\nobjective of automatically selecting the most relevant features. This process\nemploys an adaptive feature extraction method, which enables the system to\nrespond and adjust its feature set in a timely manner when the data input\nchanges, thus ensuring the efficient utilisation of data. The lightweight\nneural networks are designed in a modular fashion, comprising fast\nconvolutional layers and pruning techniques that facilitate the expeditious\ncompletion of data processing and output prediction. In contrast to\nconventional deep learning models, the neural network architecture has been\nspecifically designed to minimise the number of parameters and computational\ncomplexity, thereby markedly reducing the inference time. The experimental\nresults demonstrate that the model is capable of maintaining consistent\nperformance in the context of varying market conditions, thereby illustrating\nits advantages in terms of processing speed and revenue enhancement.",
      "generated_abstract": "In this paper, we study the problem of learning a binary classification model\nfrom binary observations, where the input is a sequence of binary observations\nand the output is a binary label. This problem is equivalent to learning a\nbinary classification model from binary observations, where the input is a\nsequence of binary observations and the output is a binary label. We introduce\na novel class of binary observation models, called sequence-to-sequence\nbinary observation models, and demonstrate that learning binary classification\nmodels from binary observations is equivalent to learning sequence-to-sequence\nbinary observation models. We also show that the learning problem of\nsequence-to-sequence binary observation models is equivalent to learning a\nbinary classification model from binary observations. We prove that the\nclassification accuracy of the binary classification model from binary\nobservations is at least as good as the classification accuracy of the\nsequence-to-sequence binary observation model, which is a sharp result. We\npresent an efficient method for learning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15217391304347827,
          "p": 0.3888888888888889,
          "f": 0.21874999595703135
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.03260869565217391,
          "f": 0.020689650840428966
        },
        "rouge-l": {
          "r": 0.13768115942028986,
          "p": 0.35185185185185186,
          "f": 0.197916662623698
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2502.19839v1",
      "true_abstract": "We consider the problem of estimating complex statistical latent variable\nmodels using variational Bayes methods. These methods are used when exact\nposterior inference is either infeasible or computationally expensive, and they\napproximate the posterior density with a family of tractable distributions. The\nparameters of the approximating distribution are estimated using optimisation\nmethods. This article develops a flexible Gaussian mixture variational\napproximation, where we impose sparsity in the precision matrix of each\nGaussian component to reflect the appropriate conditional independence\nstructure in the model. By introducing sparsity in the precision matrix and\nparameterising it using the Cholesky factor, each Gaussian mixture component\nbecomes parsimonious (with a reduced number of non-zero parameters), while\nstill capturing the dependence in the posterior distribution. Fast estimation\nmethods based on global and local variational boosting moves combined with\nnatural gradients and variance reduction methods are developed. The local\nboosting moves adjust an existing mixture component, and optimisation is only\ncarried out on a subset of the variational parameters of a new component. The\nsubset is chosen to target improvement of the current approximation in aspects\nwhere it is poor. The local boosting moves are fast because only a small number\nof variational parameters need to be optimised. The efficacy of the approach is\nillustrated by using simulated and real datasets to estimate generalised linear\nmixed models and state space models.",
      "generated_abstract": "We study the statistical inference of two-way interaction effects using\ndata with a continuous outcome and a binary outcome. We show that, under a\nsuitable condition, the observed data can be interpreted as an empirical\nversion of the two-way interaction model and propose a new Bayesian\ninference method that is asymptotically efficient and consistent. The proposed\nmethod is general and does not require any additional assumptions on the\nassessment of the effect of the interaction variable. It is particularly\napplicable when the effect of the interaction variable on the outcome is\nunknown, as in many scenarios in psychology and neuroscience.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14393939393939395,
          "p": 0.30158730158730157,
          "f": 0.1948717904978305
        },
        "rouge-2": {
          "r": 0.014778325123152709,
          "p": 0.03333333333333333,
          "f": 0.020477811443349085
        },
        "rouge-l": {
          "r": 0.12121212121212122,
          "p": 0.25396825396825395,
          "f": 0.16410255972859972
        }
      }
    },
    {
      "paper_id": "astro-ph.IM.astro-ph/CO/2503.09842v1",
      "true_abstract": "Radiowave Observations on the Lunar Surface of the photo-Electron Sheath\ninstrument (ROLSES- 1) onboard the Intuitive Machines' Odysseus lunar lander\nrepresents NASA's first radio telescope on the Moon, and the first United\nStates spacecraft landing on the lunar surface in five decades. Despite a host\nof challenges, ROLSES-1 managed to collect a small amount of data over\nfractions of one day during cruise phase and two days on the lunar surface with\nfour monopole stacer antennas that were in a non-ideal deployment. All antennas\nrecorded shortwave radio transmissions breaking through the Earth's ionosphere\n-- or terrestrial technosignatures -- from spectral and raw waveform data.\nThese technosignatures appear to be modulated by density fluctuations in the\nEarth's ionosphere and could be used as markers when searching for\nextraterrestrial intelligence from habitable exoplanets. After data reduction\nand marshaling a host of statistical and sampling techniques, five minutes of\nraw waveforms from the least noisy antenna were used to generate covariances\nconstraining both the antenna parameters and the amplitude of the low-frequency\nisotropic galactic spectrum. ROLSES- 2 and LuSEE-Night, both lunar radio\ntelescopes launching later in the decade, will have significant upgrades from\nROLSES-1 and will be set to take unprecedented measurements of the\nlow-frequency sky, lunar surface, and constrain the cosmological 21-cm signal.\nROLSES-1 represents a trailblazer for lunar radio telescopes, and many of the\nstatistical tools and data reduction techniques presented in this work will be\ninvaluable for upcoming lunar radio telescope missions.",
      "generated_abstract": "The Large Synoptic Survey Telescope (LSST) will observe over 300 billion\nastronomical objects (AO) per year, with the goal of cataloging 100% of\nextragalactic objects by 2025. This represents an unprecedented challenge for\ndata analysis. To address this challenge, we developed a large-scale\nparallelization framework to analyze the LSST AO data. This framework leverages\nparallelization techniques to reduce the time required for processing a single\nAO to less than 10 minutes, compared to up to 25 hours for a single-threaded\nimplementation. This approach enables massive parallelization, achieving\nthroughputs of 1.2 TFLOPS per AO on a single node. This is equivalent to\nprocessing 1.2 million AOs per second, making it possible to analyze all\nobserving seasons in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11409395973154363,
          "p": 0.19540229885057472,
          "f": 0.14406779195525726
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10067114093959731,
          "p": 0.1724137931034483,
          "f": 0.12711863941288443
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2502.13398v1",
      "true_abstract": "Despite recent advancements, most computational methods for molecule\noptimization are constrained to single- or double-property optimization tasks\nand suffer from poor scalability and generalizability to novel optimization\ntasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable\nout-of-domain generalizability to novel tasks. To demonstrate LLMs' potential\nfor molecule optimization, we introduce $\\mathtt{MoMUInstruct}$, the first\nhigh-quality instruction-tuning dataset specifically focused on complex\nmulti-property molecule optimization tasks. Leveraging $\\mathtt{MoMUInstruct}$,\nwe develop $\\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for\nmolecule optimization. Extensive evaluations across 5 in-domain and 5\nout-of-domain tasks demonstrate that $\\mathtt{GeLLM^3O}$s consistently\noutperform state-of-the-art baselines. $\\mathtt{GeLLM^3O}$s also exhibit\noutstanding zero-shot generalization to unseen tasks, significantly\noutperforming powerful closed-source LLMs. Such strong generalizability\ndemonstrates the tremendous potential of $\\mathtt{GeLLM^3O}$s as foundational\nmodels for molecule optimization, thereby tackling novel optimization tasks\nwithout resource-intensive retraining. $\\mathtt{MoMUInstruct}$, models, and\ncode are accessible through https://github.com/ninglab/GeLLMO.",
      "generated_abstract": "We propose a novel neural architecture for unsupervised learning of\ncompound-protein interactions using deep generative models. Our approach\ncombines a variational autoencoder with a generative adversarial network\narchitecture. The encoder of the autoencoder extracts the latent representation\nof a protein from its sequence, and the decoder uses the latent space to\nreconstruct the protein. The encoder and decoder are both trained jointly,\nwhich encourages the latent space to encode the protein information. We\nfurther introduce a novel training strategy that improves the performance of\nthe encoder. By combining our proposed strategy with the variational autoencoder\narchitecture, we demonstrate that our approach can generate high-quality protein\nrepresentations that are highly similar to the original ones, enabling the\nencoder to learn protein representations that are informative for unsupervised\nprotein-protein interaction learning. Our results show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15625,
          "p": 0.19480519480519481,
          "f": 0.173410399684587
        },
        "rouge-2": {
          "r": 0.008,
          "p": 0.00847457627118644,
          "f": 0.008230447679049247
        },
        "rouge-l": {
          "r": 0.15625,
          "p": 0.19480519480519481,
          "f": 0.173410399684587
        }
      }
    },
    {
      "paper_id": "math.LO.math/GN/2502.20887v1",
      "true_abstract": "We prove that it is consistent with ZFC that for every non-decreasing\nfunction $f:[0,1]\\to [0,1]$, each subset of $[0,1]$ of cardinality $\\mathfrak\nc$ contains a set of cardinality $\\mathfrak c$ on which $f$ is uniformly\ncontinuous. We show that this statement follows from the assumptions that\n$\\mathfrak d^* < \\mathfrak c$ and $\\mathfrak c$ is regular, where $\\mathfrak\nd^*\\leq \\mathfrak d$ is the smallest cardinality $\\kappa$ such that any two\ndisjoint countable dense sets in the Cantor set can be separated by sets each\nof which is an intersection of at most $\\kappa$-many open sets in the Cantor\nset. We establish also that $\\mathfrak d^*=\\min\\{\\mathfrak u, \\mathfrak\nd\\}=\\min\\{\\mathfrak r, \\mathfrak d\\}$, thus giving an alternative proof of the\nlatter equality established by J. Aubrey in 2004.",
      "generated_abstract": "In this article, we study the existence of solutions to a system of\nvariational equations of the form\n  \\begin{equation*}\n  \\begin{cases}\n  \\partial_t u_1 + \\partial_x (uv_1 u_2) + \\partial_x (uv_2 u_1) = 0,\\\\\n  \\partial_t u_2 + \\partial_x (uv_1 u_2) + \\partial_x (uv_2 u_1) = 0,\\\\\n  \\partial_t u_3 + \\partial_x (uv_1 u_3) + \\partial_x (uv_2 u_3) = 0,\n  \\end{cases}\n  \\end{equation*}\n  with $u_1, u_2, u_3$ being given functions and $v_1, v_2$ and $v",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07317073170731707,
          "p": 0.13636363636363635,
          "f": 0.09523809069286994
        },
        "rouge-2": {
          "r": 0.008620689655172414,
          "p": 0.018518518518518517,
          "f": 0.01176470154740644
        },
        "rouge-l": {
          "r": 0.04878048780487805,
          "p": 0.09090909090909091,
          "f": 0.06349205894683832
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.02768v1",
      "true_abstract": "We develop a denotational model for programs that have standard programming\nconstructs such as conditionals and while-loops, as well as probabilistic and\nconcurrent commands. Whereas semantic models for languages with either\nconcurrency or randomization are well studied, their combination is limited to\nlanguages with bounded loops. Our work is the first to consider both\nrandomization and concurrency for a language with unbounded looping constructs.\nThe interaction between Boolean tests (arising from the control flow\nstructures), probabilistic actions, and concurrent execution creates challenges\nin generalizing previous work on pomsets and convex languages, prominent models\nfor those effects, individually. To illustrate the generality of our model, we\nshow that it recovers a typical powerdomain semantics for concurrency, as well\nas the convex powerset semantics for probabilistic nondeterminism.",
      "generated_abstract": "This paper introduces a novel approach to the problem of identifying the\nfewest steps that transform an input state to an output state, referred to as\nthe $n$-step transform problem. We propose a novel transformation of the\n$n$-step transform problem into a sequence of problems of the form\n$n$-step transform $k$-best. This transformation enables efficient solution\nmethods for the $n$-step transform problem, such as the algorithm proposed by\nthe authors. We show that the $n$-best problem can be solved in polynomial\ntime in the number of queries and the size of the input, while the $n$-step\ntransform problem cannot be solved in polynomial time in the size of the\ninput. We also show that the $n$-best problem can be solved in linear time in\nthe size of the input, while the $n$-step transform problem cannot be solved\nin linear time in the size of the input. We present an",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1348314606741573,
          "p": 0.2,
          "f": 0.16107382069276174
        },
        "rouge-2": {
          "r": 0.025423728813559324,
          "p": 0.031914893617021274,
          "f": 0.028301881856533435
        },
        "rouge-l": {
          "r": 0.1348314606741573,
          "p": 0.2,
          "f": 0.16107382069276174
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.09903v1",
      "true_abstract": "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
      "generated_abstract": "We provide a new perspective on the relationship between randomness and\nrandomized\n  algorithms by considering the notion of randomized algorithmic information\ngap. This notion is defined in terms of the relative entropy of the\ndistribution of outputs of randomized algorithms under the distribution of the\ninputs of randomized algorithms, where the relative entropy is measured with\nrespect to a suitable probability distribution. We characterize the\nrandomized algorithmic information gap in terms of the relative entropy of the\ndistribution of outputs of randomized algorithms under the distribution of the\ninputs of randomized algorithms. We prove that for a large class of\nrandomized algorithms, the randomized algorithmic information gap is\nexponential-in-logarithmic-size. Furthermore, we show that the\nrandomized algorithmic information gap is polynomial-time-hard to\ncharacterize, and in fact, the randomized algorithmic information gap is\nequivalent to the problem of characterizing",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0945945945945946,
          "p": 0.2545454545454545,
          "f": 0.1379310305321654
        },
        "rouge-2": {
          "r": 0.0091324200913242,
          "p": 0.023255813953488372,
          "f": 0.013114750049127828
        },
        "rouge-l": {
          "r": 0.08783783783783784,
          "p": 0.23636363636363636,
          "f": 0.12807881378339697
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.10078v1",
      "true_abstract": "Image Quality Assessment (IQA) based on human subjective preferences has\nundergone extensive research in the past decades. However, with the development\nof communication protocols, the visual data consumption volume of machines has\ngradually surpassed that of humans. For machines, the preference depends on\ndownstream tasks such as segmentation and detection, rather than visual appeal.\nConsidering the huge gap between human and machine visual systems, this paper\nproposes the topic: Image Quality Assessment for Machine Vision for the first\ntime. Specifically, we (1) defined the subjective preferences of machines,\nincluding downstream tasks, test models, and evaluation metrics; (2)\nestablished the Machine Preference Database (MPD), which contains 2.25M\nfine-grained annotations and 30k reference/distorted image pair instances; (3)\nverified the performance of mainstream IQA algorithms on MPD. Experiments show\nthat current IQA metrics are human-centric and cannot accurately characterize\nmachine preferences. We sincerely hope that MPD can promote the evolution of\nIQA from human to machine preferences. Project page is on:\nhttps://github.com/lcysyzxdxc/MPD.",
      "generated_abstract": "This paper proposes a novel method to enable real-time 3D face reconstruction\nusing only 2D face images, without relying on depth or pose information. We\npropose to leverage the 2D face images as a \"pixel-to-pixel\" translation\nmodel, enabling the reconstruction of 3D faces from the translated images.\nSpecifically, we first translate the face image to a 3D mesh by transforming\nit from the input image's coordinate system to a coordinate system that\ncorresponds to the 3D face mesh. The translation is then performed by\ninverting the mesh to its original coordinate system. Our method is based on\na novel pixel-to-mesh translation model that transforms 2D face images into\n3D meshes by leveraging the similarity between the two images.\n  The proposed method is tested on three different datasets: the\nKIT",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1452991452991453,
          "p": 0.2236842105263158,
          "f": 0.17616579833445206
        },
        "rouge-2": {
          "r": 0.019230769230769232,
          "p": 0.025423728813559324,
          "f": 0.021897805315148414
        },
        "rouge-l": {
          "r": 0.13675213675213677,
          "p": 0.21052631578947367,
          "f": 0.16580310403393395
        }
      }
    },
    {
      "paper_id": "math.GM.math/GM/2502.14876v4",
      "true_abstract": "Series involving hypergeometric functions are used to derive, extend and\nevaluate integrals involving the product of two Bessel functions of the first\nkind $J_{u}(a z)$ $J_{v}(b z)$ with order $u,v$, studied by Landau et al. The\nmethod used in this work is contour integration.",
      "generated_abstract": "We prove that, for $n\\geq 2$, the set of all subgraphs of the graph $K_{n,n}$\nis in bijection with the set of all subsets of $[n",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13157894736842105,
          "p": 0.2631578947368421,
          "f": 0.17543859204678372
        },
        "rouge-2": {
          "r": 0.023255813953488372,
          "p": 0.045454545454545456,
          "f": 0.030769226291124916
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.21052631578947367,
          "f": 0.14035087274853814
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.10208v1",
      "true_abstract": "We explore the Jordan-Chevalley decomposition problem for an operator field\nin small dimensions. In dimensions three and four, we find tensorial conditions\nfor an operator field $L$, similar to a nilpotent Jordan block, to possess\nlocal coordinates in which $L$ takes a strictly upper triangular form. We prove\nthe Tempesta-Tondo conjecture for higher order brackets of\nFr\\\"olicher-Nijenhuis type.",
      "generated_abstract": "In this paper, we study the $L^2$-$L^p$ boundedness of the resolvent operator\nof a self-adjoint differential operator in the class of maximal operators with\nrespect to the norm of $L^p$, which are defined by the resolvent. We establish\nthat the operator is bounded on $L^p$ when $1<p<\\infty$ or $0<p<\\infty$ and\n$1/p+1/2=s$ for some $s\\in(0,1)$. Moreover, we prove that the operator is\nbounded on $L^p$ for $1/p=1/2$ and $1/p=3/2$, which are the natural cases of\n$1/p+1/2=s$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2765957446808511,
          "p": 0.2653061224489796,
          "f": 0.27083332833550355
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2553191489361702,
          "p": 0.24489795918367346,
          "f": 0.2499999950021702
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2412.04505v2",
      "true_abstract": "Accurately interpreting words is vital in political science text analysis;\nsome tasks require assuming semantic stability, while others aim to trace\nsemantic shifts. Traditional static embeddings, like Word2Vec effectively\ncapture long-term semantic changes but often lack stability in short-term\ncontexts due to embedding fluctuations caused by unbalanced training data.\nBERT, which features transformer-based architecture and contextual embeddings,\noffers greater semantic consistency, making it suitable for analyses in which\nstability is crucial. This study compares Word2Vec and BERT using 20 years of\nPeople's Daily articles to evaluate their performance in semantic\nrepresentations across different timeframes. The results indicate that BERT\noutperforms Word2Vec in maintaining semantic stability and still recognizes\nsubtle semantic variations. These findings support BERT's use in text analysis\ntasks that require stability, where semantic changes are not assumed, offering\na more reliable foundation than static alternatives.",
      "generated_abstract": "This paper introduces the first method for automatic identification of\nnon-English and non-Latin characters in images. We present an innovative\nmethod for segmenting characters and their boundaries based on convolutional\nneural networks (CNNs) trained on large datasets. The method leverages\ncharacter-based character recognition, which is more accurate than\ncharacter-based image segmentation. It also addresses the challenge of\nidentifying characters in out-of-domain images, a crucial consideration for\napplications that require the recognition of non-English and non-Latin\ncharacters. Our method achieves an accuracy of 94.1% for Spanish and 88.7% for\nChinese characters, and outperforms existing methods in terms of accuracy,\nspeed, and size. We provide an open-source implementation of the proposed\nmethod on GitHub.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1509433962264151,
          "p": 0.2,
          "f": 0.17204300585038745
        },
        "rouge-2": {
          "r": 0.007407407407407408,
          "p": 0.009345794392523364,
          "f": 0.008264457876855619
        },
        "rouge-l": {
          "r": 0.14150943396226415,
          "p": 0.1875,
          "f": 0.16129031767834448
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06878v1",
      "true_abstract": "Let $G$ be a finite group and $p$ be a prime. We prove that if $G$ has three\ncodegrees, then $G$ is an $M$-group. We prove for some prime $p$ that if every\nirreducible Brauer character of $G$ is a prime, then for every normal subgroup\n$N$ of $G$ either $G/N$ or $N$ is an $M_p$-group.",
      "generated_abstract": "In this paper, we establish the equivariant Hodge-Riemann relations on\nequivariant cohomology of the symmetric space of type $A_1^{(1)}$ and the\nsymmetric space of type $A_2^{(1)}$ with the help of equivariant Hodge theory.\nWe also study the equivariant Hodge-Riemann relations on equivariant cohomology\nof the symmetric space of type $A_1^{(1)}$ and the symmetric space of type $A_2$\nwith the help of equivariant Hodge theory.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08571428571428572,
          "p": 0.11538461538461539,
          "f": 0.09836065084654688
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08571428571428572,
          "p": 0.11538461538461539,
          "f": 0.09836065084654688
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/AP/2503.01566v1",
      "true_abstract": "Extreme response assessment is important in the design and operation of\nengineering structures, and is a crucial part of structural risk and\nreliability analyses. Structures should be designed in a way that enables them\nto withstand the environmental loads they are expected to experience over their\nlifetime, without designs being unnecessarily conservative and costly. An\naccurate risk estimate is essential but difficult to obtain because the\nlong-term behaviour of a structure is typically too complex to calculate\nanalytically or with brute force Monte Carlo simulation. Therefore,\napproximation methods are required to estimate the extreme response using only\na limited number of short-term conditional response calculations. Combining\nsurrogate models with Design of Experiments is an approximation approach that\nhas gained popularity due to its ability to account for both long-term\nenvironment variability and short-term response variability. In this paper, we\npropose a method for estimating the extreme response of black-box, stochastic\nmodels with heteroscedastic non-Gaussian noise. We present a mathematically\nfounded extreme response estimation process that enables Design of Experiment\napproaches that are prohibitively expensive with surrogate Monte Carlo. The\ntheory leads us to speculate this method can robustly produce more confident\nextreme response estimates, and is suitable for a variety of domains. While\nthis needs to be further validated empirically, the method offers a promising\ntool for reducing the uncertainty decision-makers face, allowing them to make\nbetter informed choices and create more optimal structures.",
      "generated_abstract": "The application of artificial intelligence (AI) to healthcare is growing\nat an exponential rate. The field is expected to generate over $30 billion in\nrevenues by 2030, with AI in Healthcare (AIiH) set to grow by 32% to reach\n$26.8 billion by 2030. The AIiH market is highly concentrated with 21 companies\nholding over $1.5 billion in market capitalization. This paper analyses the\nevolution of the AIiH market from 2016 to 2024. Using a bottom-up approach,\nthis paper determines the market size by estimating the revenue generated from\nthe sale of AIiH software and hardware, and services. The paper estimates the\nmarket size of AIiH software and hardware by applying the top-down\nprojection method. The paper",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1038961038961039,
          "p": 0.21621621621621623,
          "f": 0.14035087280855663
        },
        "rouge-2": {
          "r": 0.008968609865470852,
          "p": 0.01834862385321101,
          "f": 0.012048188360612734
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.1891891891891892,
          "f": 0.12280701315943383
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2412.05323v1",
      "true_abstract": "In application-specific designs, owing to the trade-off between power\nconsumption and speed, optimization of various circuit parameters has become a\nchallenging task. Several of the performance metrics, viz. energy efficiency,\ngain, performance, and noise immunity, are interrelated and difficult to tune.\nSuch efforts may result in a great deal of manual iterations which in turn\nincrease the computational overhead. Thus, it is important to develop a\nmethodology that not only explores large design space but also reduces the\ncomputational time. In this work, we investigate the viability of using a SPICE\nand Python IDE (PIDE) interface to optimize integrated circuits. The SPICE\nsimulations are carried out using 22 nm technology node with a nominal supply\nvoltage of 0.8 V. The SPICE-PIDE optimizer, as delineated in this work, is able\nto provide the best solution sets considering various performance metrics and\ndesign complexities for 5 transistor level converters.",
      "generated_abstract": "This paper presents a novel method for generating 3D synthetic\ndata from sparse 2D image-text pairs. The proposed method, called SynthText,\nextends the conventional synthesis pipeline by incorporating textual\ninformation to generate synthetic 3D geometry. We propose a novel approach for\nusing 2D image-text pairs to generate 3D mesh and point clouds. The proposed\nproposed method, called SynthText, extends the conventional synthesis pipeline\nby incorporating textual information to generate synthetic 3D geometry. We\npropose a novel approach for using 2D image-text pairs to generate 3D mesh and\npoint clouds. The proposed method, called SynthText, extends the conventional\nsynthesis pipeline by incorporating textual information to generate synthetic\n3D geometry. We propose a novel approach for using 2D image-text pairs to\ngenerate 3D mesh and point clouds.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.061946902654867256,
          "p": 0.17073170731707318,
          "f": 0.09090908700202412
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.05309734513274336,
          "p": 0.14634146341463414,
          "f": 0.07792207401501117
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.05098v1",
      "true_abstract": "Information-directed sampling (IDS) is a powerful framework for solving\nbandit problems which has shown strong results in both Bayesian and frequentist\nsettings. However, frequentist IDS, like many other bandit algorithms, requires\nthat one have prior knowledge of a (relatively) tight upper bound on the norm\nof the true parameter vector governing the reward model in order to achieve\ngood performance. Unfortunately, this requirement is rarely satisfied in\npractice. As we demonstrate, using a poorly calibrated bound can lead to\nsignificant regret accumulation. To address this issue, we introduce a novel\nfrequentist IDS algorithm that iteratively refines a high-probability upper\nbound on the true parameter norm using accumulating data. We focus on the\nlinear bandit setting with heteroskedastic subgaussian noise. Our method\nleverages a mixture of relevant information gain criteria to balance\nexploration aimed at tightening the estimated parameter norm bound and directly\nsearching for the optimal action. We establish regret bounds for our algorithm\nthat do not depend on an initially assumed parameter norm bound and demonstrate\nthat our method outperforms state-of-the-art IDS and UCB algorithms.",
      "generated_abstract": "This paper introduces the concept of \\emph{multi-scale} graph convolution\n(MGC), a novel graph-based framework for representing complex data structures\nsuch as protein-protein interaction (PPI) networks. MGC consists of a\nmulti-scale convolutional neural network (CNN) and a multi-scale graph\nrepresentation, which is derived from a multi-scale graph convolution. Unlike\ntraditional graph convolutional networks, MGC employs a multi-scale graph\nrepresentation to capture both short-range and long-range dependencies in the\ngraph structure, enabling the model to effectively capture structural information\nand dynamics at different scales simultaneously. Experimental results on\nprotein-protein interaction (PPI) datasets demonstrate that MGC significantly\nimproves performance in predicting protein-protein interactions. The effect of\nMGC on PPI prediction is comparable to that of traditional graph convolutional\nnetworks",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1721311475409836,
          "p": 0.2727272727272727,
          "f": 0.211055271637585
        },
        "rouge-2": {
          "r": 0.024390243902439025,
          "p": 0.0380952380952381,
          "f": 0.029739772192203744
        },
        "rouge-l": {
          "r": 0.14754098360655737,
          "p": 0.23376623376623376,
          "f": 0.18090451786874082
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2409.16613v2",
      "true_abstract": "Oral exams are a powerful tool for educators to gauge student's learning.\nThis is particularly important in introductory statistics classes where many\nstudents struggle to grasp a deep meaning of topics like $p$-values, confidence\nintervals, hypothesis testing, and more. These challenges are only heightened\nin a context where students are learning in a second language. In this paper, I\nshare my experience administering oral exams to an introductory statistics\nclass of non-native English speakers at a Japanese university. I explain the\ncontext of the university and course that the exam was given in, before sharing\ndetails about the two exams. Despite the challenges the students (and I myself)\nfaced, the exams seemed to truly test their statistical knowledge and not\nmerely their English proficiency, as I found little relationship between a\nstudent's English ability and performance. I close with encouragements and\nrecommendations for practitioners hoping to implement these exams, all while\nkeeping an eye towards the unique difficulties faced by students not learning\nin their mother tongue.",
      "generated_abstract": "The use of the generalized method of moments (GMM) is a common approach for\nconducting hypothesis tests in statistical modeling. However, the GMM\ndistribution is not a closed form and its numerical evaluation requires the\ncomputation of the Cholesky decomposition of a covariance matrix. In this\narticle, we propose a new numerical algorithm to compute the GMM distribution\nefficiently. The algorithm is based on a combination of the fast Fourier\ntransform (FFT) and the fast multipole method (FMM). It can be implemented in\nC, and the efficiency of the algorithm is demonstrated on numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11403508771929824,
          "p": 0.20967741935483872,
          "f": 0.14772726816373982
        },
        "rouge-2": {
          "r": 0.012269938650306749,
          "p": 0.023255813953488372,
          "f": 0.016064252506251174
        },
        "rouge-l": {
          "r": 0.10526315789473684,
          "p": 0.1935483870967742,
          "f": 0.13636363180010344
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.01760v1",
      "true_abstract": "Adding a capacity constraint to a hidden-action principal-agent problem\nresults in the same set of Pareto optimal contracts as the unconstrained\nproblem where output is scaled down by a constant factor. This scaling factor\nis increasing in the agent's capacity to exert effort.",
      "generated_abstract": "The use of randomization to study social welfare in game theory has been\nprominent in the field for decades. However, the classical approach to randomization\nin game theory does not fully capture the concept of social welfare. In this paper,\nwe introduce the notion of social welfare that incorporates the objective of\nmaximizing the average welfare. We show that the classical approach to randomization\nis equivalent to the well-known framework of the maximin rule, and we provide an\nalternative derivation of the maximin rule that uses social welfare as the objective\nfunction. We further show that the classical approach to randomization is\nequivalent to the well-known framework of the Bayesian rule and that the Bayesian\nrule is equivalent to the maximin rule. We also show that the classical approach\nto randomization is equivalent to the well-known framework of the Nash rule and\nthat the Nash rule is equivalent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.10344827586206896,
          "f": 0.13186812724550193
        },
        "rouge-2": {
          "r": 0.04878048780487805,
          "p": 0.02197802197802198,
          "f": 0.030303026020432198
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.10344827586206896,
          "f": 0.13186812724550193
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/dis-nn/2503.09389v2",
      "true_abstract": "We study canonical-equilibrium properties of Random Field $O(n)$ Models\ninvolving classical continuous vector spins of $n$ components with mean-field\ninteractions and subject to disordered fields acting on individual spins. To\nthis end, we employ two complementary approaches: the mean-field approximation,\nvalid for any disorder distribution, and the replica trick, applicable when the\ndisordered fields are sampled from a Gaussian distribution. On the basis of an\nexact analysis, we demonstrate that when replica symmetry holds, both the\napproaches yield identical expression for the free energy per spin of the\nsystem. As consequences, we study the case of $n=2$ ($XY$ spins) and that of\n$n=3$ (Heisenberg spins) for two representative choices of the disorder\ndistribution, namely, a Gaussian and a symmetric bimodal distribution. For both\n$n=2$ and $n=3$, we demonstrate that while the magnetization exhibits a\ncontinuous phase transition as a function of temperature for the Gaussian case,\nthe transition could be either continuous or first-order with an emergent\ntricriticality when the disorder distribution is bimodal. We also discuss in\nthe context of our models the issue of self-averaging of extensive variables\nnear the critical point of a continuous phase transition.",
      "generated_abstract": "We study the emergence of collective behavior in a large class of\nnetworks that display a rich variety of interactions, ranging from simple\nsummation rules to more complex interconnections. We focus on a broad class of\nmodels for which the mean-field behavior is well-understood, and we establish a\nconnection between the mean-field behavior and the emergent collective\nbehavior. We show that the mean-field model can be extended to include\ninteractions with arbitrary weights, and we use it to study the emergence of\nnon-trivial collective behavior in such systems. We find that the non-trivial\ncollective behavior is characterized by a phase transition, and that the\ntransition is driven by the competition between the interactions and the\nsummation rules. We also show that the emergence of collective behavior is\nindependent of the number of interacting nodes, and that the emergent\ncollective behavior",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19827586206896552,
          "p": 0.3333333333333333,
          "f": 0.24864864397136607
        },
        "rouge-2": {
          "r": 0.05113636363636364,
          "p": 0.08333333333333333,
          "f": 0.06338027697679068
        },
        "rouge-l": {
          "r": 0.16379310344827586,
          "p": 0.2753623188405797,
          "f": 0.20540540072812283
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.q-bio/TO/2412.15774v1",
      "true_abstract": "A key process during animal morphogenesis is oriented tissue deformation,\nwhich is often driven by internally generated active stresses. Yet, such active\noriented materials are prone to well-known instabilities, raising the question\nof how oriented tissue deformation can be robust during morphogenesis. In a\nsimple scenario, we recently showed that active oriented deformation can be\nstabilized by the boundary-imposed gradient of a scalar field, which\nrepresents, e.g., a morphogen gradient in a developing embryo. Here, we discuss\na more realistic scenario, where the morphogen is produced by a localized\nsource region, diffuses across the tissue, and degrades. Consistent with our\nearlier results, we find that oriented tissue deformation is stable in the\ngradient-extensile case, i.e. when active stresses act to extend the tissue\nalong the direction of the gradient, but it is unstable in the\ngradient-contractile case. In addition, we now show that gradient-contractile\ntissues can not be stabilized even by morphogen diffusion. Finally, we point\nout the existence of an additional instability, which results from the\ninterplay of tissue shear and morphogen diffusion. Our theoretical results\nexplain the lack of gradient-contractile tissues in the biological literature,\nsuggesting that the active matter instability acts as an evolutionary selection\ncriterion.",
      "generated_abstract": "The rapid development of biomaterials in medicine and engineering has\nprompted the need for a better understanding of their biological properties\nand interactions. The study of cellular responses to biomaterials is a\nfundamental challenge in biomaterials science. However, current approaches\noften fail to capture the complex nature of biological responses and lack\nquantitative frameworks. In this work, we introduce a novel approach based on\nthe Biomaterials-Cell Interactions (BIC) framework to study cellular\nresponse to biomaterials. The framework integrates experimental data,\ncomputational models, and machine learning to model cellular responses to\nbiomaterials. We demonstrate the potential of the approach by studying the\nbiological response of human umbilical vein endothelial cells (HUVECs) to\nsilicon wafers. We demonstrate that the BIC framework can capture the complex\nresponse of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12195121951219512,
          "p": 0.19230769230769232,
          "f": 0.14925372659389638
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.02586206896551724,
          "f": 0.019672126433970493
        },
        "rouge-l": {
          "r": 0.11382113821138211,
          "p": 0.1794871794871795,
          "f": 0.13930347783767744
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PM/2410.03552v1",
      "true_abstract": "The growth of the tech startup ecosystem in Latin America (LATAM) is driven\nby innovative entrepreneurs addressing market needs across various sectors.\nHowever, these startups encounter unique challenges and risks that require\nspecific management approaches. This paper explores a case study with the Total\nAddressable Market (TAM), Serviceable Available Market (SAM), and Serviceable\nObtainable Market (SOM) metrics within the context of the online food delivery\nindustry in LATAM, serving as a model for valuing startups using the Discounted\nCash Flow (DCF) method. By analyzing key emerging powers such as Argentina,\nColombia, Uruguay, Costa Rica, Panama, and Ecuador, the study highlights the\npotential and profitability of AI-driven startups in the region through the\ndevelopment of a ranking of emerging powers in Latin America for tech startup\ninvestment. The paper also examines the political, economic, and competitive\nrisks faced by startups and offers strategic insights on mitigating these risks\nto maximize investment returns. Furthermore, the research underscores the value\nof diversifying investment portfolios with startups in emerging markets,\nemphasizing the opportunities for substantial growth and returns despite\ninherent risks.",
      "generated_abstract": "The goal of this paper is to derive the optimal trading strategy in a\nmarket with multiple liquidity providers (LPs) that are subject to transaction\ncosts and are constrained by a fixed capital budget. We model the\ntransactions costs as a cost-to-go (CTG) function, which is a function of\ntransaction size and the price of the underlying asset. We show that the\noptimal trading strategy is an equal weight strategy and derive the optimal\nwealth and capital. Our findings highlight the importance of CTG in the\noptimization of a liquidity-based market.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1271186440677966,
          "p": 0.26785714285714285,
          "f": 0.17241378873827462
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.047058823529411764,
          "f": 0.03137254457516403
        },
        "rouge-l": {
          "r": 0.11016949152542373,
          "p": 0.23214285714285715,
          "f": 0.14942528299114824
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PM/2410.20597v1",
      "true_abstract": "We investigate the effectiveness of a momentum trading signal based on the\ncoverage network of financial analysts. This signal builds on the key\ninformation-brokerage role financial sell-side analysts play in modern stock\nmarkets. The baskets of stocks covered by each analyst can be used to construct\na network between firms whose edge weights represent the number of analysts\njointly covering both firms. Although the link between financial analysts\ncoverage and co-movement of firms' stock prices has been investigated in the\nliterature, little effort has been made to systematically learn the most\neffective combination of signals from firms covered jointly by analysts in\norder to benefit from any spillover effect. To fill this gap, we build a\ntrading strategy which leverages the analyst coverage network using a graph\nattention network. More specifically, our model learns to aggregate information\nfrom individual firm features and signals from neighbouring firms in a\nnode-level forecasting task. We develop a portfolio based on those predictions\nwhich we demonstrate to exhibit an annualized returns of 29.44% and a Sharpe\nratio of 4.06 substantially outperforming market baselines and existing graph\nmachine learning based frameworks. We further investigate the performance and\nrobustness of this strategy through extensive empirical analysis. Our paper\nrepresents one of the first attempts in using graph machine learning to extract\nactionable knowledge from the analyst coverage network for practical financial\napplications.",
      "generated_abstract": "This paper addresses the issue of financial market prediction. The focus of\nthe study is on the prediction of the next three-month S&P 500 index futures\ncontracts. The approach used is a combination of an Artificial Neural Network\n(ANN) and the Backtesting approach. The results of the research show that the\nANN model is more accurate than the Backtesting approach, which is confirmed by\nthe results of the Backtesting method. The results show that the ANN model\nprovides a better predictive performance for the next three-month S&P 500\nfutures contracts than the Backtesting method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.35294117647058826,
          "f": 0.18749999609863285
        },
        "rouge-2": {
          "r": 0.018604651162790697,
          "p": 0.05405405405405406,
          "f": 0.02768165708983422
        },
        "rouge-l": {
          "r": 0.12056737588652482,
          "p": 0.3333333333333333,
          "f": 0.17708332943196622
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2502.17906v2",
      "true_abstract": "In econophysics, there are several enigmatic empirical laws: (i)~the\nmarket-order flow has strong persistence (long-range order-sign correlation),\nwell formulated as the Lillo-Mike-Farmer model. This phenomenon seems\nparadoxical given the diffusive and unpredictable price dynamics; (ii)~the\nprice impact $I(Q)$ of a large metaorder $Q$ follows the square-root law,\n$I(Q)\\propto \\sqrt{Q}$. In this Letter, we propose an exactly solvable model of\nthe nonlinear price-impact dynamics that unifies these enigmas. We generalize\nthe Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped\nto an exactly solvable L\\'evy-walk model. Our exact solution and numerical\nsimulations reveal three important points: First, the price dynamics remains\ndiffusive under the square-root law, even under the long-range correlation.\nSecond, price-movement statistics follows truncated power laws with typical\nexponent around three. Third, volatility has long memory. While this simple\nmodel lacks adjustable free parameters, it naturally aligns even with other\nenigmatic empirical laws, such as (iii)~the inverse-cubic law for price\nstatistics and (iv)~volatility clustering. This work illustrates the crucial\nrole of the square-root law in understanding rich and complex financial price\ndynamics from a single coherent viewpoint.",
      "generated_abstract": "We present a novel, Bayesian framework for optimal portfolio selection and\nmarket portfolio allocation. Our approach leverages an off-the-shelf\nMarkov-Chain Monte Carlo (MCMC) algorithm for efficient sampling of optimal\nwealth distributions, which we combine with a novel Bayesian algorithm for\nefficiently estimating the optimal expected return. By incorporating both\nlikelihood-based and prior-based information, we improve the efficiency and\nrobustness of our method. We apply our framework to the U.S. equity market,\nevaluating the performance of market portfolios across different time horizons.\nOur results demonstrate that our method yields a more robust and efficient\nportfolio selection and market portfolio allocation methodology than\ntraditional approaches, offering a practical solution for portfolio management\nand investment strategies.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.109375,
          "p": 0.1794871794871795,
          "f": 0.13592232539164875
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1015625,
          "p": 0.16666666666666666,
          "f": 0.1262135875275711
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/TR/2501.07489v1",
      "true_abstract": "The efficient market hypothesis (EMH) famously stated that prices fully\nreflect the information available to traders. This critically depends on the\ntransfer of information into prices through trading strategies. Traders\noptimise their strategy with models of increasing complexity that identify the\nrelationship between information and profitable trades more and more\naccurately. Under specific conditions, the increased availability of low-cost\nuniversal approximators, such as AI systems, should be naturally pushing\ntowards more advanced trading strategies, potentially making it harder and\nharder for inefficient traders to profit. In this paper, we leverage on a\ngeneralised notion of market efficiency, based on the definition of an\nequilibrium price process, that allows us to distinguish different levels of\nmodel complexity through investors' beliefs, and trading strategies\noptimisation, and discuss the relationship between AI-powered trading and the\ntime-evolution of market efficiency. Finally, we outline the need for and the\nchallenge of describing out-of-equilibrium market dynamics in an adaptive\nmulti-agent environment.",
      "generated_abstract": "We introduce a novel framework for risk-averse investors to invest in\nrisky assets that are sensitive to the price of a correlated asset. We show\nthat the risk-averse investor can derive an optimal trading strategy to\nmitigate the risk of the correlated asset. Moreover, we show that under\nassumptions that the correlated asset is a log-normal random variable, the\nrisk-averse investor can derive an optimal trading strategy to mitigate the\nrisk of the correlated asset. The optimal trading strategy is obtained by\nsolving an optimal control problem, which can be efficiently solved using\nexisting methods in optimal control.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1308411214953271,
          "p": 0.2692307692307692,
          "f": 0.1761006245290931
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.102803738317757,
          "p": 0.21153846153846154,
          "f": 0.13836477547248938
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.19391v1",
      "true_abstract": "Antibody co-design represents a critical frontier in drug development, where\naccurate prediction of both 1D sequence and 3D structure of\ncomplementarity-determining regions (CDRs) is essential for targeting specific\nepitopes. Despite recent advances in equivariant graph neural networks for\nantibody design, current approaches often fall short in capturing the intricate\ninteractions that govern antibody-antigen recognition and binding specificity.\nIn this work, we present Igformer, a novel end-to-end framework that addresses\nthese limitations through innovative modeling of antibody-antigen binding\ninterfaces. Our approach refines the inter-graph representation by integrating\npersonalized propagation with global attention mechanisms, enabling\ncomprehensive capture of the intricate interplay between local chemical\ninteractions and global conformational dependencies that characterize effective\nantibody-antigen binding. Through extensive validation on epitope-binding CDR\ndesign and structure prediction tasks, Igformer demonstrates significant\nimprovements over existing methods, suggesting that explicit modeling of\nmulti-scale residue interactions can substantially advance computational\nantibody design for therapeutic applications.",
      "generated_abstract": "The evolution of living organisms has been driven by the accumulation of\nmolecular modifications, and the formation of macromolecular complexes.\nComplexity arises from the combination of individual molecular components, and\nthe evolutionary forces driving the selection of functional units. Here, we\nreview the evolutionary forces that drive the selection of functional units in\nbacterial communities. We focus on the interactions between genes, the\ncomposition of communities, and the emergence of co-evolutionary patterns. The\ninteractions between genes and the composition of communities are driven by\nthe evolution of gene content, the evolution of the environment, and the\nselection of functional units. The emergence of co-evolutionary patterns is\ndriven by the evolution of functional units, the evolution of the environment,\nand the evolution of genes. We discuss the role of genetic variation in\ncontrolling the evolution of functional units and gene",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09401709401709402,
          "p": 0.18333333333333332,
          "f": 0.12429378082926378
        },
        "rouge-2": {
          "r": 0.006993006993006993,
          "p": 0.010416666666666666,
          "f": 0.008368196030184304
        },
        "rouge-l": {
          "r": 0.07692307692307693,
          "p": 0.15,
          "f": 0.10169491077276666
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.12860v4",
      "true_abstract": "We study the problem of an organization that matches agents to objects where\nagents have preference rankings over objects and the organization uses\nalgorithms to construct a ranking over objects on behalf of each agent. Our new\nframework carries the interpretation that the organization and its agents may\nbe misaligned in pursuing some underlying matching goal. We design matching\nmechanisms that integrate agent decision-making and the algorithm by avoiding\nmatches that are unanimously disagreeable between the two parties. Our\nmechanisms also satisfy restricted efficiency properties. Subsequently, we\nprove that no unanimous mechanism is strategy-proof but that ours can be\nnon-obviously manipulable. We generalize our framework to allow for any\npreference aggregation rules and extend the famed Gibbard-Satterthwaite Theorem\nto our setting. We apply our framework to place foster children in foster homes\nto maximize welfare. Using a machine learning model that predicts child welfare\nin placements and a (planned) novel lab-in-the-field eliciting real\ncaseworkers' preferences, we empirically demonstrate that there are important\nmatch-specific welfare gains that our mechanisms extract that are not realized\nunder the status quo.",
      "generated_abstract": "We present a novel approach to the multi-agent reinforcement learning problem\nwith a general, non-linear, and non-convex cost function. Our approach\ncombines the so-called Pareto frontier with a robust approach to\nassessing convergence to a local optimum. We establish convergence guarantees\nunder a mild assumption that the cost function is Lipschitz and non-decreasing\nin the state. Our approach is applicable to a broad class of cost functions,\nincluding both convex and non-convex ones. We apply our method to a\nmulti-agent reinforcement learning problem with a general, non-linear, and\nnon-convex cost function. We show that our approach can improve the\nperformance of reinforcement learning algorithms compared to the traditional\nPareto frontier approach.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15254237288135594,
          "p": 0.3103448275862069,
          "f": 0.2045454501265497
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.0449438202247191,
          "f": 0.03088802637706719
        },
        "rouge-l": {
          "r": 0.13559322033898305,
          "p": 0.27586206896551724,
          "f": 0.18181817739927697
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2410.04149v1",
      "true_abstract": "This paper introduces Mov-Avg, the Python software package for time series\nanalysis that requires little computer programming experience from the user.\nThe package allows the identification of trends, patterns, and the prediction\nof future events based on data collected over time. In this regard, the Mov-Avg\nimplementation provides three indicators to apply, namely: Simple Moving\nAverage, Weighted Moving Average and Exponential Moving Average. Due to its\ngeneric design, the Mov-Avg software package can be used in any field where the\napplication of moving averages is valid. In general, the Mov-Avg library for\ntime series analysis contributes to a better understanding of data-driven\nprocesses over time by taking advantage of moving averages in any way adapted\nto the research context.",
      "generated_abstract": "In this paper, we explore the use of AI in healthcare to provide patient\ninformation to clinicians and support the provision of care. We discuss the\nchallenges and opportunities for AI in healthcare, focusing on the potential\nbenefits and risks of AI-driven decision-making. We highlight the need for\nresearch and policy to ensure that AI-based tools are designed and\ndeployed appropriately, with a focus on ensuring equitable access to care and\naddressing the social and ethical implications of AI-driven decision-making.\n  We introduce a framework for considering the integration of AI in healthcare\nthat includes five key elements: (1) alignment with ethical principles, (2)\ntransparency and accountability, (3) patient empowerment and engagement, (4)\ndata access and privacy, and (5) robust governance and o",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1566265060240964,
          "p": 0.16455696202531644,
          "f": 0.1604938221635423
        },
        "rouge-2": {
          "r": 0.009345794392523364,
          "p": 0.008771929824561403,
          "f": 0.00904976876067514
        },
        "rouge-l": {
          "r": 0.13253012048192772,
          "p": 0.13924050632911392,
          "f": 0.13580246413885097
        }
      }
    },
    {
      "paper_id": "math.RT.math/RA/2503.10461v1",
      "true_abstract": "This article studies the compatibility of Koenig's notion of an exact Borel\nsubalgebra of a quasi-hereditary or, more generally, standardly stratified\nalgebra with taking idempotent subalgebras or quotients. As an application, we\nprovide bounds for the multiplicities of indecomposable projectives in the\nprincipal blocks of BGG category $\\mathcal{O}$ having basic regular exact Borel\nsubalgebras.",
      "generated_abstract": "We consider the class of $n$-dimensional hyperbolic spaces, and we investigate\ntheir asymptotic behavior as $n$ tends to infinity. We prove that the\nperimeter-area law holds for the hyperbolic spaces of genus $g\\geq 2$ and for\ntheir generalizations. Moreover, we prove the existence of an intrinsic\nvolume-area scaling law for the hyperbolic spaces of genus $g\\geq 2$ as well as\nfor their generalizations. Finally, we prove the existence of an intrinsic\nvolume-volume scaling law for the hyperbolic spaces of genus $g\\geq 2$ as well\nas for their generalizations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11363636363636363,
          "p": 0.1282051282051282,
          "f": 0.12048192272898843
        },
        "rouge-2": {
          "r": 0.038461538461538464,
          "p": 0.03508771929824561,
          "f": 0.0366972427169437
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.10256410256410256,
          "f": 0.09638553718681983
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10114v1",
      "true_abstract": "We design specific neural networks (NNs) for the identification of switching\nnonlinear systems in the state-space form, which explicitly model the switching\nbehavior and address the inherent coupling between system parameters and\nswitching modes. This coupling is specifically addressed by leveraging the\nexpectation-maximization (EM) framework. In particular, our technique will\ncombine a moving window approach in the E-step to efficiently estimate the\nswitching sequence, together with an extended Kalman filter (EKF) in the M-step\nto train the NNs with a quadratic convergence rate. Extensive numerical\nsimulations, involving both academic examples and a battery charge management\nsystem case study, illustrate that our technique outperforms available ones in\nterms of parameter estimation accuracy, model fitting, and switching sequence\nidentification.",
      "generated_abstract": "The emergence of autonomous vehicles (AVs) has raised concerns about the\npotential impact on the safety of pedestrians and cyclists, particularly in\nurban areas. This study examines the effects of AVs on pedestrian and\ncyclist safety in urban environments. We introduce a novel framework that\ncombines the Dynamic Highway Network (DHN) model with a risk-based\nsimulation framework to assess the effects of AVs on pedestrian and\ncyclist safety. Our results show that AVs can reduce traffic accidents,\nparticularly among pedestrians and cyclists, by 30.2% and 47.1%, respectively.\nHowever, the reduction in traffic accidents is limited when the AV density is\nlow. Our findings highlight the importance of considering AVs in urban\ntransportation systems to ensure safe, sustainable, and equitable transport\noptions for all users",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.18823529411764706,
          "f": 0.18497109326739966
        },
        "rouge-2": {
          "r": 0.009009009009009009,
          "p": 0.008849557522123894,
          "f": 0.008928566428972825
        },
        "rouge-l": {
          "r": 0.1590909090909091,
          "p": 0.16470588235294117,
          "f": 0.16184970598416268
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.04555v1",
      "true_abstract": "This article analyzes a key exchange protocol based on the triad tropical\nsemiring, recently proposed by Jackson, J. and Perumal, R. We demonstrate that\nthe triad tropical semiring is isomorphic to a circulant matrix over tropical\nnumbers. Consequently, matrices in this semiring can be represented as tropical\nmatrices. As a result, we conduct a cryptanalysis of the key exchange protocol\nusing an algorithm introduced by Sulaiman Alhussaini, Craig Collett, and Sergei\nSergeev to solve the double discrete logarithm problem over tropical matrices",
      "generated_abstract": "We prove that the class of $\\mathrm{MCG}(S^3)$-invariant functions on\nS^3 is a homogeneous variety of dimension $24$ (which we call $\\mathrm{MCG}(S^3)$).\nThis is the first result of this kind for a surface group. We also describe an\nalgebraic construction for the group, and prove that $\\mathrm{MCG}(S^3)$ is\nisomorphic to the universal cover of the symmetric group $\\mathrm{Sym}(4)$.\nMoreover, we give a complete classification of $\\mathrm{MCG}(S^3)$-invariant\nfunctions on S^3. In particular, the function $f$ obtained in our construction\nfor the symmetric group $\\mathrm{Sym}(4)$ is a power of $f_2$, which is the\nclassical Weierstrass $p$-function.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2459016393442623,
          "p": 0.2542372881355932,
          "f": 0.24999999500138892
        },
        "rouge-2": {
          "r": 0.05333333333333334,
          "p": 0.05,
          "f": 0.05161289823100986
        },
        "rouge-l": {
          "r": 0.2459016393442623,
          "p": 0.2542372881355932,
          "f": 0.24999999500138892
        }
      }
    },
    {
      "paper_id": "math.GR.math/AT/2503.08411v1",
      "true_abstract": "In this article, we prove that, given two finite connected graphs $\\Gamma_1$\nand $\\Gamma_2$, if the two right-angled Artin groups $A(\\Gamma_1)$ and\n$A(\\Gamma_2)$ are quasi-isometric, then the infinite pointed sums\n$\\bigvee_\\mathbb{N} \\Gamma_1^{\\bowtie}$ and $\\bigvee_\\mathbb{N}\n\\Gamma_2^{\\bowtie}$ are homotopy equivalent, where $\\Gamma_i^{\\bowtie}$ denotes\nthe simplicial complex whose vertex-set is $\\Gamma_i$ and whose simplices are\ngiven by joins. These invariants are extracted from a study, of independent\ninterest, of the homotopy types of several complexes of hyperplanes in\nquasi-median graphs (such as one-skeleta of CAT(0) cube complexes). For\ninstance, given a quasi-median graph $X$, the \\emph{crossing complex}\n$\\mathrm{Cross}^\\triangle(X)$ is the simplicial complex whose vertices are the\nhyperplanes (or $\\theta$-classes) of $X$ and whose simplices are collections of\npairwise transverse hyperplanes. When $X$ has no cut-vertex, we show that\n$\\mathrm{Cross}^\\triangle(X)$ is homotopy equivalent to the pointed sum of the\nlinks of all the vertices in the prism-completion $X^\\square$ of $X$.",
      "generated_abstract": "The $n$-dimensional complete flag manifold $\\mathcal{F}(n)$ is a complex\nalgebraic\n  variety defined by a system of linear equations in homogeneous coordinates.\nThis article introduces a new method for computing the complex dimension of\n$\\mathcal{F}(n)$ by using the geometry of algebraic varieties and their\nsymplectic dual spaces. The method is based on the theory of symplectic\ninvariants of algebraic varieties, and it also uses the geometry of the\nBirkhoff--G\\\"unther family of symplectic manifolds. This approach yields a\ngeneral formula for computing the complex dimension of $\\mathcal{F}(n)$ in terms\nof the symmetric powers of the dual spaces of the varieties in the Birkhoff--G\\\"unther\nfamily. This formula is valid for any integer $n$. We also show that the\ncomplex dimension of $\\mathcal{F}(n)$ is equal to the number of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.18461538461538463,
          "f": 0.15286623718609288
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.038834951456310676,
          "f": 0.03347279844260502
        },
        "rouge-l": {
          "r": 0.10869565217391304,
          "p": 0.15384615384615385,
          "f": 0.1273885301797235
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2502.13720v1",
      "true_abstract": "Ecosystems often demonstrate the coexistence of numerous species competing\nfor limited resources, with pronounced rarity and abundance patterns. A\npotential driver of such coexistence is environmental fluctuations that favor\ndifferent species over time. However, how to include and treat such temporal\nvariability in existing consumer-resource models is still an open problem. In\nthis study, we examine the role of correlated temporal fluctuations in\nmetabolic strategies within a stochastic consumer-resource framework,\nreflecting change of species behavior in response to the environment. In some\nconditions, we are able to solve analytically the species abundance\ndistributions, through path integral formalism. Our results reveal that\nstochastic dynamic metabolic strategies induce community structures that align\nmore closely with empirical ecological observations and contribute to the\nviolation of the Competitive Exclusion Principle (CEP). The degree of CEP\nviolation is maximized under intermediate competition strength, leading to an\nintermediate competition hypothesis. Furthermore, when non-neutral effects are\npresent, maximal biodiversity is achieved for intermediate values of the\namplitude of fluctuations. This work not only challenges traditional ecological\nparadigms, but also establishes a robust theoretical framework for exploring\nhow temporal dynamics and stochasticity drive biodiversity and community.",
      "generated_abstract": "In recent years, artificial intelligence (AI) has become a popular approach\nin computational biology. However, there is still a lack of robust AI models\nthat can effectively predict and simulate cellular behaviors, especially\nthose that can accurately simulate cellular responses to external stimuli. In\nthis paper, we propose a novel approach that integrates a recurrent neural\nnetwork (RNN) with an artificial neural network (ANN) to predict the cellular\nbehavior of a cell population. The RNN model captures the temporal dynamics of\nthe cell population, while the ANN model is used to predict the cellular\nresponse to external stimuli. The model was evaluated using a cancer model\nthat has been extensively studied in the literature. Our results show that the\nmodel can accurately predict the cellular behavior of the cancer model and\nprovide valuable insights into the cellular response to external stimuli. The\nmodel also demonstr",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19230769230769232,
          "p": 0.30120481927710846,
          "f": 0.23474177928100695
        },
        "rouge-2": {
          "r": 0.03278688524590164,
          "p": 0.04918032786885246,
          "f": 0.03934425749508255
        },
        "rouge-l": {
          "r": 0.18461538461538463,
          "p": 0.2891566265060241,
          "f": 0.22535210791950464
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.16407v1",
      "true_abstract": "Various factors influence why some countries are more open to immigration\nthan others. Policy is only one of them. We design country-specific measures of\nopenness to immigration that aim to capture de facto levels of openness to\nimmigration, complementing existing de jure measures of immigration, based on\nenacted immigration laws and policy measures. We estimate these for 148\ncountries and three years (2000, 2010, and 2020). For a subset of countries, we\nalso distinguish between openness towards tertiary-educated migrants and less\nthan tertiary-educated migrants. Using the measures, we show that most places\nin the World today are closed to immigration, and a few regions are very open.\nThe World became more open in the first decade of the millennium, an opening\nmainly driven by the Western World and the Gulf countries. Moreover, we show\nthat other factors equal, countries that increased their openness to\nimmigration, reduced their old-age dependency ratios, and experienced slower\nreal wage growth, arguably a sign of relaxing labor and skill shortages.",
      "generated_abstract": "The recent COVID-19 pandemic has revealed the importance of social\ndynamics, such as social networks, in shaping the epidemic. A large body of\nresearch has demonstrated that network centrality metrics are highly correlated\nwith the spread of the epidemic, and the social influence of individuals can be\ndetermined by the network topology. However, existing studies often fail to\ndistinguish between different network topologies, such as the social network\n(SN) and the complex network (CN), thereby missing the role of the underlying\ncomplex network structure in influencing the spread of the epidemic. To fill\nthis gap, we introduce a novel framework for studying the influence of complex\nnetwork structures on the spread of the epidemic. Using the COVID-19 dataset\nfrom China, we first construct a complex network structure by integrating\ndifferent types of network topology, including the SN, CN, and the network",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1792452830188679,
          "p": 0.2261904761904762,
          "f": 0.19999999506703614
        },
        "rouge-2": {
          "r": 0.03289473684210526,
          "p": 0.04132231404958678,
          "f": 0.03663003169450885
        },
        "rouge-l": {
          "r": 0.16981132075471697,
          "p": 0.21428571428571427,
          "f": 0.18947367927756245
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2410.15097v1",
      "true_abstract": "This paper advances a variable screening approach to enhance conditional\nquantile forecasts using high-dimensional predictors. We have refined and\naugmented the quantile partial correlation (QPC)-based variable screening\nproposed by Ma et al. (2017) to accommodate $\\beta$-mixing time-series data.\nOur approach is inclusive of i.i.d scenarios but introduces new convergence\nbounds for time-series contexts, suggesting the performance of QPC-based\nscreening is influenced by the degree of time-series dependence. Through Monte\nCarlo simulations, we validate the effectiveness of QPC under weak dependence.\nOur empirical assessment of variable selection for growth-at-risk (GaR)\nforecasting underscores the method's advantages, revealing that specific labor\nmarket determinants play a pivotal role in forecasting GaR. While prior\nempirical research has predominantly considered a limited set of predictors, we\nemploy the comprehensive Fred-QD dataset, retaining a richer breadth of\ninformation for GaR forecasts.",
      "generated_abstract": "We propose a novel method for estimating the mean and variance of the\nregression coefficients in a linear regression model. This method is based on\nthe use of a finite-dimensional approximation of the inverse Fisher information\nmatrix (FIM). Our approach is illustrated using simulation studies and a\nreal data application on the determinants of the price elasticity of energy\ndemand. We demonstrate that our method is more efficient than the standard\nFisher information estimator and also offers a more flexible option for\nestimating the FIM.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15841584158415842,
          "p": 0.2857142857142857,
          "f": 0.20382165146172265
        },
        "rouge-2": {
          "r": 0.014925373134328358,
          "p": 0.025974025974025976,
          "f": 0.0189573413364491
        },
        "rouge-l": {
          "r": 0.15841584158415842,
          "p": 0.2857142857142857,
          "f": 0.20382165146172265
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00290v1",
      "true_abstract": "I present a novel uniform law of large numbers (ULLN) for network-dependent\ndata. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive\nsuite of limit theorems and a robust variance estimator for network-dependent\nprocesses, their analysis focuses on pointwise convergence. On the other hand,\nuniform convergence is essential for nonlinear estimators such as M and GMM\nestimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I\nestablish the ULLN under network dependence and demonstrate its utility by\nproving the consistency of both M and GMM estimators. A byproduct of this work\nis a novel maximal inequality for network data, which may prove useful for\nfuture research beyond the scope of this paper.",
      "generated_abstract": "We propose a novel method for estimating dynamic panel data models with\nstructural heterogeneity using a simple methodology. We focus on a class of\ndynamic panel data models that are widely used in econometrics, and we propose\nan efficient estimator for these models. We first introduce a novel class of\ndynamic panel data models that include a structural heterogeneity. We then\npropose an efficient estimator for this class of dynamic panel data models. We\nshow that our estimator is consistent and asymptotically normal. We further\nshow that the estimator is consistent and asymptotically normal in the\nhigh-dimensional setting. We further show that our estimator is consistent and\nasymptotically normal in the high-dimensional setting, and we also show that\nour estimator is consistent and asymptotically normal in the high-dimensional\nsetting when the number of observations is large. We provide simulation\nresults to illustrate the performance of our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15294117647058825,
          "p": 0.21666666666666667,
          "f": 0.17931033997621892
        },
        "rouge-2": {
          "r": 0.01818181818181818,
          "p": 0.021739130434782608,
          "f": 0.019801975237723035
        },
        "rouge-l": {
          "r": 0.15294117647058825,
          "p": 0.21666666666666667,
          "f": 0.17931033997621892
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.10575v1",
      "true_abstract": "We discuss non-reversible Markov-chain Monte Carlo algorithms that, for\nparticle systems, rigorously sample the positional Boltzmann distribution and\nthat have faster than physical dynamics. These algorithms all feature a\nnon-thermal velocity distribution. They are exemplified by the lifted TASEP\n(totally asymmetric simple exclusion process), a one-dimensional lattice\nreduction of event-chain Monte Carlo. We analyze its dynamics in terms of a\nvelocity trapping that arises from correlations between the local density and\nthe particle velocities. This allows us to formulate a conjecture for its\nout-of-equilibrium mixing time scale, and to rationalize its equilibrium\nsuperdiffusive time scale. Both scales are faster than for the (unlifted)\nTASEP. They are further justified by our analysis of the lifted TASEP in terms\nof many-particle realizations of true self-avoiding random walks. We discuss\nvelocity trapping beyond the case of one-dimensional lattice models and in more\nthan one physical dimensions. Possible applications beyond physics are pointed\nout.",
      "generated_abstract": "We investigate the properties of a two-dimensional (2D) interacting\nnetwork with an anisotropic coupling matrix and a single-site repulsive\ninteraction. We consider the case of an infinite system and analyze the\ndependence of the stationary state on the anisotropy parameter and the\nrepulsive interaction strength. We also perform a numerical study of the\nstatistical properties of the system, such as the stationary density,\ncorrelation functions, and the average correlation time. Our results reveal\nthat the anisotropy parameter and the repulsive interaction strength have\nsignificant effects on the stationary state, with a strong anisotropy\ninfluencing the stationary state, while a weak repulsive interaction has\nlittle effect. We also find that the stationary density exhibits a\ncharacteristic bimodal distribution, with a larger population of the lower\ndensity component.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11458333333333333,
          "p": 0.15942028985507245,
          "f": 0.13333332846721782
        },
        "rouge-2": {
          "r": 0.03597122302158273,
          "p": 0.049019607843137254,
          "f": 0.04149377105146319
        },
        "rouge-l": {
          "r": 0.11458333333333333,
          "p": 0.15942028985507245,
          "f": 0.13333332846721782
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2412.18776v1",
      "true_abstract": "The Virtual Traffic Light (VTL) eliminates the need for physical traffic\nsignal infrastructure at intersections, leveraging Connected Vehicles (CVs) to\noptimize traffic flow. VTL assigns right-of-way dynamically based on factors\nsuch as estimated times of arrival (ETAs), the number of CVs in various lanes,\nand emission rates. These factors are considered in line with the objectives of\nthe VTL application. Aiming to optimize traffic flow and reduce delays, the VTL\nsystem generates Signal Phase and Timing (SPaT) data for CVs approaching an\nintersection, while considering the impact of each CV movement on others.\nHowever, the stochastic nature of vehicle arrivals at intersections complicates\nreal-time optimization, challenging classical computing methods. To address\nthis limitation, we develop a VTL method that leverages quantum computing to\nminimize stopped delays for CVs. The method formulates the VTL problem as a\nQuadratic Unconstrained Binary Optimization (QUBO) problem, a mathematical\nframework well-suited for quantum computing. Using D-Wave cloud-based quantum\ncomputer, our approach determines optimal solutions for right-of-way\nassignments under the standard National Electrical Manufacturers Association\n(NEMA) phasing system. The system was evaluated using the microscopic traffic\nsimulator SUMO under varying traffic volumes. Our results demonstrate that the\nquantum-enabled VTL system reduces stopped delays and travel times compared to\nclassical optimization-based systems. This approach not only enhances traffic\nmanagement efficiency but also reduces the infrastructure costs associated with\ntraditional traffic signals. The quantum computing-supported VTL system offers\na transformative solution for large-scale traffic control, providing superior\nperformance across diverse traffic scenarios and paving the way for advanced,\ncost-effective traffic management.",
      "generated_abstract": "The current state of the art in autonomous driving is that sensors and\ncomputational\n  algorithms in the vehicle are capable of perceiving the surrounding environment,\nbut the vehicle is still\n  incapable of making decisions based on the perceptions it has collected. This\nlimitation\n  is due to the fact that the perception process itself is computationally\nintensive and\n  time-consuming, while the decision-making process, which is the crucial\ncomponent of\n  autonomous driving, is relatively cheap. The primary challenge in developing\na\n  practical and robust decision-making algorithm is the need to consider a\nwide\n  variety of scenarios and their corresponding decisions. In this paper, we\npropose\n  a novel framework that combines reinforcement learning (RL) with\ndynamic-programming to address this challenge. Our approach consists of three\nparts. The first part is a reinforcement learning module",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13953488372093023,
          "p": 0.2727272727272727,
          "f": 0.18461538013727818
        },
        "rouge-2": {
          "r": 0.024896265560165973,
          "p": 0.047244094488188976,
          "f": 0.032608691132000864
        },
        "rouge-l": {
          "r": 0.11627906976744186,
          "p": 0.22727272727272727,
          "f": 0.15384614936804744
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/ET/2503.07799v1",
      "true_abstract": "Congenital Heart Disease (CHD) is one of the leading causes of fetal\nmortality, yet the scarcity of labeled CHD data and strict privacy regulations\nsurrounding fetal ultrasound (US) imaging present significant challenges for\nthe development of deep learning-based models for CHD detection. Centralised\ncollection of large real-world datasets for rare conditions, such as CHD, from\nlarge populations requires significant co-ordination and resource. In addition,\ndata governance rules increasingly prevent data sharing between sites. To\naddress these challenges, we introduce, for the first time, a novel\nprivacy-preserving, zero-shot CHD detection framework that formulates CHD\ndetection as a normality modeling problem integrated with model merging. In our\nframework dubbed Sparse Tube Ultrasound Distillation (STUD), each hospital site\nfirst trains a sparse video tube-based self-supervised video anomaly detection\n(VAD) model on normal fetal heart US clips with self-distillation loss. This\nenables site-specific models to independently learn the distribution of healthy\ncases. To aggregate knowledge across the decentralized models while maintaining\nprivacy, we propose a Divergence Vector-Guided Model Merging approach,\nDivMerge, that combines site-specific models into a single VAD model without\ndata exchange. Our approach preserves domain-agnostic rich spatio-temporal\nrepresentations, ensuring generalization to unseen CHD cases. We evaluated our\napproach on real-world fetal US data collected from 5 hospital sites. Our\nmerged model outperformed site-specific models by 23.77% and 30.13% in accuracy\nand F1-score respectively on external test sets.",
      "generated_abstract": "In the field of computer vision, a significant challenge is the development of\ndiverse and efficient models for various tasks. This paper introduces a\nnovel, efficient model architecture that combines the power of a Vision Transformer\nwith the performance of a linear classifier, known as the Vision-Linear\n(ViL) model. The ViL model consists of a Vision Transformer encoder and a linear\nclassifier decoder. It is capable of generalizing across a variety of tasks and\napplications, including image classification, object detection, and semantic\nsegmentation. To further improve its efficiency, we introduce a novel\noptimization strategy that enhances the model's performance by improving the\nalignment of the encoder and decoder layers. We demonstrate the effectiveness\nof our proposed method by conducting extensive experiments on various benchmark\ndatasets, including CIFAR-10, CIFAR-100, Image",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14906832298136646,
          "p": 0.27586206896551724,
          "f": 0.1935483825419486
        },
        "rouge-2": {
          "r": 0.027149321266968326,
          "p": 0.04918032786885246,
          "f": 0.03498541815706101
        },
        "rouge-l": {
          "r": 0.14906832298136646,
          "p": 0.27586206896551724,
          "f": 0.1935483825419486
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.02802v1",
      "true_abstract": "In this work, we show the first average-case reduction transforming the\nsparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a\nconsequence obtain the first computational equivalence result between two\nwell-studied high-dimensional statistics models. Our approach leverages a new\nperturbation equivariance property for Gram-Schmidt orthogonalization, enabling\nremoval of dependence in the noise while preserving the signal.",
      "generated_abstract": "This paper studies the problem of estimating a function of two random variables\nwith a small number of observations. The function of interest is a\nnonparametric function of the form $f(x,y) = h(x) g(y)$, where $x$ is a\nrandom variable and $y$ is a random variable taking values in a discrete set.\nThe functions $h$ and $g$ are assumed to be measurable with respect to the\nobservation space. The goal is to estimate $h$ and $g$ based on a small number\nof data points, using only the information contained in the observations. In\nthis paper, we propose a Bayesian approach for estimating $h$ and $g$ based on\nthe data. The proposed approach is based on the conditional posterior\ndistribution, which is obtained by integrating the prior over the data. We\nprove that the proposed Bayesian approach achieves the same asymptotic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22448979591836735,
          "p": 0.1375,
          "f": 0.17054263094765956
        },
        "rouge-2": {
          "r": 0.03636363636363636,
          "p": 0.016666666666666666,
          "f": 0.02285713854693959
        },
        "rouge-l": {
          "r": 0.20408163265306123,
          "p": 0.125,
          "f": 0.1550387549786673
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.10360v1",
      "true_abstract": "Time-frequency concentration and resolution of the Cohen's class\ntime-frequency distribution (CCTFD) has attracted much attention in\ntime-frequency analysis. A variety of uncertainty principles of the CCTFD is\ntherefore derived, including the weak Heisenberg type, the Hardy type, the\nNazarov type, and the local type. However, the standard Heisenberg type still\nremains unresolved. In this study, we address the question of how the standard\nHeisenberg's uncertainty principle of the CCTFD is affected by fundamental\nproperties. The investigated distribution properties are Parseval's relation\nand the concise frequency domain definition (i.e., only frequency variables are\nexplicitly found in the tensor product), based on which we confine our\nattention to the CCTFD with some specific kernels. That is the unit modulus and\nv-independent time translation, reversal and scaling invariant kernel CCTFD\n(UMITRSK-CCTFD). We then extend the standard Heisenberg's uncertainty\nprinciples of the Wigner distribution to those of the UMITRSK-CCTFD, giving\nbirth to various types of attainable lower bounds on the uncertainty product in\nthe UMITRSK-CCTFD domain. The derived results strengthen the existing weak\nHeisenberg type and fill gaps in the standard Heisenberg type.",
      "generated_abstract": "In recent years, the rapid development of deep learning has enabled the\ndevelopment of large language models (LLMs) for a wide range of tasks, such as\nnatural language processing (NLP), speech recognition, and music generation.\nHowever, existing LLM-based music generation models are often trained on\nlarge-scale datasets, such as OpenAudio, which are usually unavailable to\nsmaller music generation tasks, such as music generation for music\ncomposition. In this work, we propose a novel framework, called MusicGPT,\nwhich leverages a pre-trained Large Language Model (LLM) to generate music\ninstructions for music composition. Specifically, we introduce a music\ncomposition language model (MusicGPT), which takes music-related prompts as\ninput and generates music instructions. Then, we propose a MusicGPT-M\narchitecture, which incorporates a music-related LLM, a music-related prompt",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11711711711711711,
          "p": 0.16883116883116883,
          "f": 0.13829786750396125
        },
        "rouge-2": {
          "r": 0.006329113924050633,
          "p": 0.009174311926605505,
          "f": 0.007490631872522018
        },
        "rouge-l": {
          "r": 0.11711711711711711,
          "p": 0.16883116883116883,
          "f": 0.13829786750396125
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.12453v1",
      "true_abstract": "Drug discovery is crucial for identifying candidate drugs for various\ndiseases.However, its low success rate often results in a scarcity of\nannotations, posing a few-shot learning problem. Existing methods primarily\nfocus on single-scale features, overlooking the hierarchical molecular\nstructures that determine different molecular properties. To address these\nissues, we introduce Universal Matching Networks (UniMatch), a dual matching\nframework that integrates explicit hierarchical molecular matching with\nimplicit task-level matching via meta-learning, bridging multi-level molecular\nrepresentations and task-level generalization. Specifically, our approach\nexplicitly captures structural features across multiple levels, such as atoms,\nsubstructures, and molecules, via hierarchical pooling and matching,\nfacilitating precise molecular representation and comparison. Additionally, we\nemploy a meta-learning strategy for implicit task-level matching, allowing the\nmodel to capture shared patterns across tasks and quickly adapt to new ones.\nThis unified matching framework ensures effective molecular alignment while\nleveraging shared meta-knowledge for fast adaptation. Our experimental results\ndemonstrate that UniMatch outperforms state-of-the-art methods on the\nMoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and\n6.52% in delta AUPRC. UniMatch also shows excellent generalization ability on\nthe Meta-MolNet benchmark.",
      "generated_abstract": "Recent advances in deep learning have demonstrated remarkable capabilities\nin predicting the molecular properties of chemical compounds. However,\ntraditional neural network models face limitations in accurately capturing\ncomplex molecular structures, such as the spatial relationships between atoms.\nTo address this challenge, we propose the Cylinder-Walk Neural Network (CWN),\nwhich uses cylindrical layers to capture the structural information of a molecule\nand employs the walk-updating algorithm to enhance the molecular structure\nrepresentation. Our experiments demonstrate that the CWN achieves superior\nperformance in predicting molecular properties compared to the state-of-the-art\nmethods. Additionally, we introduce a novel benchmark, Mol2Cyl, which includes\nmolecular structures with varying spatial relationships, enabling the\ncomparison of molecular modeling capabilities across different models and\narchitectures. These results highlight the potential of cyl",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22627737226277372,
          "p": 0.33695652173913043,
          "f": 0.27074235327167684
        },
        "rouge-2": {
          "r": 0.05056179775280899,
          "p": 0.07563025210084033,
          "f": 0.06060605580337645
        },
        "rouge-l": {
          "r": 0.21897810218978103,
          "p": 0.32608695652173914,
          "f": 0.26200872881752835
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.17911v1",
      "true_abstract": "Speech enhancement plays an essential role in improving the quality of speech\nsignals in noisy environments. This paper investigates the efficacy of\nintegrating Bidirectional Gated Recurrent Units (BGRU) and Transformer models\nfor speech enhancement tasks. Through a comprehensive experimental evaluation,\nour study demonstrates the superiority of this hybrid architecture over\ntraditional methods and standalone models. The combined BGRU-Transformer\nframework excels in capturing temporal dependencies and learning complex signal\npatterns, leading to enhanced noise reduction and improved speech quality.\nResults show significant performance gains compared to existing approaches,\nhighlighting the potential of this integrated model in real-world applications.\nThe seamless integration of BGRU and Transformer architectures not only\nenhances system robustness but also opens the road for advanced speech\nprocessing techniques. This research contributes to the ongoing efforts in\nspeech enhancement technology and sets a solid foundation for future\ninvestigations into optimizing model architectures, exploring many application\nscenarios, and advancing the field of speech processing in noisy environments.",
      "generated_abstract": "We present the first large-scale dataset of speech-driven musical\ninspiration. We collected 10,000 samples of human-composed melodic fragments,\nspanning a wide range of musical styles and instrumentations. We then\ntransformed each sample into a 128-dimensional embedding using the\nstate-of-the-art Audio2Vec model. Finally, we compiled the dataset into a\ncomprehensive collection of musical fragments, enabling the training of\nneural generative models for music composition. Our dataset and code are\navailable at https://github.com/GustavoBrito/music-inspiration-dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07964601769911504,
          "p": 0.16666666666666666,
          "f": 0.10778442676180591
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.028985507246376812,
          "f": 0.01834861952655602
        },
        "rouge-l": {
          "r": 0.07964601769911504,
          "p": 0.16666666666666666,
          "f": 0.10778442676180591
        }
      }
    },
    {
      "paper_id": "cs.HC.q-bio/NC/2502.17172v1",
      "true_abstract": "Affective computing has made significant strides in emotion recognition and\ngeneration, yet current approaches mainly focus on short-term pattern\nrecognition and lack a comprehensive framework to guide affective agents toward\nlong-term human well-being. To address this, we propose a teleology-driven\naffective computing framework that unifies major emotion theories (basic\nemotion, appraisal, and constructivist approaches) under the premise that\naffect is an adaptive, goal-directed process that facilitates survival and\ndevelopment. Our framework emphasizes aligning agent responses with both\npersonal/individual and group/collective well-being over extended timescales.\nWe advocate for creating a \"dataverse\" of personal affective events, capturing\nthe interplay between beliefs, goals, actions, and outcomes through real-world\nexperience sampling and immersive virtual reality. By leveraging causal\nmodeling, this \"dataverse\" enables AI systems to infer individuals' unique\naffective concerns and provide tailored interventions for sustained well-being.\nAdditionally, we introduce a meta-reinforcement learning paradigm to train\nagents in simulated environments, allowing them to adapt to evolving affective\nconcerns and balance hierarchical goals - from immediate emotional needs to\nlong-term self-actualization. This framework shifts the focus from statistical\ncorrelations to causal reasoning, enhancing agents' ability to predict and\nrespond proactively to emotional challenges, and offers a foundation for\ndeveloping personalized, ethically aligned affective systems that promote\nmeaningful human-AI interactions and societal well-being.",
      "generated_abstract": "The rapid development of biological and clinical data has enabled the\ndevelopment of large-scale bio-computational models that leverage these data\nfor more accurate predictions. However, the vast amounts of data can overwhelm\ncomputational resources, making it difficult to explore complex networks and\ngenerate interpretable results. In this work, we propose a novel approach to\nexplore complex networks, called \"graph-aware modeling\", which employs\ngraph-structured data to capture the dynamic interactions between genes and\npathways. Our method employs a multi-task learning strategy that simultaneously\npredicts gene-level outcomes, pathway-level outcomes, and gene-pathway\ninteractions. To address the computational challenges, we develop a graph\nneural network (GNN) model that integrates both gene-level and pathway-level\ndata. The GNN model is trained using a two-stage supervised learning",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13815789473684212,
          "p": 0.2413793103448276,
          "f": 0.17573221294305086
        },
        "rouge-2": {
          "r": 0.014778325123152709,
          "p": 0.02586206896551724,
          "f": 0.01880877280136905
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.22988505747126436,
          "f": 0.16736401210623075
        }
      }
    },
    {
      "paper_id": "math.PR.nlin/CG/2411.15954v1",
      "true_abstract": "We introduce and study a symmetric, gradient exclusion process, in the class\nof non-cooperative kinetically constrained lattice gases, modelling a\nnon-linear diffusivity where mass transport is constrained by the local density\nnot being too small or too large. Maintaining the gradient property is the main\ntechnical challenge. The resulting model enjoys of properties in common with\nthe Bernstein polynomial basis, and is associated with the diffusion\ncoefficient $D_{n,k}(\\rho)=\\binom{n+k}{k}\\rho^n(1-\\rho)^k$, for $n,k$ arbitrary\nnatural numbers. The dynamics generalizes the Porous Media Model, and we show,\nvia the entropy method, the hydrodynamic limit for the empirical measure\nassociated with a perturbed, irreducible version of the process. The\nhydrodynamic equation is proved to be a Generalized Porous Media Equation.",
      "generated_abstract": "We present a new perspective on the problem of the asymptotic behavior of\nthe solution of a nonlinear ordinary differential equation in the limit of\ninfinite delay. We show that the asymptotic behavior of the solution of this\nequation is a function of the value of the delay. The result is obtained by\nintroducing a new notion of asymptotic behavior, which we call asymptotic\nbehavior in the infinite delay. We also prove that the asymptotic behavior of\nthe solution of a nonlinear ordinary differential equation in the limit of\nfinite delay is a function of the value of the delay. Finally, we give an\napplication of the results obtained.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13414634146341464,
          "p": 0.25,
          "f": 0.17460317005794923
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.043478260869565216,
          "f": 0.033333328605556226
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.25,
          "f": 0.17460317005794923
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/ST/2411.19444v3",
      "true_abstract": "The Capital Asset Pricing Model (CAPM) relates a well-diversified stock\nportfolio to a benchmark portfolio. We insert size effect in CAPM, capturing\nthe observation that small stocks have higher risk and return than large\nstocks, on average. Dividing stock index returns by the Volatility Index makes\nthem independent and normal. In this article, we combine these ideas to create\na new discrete-time model, which includes volatility, relative size, and CAPM.\nWe fit this model using real-world data, prove the long-term stability, and\nconnect this research to Stochastic Portfolio Theory. We fill important gaps in\nour previous article on CAPM with the size factor.",
      "generated_abstract": "We consider a continuous-time model driven by an L\\'evy stochastic\nprocess. The model is driven by an innovation process that depends on the\nprice process of a financial option. The option payoff is linear in the\ninnovation process, and the model is driven by an unobservable stochastic\nprocess. We assume that the model is under the control of a financial agent\nthat can use the innovation process to estimate the option payoff. The agent\ncan use this information to optimally trade the option. We show that under\ncertain conditions, the optimal trading strategy is linear in the innovation\nprocess. Our results extend the results of [J. Math. Anal. Appl. 399 (2012)\n122-137",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14457831325301204,
          "p": 0.21428571428571427,
          "f": 0.1726618656922521
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.21428571428571427,
          "f": 0.1726618656922521
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10566v1",
      "true_abstract": "Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose an architectural change,\nASIDE, that allows the model to clearly separate between instructions and data\nby using separate embeddings for them. Instead of training the embeddings from\nscratch, we propose a method to convert an existing model to ASIDE form by\nusing two copies of the original model's embeddings layer, and applying an\northogonal rotation to one of them. We demonstrate the effectiveness of our\nmethod by showing (1) highly increased instruction-data separation scores\nwithout a loss in model capabilities and (2) competitive results on prompt\ninjection benchmarks, even without dedicated safety training. Additionally, we\nstudy the working mechanism behind our method through an analysis of model\nrepresentations.",
      "generated_abstract": "In this paper, we introduce a new method to train a neural language model\n(NLM) that learns to detect the presence of a given word in a sentence. Our\napproach is based on a two-step process: First, we train a pre-trained NLM\nto produce a set of token embeddings for all words in a sentence. Second, we\napply a linear classifier to learn the probability of each word being present\nin a sentence. This method is novel in that it trains a NLM to identify the\npresence of a given word in a sentence without using any additional data.\nExperimental results show that our method consistently outperforms baselines\nthat use additional data, achieving comparable or superior performance on\ndifferent datasets. We also demonstrate that our method is capable of\ngeneralizing to new words, achieving comparable or superior performance on\nunseen words. This work provides a new approach to train",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22115384615384615,
          "p": 0.27058823529411763,
          "f": 0.24338623843677398
        },
        "rouge-2": {
          "r": 0.02666666666666667,
          "p": 0.03225806451612903,
          "f": 0.029197075336992687
        },
        "rouge-l": {
          "r": 0.19230769230769232,
          "p": 0.23529411764705882,
          "f": 0.21164020669074224
        }
      }
    },
    {
      "paper_id": "physics.atom-ph.physics/atm-clus/2503.08972v1",
      "true_abstract": "Out-of-equilibrium Rydberg gases exhibit emergent many-body phases due to\nmode competition. Sustained limit cycle oscillations (OSC) emerge when driven\nby B-fields at room-temperature, forming robust Rydberg dissipative time\ncrystals (DTC). These driven-dissipative Rydberg DTC have recently been shown\nto develop an effective transition centered at the OSC frequency (-10dB\nbandwidth of ~1.7kHz, centered at 9.8kHz). Weak RF signals injected within this\nemergent transition perturb and emerge on the OSC spectrum, from which\nsensitive and high-resolution sensing of E-fields (~1.6-2.3 uVcm-1Hz-1/2) near\nthe OSC frequencies can be achieved. In this article, it is demonstrated that\nDC and AC Stark fields in the sub-kHz regime can be used effectively to shift\n(DC) or modulate (AC) the OSC frequency of Rydberg DTC at room-temperature. The\nAC-Stark driven modulation of the OSC is shown as an effective technique to\nsense weak AC E-fields in the sub-kHz regime. With a modest setup, a\nsensitivity of ~7.8 uVcm-1Hz-1/2 for AC signals at 300Hz (~8.7x improvement\nover state-of-art Rydberg atom techniques), and high-resolution detection to as\nlow as sub-Hz is demonstrated. This approach enables the development of\nultra-compact, extremely low-frequency E-field detectors for applications in\nremote sensing, communications, navigation, and bio-medical technologies.",
      "generated_abstract": "In this work, we investigate the nonlinear interaction between a strongly\ninteracting ultracold Fermi gas and a linearly polarized electromagnetic field.\nWe find that the nonlinear interaction is mediated by the linearly polarized\nelectromagnetic field, and the nonlinear interaction can be described by the\nelectromagnetic response function. We show that the nonlinear interaction\nbetween the ultracold Fermi gas and the linearly polarized electromagnetic\nfield is similar to the interaction between a strongly interacting Fermi gas\nand an electromagnetic field. We calculate the two-body scattering and\nsingle-body scattering cross sections of the ultracold Fermi gas and the\nlinearly polarized electromagnetic field, and compare them with the experimental\ndata. We also calculate the two-body scattering and single-body scattering\ncross sections of the ultracold Fermi gas and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09285714285714286,
          "p": 0.2653061224489796,
          "f": 0.13756613372525978
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.041666666666666664,
          "f": 0.02325580992969242
        },
        "rouge-l": {
          "r": 0.08571428571428572,
          "p": 0.24489795918367346,
          "f": 0.1269841231432492
        }
      }
    },
    {
      "paper_id": "cond-mat.supr-con.cond-mat/supr-con/2503.10040v1",
      "true_abstract": "Delineating the superconducting order parameters is a pivotal task in\ninvestigating superconductivity for probing pairing mechanisms, as well as\ntheir symmetry and topology. Point-contact Andreev reflection (PCAR)\nmeasurement is a simple yet powerful tool for identifying the order parameters.\nThe PCAR spectra exhibit significant variations depending on the type of the\norder parameter in a superconductor, including its magnitude\n($\\mathit{\\Delta}$), as well as temperature, interfacial quality, Fermi\nvelocity mismatch, and other factors. The information on the order parameter\ncan be obtained by finding the combination of these parameters, generating a\ntheoretical spectrum that fits a measured experimental spectrum. However, due\nto the complexity of the spectra and the high dimensionality of parameters,\nextracting the fitting parameters is often time-consuming and labor-intensive.\nIn this study, we employ a convolutional neural network (CNN) algorithm to\ncreate models for rapid and automated analysis of PCAR spectra of various\nsuperconductors with different pairing symmetries (conventional $s$-wave,\nchiral $p_x+ip_y$-wave, and $d_{x^2-y^2}$-wave). The training datasets are\ngenerated based on the Blonder-Tinkham-Klapwijk (BTK) theory and further\nmodified and augmented by selectively incorporating noise and peaks according\nto the bias voltages. This approach not only replicates the experimental\nspectra but also brings the model's attention to important features within the\nspectra. The optimized models provide fitting parameters for experimentally\nmeasured spectra in less than 100 ms per spectrum. Our approaches and findings\npave the way for rapid and automated spectral analysis which will help\naccelerate research on superconductors with complex order parameters.",
      "generated_abstract": "We investigate the excitation spectrum of a two-dimensional (2D) Kitaev\nmodel in the presence of a transverse magnetic field. The Hamiltonian is\ndescribed by a generalization of the Kitaev model where the hopping parameter\n$\\lambda$ is replaced by a complex-valued function $H(\\mathbf{r})$. The\nresulting model exhibits a rich spectrum of gapless and gapful excitations,\nwhich are localized in the vicinity of the magnetic flux $\\phi$ and\nnon-localized in the absence of the magnetic flux. We show that, under a\nsuitable transformation, the model can be mapped to the 2D Hubbard model in\nthe absence of magnetic field. The results are compared with the exact diagonalization\nof the Hamiltonian in the presence of a magnetic field. We find that the\nmagnetic field has a strong effect on the gapless excitations, while it has a\nneg",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11320754716981132,
          "p": 0.2571428571428571,
          "f": 0.15720523592990232
        },
        "rouge-2": {
          "r": 0.017937219730941704,
          "p": 0.037037037037037035,
          "f": 0.024169179893576066
        },
        "rouge-l": {
          "r": 0.10062893081761007,
          "p": 0.22857142857142856,
          "f": 0.13973798702160536
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09518v1",
      "true_abstract": "We generalize the computation of the capacity of exponential Hopfield model\nfrom Lucibello and M\\'ezard (2024) to more generic pattern ensembles, including\nbinary patterns and patterns generated from a hidden manifold model.",
      "generated_abstract": "We propose a novel scheme for the generation of large-scale, high-dimensional\nrandom graphs based on the use of the discrete Gaussian free field. The method\nis based on the use of the discrete Gaussian free field, which is a\nnon-stationary version of the Gaussian free field. It is a random field that\nrepresents the probability density function of the number of particles in a\nrandom graph. It is well known that the Gaussian free field is a good model\nfor the number of particles in a Poisson point process, and we show that it\nprovides a natural model for the number of particles in a large-scale random\ngraph. We apply the method to generate large-scale random graphs. We use the\ndiscrete Gaussian free field to sample the number of particles in a large-scale\nrandom graph and then generate a large number of random graphs from the\ndiscrete Gaussian free field. We use the discrete Gaussian free field to sample\nthe",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3076923076923077,
          "p": 0.13793103448275862,
          "f": 0.19047618620181417
        },
        "rouge-2": {
          "r": 0.03225806451612903,
          "p": 0.010309278350515464,
          "f": 0.015624996329346564
        },
        "rouge-l": {
          "r": 0.2692307692307692,
          "p": 0.1206896551724138,
          "f": 0.16666666239229036
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08756v1",
      "true_abstract": "The diagnosis of brain tumours is an extremely sensitive and complex clinical\ntask that must rely upon information gathered through non-invasive techniques.\nOne such technique is magnetic resonance, in the modalities of imaging or\nspectroscopy. The latter provides plenty of metabolic information about the\ntumour tissue, but its high dimensionality makes resorting to pattern\nrecognition techniques advisable. In this brief paper, an international\ndatabase of brain tumours is analyzed resorting to an ad hoc spectral frequency\nselection procedure combined with nonlinear classification.",
      "generated_abstract": "Accurate and fast segmentation of cardiac structures is crucial for\ndetailed analysis of cardiac imaging. In this paper, we propose a novel\nmulti-scale spatially adaptive deep learning framework for segmentation of\ncardiac structures using CT data. The proposed method utilizes a 2D U-Net\narchitecture with a multi-scale feature pyramid network (MS-FPN) to capture\nhigh-level features of cardiac structures. The proposed framework is trained\nand tested on an in-house dataset containing 5433 CT images from 12 patients.\nThe performance of the proposed method is compared to that of the state-of-the\nart methods in terms of Dice coefficient (DC) and Hausdorff distance (HD) for\nsegmentation of the left ventricle (LV), right ventricle (RV), and aorta, and\nthe mean Dice coefficient (MD",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.208955223880597,
          "p": 0.175,
          "f": 0.19047618551529466
        },
        "rouge-2": {
          "r": 0.012987012987012988,
          "p": 0.009523809523809525,
          "f": 0.010989006107356354
        },
        "rouge-l": {
          "r": 0.19402985074626866,
          "p": 0.1625,
          "f": 0.17687074333842395
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.10489v1",
      "true_abstract": "Molecular pretrained representations (MPR) has emerged as a powerful approach\nfor addressing the challenge of limited supervised data in applications such as\ndrug discovery and material design. While early MPR methods relied on 1D\nsequences and 2D graphs, recent advancements have incorporated 3D\nconformational information to capture rich atomic interactions. However, these\nprior models treat molecules merely as discrete atom sets, overlooking the\nspace surrounding them. We argue from a physical perspective that only modeling\nthese discrete points is insufficient. We first present a simple yet insightful\nobservation: naively adding randomly sampled virtual points beyond atoms can\nsurprisingly enhance MPR performance. In light of this, we propose a principled\nframework that incorporates the entire 3D space spanned by molecules. We\nimplement the framework via a novel Transformer-based architecture, dubbed\nSpaceFormer, with three key components: (1) grid-based space discretization;\n(2) grid sampling/merging; and (3) efficient 3D positional encoding. Extensive\nexperiments show that SpaceFormer significantly outperforms previous 3D MPR\nmodels across various downstream tasks with limited data, validating the\nbenefit of leveraging the additional 3D space beyond atoms in MPR models.",
      "generated_abstract": "The development of computational methods for identifying and analyzing\nbiological networks from experimental data is an active area of research.\nNetwork-based inference algorithms can be used to identify functional\nrelationships in the network, such as causal relationships, and to analyze\nnetwork properties such as network size and community structure. In this\narticle, we review recent advances in network-based inference algorithms for\nidentifying and analyzing biological networks from experimental data. We\nconsider two key aspects of network inference algorithms: the analysis of\nnetwork properties, such as network size and community structure, and the\nidentification of causal relationships in the network. We discuss the\nimpact of network size and community structure on the effectiveness of\ninference algorithms, and we review recent advances in network-based inference\nalgorithms for identifying causal relationships in biological networks. We\nconclude by discussing future directions in network-based inference algorithms\nfor ident",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14492753623188406,
          "p": 0.28169014084507044,
          "f": 0.19138755532245152
        },
        "rouge-2": {
          "r": 0.005714285714285714,
          "p": 0.009708737864077669,
          "f": 0.007194239939705939
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.2535211267605634,
          "f": 0.17224879934159026
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03620v1",
      "true_abstract": "Reconfigurable antennas possess the capability to dynamically adjust their\nfundamental operating characteristics, thereby enhancing system adaptability\nand performance. To fully exploit this flexibility in modern wireless\ncommunication systems, this paper considers a novel tri-hybrid beamforming\narchitecture, which seamlessly integrates pattern-reconfigurable antennas with\nboth analog and digital beamforming. The proposed tri-hybrid architecture\noperates across three layers: (\\textit{i}) a radiation beamformer in the\nelectromagnetic (EM) domain for dynamic pattern alignment, (\\textit{ii}) an\nanalog beamformer in the radio-frequency (RF) domain for array gain\nenhancement, and (\\textit{iii}) a digital beamformer in the baseband (BB)\ndomain for multi-user interference mitigation. To establish a solid theoretical\nfoundation, we first develop a comprehensive mathematical model for the\ntri-hybrid beamforming system and formulate the signal model for a multi-user\nmulti-input single-output (MU-MISO) scenario. The optimization objective is to\nmaximize the sum-rate while satisfying practical constraints. Given the\nchallenges posed by high pilot overhead and computational complexity, we\nintroduce an innovative tri-timescale beamforming framework, wherein the\nradiation beamformer is optimized over a long-timescale, the analog beamformer\nover a medium-timescale, and the digital beamformer over a short-timescale.\nThis hierarchical strategy effectively balances performance and implementation\nfeasibility. Simulation results validate the performance gains of the proposed\ntri-hybrid architecture and demonstrate that the tri-timescale design\nsignificantly reduces pilot overhead and computational complexity, highlighting\nits potential for future wireless communication systems.",
      "generated_abstract": "This paper presents a novel deep learning-based method for the estimation of\nthe in-plane and out-of-plane (OOP) susceptibility of bulk magnetic materials. The\nproposed approach is based on the use of a two-dimensional convolutional neural\nnetwork (CNN) architecture, which is trained using a comprehensive dataset\nconsisting of 10,000 bulk magnetic materials. The performance of the proposed\nmethod is evaluated using a comprehensive dataset comprising 10,000 bulk\nmagnetic materials, including 2,000 new materials. The results show that the\nproposed method achieves high accuracy in the estimation of both in-plane and\nout-of-plane susceptibility. The proposed method is also shown to be\nrobust to variations in the input data, including material composition,\nmagnetization orientation, and magnetic field direction. The proposed method is\nalso found to be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1510791366906475,
          "p": 0.3,
          "f": 0.20095693334401696
        },
        "rouge-2": {
          "r": 0.04591836734693878,
          "p": 0.09278350515463918,
          "f": 0.06143344266980429
        },
        "rouge-l": {
          "r": 0.14388489208633093,
          "p": 0.2857142857142857,
          "f": 0.19138755535358634
        }
      }
    },
    {
      "paper_id": "stat.ME.econ/EM/2502.13238v1",
      "true_abstract": "Uncertainty quantification in causal inference settings with random network\ninterference is a challenging open problem. We study the large sample\ndistributional properties of the classical difference-in-means Hajek treatment\neffect estimator, and propose a robust inference procedure for the\n(conditional) direct average treatment effect, allowing for cross-unit\ninterference in both the outcome and treatment equations. Leveraging ideas from\nstatistical physics, we introduce a novel Ising model capturing interference in\nthe treatment assignment, and then obtain three main results. First, we\nestablish a Berry-Esseen distributional approximation pointwise in the degree\nof interference generated by the Ising model. Our distributional approximation\nrecovers known results in the literature under no-interference in treatment\nassignment, and also highlights a fundamental fragility of inference procedures\ndeveloped using such a pointwise approximation. Second, we establish a uniform\ndistributional approximation for the Hajek estimator, and develop robust\ninference procedures that remain valid regardless of the unknown degree of\ninterference in the Ising model. Third, we propose a novel resampling method\nfor implementation of robust inference procedure. A key technical innovation\nunderlying our work is a new \\textit{De-Finetti Machine} that facilitates\nconditional i.i.d. Gaussianization, a technique that may be of independent\ninterest in other settings.",
      "generated_abstract": "We consider the problem of constructing a predictive model for a dependent\nnonlinear time series process given a small number of lags. Our approach is\nbased on the identification of an autoregressive moving average (ARMA) model\nfor the process. The model is fitted using a limited number of lags. We show\nthat the predictive performance of the ARMA model is close to that of a\nsimple AR model for the same set of lags, but the ARMA model is more flexible\nthan the simple AR model. We also compare the predictive performance of the\nARMA model and the simple AR model with the performance of a simple AR model\nwith a constant term. Finally, we compare the predictive performance of the\nARMA model and the simple AR model with the performance of an AR model with a\nconstant term. Our results show that the ARMA model provides a better\npredictive performance than the simple AR model with a constant term",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14782608695652175,
          "p": 0.2833333333333333,
          "f": 0.19428570977959195
        },
        "rouge-2": {
          "r": 0.011764705882352941,
          "p": 0.02040816326530612,
          "f": 0.014925368495211958
        },
        "rouge-l": {
          "r": 0.1391304347826087,
          "p": 0.26666666666666666,
          "f": 0.1828571383510205
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/TO/2503.03783v2",
      "true_abstract": "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
      "generated_abstract": "The rapid advancements in artificial intelligence (AI) have paved the way\nfor a wide range of applications, including drug discovery. While recent\nadvances in drug discovery have yielded promising results, their efficiency is\nrestricted by a lack of diversity in the search space, which has led to the\nemergence of a knowledge gap. To address this, we introduce DrugMaker, a\ndrug-recommendation framework that integrates AI techniques with a knowledge\ngraph. Our approach incorporates a drug-recommendation engine and a\nknowledge-extraction module to generate drug-knowledge pairs, enabling\ndrug-knowledge discovery. Furthermore, we utilize knowledge-graph-based\nrecommendation methods to incorporate drug-knowledge pairs into the drug-recommendation\nengine. We evaluate DrugMaker on two drug-recommendation datasets and two\nknow",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11206896551724138,
          "p": 0.15853658536585366,
          "f": 0.13131312646056545
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11206896551724138,
          "p": 0.15853658536585366,
          "f": 0.13131312646056545
        }
      }
    },
    {
      "paper_id": "physics.flu-dyn.nlin/CG/2502.16568v1",
      "true_abstract": "Quantum computing holds great promise to accelerate scientific computations\nin fluid dynamics and other classical physical systems. While various quantum\nalgorithms have been proposed for linear flows, developing quantum algorithms\nfor nonlinear problems remains a significant challenge. We introduce a novel\nnode-level ensemble description of lattice gas for simulating nonlinear fluid\ndynamics on a quantum computer. This approach combines the advantages of the\nlattice Boltzmann method, which offers low-dimensional representation, and\nlattice gas cellular automata, which provide linear collision treatment.\nBuilding on this framework, we propose a quantum lattice Boltzmann method that\nrelies on linear operations with medium dimensionality. We validated the\nalgorithm through comprehensive simulations of benchmark cases, including\nvortex-pair merging and decaying turbulence on $2048^2$ computational grid\npoints. The results demonstrate remarkable agreement with direct numerical\nsimulation, effectively capturing the essential nonlinear mechanisms of fluid\ndynamics. This work offers valuable insights into developing quantum algorithms\nfor other nonlinear problems, and potentially advances the application of\nquantum computing across various transport phenomena in engineering.",
      "generated_abstract": "This work presents a novel approach to investigate the non-linear\nreaction-diffusion interaction between a diffusive and a diffusive-reaction\nspecies, as well as the non-linear interaction between a diffusive and a\ndiffusive-reaction species, in a 2D periodic environment. The system is\ndescribed by a set of coupled ordinary differential equations. A systematic\nstudy of the dynamical behavior of the system is performed, starting from the\nequilibrium state, by means of numerical simulations and finite element methods\n(FEM). The obtained results highlight the non-linear interaction between the\nspecies, as well as the non-linear interaction between the species and the\nenvironment. The study also reveals the existence of a critical point, which is\ncharacterized by the non-linear reaction-diffusion interaction. Finally, we\ninvestigate the effects of the parameters involved in the system, such as the\ndiff",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14035087719298245,
          "p": 0.22857142857142856,
          "f": 0.17391303876417785
        },
        "rouge-2": {
          "r": 0.01935483870967742,
          "p": 0.02912621359223301,
          "f": 0.023255809156602157
        },
        "rouge-l": {
          "r": 0.12280701754385964,
          "p": 0.2,
          "f": 0.15217390832939523
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/GR/2503.10624v1",
      "true_abstract": "Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings. Qualitative results demonstrate\nstrong generalization of ETCH, regardless of challenging poses, unseen shapes,\nloose clothing, and non-rigid dynamics. We will release the code and models\nsoon for research purposes at https://boqian-li.github.io/ETCH/.",
      "generated_abstract": "Graph-based methods have revolutionized the representation of visual\nworlds by allowing for efficient and accurate modeling of both high-level\nstructures and complex relationships. However, their ability to capture\ncomplex, multi-scale relationships remains limited. To address this challenge,\nwe propose GRAW, a novel graph-based method that introduces a novel attention\nlayer designed to capture and refine multi-scale relationships. GRAW leverages a\ngraph attention mechanism to enable a better understanding of the\nmulti-scale relationships, which can be encoded by the graph itself. Our\nexperiments demonstrate that GRAW outperforms existing graph-based methods on\nstate-of-the-art benchmarks, with significant improvements in both pixel-wise\nand semantic performance. In addition, we show that GRAW is able to generalize\nwell across different datasets and tasks, with significant improvements in\nvisual quality, semantic, and instance discrimination.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16778523489932887,
          "p": 0.2777777777777778,
          "f": 0.2092050162252063
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.01680672268907563,
          "f": 0.013114749339641608
        },
        "rouge-l": {
          "r": 0.15436241610738255,
          "p": 0.25555555555555554,
          "f": 0.19246861455156608
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2407.18237v2",
      "true_abstract": "Transport of dense core vesicles (DCVs) in neurons is crucial for\ndistributing molecules like neuropeptides and growth factors. We studied the\nexperimental trajectories of dynein-driven directed movement of DCVs in the ALA\nneuron C. elegans over a duration of up to 6 seconds. We analysed the DCV\nmovement in three strains of C. elegans: 1) with normal kinesin-1 function, 2)\nwith reduced function in kinesin light chain 2 (KLC-2), and 3) a null mutation\nin kinesin light chain 1 (KLC-1). We find that DCVs move superdiffusively with\ndisplacement variance $var(x) \\sim t^2$ in all three strains with low reversal\nrates and frequent immobilization of DCVs. The distribution of DCV\ndisplacements fits a beta-binomial distribution with the mean and the variance\nfollowing linear and quadratic growth patterns, respectively. We propose a\nsimple heterogeneous random walk model to explain the observed superdiffusive\nretrograde transport behaviour of DCV movement. This model involves a random\nprobability with the beta density for a DCV to resume its movement or remain in\nthe same position.",
      "generated_abstract": "A key challenge in the field of cancer research is to identify biomarkers that\ncan predict a patient's response to a treatment. Existing machine learning\ntechniques often require large training datasets and complex model architectures\nto learn the complex relationships between the biomarkers and the patient's\nresponse to a treatment. In this paper, we propose a novel methodology,\ncalled Biomarker-Guided Deep Learning, to overcome these limitations. Our\nmethodology leverages biomarker data to guide the learning process, enabling\nthe model to predict the response to a treatment without training on a large\namount of biomarker data. The model uses an encoder-decoder architecture that\nenables it to learn the complex relationships between biomarkers and the\nresponse to a treatment. We validate our approach on the Cancer Genomic\nDataset (CGD), a dataset that contains 79 biom",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10909090909090909,
          "p": 0.15,
          "f": 0.12631578459833812
        },
        "rouge-2": {
          "r": 0.025157232704402517,
          "p": 0.035398230088495575,
          "f": 0.029411759848887047
        },
        "rouge-l": {
          "r": 0.10909090909090909,
          "p": 0.15,
          "f": 0.12631578459833812
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "The integration of data from multiple sources, such as satellite imagery,\naccurate ground-based measurements, and large-scale models, is an essential\ncomponent of Earth system models. In this study, we introduce a novel data\nintegration methodology for Earth system models that leverages satellite\nimagery to improve the representation of land use and land cover (LULC). We\ndevelop a novel satellite-based LULC dataset that combines ground-based\nimagery with satellite data, capturing changes in land cover over time. We\nintegrate this dataset into the Community Land Model (CLM5) to improve\nrepresentations of forest and agricultural land. The results demonstrate that\nthe integration of satellite data improves the accuracy of land cover estimates\nand enhances the representation of vegetation and cropland, resulting in\nimprovements in simulated climate and hydrological predictions. The integration\nof satellite data into Earth system",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1794871794871795,
          "p": 0.2625,
          "f": 0.21319796471952393
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.00847457627118644,
          "f": 0.007142852266329859
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.225,
          "f": 0.1827411119276458
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.04100v1",
      "true_abstract": "We show how to improve the discrepancy of an iid sample by moving only a few\npoints. Specifically, modifying \\( O(m) \\) sample points on average reduces the\nKolmogorov-Smirnov distance to the population distribution to \\(1/m\\).",
      "generated_abstract": "In this work, we propose a new approach for the estimation of the\nparameter of the Weibull distribution. We first derive the closed-form\nestimator of the parameter. Then, we propose a modified weighted least\nsquares estimator. The latter is derived using a weighted least squares\nformulation that accounts for the shape parameters of the Weibull distribution.\nIn addition, we propose an adaptive version of the proposed estimator that\naccounts for the shape parameters of the Weibull distribution. The adaptive\nproposed estimator is derived using a weighted least squares formulation. The\nproposed estimator is shown to have an asymptotic normality property. Numerical\nsimulations are carried out to assess the performance of the proposed\nestimators. The results indicate that the proposed estimator has an asymptotic\nnormality property and is a more efficient estimator than the classical\nestimators of the Weib",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.1111111111111111,
          "f": 0.15053763003815482
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.23333333333333334,
          "p": 0.1111111111111111,
          "f": 0.15053763003815482
        }
      }
    },
    {
      "paper_id": "math.AG.math/KT/2503.09928v1",
      "true_abstract": "Given a compact Lie group $G$ acting on a space $X$, the classical\nAtiyah-Segal completion theorem identifies topological $K$-theory of the\nhomotopy quotient $X/G$ with an explicit completion of $G$-equivariant\ntopological $K$-theory of $X$. We prove an analog of this result for algebraic\n$K$-theory over a field of characteristic 0. In our setting $G$ is a reductive\ngroup that acts on a derived algebraic space $X$ with the assumption that all\nstabilizer groups are nice (in the sense of Alper). Our main result identifies\nthe value $R^{\\mathrm{dAff}}K([X/G])$ of right Kan extension of the $K$-theory\nfunctor from schemes to stacks with the completion of $K$-theory of the\ncategory $\\mathrm{Perf}([X/G])$ at the augmentation ideal of\n$K_0(\\mathrm{Rep}(G))$. The main novelty of our results is that $X$ is allowed\nto be singular or even derived. This generality is achieved by employing and\nimproving analogous versions of completion theorem for negative cyclic homology\n(after Ben-Zvi--Nadler and Chen) and for homotopy $K$-theory (after van den\nBergh--Tabuada). We also show that in the singular setting the completion\ntheorem does not necessarily hold without the nice stabilizer assumption. We\nview our results as a part of the general paradigm of extending the motivic\nfiltration on algebraic $K$-theory of schemes to algebraic $K$-theory of\nstacks.",
      "generated_abstract": "We study the localization of the category of formal $R$-modules at a given\npositive integer $d$, and generalize the classical localization result of\nDixmier-Pauter for $R=k[T",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08695652173913043,
          "p": 0.47619047619047616,
          "f": 0.14705882091803635
        },
        "rouge-2": {
          "r": 0.016042780748663103,
          "p": 0.12,
          "f": 0.028301884712086306
        },
        "rouge-l": {
          "r": 0.06956521739130435,
          "p": 0.38095238095238093,
          "f": 0.11764705621215404
        }
      }
    },
    {
      "paper_id": "math.QA.math/CT/2503.06280v1",
      "true_abstract": "Hopf braces are the quantum analogues of skew braces and, as such, their\ncocommutative counterparts provide solutions to the quantum Yang-Baxter\nequation. We investigate various properties of categories related to Hopf\nbraces. In particular, we prove that the category of Hopf braces is accessible\nwhile the category of cocommutative Hopf braces is even locally presentable. We\nalso show that functors forgetting multiple antipodes and/or multiplications\ndown to coalgebras are monadic. Colimits in the category of cocommutative Hopf\nbraces are described explicitly and a free cocommutative Hopf brace on an\narbitrary cocommutative Hopf algebra is constructed.",
      "generated_abstract": "In this paper, we study the local and global behaviour of the\ncharacteristic classes of non-singular projective varieties defined over\nalgebraic number fields, and of their homogeneous components. We also obtain\nsome new examples of non-singular projective varieties with a large number of\ncomponents, which are not locally isomorphic to a product of two projective\nvarieties.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19672131147540983,
          "p": 0.2857142857142857,
          "f": 0.2330097039080027
        },
        "rouge-2": {
          "r": 0.012658227848101266,
          "p": 0.0196078431372549,
          "f": 0.015384610616569523
        },
        "rouge-l": {
          "r": 0.18032786885245902,
          "p": 0.2619047619047619,
          "f": 0.2135922281798474
        }
      }
    },
    {
      "paper_id": "gr-qc.gr-qc/2503.09731v1",
      "true_abstract": "We conduct two searches for continuous, nearly monochromatic gravitational\nwaves originating from the central compact objects in the supernova remnants\nCassiopeia A and Vela Jr. using public LIGO data. The search for Cassiopeia A\ntargets signal frequencies between 20 Hz and 400 Hz; the Vela Jr. search\nbetween 400 Hz and 1700 Hz, and both investigate the broadest set of waveforms\never considered with highly sensitive deterministic search methods. Above 1500\nHz the Vela Jr. search is the most sensitive carried out thus far, improving on\nprevious results by over 300\\%. Above 976 Hz these results improve on existing\nones by 50\\%. In all we investigate over $10^{18}$ waveforms, leveraging the\ncomputational power donated by thousands of Einstein@Home volunteers. We\nperform a 4-stage follow-up on more than 6 million waveforms. None of the\nconsidered waveforms survives the follow-up scrutiny, indicating no significate\ndetection candidate. Our null results constrain the maximum amplitude of\ncontinuous signals as a function of signal frequency from the targets. The most\nstringent 90\\% confidence upper limit for Cas A is $h_0^{90 \\%}\\approx\n7.3\\times10^{-26}$ near 200 Hz, and for Vela Jr. it is $h_0^{90 \\%}\\approx\n8.9\\times10^{-26}$ near 400 Hz. Translated into upper limits on the ellipticity\nand r-mode amplitude, our results probe physically interesting regions: for\nexample the ellipticity of Vela Jr. is constrained to be smaller than $10^{-7}$\nacross the frequency band, with a tighter constraint of less than\n$2\\times10^{-8}$ at the highest frequencies.",
      "generated_abstract": "We present a new analysis of the 1969 Crab Nebula observed with the\nGran Telescopio Canarias (GTC) on the night of 25 August 2018. This analysis\nfocuses on the emission lines of the Crab Nebula, in particular on the\nintensity of the He-II $\\lambda$ 4686 emission line. In addition, we perform\nthe spectral fits of the 2015 and 2018 observations of the Crab Nebula, and we\npresent the new constraints on the neutron star mass and the equation of\nstate of the Crab Nebula's relativistic plasma. The new observations\nsignificantly improve the knowledge of the Crab Nebula, providing new\nconstraints on the physical properties of the Crab Nebula and the neutron star\nmass and equation of state.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08552631578947369,
          "p": 0.22413793103448276,
          "f": 0.123809519811338
        },
        "rouge-2": {
          "r": 0.008888888888888889,
          "p": 0.02247191011235955,
          "f": 0.012738849441155093
        },
        "rouge-l": {
          "r": 0.08552631578947369,
          "p": 0.22413793103448276,
          "f": 0.123809519811338
        }
      }
    },
    {
      "paper_id": "math.AP.math/FA/2503.08309v1",
      "true_abstract": "We investigate the asymptotic behavior as $\\varepsilon \\to 0$ of singularly\nperturbed phase transition models of order $n \\geq 2$, given by \\begin{align}\n  G_\\varepsilon^{\\lambda,n}[u] := \\int_I \\frac 1\\varepsilon W(u)\n-\\lambda\\varepsilon^{2n-3} (u^{(n-1)})^2 + \\varepsilon^{2n-1} (u^{(n)})^2 \\ dx,\n\\quad u \\in W^{n,2}(I), \\end{align}\n  where $\\lambda >0$ is fixed, $I \\subset \\mathbb{R}$ is an open bounded\ninterval, and $W \\in C^0(\\mathbb{R})$ is a suitable double-well potential. We\nfind that there exists a positive critical parameter depending on $W$ and $n$,\nsuch that the $\\Gamma$-limit of $G_\\varepsilon^{\\lambda,n}$ with respect to the\n$L^1$-topology is given by a sharp interface functional in the subcritical\nregime. The cornerstone for the corresponding compactness property is a novel\nnonlinear interpolation inequality involving higher-order derivatives, which is\nbased on Gagliardo-Nirenberg type inequalities.",
      "generated_abstract": "We prove the existence of a finite number of solutions of the linear\nnonlinear Schr\\\"odinger equation with a non-Hermitian potential, and the\nexistence of a unique solution in the whole space $L^2(\\mathbb{R}^d)$ for\n$d\\geq3$. Moreover, we provide a condition on the potential such that there is\na unique solution in a suitable Hilbert space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15841584158415842,
          "p": 0.43243243243243246,
          "f": 0.23188405404641885
        },
        "rouge-2": {
          "r": 0.04201680672268908,
          "p": 0.10638297872340426,
          "f": 0.06024095979605195
        },
        "rouge-l": {
          "r": 0.12871287128712872,
          "p": 0.35135135135135137,
          "f": 0.1884057931768537
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.09137v1",
      "true_abstract": "Loeb & Cloete (2025) intriguingly suggest that the near-Earth object 2005\nVL$_1$ could be the lost Soviet probe Venera 2. Here I evaluate the\nplausibility of such a claim against the available data. I have re-determined\nthe orbit of 2005 VL$_1$ (including a non-gravitational acceleration component)\nusing the astrometric observations retrieved from the Minor Planet Center (MPC)\ndatabase. By propagating the orbit of 2005 VL$_1$ over the period of the Venera\n2 mission, I compare this object's distance from the Earth and from Venus at\nthe times of the probe's launch and flyby with Venus, respectively. My\nanalysis, which takes into account realistic uncertainties on both the orbit of\n2005 VL1 and the position of Venera 2, decisively rules out the proposed\nidentification. My approach relies entirely on open-source software and\npublicly available data, and could represent a viable method to assess similar\nclaims in the future.",
      "generated_abstract": "We present a study of the stellar winds of the Cepheids of the globular\nclusters NGC 6397 and NGC 6752. The results are compared to the theoretical\npredictions of a wind model with a thermal-hydrodynamic description of the\nstellar winds. The stellar winds of the Cepheids of the two clusters are\ncharacterized by a wide range of velocities and wind masses. The mass-loss rates\nof the Cepheids in NGC 6397 and NGC 6752 are in agreement with those expected\nfrom a wind model. The velocity-integrated wind velocities and masses are in\nvery good agreement with the predictions of the wind model. However, we find a\nsignificant offset between the observed and predicted wind masses and\nvelocities, which is due to the presence of an unknown variable component in the\nobser",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0891089108910891,
          "p": 0.14516129032258066,
          "f": 0.11042944313899679
        },
        "rouge-2": {
          "r": 0.014814814814814815,
          "p": 0.0196078431372549,
          "f": 0.016877632227742703
        },
        "rouge-l": {
          "r": 0.07920792079207921,
          "p": 0.12903225806451613,
          "f": 0.09815950448869006
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/PM/2503.04662v1",
      "true_abstract": "We investigate portfolio optimization in financial markets from a trading and\nrisk management perspective. We term this task Risk-Aware Trading Portfolio\nOptimization (RATPO), formulate the corresponding optimization problem, and\npropose an efficient Risk-Aware Trading Swarm (RATS) algorithm to solve it. The\nkey elements of RATPO are a generic initial portfolio P, a specific set of\nUnique Eligible Instruments (UEIs), their combination into an Eligible\nOptimization Strategy (EOS), an objective function, and a set of constraints.\nRATS searches for an optimal EOS that, added to P, improves the objective\nfunction repecting the constraints.\n  RATS is a specialized Particle Swarm Optimization method that leverages the\nparameterization of P in terms of UEIs, enables parallel computation with a\nlarge number of particles, and is fully general with respect to specific\nchoices of the key elements, which can be customized to encode financial\nknowledge and needs of traders and risk managers.\n  We showcase two RATPO applications involving a real trading portfolio made of\nhundreds of different financial instruments, an objective function combining\nboth market risk (VaR) and profit&loss measures, constrains on market\nsensitivities and UEIs trading costs. In the case of small-sized EOS, RATS\nsuccessfully identifies the optimal solution and demonstrates robustness with\nrespect to hyper-parameters tuning. In the case of large-sized EOS, RATS\nmarkedly improves the portfolio objective value, optimizing risk and capital\ncharge while respecting risk limits and preserving expected profits.\n  Our work bridges the gap between the implementation of effective trading\nstrategies and compliance with stringent regulatory and economic capital\nrequirements, allowing a better alignment of business and risk management\nobjectives.",
      "generated_abstract": "This paper presents an efficient and scalable approach for the evaluation of\npension plan risk and return in a large number of scenarios, including those\nwith extreme values. The approach relies on a Bayesian hierarchical model for\nthe distribution of the return and risk factors, and uses a novel approach to\nestimate the posterior distribution of the parameters. The model is\nparametrized by a vector of mean and standard deviation parameters, which are\nin turn estimated using a Markov Chain Monte Carlo (MCMC) approach. The\napproach is applied to the analysis of pension plan risk and return for\nvarious hypothetical scenarios, including the extreme value cases. The\nperformance of the approach is assessed using a simulation study, and the\napplication to real pension plan data from the UK National Insurance\nScheme.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.2631578947368421,
          "f": 0.16949152105716758
        },
        "rouge-2": {
          "r": 0.028688524590163935,
          "p": 0.0603448275862069,
          "f": 0.03888888452098815
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.21052631578947367,
          "f": 0.13559321597242188
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.02058v1",
      "true_abstract": "Ribonucleic acid (RNA) plays fundamental roles in biological systems, from\ncarrying genetic information to performing enzymatic function. Understanding\nand designing RNA can enable novel therapeutic application and biotechnological\ninnovation. To enhance RNA design, in this paper we introduce RiboGen, the\nfirst deep learning model to simultaneously generate RNA sequence and all-atom\n3D structure. RiboGen leverages the standard Flow Matching with Discrete Flow\nMatching in a multimodal data representation. RiboGen is based on Euclidean\nEquivariant neural networks for efficiently processing and learning\nthree-dimensional geometry. Our experiments show that RiboGen can efficiently\ngenerate chemically plausible and self-consistent RNA samples. Our results\nsuggest that co-generation of sequence and structure is a competitive approach\nfor modeling RNA.",
      "generated_abstract": "The current COVID-19 pandemic has impacted various aspects of life, including\nthe healthcare system, the labor market, and education. This paper examines how\nthese three systems are interconnected, focusing on the educational\nenvironment. We use a network analysis approach to examine the influence of\neducational institutions on the COVID-19 pandemic, focusing on universities,\ncolleges, and schools. The results show that educational institutions are key\nnodes in the COVID-19 network, with higher connectivity in the education\nsector. The analysis also reveals that COVID-19 outbreaks are associated with\nchanges in educational practices, such as closures of schools and\ndisruptions to in-person instruction. These findings highlight the importance of\neducational institutions in shaping the COVID-19 response and suggest\ninterventions to mitigate the impact of future pandemics.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16470588235294117,
          "p": 0.175,
          "f": 0.16969696470156118
        },
        "rouge-2": {
          "r": 0.00909090909090909,
          "p": 0.008771929824561403,
          "f": 0.008928566430168613
        },
        "rouge-l": {
          "r": 0.15294117647058825,
          "p": 0.1625,
          "f": 0.15757575258034914
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.03520v1",
      "true_abstract": "In our previous paper an effective algorithm for inverting polynomial\nautomorphisms was proposed. We extend its application to the case of formal\npower series over a field of arbitrary characteristic and illustrate the\nproposed approach with some examples.",
      "generated_abstract": "We investigate the stability of finite dimensional representations of\nassociative algebras with respect to tensor products and tensor extensions.\nOur results establish that, under mild conditions, the number of irreducible\nrepresentations of a finite dimensional associative algebra is bounded above\nby the number of irreducible representations of its tensor product and tensor\nextension. In particular, the number of irreducible representations of an\nassociative algebra is bounded above by the number of irreducible representations\nof its direct sum. We also establish that, under mild conditions, the number of\nirreducible representations of a finite dimensional associative algebra is\nbounded above by the number of irreducible representations of its direct sum.\nFinally, we establish that, under mild conditions, the number of irreducible\nrepresentations of a finite dimensional associative algebra is bounded above\nby the number of irreducible representations of its tensor product. Our\nresults establish new bounds for the number of irreducible representations",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3142857142857143,
          "p": 0.2391304347826087,
          "f": 0.2716049333638166
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2571428571428571,
          "p": 0.1956521739130435,
          "f": 0.2222222173144339
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.03698v1",
      "true_abstract": "Programs written in unsafe languages such as C are prone to memory safety\nerrors, which can lead to program compromises and serious real-world security\nconsequences. Recently, Memory-Safe WebAssembly (MSWASM) is introduced as a\ngeneral-purpose intermediate bytecode with built-in memory safety semantics.\nPrograms written in C can be compiled into MSWASM to get complete memory safety\nprotection. In this paper, we present our extensions on MSWASM, which improve\nits semantics and practicality. First, we formalize MSWASM semantics in\nCoq/Iris, extending it with inter-module interaction, showing that MSWASM\nprovides fine-grained isolation guarantees analogous to WASM's coarse-grained\nisolation via linear memory. Second, we present Aegis, a system to adopt the\nmemory safety of MSWASM for C programs in an interoperable way. Aegis pipeline\ngenerates Checked C source code from MSWASM modules to enforce spatial memory\nsafety. Checked C is a recent binary-compatible extension of C which can\nprovide guaranteed spatial safety. Our design allows Aegis to protect C\nprograms that depend on legacy C libraries with no extra dependency and with\nlow overhead. Aegis pipeline incurs 67% runtime overhead and near-zero memory\noverhead on PolyBenchC programs compared to native.",
      "generated_abstract": "This paper introduces a novel approach to the task of building a logical\ndatalogger (LD) from a database of logical operators. The LD is then used to\ngenerate a model for a set of constraints and, optionally, a model for a\nquery. The approach is based on a novel way of representing logical operators,\nwhich we term as \"operator-based\". We also propose a novel way of building a\nLD from a database of operators, which we term as \"operator-based\". Finally, we\npresent a novel way of using the LD to generate a model for a set of constraints\nand, optionally, a model for a query.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092436974789916,
          "p": 0.2826086956521739,
          "f": 0.15757575355445375
        },
        "rouge-2": {
          "r": 0.005747126436781609,
          "p": 0.014492753623188406,
          "f": 0.008230448608445844
        },
        "rouge-l": {
          "r": 0.10084033613445378,
          "p": 0.2608695652173913,
          "f": 0.1454545414332416
        }
      }
    },
    {
      "paper_id": "q-fin.GN.econ/EM/2501.01763v1",
      "true_abstract": "Following an analysis of existing AI-related exchange-traded funds (ETFs), we\nreveal the selection criteria for determining which stocks qualify as\nAI-related are often opaque and rely on vague phrases and subjective judgments.\nThis paper proposes a new, objective, data-driven approach using natural\nlanguage processing (NLP) techniques to classify AI stocks by analyzing annual\n10-K filings from 3,395 NASDAQ-listed firms between 2011 and 2023. This\nanalysis quantifies each company's engagement with AI through binary indicators\nand weighted AI scores based on the frequency and context of AI-related terms.\nUsing these metrics, we construct four AI stock indices-the Equally Weighted AI\nIndex (AII), the Size-Weighted AI Index (SAII), and two Time-Discounted AI\nIndices (TAII05 and TAII5X)-offering different perspectives on AI investment.\nWe validate our methodology through an event study on the launch of OpenAI's\nChatGPT, demonstrating that companies with higher AI engagement saw\nsignificantly greater positive abnormal returns, with analyses supporting the\npredictive power of our AI measures. Our indices perform on par with or surpass\n14 existing AI-themed ETFs and the Nasdaq Composite Index in risk-return\nprofiles, market responsiveness, and overall performance, achieving higher\naverage daily returns and risk-adjusted metrics without increased volatility.\nThese results suggest our NLP-based approach offers a reliable,\nmarket-responsive, and cost-effective alternative to existing AI-related ETF\nproducts. Our innovative methodology can also guide investors, asset managers,\nand policymakers in using corporate data to construct other thematic\nportfolios, contributing to a more transparent, data-driven, and competitive\napproach.",
      "generated_abstract": "This paper introduces a novel approach to the estimation of the long-term\ncontingency of asset prices using a generalised autoregressive conditional\nheteroskedasticity (GARCH) model. The model is used to estimate the long-term\ncontiguity of asset returns in a cross-sectional setting. The proposed\nframework facilitates the identification of the long-term contingency of asset\nreturns and the construction of the implied volatility forecast, allowing for\nmore robust asset price predictions. The proposed approach is applied to the\nCBOE Volatility Index (VIX) and the S&P 500 Index, and the results are\ncompared to those obtained using other approaches.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10919540229885058,
          "p": 0.31666666666666665,
          "f": 0.16239315857988174
        },
        "rouge-2": {
          "r": 0.01276595744680851,
          "p": 0.037037037037037035,
          "f": 0.018987337959662676
        },
        "rouge-l": {
          "r": 0.09195402298850575,
          "p": 0.26666666666666666,
          "f": 0.13675213293885613
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2503.01617v1",
      "true_abstract": "Although several accounts of scientific understanding exist, the concept of\nunderstanding in relation to technology remains underexplored. This paper\naddresses this gap by proposing a philosophical account of technological\nunderstanding - the type of understanding that is required for and reflected by\nsuccessfully designing and using technological artefacts. We develop this\nnotion by building on the concept of scientific understanding. Drawing on\nparallels between science and technology, and specifically between scientific\ntheories and technological artefacts, we extend the idea of scientific\nunderstanding into the realm of technology. We argue that, just as scientific\nunderstanding involves the ability to explain a phenomenon using a theory,\ntechnological understanding involves the ability to use a technological\nartefact to realise a practical aim. Technological understanding can thus be\nconsidered a specific application of knowledge: it encompasses the cognitive\nskill of recognising how a practical aim can be achieved by using a\ntechnological artefact. In a context of design, this general notion of\ntechnological understanding is specified as the ability to design an artefact\nthat, by producing a phenomenon through its physical structure, achieves the\nintended aim. We illustrate our concept of technological understanding through\ntwo running examples: magnetic resonance imaging (MRI) and superconducting\nquantum computers. Our account highlights the epistemic dimension of engaging\nwith technology and, by allowing for context-dependent specifications, provides\nguidance for testing and improving technological understanding in specific\ncontexts.",
      "generated_abstract": "The study of the evolution of large structures in astrophysics, such as\ngalactic or extragalactic structures, is a key topic in cosmology. While\nclassical hydrodynamical simulations provide a precise representation of the\nstructure evolution, they are computationally expensive and do not take\nconsiderations of gravitational interactions into account. The study of\ngravitational collapse in hydrodynamical simulations is particularly challenging\nbecause of the numerical issues associated with the formation of shock waves. In\nthis paper, we investigate the evolution of the large-scale structure of\ncosmological models that include a non-linear dark matter component. We\nparticularly focus on the gravitational collapse of spherical density\nwaves. We use a simple model of cold dark matter with a non-linear response to\ngravitational collapse and study the effect of non-linear dark matter on the\nstructure formation of the Universe. We also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.2222222222222222,
          "f": 0.1739130387145559
        },
        "rouge-2": {
          "r": 0.010050251256281407,
          "p": 0.017241379310344827,
          "f": 0.01269840804555474
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.18518518518518517,
          "f": 0.14492753146817913
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.12026v1",
      "true_abstract": "In this paper, we consider the impact of the order flow auction (OFA) in the\ncontext of the proposer-builder separation (PBS) mechanism through a\ngame-theoretic perspective. The OFA is designed to improve user welfare by\nredistributing maximal extractable value (MEV) to the users, in which two\nauctions take place: the order flow auction and the block-building auction. We\nformulate the OFA as a multiplayer game, and focus our analyses on the case of\ntwo competing players (builders). We prove the existence and uniqueness of a\nNash equilibrium for the two-player game, and derive a closed-form solution by\nsolving a quartic equation. Our result shows that the builder with a\ncompetitive advantage pays a relatively lower cost, leading to centralization\nin the builder space. In contrast, the proposer's shares evolve as a martingale\nprocess, which implies decentralization in the proposer (or, validator) space.\nOur analyses rely on various tools from stochastic processes, convex\noptimization, and polynomial equations. We also conduct numerical studies to\ncorroborate our findings, and explore other features of the OFA under the PBS\nmechanism.",
      "generated_abstract": "We study the problem of estimating the mean of a random variable from a\nestimator that is a linear combination of two estimators, one of which is a\nparametric one and the other a nonparametric one. We derive the asymptotic\ndistribution of the estimator under the null hypothesis and prove that under\nthe alternative, the estimator is asymptotically normal. We provide a\ncomputationally efficient algorithm to compute the estimator, and show that\nassuming a particular parametric class of distributions for the parametric\nestimator, the estimator is consistent under the null hypothesis and asymptotically\nnormal under the alternative.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13675213675213677,
          "p": 0.32653061224489793,
          "f": 0.1927710801763682
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.07058823529411765,
          "f": 0.0481927665876361
        },
        "rouge-l": {
          "r": 0.10256410256410256,
          "p": 0.24489795918367346,
          "f": 0.14457830909203087
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2503.06822v1",
      "true_abstract": "Clustering is a fundamental task in network analysis, essential for\nuncovering hidden structures within complex systems. Edge clustering, which\nfocuses on relationships between nodes rather than the nodes themselves, has\ngained increased attention in recent years. However, existing edge clustering\nalgorithms often overlook the significance of edge weights, which can represent\nthe strength or capacity of connections, and fail to account for noisy\nedges--connections that obscure the true structure of the network. To address\nthese challenges, the Weighted Edge Clustering Adjusting for Noise (WECAN)\nmodel is introduced. This novel algorithm integrates edge weights into the\nclustering process and includes a noise component that filters out spurious\nedges. WECAN offers a data-driven approach to distinguishing between meaningful\nand noisy edges, avoiding the arbitrary thresholding commonly used in network\nanalysis. Its effectiveness is demonstrated through simulation studies and\napplications to real-world datasets, showing significant improvements over\ntraditional clustering methods. Additionally, the R package ``WECAN'' has been\ndeveloped to facilitate its practical implementation.",
      "generated_abstract": "The objective of this paper is to derive the asymptotic properties of the\nrandom variable $X_t = \\sum_{i=1}^{t} X_i$ where $X_i$ is a random variable\nwith known distribution, $X_i \\sim N(0,1)$ for all $i$. We use the\nconcept of the weak convergence of random variables to derive the limiting\ndistribution of $X_t$. The method used in this paper is based on a\nseminal result of Slepian, and is applicable to any finite or infinite set of\nrandom variables with known distribution. We use this method to derive the\nlimiting distribution of $X_t$. We also derive the distribution of the\nsum-of-squares of the random variables, $S_t = \\sum_{i=1}^{t} X_i^2$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09016393442622951,
          "p": 0.1896551724137931,
          "f": 0.12222221785432115
        },
        "rouge-2": {
          "r": 0.0189873417721519,
          "p": 0.03529411764705882,
          "f": 0.02469135347592761
        },
        "rouge-l": {
          "r": 0.07377049180327869,
          "p": 0.15517241379310345,
          "f": 0.09999999563209896
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2411.07978v4",
      "true_abstract": "This note introduces a doubly robust (DR) estimator for regression\ndiscontinuity (RD) designs. RD designs provide a quasi-experimental framework\nfor estimating treatment effects, where treatment assignment depends on whether\na running variable surpasses a predefined cutoff. A common approach in RD\nestimation is the use of nonparametric regression methods, such as local linear\nregression. However, the validity of these methods still relies on the\nconsistency of the nonparametric estimators. In this study, we propose the\nDR-RD estimator, which combines two distinct estimators for the conditional\nexpected outcomes. The primary advantage of the DR-RD estimator lies in its\nability to ensure the consistency of the treatment effect estimation as long as\nat least one of the two estimators is consistent. Consequently, our DR-RD\nestimator enhances robustness of treatment effect estimators in RD designs.",
      "generated_abstract": "We study the problem of estimating a stochastic frontier model using data\nrepresenting a large number of businesses. We propose a robust estimation\nmethodology that uses the bootstrap to provide a consistent estimator. We\ndemonstrate that the proposed methodology has a lower bias and higher\nvariance compared to the traditional bootstrap-based method. Furthermore, we\nshow that the proposed methodology provides a consistent estimator when the\nsample size is large. Finally, we provide a simulation study to illustrate the\neffectiveness of our methodology. The proposed methodology can be used for\nestimating frontier models using data from multiple businesses.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18823529411764706,
          "p": 0.26666666666666666,
          "f": 0.22068965032104648
        },
        "rouge-2": {
          "r": 0.01652892561983471,
          "p": 0.022727272727272728,
          "f": 0.019138751105516227
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.25,
          "f": 0.20689654687277062
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/CB/2501.08356v1",
      "true_abstract": "Recent high-throughput experiments unveil substantial electrophysiological\ndiversity among uncoupled healthy myocytes under identical conditions. To\nquantify inter-cell variability, the values of a subset of the parameters in a\nwell-regarded mathematical model of the action potential of rabbit ventricular\nmyocytes are estimated from fluorescence voltage measurements of a large number\nof cells. Statistical inference yields a population of nearly 1200\ncell-specific model variants that, on a population-level replicate\nexperimentally measured biomarker ranges and distributions, and in contrast to\nearlier studies, also match experimental biomarker values on a cell-by-cell\nbasis. This model population may be regarded as a random sample from the\nphenotype of healthy rabbit ventricular myocytes. Uni-variate and bi-variate\njoint marginal distributions of the estimated parameters are presented, and the\nparameter dependencies of several commonly utilised electrophysiological\nbiomarkers are revealed. Parameter values are weakly correlated, while summary\nmetrics such as the action potential duration are not strongly dependent on any\nsingle electrophysiological characteristic of the myocyte. Our results\ndemonstrate the feasibility of accurately and efficiently fitting entire action\npotential waveforms at scale.\n  Keywords: cellular excitability, rabbit ventricular myocytes, fluorescence\nvoltage measurements, action potential waveform, parameter estimation in\ndifferential equations, noisy time series",
      "generated_abstract": "Cellular networks are central to biological processes, but their exact\ndesign remains elusive. While network theory and computational biology\naddress this challenge through graph theory and network models, the integration\nof these approaches remains inadequate for the description of complex biological\nsystems. Here, we propose a novel approach that bridges these fields by\nintegrating the mathematical theory of networks with the foundational\nprinciples of cellular networks. Our approach focuses on the topology of\nbiological networks, exploring how topology influences the dynamics of cellular\nprocesses, such as gene expression and cell division. By integrating\nnetwork-theoretic concepts with cellular mechanisms, our approach provides a\nfoundation for the design of cellular networks that are more efficient,\nreproducible, and stable. We illustrate this approach through the example of\ncellular networks that integrate the mathematical theory of networks with the\nfoundational principles of cell",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09448818897637795,
          "p": 0.14814814814814814,
          "f": 0.1153846106291607
        },
        "rouge-2": {
          "r": 0.0056179775280898875,
          "p": 0.008264462809917356,
          "f": 0.006688958392415189
        },
        "rouge-l": {
          "r": 0.07874015748031496,
          "p": 0.12345679012345678,
          "f": 0.0961538413983915
        }
      }
    },
    {
      "paper_id": "nlin.CG.nlin/CG/2502.13735v1",
      "true_abstract": "Two kinetic exchange models are proposed to explore the dynamics of closed\neconomic markets characterized by random exchanges, saving propensities, and\ncollective transactions. Model I simulates a system where individual\ntransactions occur among agents with saving tendencies, along with collective\ntransactions between groups. Model II restricts individual transactions to\nagents within the same group, but allows for collective transactions between\ngroups. A three-step trading process--comprising intergroup transactions,\nintragroup redistribution, and individual exchanges--is developed to capture\nthe dual-layered market dynamics. The saving propensity is incorporated using\nthe Chakraborti-Chakrabarti model, applied to both individual and collective\ntransactions. Results reveal that collective transactions increase wealth\ninequality by concentrating wealth within groups, as indicated by higher Gini\ncoefficients and Kolkata indices. In contrast, individual transactions across\ngroups mitigate inequality through more uniform wealth redistribution. The\ninterplay between saving propensities and collective transactions governs\ndeviation degree and entropy, which display inverse trends. Higher saving\npropensities lead to deviations from the Boltzmann-Gibbs equilibrium, whereas\nspecific thresholds result in collective transaction dominance, producing\nnotable peaks or troughs in these metrics. These findings underscore the\ncritical influence of dual-layered market interactions on wealth distribution\nand economic dynamics.",
      "generated_abstract": "This paper studies a class of non-autonomous stochastic differential equations\n(SDEs) of the form\n  \\begin{equation*}\n    \\begin{cases}\n      \\mathrm{d} X_t = \\sigma(X_t)\\mathrm{d} W_t, \\\\\n      X_0 = x_0,\n    \\end{cases}\n  \\end{equation*}\nwhere $W$ is a standard Brownian motion and $\\sigma$ is a given function.\n  We prove that the system is strongly mixing and thus, by the classical\nRuelle-Smirnov-Gleason theorem, the corresponding Markov chain is\nergodic.\n  We then establish the Markov property of the solution of the system and\nestablish that the solution is a Markov process.\n  Finally, we use the ergodic theorem to prove that the solution is also\nergodic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07751937984496124,
          "p": 0.16393442622950818,
          "f": 0.10526315353518023
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06201550387596899,
          "p": 0.13114754098360656,
          "f": 0.08421052195623292
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.11604v1",
      "true_abstract": "In this work, we revisit the estimation of the model parameters of a Weibull\ndistribution based on iid observations, using the maximum likelihood estimation\n(MLE) method which does not yield closed expressions of the estimators. Among\nother results, it has been shown analytically that the MLEs obtained by solving\nthe highly non-linear equations do exist (i.e., finite), and are unique. We\nthen proceed to study the sampling distributions of the MLEs through both\ntheoretical as well as computational means. It has been shown that the sampling\ndistributions of the two model parameters' MLEs can be approximated fairly well\nby suitable Weibull distributions too. Results of our comprehensive simulation\nstudy corroborate some recent results on the first-order bias and first-order\nmean squared error (MSE) expressions of the MLEs.",
      "generated_abstract": "We consider the problem of estimating a function $f$ from the mean value of\nthe function evaluated at a set of points. For the case where the set of points\nconsists of $N$ equally spaced points, we consider the so-called\n$\\epsilon$-$\\delta$ method. In the context of estimating the mean value of a\nfunction, the method has been studied extensively. However, in the context of\nestimating the function itself, the literature is relatively sparse. In this\npaper, we provide a comprehensive analysis of the $\\epsilon$-$\\delta$ method\nfor estimating the mean value of a function in the case where the set of points\nconsists of $N$ equally spaced points. We show that the method performs\nwell when the set of points is sufficiently large, but that it performs poorly\nwhen the set of points is too small. In addition, we show that the method can\nbe used to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2087912087912088,
          "p": 0.2923076923076923,
          "f": 0.2435897387286326
        },
        "rouge-2": {
          "r": 0.05217391304347826,
          "p": 0.0594059405940594,
          "f": 0.055555550576560794
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.2153846153846154,
          "f": 0.1794871746260685
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.physics/space-ph/2503.05358v1",
      "true_abstract": "Comets are the most pristine planetesimals left from the formation of the\nSolar System. They carry unique information on the materials and the physical\nprocesses which led to the presence of planets and moons. Many important\nquestions about cometary physics, such as origin, constituents and mechanism of\ncometary activity, remain unanswered. The next perihelion of comet 1P/Halley,\nin 2061, is an excellent opportunity to revisit this object of outstanding\nscientific and cultural relevance. In 1986, during its latest approach to the\nSun, several flyby targeted Halley's comet to observe its nucleus and shed\nlight on its properties, origin, and evolution. However, due to its retrograde\norbit and high ecliptic inclination, the quality of data was limited by the\nlarge relative velocity and short time spent by the spacecraft inside the coma\nof the comet. A rendezvous mission like ESA/Rosetta would overcome such\nlimitations, but the trajectory design is extremely challenging due to the\nshortcomings of current propulsion technology. Given the considerable lead\ntimes of spacecraft development and the long duration of the interplanetary\ntransfer required to reach the comet, it is imperative to start mission\nplanning several decades in advance. This study presents a low-thrust\nrendezvous strategy to reach the comet before the phase of intense activity\nduring the close approach to the Sun. The trajectory design combines a\ngravity-assist maneuver with electric propulsion arcs to maximize scientific\npayload mass while constraining transfer duration. A propulsive plane change\nmaneuver would be prohibitive. To keep the propellant budget within reasonable\nlimits, most of the plane change maneuver is achieved via either a Jupiter or a\nSaturn flyby. The interplanetary low-thrust gravity-assisted trajectory design\nstrategy is described, followed by the presentation of multiple\nproof-of-concept solutions.",
      "generated_abstract": "We present a new method for the accurate reconstruction of the ionization\ntime history of interplanetary plasma in the solar wind using the\nHamilton-Jacobi equation (HJE). The HJE is an exact equation that is valid in\nthe limit of high plasma energies and in the limit of weakly ionized plasmas,\nwhere it can be approximated by the Keldysh equation. We propose a method to\nsolve the HJE by iteratively solving the Keldysh equation. We demonstrate the\nuse of this method to reconstruct the time history of ionization in the solar\nwind using data from the Parker Solar Probe. This is the first time that the\nHJE has been used to reconstruct the ionization time history in the solar\nwind, and we expect it to be a useful tool for future plasma tomography\nexperiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.2857142857142857,
          "f": 0.1599999959680001
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.03636363636363636,
          "f": 0.02139037017930247
        },
        "rouge-l": {
          "r": 0.09444444444444444,
          "p": 0.24285714285714285,
          "f": 0.1359999959680001
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2412.15225v1",
      "true_abstract": "Biochemical reaction networks are typically modeled by $\\dfrac{dx}{dt}=N\\cdot\nK(x)=Y\\cdot I_a\\cdot K(x)$, with $x$ and $K(x)$ as the concentration and rate\nvectors, respectively, and $N$, $Y$, and $I_a$ as the stoichiometric,\nmolecularity, and incidence matrices, respectively. Steady states, which\ndescribe their long-term behaviors, are determined by solving $N\\cdot K(x)=0$,\nwhile complex balanced steady states are found by solving $I_a \\cdot K(x)=0$.\nTo investigate these complex networks, decomposition techniques are important,\nin particular, for computing steady states. Previously, we identified a\nwidespread property across many networks: the existence of independent and\nincidence-independent decompositions, characterized by the ability to directly\nsum the stoichiometric and incidence matrices of the subnetworks, respectively,\nto match those of the entire network. Here, we discover the ubiquitous property\nthat we call the Finest Decomposition Coarsening (FDC), where the finest\nindependent decomposition (FID) is a coarsening of the finest\nincidence-independent decomposition (FIID). To support the analysis of this\nproperty, we introduce a MATLAB package designed to compute both these\ndecompositions. We then characterize the FDC property and its relationship to\nstructural factors such as the invertibility of the molecularity matrix. We\nalso introduce and characterize the Finest Decompositions Equality (FDE)\nproperty, where FIID equals FID. Notably, we show that all deficiency zero\nnetworks exhibit the FDE property. Furthermore, we establish important\nrelationships of the FID and FIID with decomposition of the network into its\nconnected components. Our results highlight the prevalence of the coarsening\nproperty in reaction networks and deepens the understanding of the algebraic\nstructure and dynamics of biochemical networks.",
      "generated_abstract": "Quantitative proteomics (QP) is a promising approach for the simultaneous\nanalysis of hundreds of proteins in biological samples. However, current\nmethods are limited by the use of protein standards, which are often not\nquantifiable due to low abundance, and by the lack of automated and\nstandardized workflows for data processing and analysis. This work presents a\nfully automated workflow for QP, developed using the QP-Bio-Toolkit. It\nintegrates state-of-the-art data processing methods and an automated workflow\nthat generates a database of standardized proteins for QP. The workflow\nautomates data preprocessing and includes a validation strategy to ensure\nconsistent results across different samples and conditions. The toolkit is\ndesigned to be easily integrated into existing QP pipelines and can be used\nin different contexts, including single-cell proteomics, proteomics for\nd",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10967741935483871,
          "p": 0.2,
          "f": 0.14166666209201406
        },
        "rouge-2": {
          "r": 0.01702127659574468,
          "p": 0.03225806451612903,
          "f": 0.022284118040674098
        },
        "rouge-l": {
          "r": 0.09032258064516129,
          "p": 0.16470588235294117,
          "f": 0.11666666209201407
        }
      }
    },
    {
      "paper_id": "math.CA.math/CA/2503.04604v1",
      "true_abstract": "In this paper, we explore the relationship between the operators mapping\natoms to molecules in local Hardy spaces $h^p(\\mathbb{R}^n)$ and the size\nconditions of its kernel. In particular, we show that if the kernel a\nCalder\\'on--Zygmund-type operator satisfies an integral-type size condition and\na $T^*-$type cancellation, then the operator maps $h^p(\\mathbb{R}^n)$ atoms to\nmolecules. On the other hand, assuming that $T$ is an integral type operator\nbounded on $L^2(\\mathbb{R}^n)$ that maps atoms to molecules in\n$h^p(\\mathbb{R}^n)$, then the kernel of such operator satisfies the same\nintegral-type size conditions. We also provide the $L^1(\\mathbb{R}^n)$ to\n$L^{1,\\infty}(\\mathbb{R}^n)$ boundedness for such operators connecting our\nintegral-type size conditions on the kernel with others presented in the\nliterature.",
      "generated_abstract": "We investigate the structure of the group of endomorphisms of the\nfunctional space\n  $L^1([0,1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.045454545454545456,
          "p": 0.3,
          "f": 0.07894736613573414
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.045454545454545456,
          "p": 0.3,
          "f": 0.07894736613573414
        }
      }
    },
    {
      "paper_id": "math.NT.cs/FL/2503.00959v2",
      "true_abstract": "The Riemann zeta function, and more generally the L-functions of Dirichlet\ncharacters, are among the central objects of study in number theory. We report\non a project to formalize the theory of these objects in Lean's \"Mathlib\"\nlibrary, including a proof of Dirichlet's theorem on primes in arithmetic\nprogressions and a formal statement of the Riemann hypothesis",
      "generated_abstract": "The aim of this paper is to extend the concept of a minimal number of\ncomputational steps in the proof of a conjecture, established by A. Lusky and\nD. Poulsen, to the context of the existence of a solution to a nonlinear\ndifferential equation. To do so, we propose a definition of a minimal number of\nsteps for the proof of the existence of a solution to a nonlinear differential\nequation, which is based on the number of minimal steps needed to prove the\nexistence of a solution to the differential equation by a direct computation\nof the solution. We prove the existence of a solution to a nonlinear\ndifferential equation using the method of a proof with a minimum number of\nsteps. In addition, we give a characterization of the number of minimal steps\nneeded to prove the existence of a solution to a nonlinear differential\nequation.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2682926829268293,
          "p": 0.2,
          "f": 0.2291666617730036
        },
        "rouge-2": {
          "r": 0.05357142857142857,
          "p": 0.03333333333333333,
          "f": 0.041095885682117264
        },
        "rouge-l": {
          "r": 0.2682926829268293,
          "p": 0.2,
          "f": 0.2291666617730036
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.09133v1",
      "true_abstract": "The usual approach to tropical geometry is via degeneration of amoebas of\nalgebraic subvarieties of an algebraic torus $(\\mathbb{C}^*)^n$. An amoeba is\nlogarithmic projection of the variety forgetting the angular part of\ncoordinates, called the phase. Similar degeneration can be performed without\nignoring the phase. The limit then is called phase tropical variety, and it is\na powerful tool in numerous areas. In the article is described a\nnon-commutative version of phase tropicalization in the simplest case of the\nmatrix group $PSL_2(\\mathbb{C})$, replacing here $(\\mathbb{C}^*)^n$ in the\nclassical approach.",
      "generated_abstract": "We prove that a weakly amenable group $G$ is of type $A$ if and only if its\nitself is of type $A$ and $G$ is amenable if and only if $G$ is amenable. We\nalso prove a similar result for a weakly amenable group $G$ which is not of\ntype $A$, and we give a characterization of $A$-types for a group $G$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0847457627118644,
          "p": 0.17857142857142858,
          "f": 0.1149425243704586
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0847457627118644,
          "p": 0.17857142857142858,
          "f": 0.1149425243704586
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/MF/2503.08833v1",
      "true_abstract": "We study optimal execution in markets with transient price impact in a\ncompetitive setting with $N$ traders. Motivated by prior negative results on\nthe existence of pure Nash equilibria, we consider randomized strategies for\nthe traders and whether allowing such strategies can restore the existence of\nequilibria. We show that given a randomized strategy, there is a non-randomized\nstrategy with strictly lower expected execution cost, and moreover this\nde-randomization can be achieved by a simple averaging procedure. As a\nconsequence, Nash equilibria cannot contain randomized strategies, and\nnon-existence of pure equilibria implies non-existence of randomized\nequilibria. Separately, we also establish uniqueness of equilibria. Both\nresults hold in a general transaction cost model given by a strictly positive\ndefinite impact decay kernel and a convex trading cost.",
      "generated_abstract": "We investigate the impact of volatility risk premium on the value of an\nstock option under a stochastic volatility model. We assume a log-normal\nvolatility process and derive a stochastic volatility model for the underlying\nvolatility process. We use a stochastic volatility model to calculate the\noptimal exercise price and derive a closed-form solution to the value of the\noption. Our results show that the volatility risk premium increases the value\nof the option. We also find that the optimal exercise price is different for\ndifferent values of the volatility risk premium.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1951219512195122,
          "p": 0.38095238095238093,
          "f": 0.2580645116493237
        },
        "rouge-2": {
          "r": 0.025423728813559324,
          "p": 0.045454545454545456,
          "f": 0.03260869105151294
        },
        "rouge-l": {
          "r": 0.18292682926829268,
          "p": 0.35714285714285715,
          "f": 0.24193547939125917
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.q-bio/PE/2503.07083v2",
      "true_abstract": "Motivated by the paradigm of a super-Maltusian population catastrophe, we\nstudy a simple stochastic population model which exhibits a finite-time blowup\nof the population size and is strongly affected by intrinsic noise. We focus on\nthe fluctuations of the blowup time $T$ in the asexual binary reproduction\nmodel $2A \\to 3A$, where two identical individuals give birth to a third one.\nWe determine exactly the average blowup time as well as the probability\ndistribution $\\mathcal{P}(T)$ of the blowup time and its moments. In\nparticular, we show that the long-time tail $\\mathcal{P}(T\\to \\infty)$ is\npurely exponential. The short-time tail $\\mathcal{P}(T\\to 0)$ exhibits an\nessential singularity at $T=0$, and it is dominated by a single (the most\nlikely) population trajectory which we determine analytically.",
      "generated_abstract": "We investigate the statistical properties of the dynamics of the open\nnetwork of interacting proteins that are the focus of research in biological\nnetworks. We focus on the case of a single protein interacting with several\nother proteins. We derive the joint distribution of the protein's interactions\nand its network position, and investigate the interplay between the dynamics of\nboth the protein and the network. We show that, in the absence of interactions\nbetween the network and the protein, the joint distribution is Gaussian. This\ngaussianity persists even when the protein interacts with itself. However, when\nthe protein interacts with the network, the joint distribution undergoes\nsignificant changes. We find that the joint distribution exhibits a broad\ndistribution of interaction strengths, and that there is a non-trivial\ninterplay between the interactions between the network and the protein, and\nthose between the protein and itself. We also investigate the effects",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18072289156626506,
          "p": 0.23076923076923078,
          "f": 0.20270269777666194
        },
        "rouge-2": {
          "r": 0.08695652173913043,
          "p": 0.08620689655172414,
          "f": 0.08658008158018057
        },
        "rouge-l": {
          "r": 0.1686746987951807,
          "p": 0.2153846153846154,
          "f": 0.18918918426314843
        }
      }
    },
    {
      "paper_id": "physics.ed-ph.physics/ed-ph/2502.13774v1",
      "true_abstract": "In this work, we present a teaching strategy implemented in Introduction to\nPhysics, corresponding to the first year of the Physics Teacher Degree at the\nNational University of Rosario, whose main purpose is to provide students with\ntools to understand problem statements and exercises and to incorporate habits\nthat favor their resolution and communication. For this purpose, we implemented\nthe use of certain problemsolving algorithms, which we call\nHopscotch-Algorithms, appealing to the image of this popular game in which\nsteps can be skipped, rearranged or simultaneously executed. The aim is to\nstimulate a work method in a critical and problematized way, avoiding rigid or\ndogmatic applications of resolution steps. The testimonies collected indicate\nthat the strategy was positively valued by the students.",
      "generated_abstract": "The use of modern technologies has led to the development of various\ntechnical solutions for the safe storage of radioactive waste. One of the most\nrecent and promising solutions is the use of a storage rack with a large number\nof small storage containers, where each container is filled with a few\ngranular samples of the material to be stored. The problem is that the\ncontainers are placed in a high-pressure environment. The pressure inside the\ncontainers can be measured using a pressure transducer. It is well known that\nthe pressure inside a container can be determined by the ratio between the\nvolumetric capacity of the container and the pressure. The ratio can be\nmeasured using a gauge, for example.\n  The problem is that the pressure in a container is not constant and can\nchange significantly in a short period of time. To solve this problem, a\ncontrol system is needed that can measure the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18823529411764706,
          "p": 0.19753086419753085,
          "f": 0.1927710793402527
        },
        "rouge-2": {
          "r": 0.06722689075630252,
          "p": 0.06060606060606061,
          "f": 0.06374501493373161
        },
        "rouge-l": {
          "r": 0.18823529411764706,
          "p": 0.19753086419753085,
          "f": 0.1927710793402527
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/RM/2410.11849v1",
      "true_abstract": "In this paper, we investigate a complex variation of the standard joint life\nannuity policy by introducing three distinct contingent benefits for the\nsurviving member(s) of a couple, along with a contingent benefit for their\nbeneficiaries if both members pass away. Our objective is to price this\ninnovative insurance policy and analyse its sensitivity to key model\nparameters, particularly those related to the joint mortality framework. We\nemploy the $QP$-rule (described in Section \\ref{secgenset}), which combines the\nreal-world probability measure $P$ for mortality risk with risk-neutral\nvaluation under $Q$ for financial market risks. The model enables explicit\npricing expressions, computed using efficient numerical methods. Our results\nhighlight the interdependent risks faced by couples, such as broken-heart\nsyndrome, providing valuable insights for insurers and policyholders regarding\nthe pricing influences of these factors.",
      "generated_abstract": "This paper explores the use of multi-fidelity methods in asset pricing models\nfor portfolio selection. We consider the case of an infinite-dimensional\nportfolio space, and we propose a framework that allows for the incorporation\nof both qualitative and quantitative information in the model. We then\nintroduce a novel multi-fidelity loss function that integrates both\nquantitative and qualitative information. We apply this framework to the\nwell-known asset pricing models of the Sharpe-Renior-Cramer-Heston (SRCH) and\nthe Heston model, and we demonstrate that the proposed multi-fidelity\nframework enhances the performance of the models. We further show that the\nproposed multi-fidelity framework allows for a more efficient exploration of the\nportfolio space, which can be useful in the case of large-scale portfolio\nselection problems. Finally, we provide",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1650485436893204,
          "p": 0.26153846153846155,
          "f": 0.20238094763676318
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.028037383177570093,
          "f": 0.02531645074329353
        },
        "rouge-l": {
          "r": 0.1650485436893204,
          "p": 0.26153846153846155,
          "f": 0.20238094763676318
        }
      }
    },
    {
      "paper_id": "quant-ph.physics/atom-ph/2503.05664v1",
      "true_abstract": "We experimentally and theoretically study collective emission of a dense\natomic ensemble coupled to a whispering-gallery-mode (WGM) in a nanophotonic\nmicroring resonator. Due to many cold atoms localized in a small volume, these\ntrapped atoms collectively couple not only to the WGM and but also to the\nnon-guided modes in free space. Through tuning the atom-WGM coupling and by\nadjusting the number of trapped atoms, we demonstrate superradiant emission to\nthe WGM. For photon emission via the non-guided modes, our study reveals\nsignatures of subradiance and superradiance when the system is driven to the\nsteady-state states and the timed-Dicke states, respectively. Our experimental\nplatform thus presents the first atom-light interface with selective collective\nemission behavior into a guided mode and the environment, respectively. Our\nobservation and methodology could shed light on future explorations of\ncollective emission with densely packed quantum emitters coupled to\nnanophotonic light-matter interfaces.",
      "generated_abstract": "We propose a framework for quantum computation based on the theory of\nquantum\n  phase-space functions, a generalization of the classical phase-space\nfunction. We show that a quantum phase-space function $F$ with $dF/d\\theta =\n\\theta$ can be realized as a quantum circuit that maps the quantum state\n$\\ket{\\psi}$ to $\\ket{\\psi} + \\delta \\theta \\ket{0}$. We discuss the\nproperties of $F$, including its analytic continuation and its\nphase-space-like behavior. We then demonstrate how to implement $F$ using a\nquantum circuit that maps $\\ket{\\psi}$ to $\\ket{\\psi} + \\delta \\theta\n\\ket{0}$ and shows how to implement $F$ using a quantum circuit that maps\n$\\ket{\\psi}$ to $\\ket{\\psi} + \\delta \\theta \\ket{1}$. The circuit that maps\n$\\ket",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.19298245614035087,
          "f": 0.14102563638806723
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10101010101010101,
          "p": 0.17543859649122806,
          "f": 0.1282051235675544
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.05960v2",
      "true_abstract": "A parametrized Yang-Baxter equation is a map from a group to a set of\nR-matrices, satisfying the Yang-Baxter commutation relation. For the six-vertex\nmodel, there are two main regimes of the Yang-Baxter equation: the\nfree-fermionic point, and everything else. For the free-fermionic point, there\nexists a parametrized Yang-Baxter equation with a large parameter group\nGL(2)xGL(1). For non-free-fermionic six-vertex matrices, there are also\nparametrized Yang-Baxter equations, but these do not account for all possible\ninteractions. Instead we will construct a groupoid parametrized Yang-Baxter\nequation that does reflect all possible Yang-Baxter equations in the six-vertex\nmodel.",
      "generated_abstract": "We show that the space of smooth, compactly supported, cusp forms for the\nelliptic curve $Y_0(3)$ is naturally endowed with an action of the group of\nautomorphisms of the elliptic curve $Y_0(3)$. The action is in one-to-one\ncorrespondence with the group of automorphisms of the elliptic curve $Y_0(3)$\nthat leave the curve fixed.\n  We then use this action to construct a family of cusp forms that are\nconformally related to the Kodaira--Spencer field of the elliptic curve $Y_0(3)$\nand to the family of cusp forms that are related to the Kodaira--Spencer field\nof the elliptic curve $Y_0(2)$. We also show that the action of the group of\nautomorphisms of the elliptic curve $Y_",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23333333333333334,
          "p": 0.3111111111111111,
          "f": 0.2666666617687075
        },
        "rouge-2": {
          "r": 0.024691358024691357,
          "p": 0.02857142857142857,
          "f": 0.02649006125170042
        },
        "rouge-l": {
          "r": 0.23333333333333334,
          "p": 0.3111111111111111,
          "f": 0.2666666617687075
        }
      }
    },
    {
      "paper_id": "cond-mat.soft.cond-mat/soft/2503.09056v1",
      "true_abstract": "Long range order and symmetry in heterogeneous materials architected on\ncrystal lattices lead to elastic and inelastic anisotropies and thus limit\nmechanical functionalities in particular crystallographic directions. Here, we\npresent a facile approach for designing heterogeneous disordered materials that\nexhibit nearly isotropic mechanical resilience and energy dissipation\ncapabilities. We demonstrate, through experiments and numerical simulations on\n3D-printed prototypes, that near-complete isotropy can be attained in the\nproposed heterogeneous materials with a small, finite number of random spatial\npoints. We also show that adding connectivity between random subdomains leads\nto much enhanced elastic stiffness, plastic strength, energy dissipation, shape\nrecovery, structural stability and reusability in our new heterogeneous\nmaterials. Overall, our study opens avenues for the rational design of a new\nclass of heterogeneous materials with isotropic mechanical functionalities for\nwhich the engineered disorder throughout the subdomains plays a crucial role.",
      "generated_abstract": "The intriguing phenomena of liquid crystals are well known to be driven by\nthe polarization of the liquid, which is typically described by the so-called\nfield-induced polarization (FIP). In contrast, the nematic order parameter is\nusually described by the so-called nematic order parameter (NPM), which is\nderived from the nematic order parameter tensor (NPMT). However, the NPMT is\nnot a field-induced polarization, and is more generally known as the\npolarization-dependent order parameter (PDOP). In this paper, we show that the\npolarization-dependent order parameter (PDOP) can be derived from the NPMT, and\nthis polarization-dependent order parameter (PDOP) can be derived from the\nfield-induced polarization (FIP). Thus, the FIP and PDOP are equivalent in the\nquant",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1326530612244898,
          "p": 0.22413793103448276,
          "f": 0.1666666619953979
        },
        "rouge-2": {
          "r": 0.022556390977443608,
          "p": 0.03488372093023256,
          "f": 0.02739725550426471
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.20689655172413793,
          "f": 0.1538461491748851
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.15718v1",
      "true_abstract": "It is widely assumed that increases in economic productivity necessarily lead\nto economic growth. In this paper, it is shown that this is not always the\ncase. An idealized model of an economy is presented in which a new technology\nallows capital to be utilized autonomously without labor input. This is\nmotivated by the possibility that advances in artificial intelligence (AI) will\ngive rise to AI agents that act autonomously in the economy. The economic model\ninvolves a single profit-maximizing firm which is a monopolist in the product\nmarket and a monopsonist in the labor market. The new automation technology\ncauses the firm to replace labor with capital in such a way that its profit\nincreases while total production decreases. The model is not intended to\ncapture the structure of a real economy, but rather to illustrate how basic\neconomic mechanisms can give rise to counterintuitive and undesirable outcomes.",
      "generated_abstract": "This paper examines the impact of the global financial crisis on the\nconsumer behavior of consumers in Indonesia. The research uses the\nlongitudinal data of the Consumer Behavior Survey (KKM) collected between 2009\nand 2018. The study utilizes the panel data model to analyze the impact of\nfinancial crises on the purchasing behavior of consumers. The findings show\nthat financial crises negatively impact the behavior of consumers by reducing\nconsumer purchasing power and increasing the probability of debt. The study\nconcludes that financial crises can have significant economic and social\nimplications for consumers, particularly in emerging markets. The study\nrecommends that policymakers should implement effective measures to mitigate\nthe negative effects of financial crises on consumer behavior. The findings\nprovide insights into the impact of financial crises on consumer behavior,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.16666666666666666,
          "f": 0.14634145848899482
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.11956521739130435,
          "p": 0.1527777777777778,
          "f": 0.13414633653777533
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2502.08062v1",
      "true_abstract": "We present a machine learning approach for predicting the organisation of\ncorneal, glial and fibroblast cells in 3D cultures used for tissue engineering.\nOur machine-learning-based method uses a powerful generative adversarial\nnetwork architecture called pix2pix, which we train using results from\nbiophysical contractile network dipole orientation (CONDOR) simulations. In the\nfollowing, we refer to the machine learning method as the RAPTOR (RApid\nPrediction of Tissue ORganisation) approach. A training data set containing a\nrange of CONDOR simulations is created, covering a range of underlying model\nparameters. Validation of the trained neural network is carried out by\ncomparing predictions with cultured glial, corneal, and fibroblast tissues,\nwith good agreements for both CONDOR and RAPTOR approaches. An approach is\ndeveloped to determine CONDOR model parameters for specific tissues using a fit\nto tissue properties. RAPTOR outputs a variety of tissue properties, including\ncell densities of cell alignments and tension. Since it is fast, it could be\nvaluable for the design of tethered moulds for tissue growth.",
      "generated_abstract": "The formation of the blood-brain barrier (BBB) is a complex process that\n requires the coordination of numerous biological and chemical events. The\n formation of the BBB is highly regulated, and the mechanisms driving its\n development and maintenance remain largely unexplored. Here, we propose that\n the BBB is established by a balance between the effects of two opposing\n chemical and physical forces: the osmotic pressure of the extracellular fluid\n and the inhibitory effect of the endothelial glycocalyx on the diffusion of\n molecules across the endothelial cell membrane. We use a two-dimensional\n model to study the effect of these two forces on the development of the BBB.\n Our findings show that the BBB can be established in a single cell layer by\n varying the strength of the endothelial glycocalyx and the osmotic pressure",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12844036697247707,
          "p": 0.1891891891891892,
          "f": 0.15300545966377035
        },
        "rouge-2": {
          "r": 0.006369426751592357,
          "p": 0.009174311926605505,
          "f": 0.0075187921552973625
        },
        "rouge-l": {
          "r": 0.11009174311926606,
          "p": 0.16216216216216217,
          "f": 0.13114753616650263
        }
      }
    },
    {
      "paper_id": "physics.bio-ph.q-bio/TO/2410.06002v1",
      "true_abstract": "The understanding of the mechanisms driving vascular development is still\nlimited. Techniques to generate vascular trees synthetically have been\ndeveloped to tackle this problem. However, most algorithms are limited to\nsingle trees inside convex perfusion volumes. We introduce a new framework for\ngenerating multiple trees inside general nonconvex perfusion volumes. Our\nframework combines topology optimization and global geometry optimization into\na single algorithmic approach. Our first contribution is defining a baseline\nproblem based on Murray's original formulation, which accommodates efficient\nsolution algorithms. The problem of finding the global minimum is cast into a\nnonlinear optimization problem (NLP) with merely super-linear solution effort.\nOur second contribution extends the NLP to constrain multiple vascular trees\ninside any nonconvex boundary while avoiding intersections. We test our\nframework against a benchmark of an anatomic region of brain tissue and a\nvasculature of the human liver. In all cases, the total tree energy is improved\nsignificantly compared to local approaches. By avoiding intersections globally,\nwe can reproduce key physiological features such as parallel running inflow\nvessels and tortuous vessels. The ability to generate non-intersecting vascular\ntrees inside nonconvex organs can improve the functional assessment of organs.",
      "generated_abstract": "In this paper, we introduce a new algorithm, the Transform-Map-Reverse\n(TMR) algorithm, that allows one to compute the inverse of a matrix of\ntransformations, using a sequence of maps. We prove that the TMR algorithm\npreserves the singularity of the original matrix and that the inverse of the\noriginal matrix is also the inverse of the TMR-transformed matrix. We also\nderive a formula for the inverse of the TMR-transformed matrix, which provides\na closed form expression for the inverse of a $d \\times d$ matrix with\n$d < 2 \\times 2$. Our results provide a unified treatment of inverse\ntransformations, inverse maps, and inverse matrices.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1349206349206349,
          "p": 0.2982456140350877,
          "f": 0.18579234543760648
        },
        "rouge-2": {
          "r": 0.016666666666666666,
          "p": 0.034482758620689655,
          "f": 0.022471905718975737
        },
        "rouge-l": {
          "r": 0.1349206349206349,
          "p": 0.2982456140350877,
          "f": 0.18579234543760648
        }
      }
    },
    {
      "paper_id": "hep-th.hep-th/2503.10584v1",
      "true_abstract": "We investigate the shear viscosity and butterfly velocity of a magnetic\nfield-induced quantum phase transition in five dimensional\nEinstein-Maxwell-Chern-Simons theory, which is holographically dual to a class\nof strongly coupled quantum field theories with chiral anomalies. Our analysis\nreveals that the ratio of longitudinal shear viscosity to entropy density\n$\\eta_\\parallel/s$ exhibits a pronounced non-monotonic dependence on\ntemperature $T$ when the magnetic field $B$ is slightly below the critical\nvalue $B_c$ of the quantum phase transition. In particular, it can develop a\ndistinct minimum at an intermediate temperature. This contrasts sharply with\nthe monotonic temperature scaling observed at and above $B_c$, where\n$\\eta_\\parallel/s$ follows the scaling $T^{2/3}$ at $B=B_c$ and transitions to\n$T^2$ for $B>B_c$ as $T\\to0$. The non-vanishing of $\\eta_\\parallel/s$ for\n$B<B_c$ in the zero temperature limit suggests that it could serve as a good\norder parameter of the quantum phase transition. We also find that all\nbutterfly velocities change dramatically near the quantum phase transition, and\nthus their derivatives with respect to $B$ can be independently used to detect\nthe quantum critical point.",
      "generated_abstract": "We consider a non-Abelian gauge theory with a large $N$ limit in which the\nconformal symmetry is spontaneously broken, and we analyze the mass spectrum\nof the chiral fields. We find that the lightest chiral field is stable, while\nthe heavy chiral fields have mass degeneracy. Furthermore, we prove that the\nlightest chiral field can be identified with the lightest Dirac fermion in the\nconformal theory. Our analysis reveals the non-trivial nature of the mass\nspectrum of the chiral fields in a non-Abelian gauge theory with a large $N$\nlimit.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.40425531914893614,
          "f": 0.23899370652743174
        },
        "rouge-2": {
          "r": 0.049079754601226995,
          "p": 0.11940298507462686,
          "f": 0.0695652132623821
        },
        "rouge-l": {
          "r": 0.16071428571428573,
          "p": 0.3829787234042553,
          "f": 0.22641509017523048
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2503.06348v1",
      "true_abstract": "Real-time computer-based accompaniment for human musical performances entails\nthree critical tasks: identifying what the performer is playing, locating their\nposition within the score, and synchronously playing the accompanying parts.\nAmong these, the second task (score following) has been addressed through\nmethods such as dynamic programming on string sequences, Hidden Markov Models\n(HMMs), and Online Time Warping (OLTW). Yet, the remarkably successful\ntechniques of Deep Learning (DL) have not been directly applied to this\nproblem.\n  Therefore, we introduce HeurMiT, a novel DL-based score-following framework,\nutilizing a neural architecture designed to learn compressed latent\nrepresentations that enables precise performer tracking despite deviations from\nthe score. Parallelly, we implement a real-time MIDI data augmentation toolkit,\naimed at enhancing the robustness of these learned representations.\nAdditionally, we integrate the overall system with simple heuristic rules to\ncreate a comprehensive framework that can interface seamlessly with existing\ntranscription and accompaniment technologies.\n  However, thorough experimentation reveals that despite its impressive\ncomputational efficiency, HeurMiT's underlying limitations prevent it from\nbeing practical in real-world score following scenarios. Consequently, we\npresent our work as an introductory exploration into the world of DL-based\nscore followers, while highlighting some promising avenues to encourage future\nresearch towards robust, state-of-the-art neural score following systems.",
      "generated_abstract": "This paper introduces a novel audio-visual speaker verification (AVSV)\nmethod based on deep learning techniques. The proposed approach features an\nend-to-end learning architecture with a two-stage training process, enabling\nrapid deployment in real-world applications. The first stage employs a\nconvolutional neural network (CNN) model to extract speaker features from\naudio and visual signals, while the second stage employs a transformer-based\nmodel to analyze speaker identity, classification, and temporal dynamics.\nAdditionally, the paper introduces an end-to-end learning model based on the\ntransformer architecture that enables a flexible and robust model, suitable\nfor various use cases. To evaluate the proposed method, the proposed model was\ntrained and tested on the CLEVR dataset, achieving an overall accuracy of 85.9%\nand an F1 score of 0.77, demonstrating its",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14634146341463414,
          "p": 0.2727272727272727,
          "f": 0.19047618593096505
        },
        "rouge-2": {
          "r": 0.020100502512562814,
          "p": 0.034482758620689655,
          "f": 0.025396820743966587
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.25,
          "f": 0.17460317005794923
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.08587v1",
      "true_abstract": "The increasing use of children's automatic speech recognition (ASR) systems\nhas spurred research efforts to improve the accuracy of models designed for\nchildren's speech in recent years. The current approach utilizes either\nopen-source speech foundation models (SFMs) directly or fine-tuning them with\nchildren's speech data. These SFMs, whether open-source or fine-tuned for\nchildren, often exhibit higher word error rates (WERs) compared to adult\nspeech. However, there is a lack of systemic analysis of the cause of this\ndegraded performance of SFMs. Understanding and addressing the reasons behind\nthis performance disparity is crucial for improving the accuracy of SFMs for\nchildren's speech. Our study addresses this gap by investigating the causes of\naccuracy degradation and the primary contributors to WER in children's speech.\nIn the first part of the study, we conduct a comprehensive benchmarking study\non two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised\nSFMs (Whisper and MMS) across various age groups on two children speech\ncorpora, establishing the raw data for the causal inference analysis in the\nsecond part. In the second part of the study, we analyze the impact of\nphysiological factors (age, gender), cognitive factors (pronunciation ability),\nand external factors (vocabulary difficulty, background noise, and word count)\non SFM accuracy in children's speech using causal inference. The results\nindicate that physiology (age) and particular external factor (number of words\nin audio) have the highest impact on accuracy, followed by background noise and\npronunciation ability. Fine-tuning SFMs on children's speech reduces\nsensitivity to physiological and cognitive factors, while sensitivity to the\nnumber of words in audio persists.\n  Keywords: Children's ASR, Speech Foundational Models, Causal Inference,\nPhysiology, Cognition, Pronunciation",
      "generated_abstract": "This paper introduces a novel model for the prediction of the time series of\nintensity of a 3D magnetic field in the form of a 3D Fourier Transform\nInversion (FTI) model. The model is based on a 3D Fourier Transform, which\ntransforms a 2D time series into a 3D space. It uses a novel method to\nrepresent the 3D Fourier Transform. This new method is based on a series of\nlinear transformations that transform the input 2D time series into a\n1D-time series, followed by a linear transformation back into the original\n2D-space. This process is repeated for each input time series, resulting in a\n3D-space. The model is trained on a dataset of intensity of 3D magnetic fields\nand can be used to predict intensity of 3D magnetic fields. The model is\nevaluated on test data, and results are",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08536585365853659,
          "p": 0.2028985507246377,
          "f": 0.12017166965094232
        },
        "rouge-2": {
          "r": 0.01606425702811245,
          "p": 0.03636363636363636,
          "f": 0.02228411831224233
        },
        "rouge-l": {
          "r": 0.07317073170731707,
          "p": 0.17391304347826086,
          "f": 0.10300428767669341
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SY/2503.09775v1",
      "true_abstract": "This paper introduces a data-driven graphical framework for the real-time\nsearch of risky cascading fault chains (FCs) in power-grids, crucial for\nenhancing grid resiliency in the face of climate change. As extreme weather\nevents driven by climate change increase, identifying risky FCs becomes crucial\nfor mitigating cascading failures and ensuring grid stability. However, the\ncomplexity of the spatio-temporal dependencies among grid components and the\nexponential growth of the search space with system size pose significant\nchallenges to modeling and risky FC search. To tackle this, we model the search\nprocess as a partially observable Markov decision process (POMDP), which is\nsubsequently solved via a time-varying graph recurrent neural network (GRNN).\nThis approach captures the spatial and temporal structure induced by the\nsystem's topology and dynamics, while efficiently summarizing the system's\nhistory in the GRNN's latent space, enabling scalable and effective\nidentification of risky FCs.",
      "generated_abstract": "In this paper, we propose a novel paradigm for multi-agent systems (MASs)\nthat enables dynamic collaboration between agents that are not directly\nconnected. This is achieved by incorporating an online MAS-informed\ncommunication-assisted learning (CAL) framework that adaptively selects the\ncommunication strategy for agents based on their interactions with the MAS. We\ndefine the online CAL problem as a multi-agent stochastic control problem\nwith a continuous-time decision horizon and continuous-valued state space. We\nprove that, under certain conditions, the optimal CAL policy is analytically\ncomputable, and that the CAL problem can be solved in polynomial time. We\nalso propose an offline learning algorithm that learns the optimal CAL policy\nfrom the MAS dynamics and state, which is guaranteed to converge to the\noptimal CAL policy. We demonstrate the effectiveness of the proposed CAL",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.20224719101123595,
          "f": 0.18848167041583305
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.024390243902439025,
          "f": 0.0232558089643061
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.1797752808988764,
          "f": 0.1675392620388697
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.07798v1",
      "true_abstract": "Understanding the risk and protective factors associated with Parkinsons\ndisease (PD) is crucial for improving outcomes for patients, individuals at\nrisk, healthcare providers, and healthcare systems. Studying these factors not\nonly enhances our knowledge of the disease but also aids in developing\neffective prevention, management, and treatment strategies. This paper reviews\nthe key risk and protective factors associated with PD, with a particular focus\non the biological mechanisms underlying these factors. Risk factors include\ngenetic mutations, racial predispositions, and environmental exposures, all of\nwhich contribute to an increased likelihood of developing PD or accelerating\ndisease progression. Conversely, protective factors such as regular physical\nexercise, adherence to a Mediterranean diet, and higher urate levels have\ndemonstrated potential to reduce inflammation and support mitochondrial\nfunction, thereby mitigating disease risk. However, identifying and validating\nthese factors presents significant challenges. To overcome challenges, we\npropose several solutions and recommendations. Future research should\nprioritize the development of standardized biomarkers for early diagnosis,\ninvestigate gene-environment interactions in greater depth, and refine animal\nmodels to better mimic human PD pathology. Additionally, we offer actionable\nrecommendations for PD prevention and management, tailored to healthy\nindividuals, patients diagnosed with PD, and healthcare systems. These\nstrategies aim to improve clinical outcomes, enhance quality of life, and\noptimize healthcare delivery for PD.",
      "generated_abstract": "In the field of immuno-oncology, it is becoming increasingly important to\nknow the biological impact of the immune response on tumor cells. This\ninformation is essential for designing targeted therapies and tailoring the\ntreatment to each patient. The goal of this study is to explore the impact of\nthe immune response on cancer cells, and to understand the mechanisms behind the\nimmunogenicity of the tumor. To achieve this, we used the MG-73 (a human breast\ncancer cell line) and H1299 (a human lung cancer cell line) as model systems.\nWe carried out several experiments to understand the role of the immune\nresponse in the tumor cell response and the impact of the tumor in the\nimmune response. We observed that the tumor had a significant impact on the\nimmune response in both cell lines. The tumor cells produced different",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13513513513513514,
          "p": 0.2702702702702703,
          "f": 0.18018017573573586
        },
        "rouge-2": {
          "r": 0.015151515151515152,
          "p": 0.02702702702702703,
          "f": 0.01941747112451801
        },
        "rouge-l": {
          "r": 0.12837837837837837,
          "p": 0.25675675675675674,
          "f": 0.17117116672672683
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.01761v1",
      "true_abstract": "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
      "generated_abstract": "Cellular and molecular mechanisms underlying cancer progression and\ntreatment resistance are complex and evolving. Recent advancements in\ndeep-learning models and large language models have demonstrated remarkable\nsuccess in predicting patient outcomes in oncology, offering hope for more\npersonalized and targeted treatment strategies. However, these models often\nfall short of addressing the complexities of cancer biology, particularly in\nunderstanding disease mechanisms and predicting therapeutic efficacy. In this\narticle, we introduce the Tumor Biology and Therapy Prediction (TBTP) model, a\nnovel deep-learning model that addresses these challenges. The model leverages\nstate-of-the-art deep learning techniques, including transformer-based models,\nto learn robust features that predict patient survival, response to therapy,\nand disease progression. Our model combines deep learning with a comprehensive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.19540229885057472,
          "f": 0.1416666620447918
        },
        "rouge-2": {
          "r": 0.004651162790697674,
          "p": 0.008695652173913044,
          "f": 0.006060601519746286
        },
        "rouge-l": {
          "r": 0.10457516339869281,
          "p": 0.1839080459770115,
          "f": 0.13333332871145848
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.17059v1",
      "true_abstract": "This study examines the determinants of the spousal age gap (SAG) in India,\nutilizing data from the 61st and 68th rounds of the National Sample Survey\n(NSSO). We employ regression analysis, including instrumental variables, to\naddress selection bias and account for unobservable factors. We hypothesize an\ninverted U-shaped relationship between educational assortative mating and SAG,\nwhere, keeping the husband's education constant at the graduation level, the\nSAG first widens and then narrows as the wife's education level increases from\nprimary to postgraduate. This pattern is shaped by distinct socio-economic\nfactors across rural and urban settings. In rural India, increasing prosperity,\nchanges in family structure, and educational hypergamy contribute to a wider\nage gap, with the influence of bride squeeze further exacerbating this\ndisparity. Conversely, in urban areas, while the growth of white-collar jobs\ninitially contributed to a narrowing of the SAG in 2004-05, this trend did not\npersist by 2011-12. Specifically, the influence of income on SAG becomes\nnonlinear, showing declining trends beyond the 7th income quantile, reflecting\nlimited marriage mobility opportunities for females and hinting at a possible\nthreat to the institution of marriage among the urban upper class. To the best\nof our knowledge, this is the first study to provide empirical evidence on how\nspecific social, economic, and cultural dynamics influence the spousal age gap\nin Indian society. This increasing and persistent spousal age gap has\nsignificant implications for the treatment of women, power dynamics, and\nviolence within marriage.",
      "generated_abstract": "In this paper, we investigate the role of network effects in the evolution of\nthe global economy by analyzing the effects of the COVID-19 pandemic on\nnetwork effects. We use the World Value Survey (WVS) data, which measures\npopularity of countries worldwide, as a proxy for network effects. We find that\nthe COVID-19 pandemic had a significant impact on the popularity of countries\nwith respect to the WVS. The impact is more pronounced in countries with\nstronger network effects, such as the United States, Canada, and Japan. The\nimpact is also larger in countries with a higher level of income inequality,\nsuch as the United States and Canada. The findings suggest that network\neffects can play a significant role in shaping the global economy and\ninfluencing economic growth. The COVID-19 pandemic has had a significant\nimpact on the global economy, with network effects playing a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13125,
          "p": 0.28,
          "f": 0.17872339990946143
        },
        "rouge-2": {
          "r": 0.017391304347826087,
          "p": 0.034482758620689655,
          "f": 0.023121382826022444
        },
        "rouge-l": {
          "r": 0.11875,
          "p": 0.25333333333333335,
          "f": 0.16170212331371675
        }
      }
    },
    {
      "paper_id": "math.GR.math/GR/2503.06379v1",
      "true_abstract": "Let $G$ be a finite group and $p$ be a prime. We denote by $C_p(G)$ the poset\nof all cosets of $p$-subgroups of $G$. We characterize the homotopy type of the\ngeometric realization $|\\Delta C_p(G)|$ for $p$-closed groups $G$, which is\nmotivated by K.S.Brown's Question. We will further demonstrate that\n$\\chi(C_{p}(G)) \\equiv |G|_{p'} (\\text{mod} p)$ for any finite group $G$ and\nany prime $p$.",
      "generated_abstract": "In this paper we study the behavior of the Hodge star operator in a class\nof non-compact K\\\"ahler manifolds that are conformally flat to some K\\\"ahler\nmanifolds. We prove that the Hodge star operator has a regular singularity in\nthis class of non-compact K\\\"ahler manifolds, and that the regular singularity\nis transverse to the conifold. We also prove that the Hodge star operator is\nholomorphic in this class of non-compact K\\\"ahler manifolds. We use these\nresults to construct a family of K\\\"ahler-Einstein non-compact K\\\"ahler\nmanifolds with an isolated singularity of Hodge type, and show that the\nsingularity is transverse to the conifold. We also provide a new example of a\nK\\\"ahler-Einstein non-compact K\\\"ahler manifold",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14893617021276595,
          "p": 0.14,
          "f": 0.14432989191199933
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.0125,
          "f": 0.013986009056679325
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.14,
          "f": 0.14432989191199933
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/CO/2502.03439v1",
      "true_abstract": "The pyLOT library offers a Python implementation of linearized optimal\ntransport (LOT) techniques and methods to use in downstream tasks. The pipeline\nembeds probability distributions into a Hilbert space via the Optimal Transport\nmaps from a fixed reference distribution, and this linearization allows\ndownstream tasks to be completed using off the shelf (linear) machine learning\nalgorithms. We provide a case study of performing ML on 3D scans of lemur\nteeth, where the original questions of classification, clustering, dimension\nreduction, and data generation reduce to simple linear operations performed on\nthe LOT embedded representations.",
      "generated_abstract": "We study a generalization of the popular Bayesian nonparametric regression\nmodel, which includes a smooth function of the predictor as an additive\nbackground, leading to a multivariate Gaussian distribution. This model is\nmotivated by a variety of real-world applications, such as Gaussian process\nregression and Gaussian process classification, and is of particular interest in\nmachine learning. To obtain a tractable posterior distribution, we propose a\nframework based on a novel generalized multivariate Gaussian process\nregression. This framework is inspired by the recent development of\nmultivariate Gaussian processes, where the prior distribution is determined\nthrough a single multivariate Gaussian process regression. We show that our\nframework achieves the same posterior distribution as the multivariate Gaussian\nprocess regression, and we provide a computational scheme to obtain the\nposterior distribution efficiently. We also propose a new algorithm for\nestimating the posterior mean and covariance matrix of the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19736842105263158,
          "p": 0.18518518518518517,
          "f": 0.19108279755284202
        },
        "rouge-2": {
          "r": 0.03296703296703297,
          "p": 0.0234375,
          "f": 0.02739725541669359
        },
        "rouge-l": {
          "r": 0.17105263157894737,
          "p": 0.16049382716049382,
          "f": 0.1656050905464726
        }
      }
    },
    {
      "paper_id": "hep-th.math-ph/2503.09997v1",
      "true_abstract": "This review explores recent advances in the theory of $T\\bar{T}$ deformation,\nan irrelevant yet solvable deformation of quantum field theories defined via\nthe quadratic form of the energy-momentum tensor. It addresses classical and\nquantum aspects, highlighting significant developments across various fields,\nincluding field theory, holography, and string theory. Classically, $T\\bar{T}$\ndeformation manifests through multiple geometric interpretations, notably\nrandom geometry, Jackiw-Teitelboim-like gravity, and uniform light-cone gauge\nframeworks. For quantum aspects, the deformation introduces notable features\nsuch as non-locality, UV-IR mixing, solvable renormalization structures, and\nintriguing modifications to correlation functions and entanglement properties.\nFurthermore, the paper examines the profound relationship between $T\\bar{T}$\ndeformation and holography, particularly within the mixed boundary\nconditions/cutoff AdS holography proposal and holographic entanglement entropy.\nConnections to string theory through single-trace deformations and their\nholographic duals further reveal the deformed structure of the worldsheet. This\nreview synthesizes recent developments and outlines potential directions for\nfuture research in the study of $T\\bar{T}$-like deformation.",
      "generated_abstract": "We introduce a generalization of the Heisenberg-Weyl spin chain, which\ncontains two-dimensional (2D) quantum spin chains and 2D Heisenberg spin\nchains. The 2D spin chains are connected to the 2D Heisenberg spin chains\nthrough the Heisenberg-Weyl spin chain. We also introduce a 2D Heisenberg spin\nchain with $z$ and $z'$ subsystems and show that the 2D Heisenberg spin chain\ncan be mapped to the 2D spin chain through the Heisenberg-Weyl spin chain. In\nthe 2D spin chain, the $z$ subsystem is coupled to the $z'$ subsystem by a\npair of Majorana fermions. We discuss the 2D spin chain and 2D Heisenberg spin\nchain in a dual picture, where the $z$ and $",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06481481481481481,
          "p": 0.14583333333333334,
          "f": 0.08974358548323493
        },
        "rouge-2": {
          "r": 0.00684931506849315,
          "p": 0.013333333333333334,
          "f": 0.009049769271720654
        },
        "rouge-l": {
          "r": 0.06481481481481481,
          "p": 0.14583333333333334,
          "f": 0.08974358548323493
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.04325v2",
      "true_abstract": "Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .",
      "generated_abstract": "We propose a novel framework, named V-SAM (Video-Self-Attention-Matching),\nfor video alignment. By integrating the attention mechanism with self-attention\nand alignment module, our method effectively captures the visual similarities\namong video frames and aligns them with the corresponding annotations.\nAdditionally, our method introduces a video-level alignment loss, which\nstrengthens the alignment between video frames and the corresponding annotations.\nExperimental results on four benchmark datasets demonstrate that our method\nachieves the state-of-the-art performance in both video alignment and\nalignment-aware video retrieval tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10457516339869281,
          "p": 0.2909090909090909,
          "f": 0.15384614995608367
        },
        "rouge-2": {
          "r": 0.010526315789473684,
          "p": 0.027777777777777776,
          "f": 0.015267171586738409
        },
        "rouge-l": {
          "r": 0.0915032679738562,
          "p": 0.2545454545454545,
          "f": 0.13461538072531445
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/AP/2502.17371v2",
      "true_abstract": "The integration of photovoltaic (PV) systems into greenhouses not only\noptimizes land use but also enhances sustainable agricultural practices by\nenabling dual benefits of food production and renewable energy generation.\nHowever, accurate prediction of internal environmental conditions is crucial to\nensure optimal crop growth while maximizing energy production. This study\nintroduces a novel application of Spatio-Temporal Graph Neural Networks\n(STGNNs) to greenhouse microclimate modeling, comparing their performance with\ntraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal\npattern recognition, they cannot explicitly model the directional relationships\nbetween environmental variables. Our STGNN approach addresses this limitation\nby representing these relationships as directed graphs, enabling the model to\ncapture both spatial dependencies and their directionality. Using\nhigh-frequency data collected at 15-minute intervals from a greenhouse in\nVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter\nconditions (R^2 = 0.985) but show limitations during summer cooling system\noperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),\ntheir architecture offers greater potential for integrating additional\nvariables such as PV generation and crop growth indicators.",
      "generated_abstract": "This paper examines the relationship between the number of features and the\nfinancial performance of machine learning (ML) models, focusing on\nover-parameterized models. We analyze the relationship between the number of\nfeatures and the performance of over-parameterized models using two datasets:\nthe MNIST handwritten digit recognition dataset and the Fashion-MNIST dataset.\nOur findings indicate that the number of features does not have a significant\neffect on the performance of over-parameterized models, although the\nover-parameterization strategy has a significant impact on the performance. We\nalso explore the impact of model complexity on the performance of over-parameterized\nmodels, finding that the complexity of the model does not have a significant\neffect on the performance of over-parameterized models. Our findings highlight\nthe importance of understanding the relationship between the number of features\nand the performance of over-parameterized models, providing insights for\noptim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09154929577464789,
          "p": 0.22413793103448276,
          "f": 0.12999999588200015
        },
        "rouge-2": {
          "r": 0.005780346820809248,
          "p": 0.011363636363636364,
          "f": 0.007662830779351118
        },
        "rouge-l": {
          "r": 0.07746478873239436,
          "p": 0.1896551724137931,
          "f": 0.10999999588200014
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.00432v1",
      "true_abstract": "A specialized high-precision numerical search for equal-mass collisionless\nthree-body periodic free-fall orbits with central symmetry is conducted. The\nsearch is based on Newton's method with initial approximations obtained by the\ngrid-search method. Instead of solving the standard periodicity equation on the\nentire period a quarter-period equation that also characterizes the periodic\norbits is solved. The number of the known orbits from the class is\nsignificantly enlarged. The linear stability of the orbits is also\ninvestigated. All of them are unstable. A discussion in relation to the\nefficiency of Newton's method applied with grid-search initial approximations\nis held.",
      "generated_abstract": "The search for dark matter (DM) has evolved from a fundamental physics\nexploration into a multi-disciplinary field. The discovery of DM candidates,\nsuch as WIMPs and axions, has been supported by a range of experimental\ntechniques, including cosmological, astrophysical, and terrestrial searches.\nWhile these techniques have yielded some promising results, they have also\nhighlighted the limitations of their use in the search for DM. In this paper,\nwe review the current status of astrophysical DM searches and discuss the\npossibilities for future experiments to advance the field. We also present a\ndetailed analysis of the astrophysical constraints on DM, including the\nconstraints from galactic and extragalactic DM annihilation, DM streaming, and\nDM-galaxy interactions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1935483870967742,
          "p": 0.1518987341772152,
          "f": 0.17021276103012944
        },
        "rouge-2": {
          "r": 0.03260869565217391,
          "p": 0.02702702702702703,
          "f": 0.02955664529010736
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.12658227848101267,
          "f": 0.14184396670388832
        }
      }
    },
    {
      "paper_id": "math.GT.math/GT/2503.08268v1",
      "true_abstract": "We generalise the finite biquandle colouring invariant to a polynomial\ninvariant based on labelling a knot diagram with a finite birack that reduces\nto the biquandle colouring invariant in that case. The polynomial is an\ninvariant of a class of knot theories amenable to a generalisation of theorem\nof Trace on regular homotopy. We take the opportunity to reprise the relevant\ngeneralised knot theory and the theory of generalised biracks in the light of\nthis work and recent developments.",
      "generated_abstract": "The purpose of this paper is to prove the existence of a solution to a\nnewly introduced linear first order partial differential equation of the\nsecond order in the space of all smooth functions on the unit circle $S^1$.\nMore precisely, we prove that there exists a unique solution for this equation\nwhich is a continuous function on the unit circle. We also prove that the\nsolution is smooth on the unit circle and we give a proof that the solution is\nalso smooth on any compact set of the unit circle. This is a generalization of\nthe classical result of A.W.F.F.L.K. and W.M.W. of the existence of a solution\nto a linear first order partial differential equation of the second order in\nthe space of all smooth functions on a compact set of the unit circle.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.2033898305084746,
          "f": 0.22857142364807267
        },
        "rouge-2": {
          "r": 0.05333333333333334,
          "p": 0.04081632653061224,
          "f": 0.04624276965485034
        },
        "rouge-l": {
          "r": 0.2608695652173913,
          "p": 0.2033898305084746,
          "f": 0.22857142364807267
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.13556v1",
      "true_abstract": "This study examines the relationship between the concept of serious leisure\nand user innovation. We adopted the characteristics of innovative consumers\nidentified by Luthje (2004)-product use experience, information exchange, and\nnew product adoption speed-to analyze their correlation with serious leisure\nengagement. The analysis utilized consumer behavior survey data from the\n\"Marketing Analysis Contest 2023\" sponsored by Nomura Research Institute,\nexamining the relationship between innovative consumer characteristics and the\ndegree of serious leisure (Serious Leisure Inventory and Measure: SLIM). Since\nthe contest data did not directly measure innovative consumer characteristics\nor serious leisure engagement, we established alternative variables for\nquantitative analysis. The results showed that the SLIM alternative variable\nhad positive correlations with diverse product experiences and early adoption\nof new products. However, no clear relationship was found with information\nexchange among consumers. These findings suggest that serious leisure practice\nmay serve as a potential antecedent to user innovation. The leisure career\nperspective of the serious leisure concept may capture the motivations of user\ninnovators that Okada and Nishikawa (2019) identified.",
      "generated_abstract": "We present a novel framework for modeling and predicting labor market outcomes\nin an evolving economy. We introduce a time-varying random walk with a\ndisturbance (TRWAD) model, which captures the dynamic interactions between\neconomic conditions and labor market outcomes. The model accounts for both\ninstrumental variables and error correction mechanisms. Using data from the\n2006-2016 National Longitudinal Survey of Youth 1997 (NLSY97), we test the\npredictive performance of the TRWAD model using a variety of approaches. We\nfind that the model outperforms traditional time-varying structural models,\nincluding the alternative TRWAD(0,1) model. This result is consistent across a\nrange of selection and error correction mechanisms, including a variety of\nsemiparametric instruments, exogenous variables, and a model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14545454545454545,
          "p": 0.19753086419753085,
          "f": 0.16753926213097242
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.037037037037037035,
          "f": 0.03007518314658905
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.18518518518518517,
          "f": 0.15706805794249076
        }
      }
    },
    {
      "paper_id": "physics.soc-ph.q-bio/PE/2502.02713v1",
      "true_abstract": "The spread of disinformation poses a significant threat to societal\nwell-being. We analyze this phenomenon using an evolutionary game theory model\nof the sender-receiver game, where senders aim to mislead receivers and\nreceivers aim to discern the truth. Using a combination of replicator\nequations, finite-size scaling analysis, and extensive Monte Carlo simulations,\nwe investigate the long-term evolutionary dynamics of this game. Our central\nfinding is a counterintuitive threshold phenomenon: the role (sender or\nreceiver) with the larger difference in payoffs between successful and\nunsuccessful interactions is surprisingly more likely to lose in the long run.\nWe show that this effect is robust across different parameter values and arises\nfrom the interplay between the relative speeds of evolution of the two roles\nand the ability of the slower evolving role to exploit the fixed strategy of\nthe faster evolving role. Moreover, for finite populations we find that the\ninitially less frequent strategy of the slower role is more likely to fixate in\nthe population. The initially rarer strategy in the less-rewarded role is,\nparadoxically, more likely to prevail.",
      "generated_abstract": "The rapid increase in the amount of data generated by biological experiments\n(e.g., single-cell RNA sequencing, microarray, proteomics, and other high-\nthroughput methods) has created a need for new approaches to data analysis and\nmachine learning. This paper presents a novel framework for the analysis of\nhigh-throughput datasets, focusing on the analysis of gene expression data,\nwhich is often referred to as a \"genome-scale data analysis\" (GSDA). The\nframework, termed the Genome-scale Transcriptome Analysis (Genome-TA)\nframework, integrates state-of-the-art machine learning methods with\ngenome-scale gene expression data and provides a unified approach for\ncomparing experimental conditions, analyzing the effects of genetic modifications,\nand predicting gene expression profiles. The framework includes the\napplication of standard machine learning techniques, including support vector\nmach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09009009009009009,
          "p": 0.11494252873563218,
          "f": 0.10101009608356314
        },
        "rouge-2": {
          "r": 0.006172839506172839,
          "p": 0.008695652173913044,
          "f": 0.007220211750449642
        },
        "rouge-l": {
          "r": 0.09009009009009009,
          "p": 0.11494252873563218,
          "f": 0.10101009608356314
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/TR/2412.14361v2",
      "true_abstract": "We revisit the long-only trend-following strategy presented in A Century of\nProfitable Industry Trends by Zarattini and Antonacci, which achieved\nexceptional historical performance with an 18.2% annualized return and a Sharpe\nRatio of 1.39. While the results outperformed benchmarks, practical\nimplementation raises concerns about robustness and evolving market conditions.\nThis study explores modifications addressing reliance on T-bills, alternative\nfallback allocations, and industry exclusions. Despite attempts to enhance\nadaptability through momentum signals, parameter optimization, and Walk-Forward\nAnalysis, results reveal persistent challenges. The results highlight\nchallenges in adapting historical strategies to modern markets and offer\ninsights for future trend-following frameworks.",
      "generated_abstract": "We propose a new stochastic volatility model, the Stochastic Volatility Model\n(SVM), that combines the characteristics of the Black-Scholes model and the\nMonte-Carlo method. The SVM model is based on the assumption that the mean and\nvariance of the stock price can be expressed by a single parameter, which is\ncalled the stochastic volatility coefficient. The SVM model can be used to\npredict the future stock price, and its implied volatility can be used to\nderive the stock's future volatility. To test the accuracy of the SVM model,\nwe compare it with the Black-Scholes model, Monte-Carlo method, and the\nGaussian-process based model. The results show that the SVM model has better\npredictive performance than the other models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.208955223880597,
          "f": 0.18300653102481965
        },
        "rouge-2": {
          "r": 0.010101010101010102,
          "p": 0.00980392156862745,
          "f": 0.00995024375733525
        },
        "rouge-l": {
          "r": 0.16279069767441862,
          "p": 0.208955223880597,
          "f": 0.18300653102481965
        }
      }
    },
    {
      "paper_id": "cs.IT.eess/SP/2503.07189v1",
      "true_abstract": "Reconfigurable intelligent surface (RIS)-aided cell-free (CF) massive\nmultiple-input multiple-output (mMIMO) is a promising architecture for further\nimproving spectral efficiency (SE) with low cost and power consumption.\nHowever, conventional RIS has inevitable limitations due to its capability of\nonly reflecting signals. In contrast, beyond-diagonal RIS (BD-RIS), with its\nability to both reflect and transmit signals, has gained great attention. This\ncorrespondence focuses on using BD-RIS to improve the sum SE of CF mMIMO\nsystems. This requires completing the beamforming design under the transmit\npower constraints and unitary constraints of the BD-RIS, by optimizing active\nand passive beamformer simultaneously. To tackle this issue, we introduce an\nalternating optimization algorithm that decomposes it using fractional\nprogramming and solves the subproblems alternatively. Moreover, to address the\nchallenge introduced by the unitary constraint on the beamforming matrix of the\nBD-RIS, a manifold optimization algorithm is proposed to solve the problem\noptimally. Simulation results show that BD-RISs outperform RISs\ncomprehensively, especially in the case of the full connected architecture\nwhich achieves the best performance, enhancing the sum SE by around 40%\ncompared to ideal RISs.",
      "generated_abstract": "This paper proposes a novel approach for multi-target tracking based on\nthe concept of adaptive weighting in the context of neural networks. This\napproach employs the concept of adaptive weighting to improve tracking\nperformance in a multi-target scenario. In this approach, the neural network\nis designed to adaptively weight the contributions of the targets and the\nbackground to improve tracking performance. We propose a new loss function for\nthe adaptive weighting approach that is based on the energy function and\noptimizes the weights of the neural network. This approach has been validated\non a multi-target tracking problem, which involves the detection and\ntracking of targets in a complex environment. The results show that this\napproach significantly improves the performance of the neural network in\nmulti-target tracking. The performance of the proposed approach is compared to\nthe traditional approach based on the energy function, which has been used in\nthe literature to track multi",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15503875968992248,
          "p": 0.29411764705882354,
          "f": 0.20304568075858703
        },
        "rouge-2": {
          "r": 0.04093567251461988,
          "p": 0.056451612903225805,
          "f": 0.04745762224556212
        },
        "rouge-l": {
          "r": 0.12403100775193798,
          "p": 0.23529411764705882,
          "f": 0.1624365437027495
        }
      }
    },
    {
      "paper_id": "q-bio.QM.stat/AP/2502.19206v2",
      "true_abstract": "Understanding the role of different age groups in disease transmission is\ncrucial for designing effective intervention strategies. A key parameter in\nage-structured epidemic models is the contact matrix, which defines the\ninteraction structure between age groups. However, accurately estimating\ncontact matrices is challenging, as different age groups respond differently to\nsurveys and are accessible through different channels. This variability\nintroduces significant epistemic uncertainty in epidemic models.\n  In this study, we introduce the Age Group Sensitivity Analysis (AGSA) method,\na novel framework for assessing the impact of age-structured contact patterns\non epidemic outcomes. Our approach integrates age-stratified epidemic models\nwith Latin Hypercube Sampling (LHS) and the Partial Rank Correlation\nCoefficient (PRCC) method, enabling a systematic sensitivity analysis of\nage-specific interactions. Additionally, we propose a new sensitivity\naggregation technique that quantifies the contribution of each age group to key\nepidemic parameters.\n  By identifying the age groups to which the model is most sensitive, AGSA\nhelps pinpoint those that introduce the greatest epistemic uncertainty. This\nallows for targeted data collection efforts, focusing surveys and empirical\nstudies on the most influential age groups to improve model accuracy. As a\nresult, AGSA can enhance epidemic forecasting and inform the design of more\neffective and efficient public health interventions.",
      "generated_abstract": "The current COVID-19 pandemic has brought about a severe economic crisis\nin many countries. This paper explores the effect of COVID-19 on the global\neconomy using a stochastic general equilibrium model. We apply the Bayesian\nNetwork model to analyze the impact of COVID-19 on global economic activity\nthrough the transmission channels of financial and non-financial sectors. We\nalso estimate the economic impact of COVID-19 using the Bayesian Network model\nand the DSGE model. The results show that the COVID-19 pandemic will have a\nsignificant impact on the global economy. The results suggest that the\neffects of COVID-19 on global economic activity are likely to be long-lasting.\nThe results also show that the effect of COVID-19 on global economic activity\nwill depend on the severity of the pandemic. The DSGE model and the Bayesian\nNetwork model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11940298507462686,
          "p": 0.26666666666666666,
          "f": 0.1649484493357425
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.04040404040404041,
          "f": 0.02758620239976292
        },
        "rouge-l": {
          "r": 0.11940298507462686,
          "p": 0.26666666666666666,
          "f": 0.1649484493357425
        }
      }
    },
    {
      "paper_id": "eess.IV.q-bio/TO/2502.14707v1",
      "true_abstract": "While deep learning methods have shown great promise in improving the\neffectiveness of prostate cancer (PCa) diagnosis by detecting suspicious\nlesions from trans-rectal ultrasound (TRUS), they must overcome multiple\nsimultaneous challenges. There is high heterogeneity in tissue appearance,\nsignificant class imbalance in favor of benign examples, and scarcity in the\nnumber and quality of ground truth annotations available to train models.\nFailure to address even a single one of these problems can result in\nunacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed,\ntuned, and integrated system for reliable PCa detection. Our pipeline\nintegrates self-supervised learning, multiple-instance learning aggregation\nusing transformers, random-undersampled boosting and ensembling: these address\nlabel scarcity, weak labels, class imbalance, and overconfidence, respectively.\nWe train and rigorously evaluate our method using a large, multi-center dataset\nof micro-ultrasound data. Our method outperforms previous state-of-the-art deep\nlearning methods in terms of accuracy and uncertainty calibration, with AUROC\nand balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20%\nof predictions with the highest confidence, we can achieve a balanced accuracy\nof up to 91%. The success of TRUSWorthy demonstrates the potential of\nintegrated deep learning solutions to meet clinical needs in a highly\nchallenging deployment setting, and is a significant step towards creating a\ntrustworthy system for computer-assisted PCa diagnosis.",
      "generated_abstract": "In this paper, we propose a novel deep learning approach to identify\nremaining cells in multi-channel flow cytometry data. The proposed method\nintegrates deep neural networks and random forests to learn the complex\nrelationship between cellular morphology and cellular state, which are\nintensity-based features. The proposed method is trained on a dataset of\nmulti-channel flow cytometry data, which has been previously used for cell\ncounting and cell counting with intensity. The proposed method achieves\nstate-of-the-art accuracy in cell identification, and is capable of\nidentifying 90% of cells, and 70% of cells in the presence of noisy data. The\nresults demonstrate the effectiveness of the proposed method, with potential\napplications in cell counting and cell counting with intensity.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1476510067114094,
          "p": 0.3055555555555556,
          "f": 0.19909501823140405
        },
        "rouge-2": {
          "r": 0.028708133971291867,
          "p": 0.06,
          "f": 0.038834947078476845
        },
        "rouge-l": {
          "r": 0.1476510067114094,
          "p": 0.3055555555555556,
          "f": 0.19909501823140405
        }
      }
    },
    {
      "paper_id": "cond-mat.quant-gas.nlin/PS/2503.10519v1",
      "true_abstract": "Driven systems are of fundamental scientific interest, as they can exhibit\nproperties that are radically different from the same system at equilibrium. In\ncertain cases, long-lived states of driven matter can emerge, which exhibit new\nmaterial properties. In this work, we probe the excitation spectrum of an\nemergent patterned state in a driven superfluid, finding that its response is\nidentical to that of a one-dimensional supersolid. In order to extract physical\nquantities that parametrize the observed sound modes, we apply an effective\nhydrodynamic theory of superfluid smectics, which is agnostic to microscopic\nprocesses. We therefore use the conceptual framework of supersolids to\ncharacterize an otherwise dynamic and far-from-equilibrium state.",
      "generated_abstract": "We investigate the nonlinear evolution of a Bose-Einstein condensate\nunder a laser field. We analyze the initial conditions, the initial profile of\nthe laser field, and the driving strength. We find that the dynamics of the\ncondensate are strongly affected by the initial conditions. A higher driving\nstrength allows us to drive the system toward a more chaotic state, while an\nincrease in the initial profile leads to a more ordered state. The results\nsuggest that the initial profile of the laser field and the driving strength\nhave a significant impact on the final state of the condensate. These\nfindings may have important implications for future experiments.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.1935483870967742,
          "f": 0.16783216292043635
        },
        "rouge-2": {
          "r": 0.009259259259259259,
          "p": 0.011494252873563218,
          "f": 0.010256405314400802
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.1935483870967742,
          "f": 0.16783216292043635
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2503.03485v1",
      "true_abstract": "Understanding the biological mechanism of disease is critical for medicine,\nand in particular drug discovery. AI-powered analysis of genome-scale\nbiological data hold great potential in this regard. The increasing\navailability of single-cell RNA sequencing data has enabled the development of\nlarge foundation models for disease biology. However, existing foundation\nmodels either do not improve or only modestly improve over task-specific models\nin downstream applications. Here, we explored two avenues for improving the\nstate-of-the-art. First, we scaled the pre-training dataset to 116 million\ncells, which is larger than those used by previous models. Second, we leveraged\nthe availability of large-scale biological annotations as a form of supervision\nduring pre-training. We trained the TEDDY family of models comprising six\ntransformer-based state-of-the-art single-cell foundation models with 70\nmillion, 160 million, and 400 million parameters. We vetted our models on two\ndownstream evaluation tasks -- identifying the underlying disease state of\nheld-out donors not seen during training and distinguishing healthy cells from\ndiseased ones for disease conditions and donors not seen during training.\nScaling experiments showed that performance improved predictably with both data\nvolume and parameter count. Our models showed substantial improvement over\nexisting work on the first task and more muted improvements on the second.",
      "generated_abstract": "This work presents a framework for predicting the structure of protein\nprotein interaction networks, called the Protein Interaction Network (PIN)\nprediction framework, which leverages a deep learning approach for protein\ninteraction detection and analysis. The framework consists of three main\ncomponents: (i) the detection of protein interaction sites through a\nhigh-performance neural network, (ii) the prediction of the interaction\nprobabilities between these sites using a deep learning approach, and (iii) the\ncalculation of the interaction network structure through a graph-based\nrepresentation. To evaluate the performance of the framework, we compare its\nresults with state-of-the-art approaches, including a traditional\nsupervised-learning model, a deep learning-based model, and a model that uses\nboth approaches. The results show that the proposed framework achieves the\nhighest accuracy for the prediction of the interaction probabilities and the\ninteraction network structure, out",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.19480519480519481,
          "f": 0.1415094293365078
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08888888888888889,
          "p": 0.15584415584415584,
          "f": 0.113207542544055
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10636v1",
      "true_abstract": "Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT",
      "generated_abstract": "The development of large language models (LLMs) has fundamentally changed how\ncomputational tasks are executed. This paper introduces a novel framework\nthat leverages LLMs to address complex real-world problems. The framework\nintegrates LLMs with machine learning (ML) models to enable data-driven\ndiscovery and problem-solving. By integrating LLMs with ML models, the\nframework enables data-driven discovery of novel solutions to complex problems.\nThe framework can be applied in various domains, such as security, healthcare,\nand manufacturing, where traditional ML-based approaches are limited by\nresource constraints or data availability. The paper presents the framework's\narchitecture, demonstrates its effectiveness in solving complex real-world\nproblems, and discusses its potential for future applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10569105691056911,
          "p": 0.16049382716049382,
          "f": 0.12745097560409474
        },
        "rouge-2": {
          "r": 0.006329113924050633,
          "p": 0.00980392156862745,
          "f": 0.007692302924263309
        },
        "rouge-l": {
          "r": 0.0975609756097561,
          "p": 0.14814814814814814,
          "f": 0.11764705403546732
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/OT/2407.18835v3",
      "true_abstract": "Polychoric correlation is often an important building block in the analysis\nof rating data, particularly for structural equation models. However, the\ncommonly employed maximum likelihood (ML) estimator is highly susceptible to\nmisspecification of the polychoric correlation model, for instance through\nviolations of latent normality assumptions. We propose a novel estimator that\nis designed to be robust to partial misspecification of the polychoric model,\nthat is, the model is only misspecified for an unknown fraction of\nobservations, for instance (but not limited to) careless respondents. In\ncontrast to existing literature, our estimator makes no assumption on the type\nor degree of model misspecification. It furthermore generalizes ML estimation,\nis consistent as well as asymptotically normally distributed, and comes at no\nadditional computational cost. We demonstrate the robustness and practical\nusefulness of our estimator in simulation studies and an empirical application\non a Big Five administration. In the latter, the polychoric correlation\nestimates of our estimator and ML differ substantially, which, after further\ninspection, is likely due to the presence of careless respondents that the\nestimator helps identify.",
      "generated_abstract": "This paper presents a novel method for generating a single summary statistic\nfrom a collection of individual summaries. The method is based on the\nrelationship between the sample covariance matrix and the sample correlation\nmatrix. The covariance matrix is defined as a function of the sample sums,\nwhile the correlation matrix is a function of the sample differences. These\nrelationships are established using the so-called \"asymptotic formula\". We\ndemonstrate that the asymptotic formula for the covariance matrix,\nconsequently, also holds for the sample covariance matrix. The method is\nexplicitly illustrated for the case of the sample correlation matrix, but\ngeneralizations to other types of covariance matrices and correlation matrices\nare also possible. The proposed method is based on the central limit theorem,\nand as a result, it is relatively simple to implement. We demonstrate its\napplicability in a variety of scenarios, including",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13274336283185842,
          "p": 0.19736842105263158,
          "f": 0.15873015392178283
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.043478260869565216,
          "f": 0.03571428087372515
        },
        "rouge-l": {
          "r": 0.13274336283185842,
          "p": 0.19736842105263158,
          "f": 0.15873015392178283
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.00586v1",
      "true_abstract": "Early diagnosis of Alzheimer's disease (AD) is critical for intervention\nbefore irreversible neurodegeneration occurs. Structural MRI (sMRI) is widely\nused for AD diagnosis, but conventional deep learning approaches primarily rely\non intensity-based features, which require large datasets to capture subtle\nstructural changes. Jacobian determinant maps (JSM) provide complementary\ninformation by encoding localized brain deformations, yet existing multimodal\nfusion strategies fail to fully integrate these features with sMRI. We propose\na cross-attention fusion framework to model the intrinsic relationship between\nsMRI intensity and JSM-derived deformations for AD classification. Using the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) dataset, we compare\ncross-attention, pairwise self-attention, and bottleneck attention with four\npre-trained 3D image encoders. Cross-attention fusion achieves superior\nperformance, with mean ROC-AUC scores of 0.903 (+/-0.033) for AD vs.\ncognitively normal (CN) and 0.692 (+/-0.061) for mild cognitive impairment\n(MCI) vs. CN. Despite its strong performance, our model remains highly\nefficient, with only 1.56 million parameters--over 40 times fewer than\nResNet-34 (63M) and Swin UNETR (61.98M). These findings demonstrate the\npotential of cross-attention fusion for improving AD diagnosis while\nmaintaining computational efficiency.",
      "generated_abstract": "This paper proposes a novel hybrid approach for 3D ultrasound image segmentation\nusing a pre-trained ResNet-50 model. The segmentation model is first pre-trained\non a large-scale public dataset to recognize key anatomical features. Then, it\nis fine-tuned on the target 3D ultrasound dataset, where a custom loss is\nintroduced to incorporate the segmentation information. The performance of the\nproposed method is evaluated on a public benchmark dataset and the results\ndemonstrate that the proposed method achieves state-of-the-art performance.\nFurthermore, the proposed method is also validated on a new dataset to\nillustrate its robustness and applicability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.3,
          "f": 0.17142856734693887
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.25,
          "f": 0.14285713877551035
        }
      }
    },
    {
      "paper_id": "math.CO.math/CO/2503.09795v1",
      "true_abstract": "An isolating set of a graph is a set of vertices $S$ such that, if $S$ and\nits neighborhood is removed, only isolated vertices remain; and the isolation\nnumber is the minimum size of such a set. It is known that for every connected\ngraph apart from $K_2$ and $C_5$, the isolation number is at most one-third the\norder and indeed such a graph has three disjoint isolating sets. In this paper\nwe consider isolating sets where $S$ is required to be an independent set and\ncall the minimum size thereof the independent isolation number. While for\ngeneral graphs of order $n$ the independent isolation number can be arbitrarily\nclose to $n/2$, we show that in bipartite graphs the vertex set can be\npartitioned into three disjoint independent isolating sets, whence the\nindependent isolation number is at most $n/3$; while for $3$-colorable graphs\nthe maximum value of the independent isolation number is $(n+1)/3$. We also\nprovide a bound for $k$-colorable graphs.",
      "generated_abstract": "In this paper, we give a new proof of the classical result of J. L. Lp that the\nalgebraic entropy of a probability space is equal to the logarithm of its\nergodic entropy. More precisely, we prove that for every probability space\n$(X,\\mathcal{B},\\mu)$, the following identity holds:\n$\\log\\alpha_p(X,\\mathcal{B},\\mu)=\\alpha_p(X,\\mathcal{B},\\mu)+(1-p)\\log\n\\mu(\\cdot)$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14457831325301204,
          "p": 0.32432432432432434,
          "f": 0.19999999573472232
        },
        "rouge-2": {
          "r": 0.03759398496240601,
          "p": 0.10638297872340426,
          "f": 0.055555551696913855
        },
        "rouge-l": {
          "r": 0.13253012048192772,
          "p": 0.2972972972972973,
          "f": 0.18333332906805566
        }
      }
    },
    {
      "paper_id": "cs.CR.eess/IV/2503.09302v1",
      "true_abstract": "This paper investigates the critical issue of data poisoning attacks on AI\nmodels, a growing concern in the ever-evolving landscape of artificial\nintelligence and cybersecurity. As advanced technology systems become\nincreasingly prevalent across various sectors, the need for robust defence\nmechanisms against adversarial attacks becomes paramount. The study aims to\ndevelop and evaluate novel techniques for detecting and preventing data\npoisoning attacks, focusing on both theoretical frameworks and practical\napplications. Through a comprehensive literature review, experimental\nvalidation using the CIFAR-10 and Insurance Claims datasets, and the\ndevelopment of innovative algorithms, this paper seeks to enhance the\nresilience of AI models against malicious data manipulation. The study explores\nvarious methods, including anomaly detection, robust optimization strategies,\nand ensemble learning, to identify and mitigate the effects of poisoned data\nduring model training. Experimental results indicate that data poisoning\nsignificantly degrades model performance, reducing classification accuracy by\nup to 27% in image recognition tasks (CIFAR-10) and 22% in fraud detection\nmodels (Insurance Claims dataset). The proposed defence mechanisms, including\nstatistical anomaly detection and adversarial training, successfully mitigated\npoisoning effects, improving model robustness and restoring accuracy levels by\nan average of 15-20%. The findings further demonstrate that ensemble learning\ntechniques provide an additional layer of resilience, reducing false positives\nand false negatives caused by adversarial data injections.",
      "generated_abstract": "Recent advancements in large language models (LLMs) have demonstrated their\nability to synthesize high-quality audio content. However, this capability can\nbe exploited for malicious purposes, especially when LLMs are integrated into\nspeech-to-speech (S2S) systems. In this work, we propose the first LLM-based\nattack that synthesizes audio content without the knowledge of the LLM. This\nattack leverages the power of audio-to-audio (A2A) translation, enabling a\nspeaker-independent attack where an LLM-generated prompt is used to synthesize\naudio content, which is then decoded by the target system. The attack\ndemonstrates that the LLM-generated prompt can be used to synthesize audio\ncontent with high fidelity, while bypassing the LLM's ability to generate\nhigh-quality",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10344827586206896,
          "p": 0.189873417721519,
          "f": 0.13392856686264365
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0896551724137931,
          "p": 0.16455696202531644,
          "f": 0.1160714240055008
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/CV/2503.10639v1",
      "true_abstract": "Current image generation and editing methods primarily process textual\nprompts as direct inputs without reasoning about visual composition and\nexplicit operations. We present Generation Chain-of-Thought (GoT), a novel\nparadigm that enables generation and editing through an explicit language\nreasoning process before outputting images. This approach transforms\nconventional text-to-image generation and editing into a reasoning-guided\nframework that analyzes semantic relationships and spatial arrangements. We\ndefine the formulation of GoT and construct large-scale GoT datasets containing\nover 9M samples with detailed reasoning chains capturing semantic-spatial\nrelationships. To leverage the advantages of GoT, we implement a unified\nframework that integrates Qwen2.5-VL for reasoning chain generation with an\nend-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance\nModule. Experiments show our GoT framework achieves excellent performance on\nboth generation and editing tasks, with significant improvements over\nbaselines. Additionally, our approach enables interactive visual generation,\nallowing users to explicitly modify reasoning steps for precise image\nadjustments. GoT pioneers a new direction for reasoning-driven visual\ngeneration and editing, producing images that better align with human intent.\nTo facilitate future research, we make our datasets, code, and pretrained\nmodels publicly available at https://github.com/rongyaofang/GoT.",
      "generated_abstract": "The advent of large language models (LLMs) has led to a surge in the\ndevelopment of large-scale visual question answering (VQA) models. However,\ncurrent VQA models often struggle to effectively integrate visual and textual\ninformation due to their focus on text-only prompts. To address this limitation,\nwe introduce a novel paradigm for VQA, called \\textit{Visual Textual Question\nAnswering (VTQA)}. This paradigm leverages visual question-answering techniques\nto guide the generation of question-answer pairs, enabling the model to\nincorporate visual and textual information in a unified manner. We introduce\n\\textit{VTQA-L} and \\textit{VTQA-H}, the first and second versions of VTQA,\nrespectively. The VTQA-L model achieves state-of-the-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17424242424242425,
          "p": 0.3026315789473684,
          "f": 0.2211538415162723
        },
        "rouge-2": {
          "r": 0.01675977653631285,
          "p": 0.030303030303030304,
          "f": 0.021582729227007855
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2894736842105263,
          "f": 0.21153845690088768
        }
      }
    },
    {
      "paper_id": "cs.CL.q-fin/CP/2502.02199v1",
      "true_abstract": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect.",
      "generated_abstract": "In this paper, we introduce the first model that integrates latent representation\nwith the knowledge of financial time series. By capturing the temporal\ndependencies of financial time series with a latent space, our model is able to\nlearn the key features and patterns in financial time series, enabling more\neffective forecasting. Additionally, we propose a novel method for measuring\nthe uncertainty of financial time series predictions, which is essential for\nefficient and accurate decision-making. Through extensive experiments on\ndifferent financial datasets, we demonstrate the effectiveness of our method.\nThe code will be released in GitHub.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.2878787878787879,
          "f": 0.2360248398827207
        },
        "rouge-2": {
          "r": 0.03076923076923077,
          "p": 0.04597701149425287,
          "f": 0.03686635464333559
        },
        "rouge-l": {
          "r": 0.16842105263157894,
          "p": 0.24242424242424243,
          "f": 0.1987577591373791
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.07343v1",
      "true_abstract": "Seismic fragility curves express the probability of failure of a mechanical\nequipment conditional to an intensity measure derived from a seismic signal.\nAlthough based on a strong assumption, the probit-lognormal model is very\npopular among practitioners for estimating such curves, judging by its abundant\nuse in the literature. However, as this model is likely to lead to biased\nestimates, its use should be limited to cases for which only few data are\navailable. In practice, this involves having to resort to binary data which\nindicate the state of the structure when it has been subjected to a seismic\nloading, namely failure or non-failure. The question then arises of the choice\nof data that must be used to obtain an optimal estimate, that is to say the\nmost precise possible with the minimum of data. To answer this question, we\npropose a methodology for design of experiments in a Bayesian framework based\non the reference prior theory. This theory aims to define a so-called objective\nprior that favors data learning, which is slighty constrained in this work in\norder tackle the problems of likelihood degeneracy that are ubiquitous with\nsmall data sets. The novelty of our work is then twofold. First, we rigorously\npresent the problem of likelihood degeneracy which hampers frequentist\napproaches such as the maximum likelihood estimation. Then, we propose our\nstrategy inherited from the reference prior theory to build the data set. This\nstrategy aims to maximize the impact of the data on the posterior distribution\nof the fragility curve. Our method is applied to a case study of the nuclear\nindustry. The results demonstrate its ability to efficiently and robustly\nestimate the fragility curve, and to avoid degeneracy even with a limited\nnumber of experiments. Additionally, we demonstrate that the estimates quickly\nreach the model bias induced by the probit-lognormal modeling.",
      "generated_abstract": "This paper introduces a novel approach to the problem of estimating the\nparameter of the Gumbel distribution from a finite sample. It is based on a\nnon-parametric bootstrap method, which is capable of estimating the parameter\nof the Gumbel distribution in the case of very large samples, and it is\ndemonstrated that the estimator is consistent and asymptotically normal. The\nestimator is also shown to be robust to outliers and to be asymptotically\ncorrectly specified in the case of the parameter being zero. This approach is\nused to estimate the parameter of the Gumbel distribution in a variety of\napplications, including estimating the mean and variance of a Gumbel random\nvariable, and to perform parameter estimation for other distributions that are\nwell-known to be closely related to the Gumbel distribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15028901734104047,
          "p": 0.3880597014925373,
          "f": 0.21666666264201392
        },
        "rouge-2": {
          "r": 0.05319148936170213,
          "p": 0.14563106796116504,
          "f": 0.0779220740029012
        },
        "rouge-l": {
          "r": 0.13872832369942195,
          "p": 0.3582089552238806,
          "f": 0.1999999959753473
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.12393v1",
      "true_abstract": "This study investigates the emergence of power-law and other concentrated\ndistributions through a feedback loop model in crowd interactions. Agents act\nby their response functions to observations and external forces, while\nobservations change by the aggregated actions of all agents, weighted by their\nrespective influence, i.e. power or wealth. Agents wealth dynamically adjust\nbased on the alignment between an agents actions and observation outcomes:\nagents gain wealth when their actions align with observed trends and lose\nwealth otherwise. A reward function, that describes the change of agents wealth\nat each time step, manifests the differences of response functions of agents to\nobservations. When all agents responses are set to zero and feedback loop is\nbroken, agents wealth follow a normal or lognormal distribution. Otherwise,\nthis response-reward iterative feedback mechanism results in concentrated\nwealth distributions, characterized by a small number of dominant agents and\nthe marginalization of the majority. Contrasted to past studies, such\nconcentration is not limited only to asymptotic behavior at the upper tail for\nlarge variables, nor does it require the reward function to be linear to agents\nprevious wealth as formulated in random growth model and network preferential\nattachment. Probability density functions for various distributions are more\nvisually distinguishable for small values at the lower tail. In application of\nthis model, key differences in income and wealth distributions in the US vs\nJapan are attributed to different response functions of agents in the two\ncountries. The model applicability extends beyond social systems to other\nmany-body systems with analogous feedback mechanisms, where power-law\ndistributions represent a rare subset of general concentrated outcomes.",
      "generated_abstract": "This paper examines the potential of AI and big data for advancing\nindigenous economic development in Indonesia. It examines the potential of AI\nand big data for advancing indigenous economic development in Indonesia. It\nexamines the potential of AI and big data for advancing indigenous economic\ndevelopment in Indonesia. It examines the potential of AI and big data for\nadvancing indigenous economic development in Indonesia. It examines the\npotential of AI and big data for advancing indigenous economic development in\nIndonesia. It examines the potential of AI and big data for advancing indigenous\neconomic development in Indonesia. It examines the potential of AI and big data\nfor advancing indigenous economic development in Indonesia. It examines the\npotential of AI and big data for advancing indigenous economic development in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.037267080745341616,
          "p": 0.3333333333333333,
          "f": 0.06703910433631914
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.037267080745341616,
          "p": 0.3333333333333333,
          "f": 0.06703910433631914
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2503.08674v1",
      "true_abstract": "Machine Learning Force Fields (MLFFs) are a promising alternative to\nexpensive ab initio quantum mechanical molecular simulations. Given the\ndiversity of chemical spaces that are of interest and the cost of generating\nnew data, it is important to understand how MLFFs generalize beyond their\ntraining distributions. In order to characterize and better understand\ndistribution shifts in MLFFs, we conduct diagnostic experiments on chemical\ndatasets, revealing common shifts that pose significant challenges, even for\nlarge foundation models trained on extensive data. Based on these observations,\nwe hypothesize that current supervised training methods inadequately regularize\nMLFFs, resulting in overfitting and learning poor representations of\nout-of-distribution systems. We then propose two new methods as initial steps\nfor mitigating distribution shifts for MLFFs. Our methods focus on test-time\nrefinement strategies that incur minimal computational cost and do not use\nexpensive ab initio reference labels. The first strategy, based on spectral\ngraph theory, modifies the edges of test graphs to align with graph structures\nseen during training. Our second strategy improves representations for\nout-of-distribution systems at test-time by taking gradient steps using an\nauxiliary objective, such as a cheap physical prior. Our test-time refinement\nstrategies significantly reduce errors on out-of-distribution systems,\nsuggesting that MLFFs are capable of and can move towards modeling diverse\nchemical spaces, but are not being effectively trained to do so. Our\nexperiments establish clear benchmarks for evaluating the generalization\ncapabilities of the next generation of MLFFs. Our code is available at\nhttps://tkreiman.github.io/projects/mlff_distribution_shifts/.",
      "generated_abstract": "The development of a novel method for analyzing gene expression data is\nrequired for developing a personalized medicine treatment regimen for\ncancer patients. In this study, we propose a novel method that integrates\ngene expression data with protein-protein interaction data to predict the\ntherapeutic efficacy of cancer drugs. Specifically, we use a neural network\nwith hidden layers to analyze the gene expression data and an evolutionary\nalgorithm to predict the interaction potential between proteins and\ngene-expression signals. We evaluated our method on the CancerGSEA dataset, a\npublicly available dataset that contains gene expression data and protein\ninteraction data from 142 cancer patients. The results show that our method\noutperforms the existing methods in terms of accuracy and precision. Furthermore,\nthe proposed method is applicable to other types of cancer data, such as\nmultiple myeloma and ovarian cancer. These results demonstrate the potential of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14545454545454545,
          "p": 0.27586206896551724,
          "f": 0.1904761859552155
        },
        "rouge-2": {
          "r": 0.00425531914893617,
          "p": 0.007874015748031496,
          "f": 0.005524857323498154
        },
        "rouge-l": {
          "r": 0.12727272727272726,
          "p": 0.2413793103448276,
          "f": 0.16666666214569173
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.18008v4",
      "true_abstract": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
      "generated_abstract": "The rapid advancement of Artificial Intelligence (AI) and Machine Learning\n(ML) technologies has led to the development of advanced speech and audio\nprocessing systems. In this work, we propose a novel framework for multi-channel\nspeech enhancement using a unified transformer architecture. Our approach\nintroduces a new encoder-decoder design that integrates the speech enhancement\nand the speech source separation into a single end-to-end framework. We\ndemonstrate the effectiveness of our proposed framework by evaluating it on a\nvariety of benchmark datasets. Our results show that the proposed approach\noutperforms state-of-the-art methods in terms of both speech quality and\nspeech separation performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1717171717171717,
          "p": 0.2361111111111111,
          "f": 0.19883040448137898
        },
        "rouge-2": {
          "r": 0.032520325203252036,
          "p": 0.041237113402061855,
          "f": 0.036363631433471745
        },
        "rouge-l": {
          "r": 0.16161616161616163,
          "p": 0.2222222222222222,
          "f": 0.18713449804863047
        }
      }
    },
    {
      "paper_id": "physics.geo-ph.stat/AP/2502.19549v1",
      "true_abstract": "This paper presents a comparative analysis of structural seismic responses\nunder two types of ground motion inputs: (i) synthetic motions generated by\nstochastic ground motion models and (ii) recorded motions from an earthquake\ndatabase. Five key seismic response metrics - probability distributions,\nstatistical moments, correlations, tail indices, and variance-based global\nsensitivity indices - are systematically evaluated for two archetypal\nstructures: a 12-story medium-period building and a high-rise long-period\ntower. Both ground motion datasets are calibrated to a shared response\nspectrum, ensuring consistency in spectral characteristics, including spectral\nmedian, variance, and correlation structure. The analysis incorporates both\naleatory uncertainties from ground motion variability and epistemic\nuncertainties associated with structural parameters, providing a comprehensive\ncomparison of seismic responses. The results demonstrate close agreement in\nglobal response characteristics, including distributions, correlations, and\nsensitivity indices, between synthetic and recorded motions, with differences\ntypically within 15\\%. However, significant discrepancies are observed under\nextreme conditions, particularly in tail behavior, higher-order moments, and\ndrift responses of long-period structures, with differences exceeding 50\\%.\nThese discrepancies are attributed to the non-Gaussian features and complex\ncharacteristics inherent in recorded motions, which are less pronounced in\nsynthetic datasets. The findings support the use of synthetic ground motions\nfor evaluating global seismic response characteristics, while highlighting\ntheir limitations in capturing rare-event behavior and long-period structural\ndynamics.",
      "generated_abstract": "The effect of the mean sea level (MSL) on the distribution of soil\nwater (SW) concentration is analyzed. We consider the case of a flat,\nconducive, and dry environment with a constant MSL. The model is\nrepresentative of a typical urban setting. We first determine the\ndistribution of SW concentration in a flat, conducive, dry environment with a\nconstant MSL. Then, we derive the distribution of the SW concentration when\nthe MSL changes. We then derive the distribution of the SW concentration when\nthe MSL is varying. Our results indicate that the SW concentration in a flat,\nconducive, dry environment with a constant MSL is not uniform. This\ndifference in SW concentration is due to the variation in the MSL. In a\nflat, conducive, dry environment with a constant MSL, the distribution of\nSW concentration is uniform. However, in a flat, condu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08270676691729323,
          "p": 0.1864406779661017,
          "f": 0.11458332907606353
        },
        "rouge-2": {
          "r": 0.0049261083743842365,
          "p": 0.011363636363636364,
          "f": 0.006872848014551308
        },
        "rouge-l": {
          "r": 0.08270676691729323,
          "p": 0.1864406779661017,
          "f": 0.11458332907606353
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.02292v1",
      "true_abstract": "Selecting the right monitoring level in Remote Patient Monitoring (RPM)\nsystems for e-healthcare is crucial for balancing patient outcomes, various\nresources, and patient's quality of life. A prior work has used one-dimensional\nhealth representations, but patient health is inherently multidimensional and\ntypically consists of many measurable physiological factors. In this paper, we\nintroduce a multidimensional health state model within the RPM framework and\nuse dynamic programming to study optimal monitoring strategies. Our analysis\nreveals that the optimal control is characterized by switching curves (for\ntwo-dimensional health states) or switching hyper-surfaces (in general):\npatients switch to intensive monitoring when health measurements cross a\nspecific multidimensional surface. We further study how the optimal switching\ncurve varies for different medical conditions and model parameters. This\nfinding of the optimal control structure provides actionable insights for\nclinicians and aids in resource planning. The tunable modeling framework\nenhances the applicability and effectiveness of RPM services across various\nmedical conditions.",
      "generated_abstract": "In this paper, we propose a novel approach for the control of stochastic\ntransient systems with state-dependent delays. The proposed control strategy\nis based on a linear-quadratic Gaussian (LQG) control design with a finite\nstate-delay feedback controller and a fixed state feedback controller. The\nproposed controller is designed to achieve a desired average delay performance\nwhile preserving the stability of the system. We also propose a novel\nbackstepping-based control design for the proposed LQG controller. The\nbackstepping transformation is used to obtain a quadratic cost functional. The\nbackstepping method is then applied to design a backstepping control law for\nthe proposed LQG controller. Numerical results are provided to demonstrate the\neffectiveness of the proposed controller.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16216216216216217,
          "p": 0.26865671641791045,
          "f": 0.20224718631675304
        },
        "rouge-2": {
          "r": 0.03355704697986577,
          "p": 0.050505050505050504,
          "f": 0.04032257584840068
        },
        "rouge-l": {
          "r": 0.13513513513513514,
          "p": 0.22388059701492538,
          "f": 0.16853932114821374
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/TH/2502.10317v1",
      "true_abstract": "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology.",
      "generated_abstract": "We consider the problem of estimating the mean of a random variable by\nestimating its conditional expectation given a random set. This problem arises\nin a variety of settings, including the estimation of the mean of a random\nvariable given a subset of its observations. We consider the case in which the\nrandom set of observations is a finite set of indices and the conditional\nexpectation is defined as the average of the conditional expectation of the\nrandom variable with respect to the joint distribution of the observations. We\nprovide a closed-form solution for the case in which the joint distribution is\nindependent. We also provide a closed-form solution for the case in which the\njoint distribution is the product of two independent distributions. Finally, we\npresent a numerical simulation study that illustrates the effectiveness of the\nclosed-form solution in a number of settings.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1258741258741259,
          "p": 0.3050847457627119,
          "f": 0.17821781764679945
        },
        "rouge-2": {
          "r": 0.0049504950495049506,
          "p": 0.00980392156862745,
          "f": 0.006578942909455931
        },
        "rouge-l": {
          "r": 0.11188811188811189,
          "p": 0.2711864406779661,
          "f": 0.15841583744877966
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2502.20943v1",
      "true_abstract": "Reference-based image super-resolution (RefSR) represents a promising\nadvancement in super-resolution (SR). In contrast to single-image\nsuper-resolution (SISR), RefSR leverages an additional reference image to help\nrecover high-frequency details, yet its vulnerability to backdoor attacks has\nnot been explored. To fill this research gap, we propose a novel attack\nframework called BadRefSR, which embeds backdoors in the RefSR model by adding\ntriggers to the reference images and training with a mixed loss function.\nExtensive experiments across various backdoor attack settings demonstrate the\neffectiveness of BadRefSR. The compromised RefSR network performs normally on\nclean input images, while outputting attacker-specified target images on\ntriggered input images. Our study aims to alert researchers to the potential\nbackdoor risks in RefSR. Codes are available at\nhttps://github.com/xuefusiji/BadRefSR.",
      "generated_abstract": "Conventional RGB-D image-to-image translation models fail to capture the\nrepresentation diversity and scene-level details of complex scenes, hindering\nthe development of real-world applications. To address this challenge, we\npropose a multimodal scene-level representation learning model, named\nMultimodal Scene-Level Representation (MSLR), which integrates multi-modal\nscene representations with RGB-D image inputs. Specifically, we integrate\nmultimodal scene representations, including RGB, depth, semantic segmentation,\nand textual information, into a unified feature space. Through a\nmultimodal-to-multimodal alignment, the MSLR model learns a shared representation\nspace that captures the scene-level details. Extensive experiments on\nimage-to-image translation benchmarks demonstrate that the MSLR model\nsignificantly outperforms the state-of-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17708333333333334,
          "p": 0.22666666666666666,
          "f": 0.1988304044321331
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.031578947368421054,
          "f": 0.02790697181179103
        },
        "rouge-l": {
          "r": 0.17708333333333334,
          "p": 0.22666666666666666,
          "f": 0.1988304044321331
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.07359v1",
      "true_abstract": "In this paper, we present an advanced wind turbine control scheme for power\nmaximization as well as for active power control, which is designed using\n$\\mathcal{H}_\\infty$ loop-shaping. Our approach involves the synthesis of two\nseparate controllers for two different operating modes. To ensure smooth\ntransitions between these modes, we implement a bumpless transfer strategy that\nreduces transient effects. A comprehensive case study demonstrates the efficacy\nof our control scheme, showing significant improvements in power tracking\naccuracy and a reduction in mechanical wear. Moreover, our control strategy\ncomes with robust stability guarantees.",
      "generated_abstract": "This paper proposes a novel adaptive multi-agent control strategy for\ncontingent-value control (CVC) under uncertainties. The proposed strategy\ncombines the adaptive weighted-average control (AWAC) with the multi-agent\nstructured learning (MASL). The AWAC method is used for the state estimation,\nwhile the MASL method is employed for the control. The effectiveness of the\nproposed method is verified by simulation. The simulation results demonstrate\nthat the proposed method can effectively adapt to the uncertainties,\naccurately estimate the state, and obtain robust control.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.21153846153846154,
          "f": 0.1718749951757814
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.013513513513513514,
          "f": 0.01226993369265133
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.21153846153846154,
          "f": 0.1718749951757814
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.q-bio/MN/2409.05827v1",
      "true_abstract": "Many biological decision-making processes can be viewed as performing a\nclassification task over a set of inputs, using various chemical and physical\nprocesses as \"biological hardware.\" In this context, it is important to\nunderstand the inherent limitations on the computational expressivity of\nclassification functions instantiated in biophysical media. Here, we model\nbiochemical networks as Markov jump processes and train them to perform\nclassification tasks, allowing us to investigate their computational\nexpressivity. We reveal several unanticipated limitations on the input-output\nfunctions of these systems, which we further show can be lifted using\nbiochemical mechanisms like promiscuous binding. We analyze the flexibility and\nsharpness of decision boundaries as well as the classification capacity of\nthese networks. Additionally, we identify distinctive signatures of networks\ntrained for classification, including the emergence of correlated subsets of\nspanning trees and a creased \"energy landscape\" with multiple basins. Our\nfindings have implications for understanding and designing physical computing\nsystems in both biological and synthetic chemical settings.",
      "generated_abstract": "We present a study of the properties of the quantum phase transition (QPT) in\nthe system of a single-strand DNA molecule in an external magnetic field. We\npropose a method to measure the anisotropy of the QPT, which is of great\ninterest in biological systems. By using a spin-echo technique, we\ndetermine the anisotropy of the QPT in the molecular dynamics of DNA in\nmagnetic fields ranging from 0 to 1000 G. We also study the anisotropy of the\nmagnetic response of the DNA molecule in the presence of external magnetic\nfields and the magnetic response of the molecule in the absence of an external\nmagnetic field. We compare the results with the theoretical calculations of\nthe anisotropy of the QPT in DNA molecules under external magnetic fields and\nfind a good agreement.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12727272727272726,
          "p": 0.22580645161290322,
          "f": 0.1627906930638184
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09090909090909091,
          "p": 0.16129032258064516,
          "f": 0.11627906515684173
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/GN/2411.10325v1",
      "true_abstract": "Bitcoin, launched in 2008 by Satoshi Nakamoto, established a new digital\neconomy where value can be stored and transferred in a fully decentralized\nmanner - alleviating the need for a central authority. This paper introduces a\nlarge scale dataset in the form of a transactions graph representing\ntransactions between Bitcoin users along with a set of tasks and baselines. The\ngraph includes 252 million nodes and 785 million edges, covering a time span of\nnearly 13 years of and 670 million transactions. Each node and edge is\ntimestamped. As for supervised tasks we provide two labeled sets i. a 33,000\nnodes based on entity type and ii. nearly 100,000 Bitcoin addresses labeled\nwith an entity name and an entity type. This is the largest publicly available\ndata set of bitcoin transactions designed to facilitate advanced research and\nexploration in this domain, overcoming the limitations of existing datasets.\nVarious graph neural network models are trained to predict node labels,\nestablishing a baseline for future research. In addition, several use cases are\npresented to demonstrate the dataset's applicability beyond Bitcoin analysis.\nFinally, all data and source code is made publicly available to enable\nreproducibility of the results.",
      "generated_abstract": "We propose a novel approach for building a deep learning-based risk\ninference model for financial institutions. The key contribution of our\napproach is the integration of a neural network-based methodology with a\nprobability-based approach. We employ a neural network to learn the risk\nrepresentation function, which allows us to map the risk distribution to a\nprobability distribution. This is crucial in the risk-inference process, as it\nallows us to model the uncertainty of our model. We then introduce a\nprobability-based approach to build a model for financial institutions. The\nkey insight is that the probability distribution is not only a measure of the\nrisk distribution, but also a measure of the uncertainty of the model. By\nutilizing this approach, we are able to develop a more effective model for\nbuilding a risk-inference model for financial institutions. To evaluate the\neffectiveness of our model, we utilize it to assess the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11363636363636363,
          "p": 0.20270270270270271,
          "f": 0.1456310633575268
        },
        "rouge-2": {
          "r": 0.03684210526315789,
          "p": 0.05785123966942149,
          "f": 0.045016072416538794
        },
        "rouge-l": {
          "r": 0.11363636363636363,
          "p": 0.20270270270270271,
          "f": 0.1456310633575268
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/PR/2403.15810v1",
      "true_abstract": "National football teams increasingly issue tradeable blockchain-based fan\ntokens to strategically enhance fan engagement. This study investigates the\nimpact of 2022 World Cup matches on the dynamic performance of each team's fan\ntoken. The event study uncovers fan token returns surged six months before the\nWorld Cup, driven by positive anticipation effects. However, intraday analysis\nreveals a reversal of fan token returns consistently declining and trading\nvolumes rising as matches unfold. To explain findings, we uncover asymmetries\nwhereby defeats in high-stake matches caused a plunge in fan token returns,\ncompared to low-stake matches, intensifying in magnitude for knockout matches.\nContrarily, victories enhance trading volumes, reflecting increased market\nactivity without a corresponding positive effect on returns. We align findings\nwith the classic market adage \"buy the rumor, sell the news,\" unveiling\ncognitive biases and nuances in investor sentiment, cautioning the dichotomy of\npre-event optimism and subsequent performance declines.",
      "generated_abstract": "We propose a novel approach for modeling the joint behavior of financial\nassets and risk factors. Our framework introduces a hierarchical structure to\naccount for the temporal relationships between assets and risk factors, and\ntakes into account both the unobserved and observable components of risk\nfactors. Specifically, we define the conditional conditional-expectation\nprocess of a financial asset, which captures the joint dynamics of the asset\nand risk factors. The conditional conditional-expectation process is\nparameterized by an unobserved factor, which captures the unobserved\nrelationship between the asset and risk factors. In contrast, the observable\nfactor captures the relationship between the asset and the market, and we\nfurther extend our framework to accommodate both unobserved and observable\nfactors. Our model is characterized by a system of coupled ordinary differential\nequations, and the parameter estimates are obtained through a maximum likelihood",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09174311926605505,
          "p": 0.136986301369863,
          "f": 0.10989010508573865
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09174311926605505,
          "p": 0.136986301369863,
          "f": 0.10989010508573865
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.00492v1",
      "true_abstract": "We introduce a nonparametric spectral density estimator for fully irregularly\nsampled points in one or more dimensions, constructed using a weighted\nnonuniform Fourier sum whose weights yield a high-accuracy quadrature rule for\na user-specified window function. The resulting estimator significantly reduces\nthe aliasing seen in periodogram approaches and least squares spectral\nanalysis, sidesteps the dangers of ill-conditioning of the nonuniform Fourier\ninverse problem, and can be adapted to a wide variety of irregular sampling\nsettings. After a discussion of methods for computing the necessary weights and\na theoretical analysis of sources of bias, we close with demonstrations of the\nmethod's efficacy, including for processes that exhibit very slow spectral\ndecay and for processes in multiple dimensions.",
      "generated_abstract": "We consider the problem of determining the number of clusters in a data\npoint set $X$ by comparing the empirical distribution of cluster membership\n$Z$ with the theoretical distribution of $Z$ given by a normal distribution.\nIn the case of an unknown number of clusters, we consider the problem of\ndetermining the optimal number of clusters from the empirical distribution of\n$Z$. In the case of a known number of clusters, we study the problem of\ndetermining the optimal number of clusters from the theoretical distribution of\n$Z$. We derive necessary and sufficient conditions for these problems to have\nthe same optimal solution. We establish that the optimal number of clusters in\nthe case of an unknown number of clusters is equal to the maximum of the\nempirical and theoretical number of clusters. We show that the optimal number\nof clusters in the case of a known number of clusters is equal to the\nminimum of the empirical and theoretical number of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15476190476190477,
          "p": 0.24074074074074073,
          "f": 0.1884057923377443
        },
        "rouge-2": {
          "r": 0.008928571428571428,
          "p": 0.011235955056179775,
          "f": 0.009950243821690028
        },
        "rouge-l": {
          "r": 0.13095238095238096,
          "p": 0.2037037037037037,
          "f": 0.1594202850913675
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.15275v1",
      "true_abstract": "Factor-based forecasting using Principal Component Analysis (PCA) is an\neffective machine learning tool for dimension reduction with many applications\nin statistics, economics, and finance. This paper introduces a Supervised\nScreening and Regularized Factor-based (SSRF) framework that systematically\naddresses high-dimensional predictor sets through a structured four-step\nprocedure integrating both static and dynamic forecasting mechanisms. The\nstatic approach selects predictors via marginal correlation screening and\nscales them using univariate predictive slopes, while the dynamic method\nscreens and scales predictors based on time series regression incorporating\nlagged predictors. PCA then extracts latent factors from the scaled predictors,\nfollowed by LASSO regularization to refine predictive accuracy. In the\nsimulation study, we validate the effectiveness of SSRF and identify its\nparameter adjustment strategies in high-dimensional data settings. An empirical\nanalysis of macroeconomic indices in China demonstrates that the SSRF method\ngenerally outperforms several commonly used forecasting techniques in\nout-of-sample predictions.",
      "generated_abstract": "We propose a novel method for estimating the mean of a multivariate normal\ndistribution under the condition that the mean is known. We derive the\ndistributional properties of the estimator, which is consistent and asymptotically\nnormal. We then propose an estimator of the mean that is consistent and\nasymptotically normal. Our estimator is based on a modified maximum likelihood\nestimator. Our estimator is consistent and asymptotically normal, and it\nprovides a simple procedure for estimating the mean of a multivariate normal\ndistribution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1111111111111111,
          "p": 0.325,
          "f": 0.16560509174408705
        },
        "rouge-2": {
          "r": 0.013986013986013986,
          "p": 0.03278688524590164,
          "f": 0.01960783894511816
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.325,
          "f": 0.16560509174408705
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09530v1",
      "true_abstract": "In this paper, we present a comprehensive review of the analysis of the\nwell-known $1 - 1/e$ upper bound on the competitiveness that any online\nalgorithm can achieve, as established in the classical paper by Karp, Vazirani,\nand Vazirani (STOC 1990). We discuss in detail all the minor and major\ntechnical issues in their approach and present a \\emph{simple yet rigorous}\nmethod to address them. Specifically, we show that the upper bound of $n(1 -\n1/e) + o(n)$ on the performance of any online algorithm, as shown in the paper,\ncan be replaced by $\\lceil n \\cdot (1 - 1/e) + 2 - 1/e \\rceil$. Our approach is\nnotable for its simplicity and is significantly less technically involved than\nexisting ones.",
      "generated_abstract": "In this paper, we study the optimal load balancing problem in a\nnetwork of $n$ servers, where the server's load is denoted by $l_i$, $i\\in[n",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.43478260869565216,
          "f": 0.18518518183299046
        },
        "rouge-2": {
          "r": 0.026785714285714284,
          "p": 0.125,
          "f": 0.04411764415224933
        },
        "rouge-l": {
          "r": 0.10588235294117647,
          "p": 0.391304347826087,
          "f": 0.16666666331447194
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.10456v1",
      "true_abstract": "Aims. In the low-collisional, partially ionized plasma (PIP) of solar\nprominences, uncharged emitters might show different signatures of magnetic\nline broadening than charged emitters. We investigate if the widths of weak\nmetall emissions in prominences exceed the thermal line broadening by a\ndifferent amount for charged and for uncharged emitters.\n  Methods. We simultaneously observe five optically thin, weak metall lines in\nthe brightness center of a quiescent prominence and compare their observed\nwidths with the thermal broadening.\n  Results. The inferred non-thermal broadening of the metall lines does not\nindicate systematic differences between the uncharged Mg b2 and Na D1 and the\ncharged Fe II emitters, only Sr II is broader.\n  Conclusions. The additional line broadening of charged emitters is reasonably\nattributed to magnetic forces. That of uncharged emitters can then come from\ntheir temporary state as ion before recombination. Magnetically induced\nvelocities will retain some time after recombination. Modelling partially\nionized plasmas then requires consideration of a memory of previous ionization\nstates.",
      "generated_abstract": "The GAIA mission provides high-resolution parallaxes and precise positions\nfor thousands of stars. The GAIA-EDR3 release (2022b) includes a catalogue of\n24,000 radial velocity (RV) stars, based on their astrometric and parallactic\nobservations. We describe a pipeline for identifying, extracting, and\nanalysing the RV stars, which we have used to study the RV of the Cepheid\nV405 Cygni and the Cepheid V405 Cygni II. The pipeline includes the selection\nof the most reliable parallaxes from the GAIA-EDR3 catalogue, identification\nof the most suitable stellar spectra from the GAIA-DR2 catalogue, and\nextraction of the RVs. The results show that the RV of V405 Cyg",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11214953271028037,
          "p": 0.1875,
          "f": 0.14035087250914827
        },
        "rouge-2": {
          "r": 0.013333333333333334,
          "p": 0.02197802197802198,
          "f": 0.016597505673113022
        },
        "rouge-l": {
          "r": 0.11214953271028037,
          "p": 0.1875,
          "f": 0.14035087250914827
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10511v1",
      "true_abstract": "One of the main challenges in children's speaker verification (C-SV) is the\nsignificant change in children's voices as they grow. In this paper, we propose\ntwo approaches to improve age-related robustness in C-SV. We first introduce a\nFeature Transform Adapter (FTA) module that integrates local patterns into\nhigher-level global representations, reducing overfitting to specific local\nfeatures and improving the inter-year SV performance of the system. We then\nemploy Synthetic Audio Augmentation (SAA) to increase data diversity and size,\nthereby improving robustness against age-related changes. Since the lack of\nlongitudinal speech datasets makes it difficult to measure age-related\nrobustness of C-SV systems, we introduce a longitudinal dataset to assess\ninter-year verification robustness of C-SV systems. By integrating both of our\nproposed methods, the average equal error rate was reduced by 19.4%, 13.0%, and\n6.1% in the one-year, two-year, and three-year gap inter-year evaluation sets,\nrespectively, compared to the baseline.",
      "generated_abstract": "The development of large language models (LLMs) has revolutionized various\nclimate-related applications, such as automated weather forecasting and\nclimate-aware energy management. However, these models often lack domain\nawareness, which hinders their ability to understand and predict regional\nclimate patterns. This paper introduces LLM-Climate, a novel approach that\nintegrates LLMs with climate-aware semantic embeddings to improve regional\nclimate prediction accuracy. The framework integrates LLMs with semantic\nembeddings by using a pre-trained LLM and a semantic embedding model to\nenhance the predictive power of the LLM. The LLM is then used to generate\nclimate-aware semantic embeddings, which are then used as inputs to the\nsemantic embedding model. This approach improves the LLM's ability to\nunderstand and predict regional climate patterns, enabling",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12727272727272726,
          "p": 0.18666666666666668,
          "f": 0.15135134653031423
        },
        "rouge-2": {
          "r": 0.027777777777777776,
          "p": 0.038461538461538464,
          "f": 0.03225805964620261
        },
        "rouge-l": {
          "r": 0.10909090909090909,
          "p": 0.16,
          "f": 0.12972972490869267
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2410.20644v1",
      "true_abstract": "Fungal keratitis is a severe vision-threatening corneal infection with a\nprognosis influenced by fungal virulence and the host's immune defense\nmechanisms. The immune system, through its regulation of the inflammatory\nresponse, ensures cells and tissues can effectively activate defense mechanisms\nin response to infection and injury. However, there is still a lack of\neffective drugs that attenuate fungal virulence while relieving the\ninflammatory response caused by fungal keratitis. Therefore, finding effective\ntreatments to solve these problems is particularly important.\n  We synthesized ZIF-90 by water-based synthesis and characterized by SEM, XRD\netc. In vitro experiments included CCK-8 and ELISA. These evaluations verified\nthe disruptive effects of ZIF-90 on Aspergillus. fumigatus spore adhesion,\nmorphology, cell membrane, and the effect of ZIF-90 on apoptosis. In addition,\nto investigate whether the metal-ligand zinc and the organic ligand imidazole\nact as essential factors in ZIF-90, we investigated the in vitro antimicrobial\nand anti-inflammatory effects of ZIF-8, ZIF-67, and MOF-74 (Zn) by MIC and\nELISA experiments.\n  ZIF-90 has therapeutic effects on fungal keratitis, which could break the\nprotective organelles of Aspergillus. fumigatus, such as the cell wall. In\naddition, ZIF-90 can avoid excessive inflammatory response by promoting\napoptosis of inflammatory cells. The results demonstrated that both zinc ions\nand imidazole possessed antimicrobial and anti-inflammatory effects. In\naddition, ZIF-90 exhibited better biocompatibility compared to ZIF-8, ZIF-67,\nand MOF-74 (Zn).\n  ZIF-90 has anti-inflammatory and antifungal effects and preferable\nbiocompatibility, and has great potential for the treatment of fungal\nkeratitis.",
      "generated_abstract": "The emergence of a new generation of computational biologists has led to\nthe development of various new methods to perform Bayesian analysis of\nexperimental data, such as Bayesian networks and Markov chain Monte Carlo\nmethods. In this paper, we present a unified framework for combining these\nmethods in a Bayesian framework, with the objective of producing more\nprobabilistic models of the data. We show that the Bayesian network approach\ncan be combined with the Markov chain Monte Carlo method for parameter\nestimation, and we illustrate this methodology on the problem of inferring\nthe stochastic kinetics of a chemical reaction network. We then apply our\nmethodology to the problem of inferring the stochastic kinetics of a chemical\nreaction network.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1366906474820144,
          "p": 0.2714285714285714,
          "f": 0.18181817736315575
        },
        "rouge-2": {
          "r": 0.009216589861751152,
          "p": 0.02,
          "f": 0.012618292211089259
        },
        "rouge-l": {
          "r": 0.1366906474820144,
          "p": 0.2714285714285714,
          "f": 0.18181817736315575
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2410.23297v1",
      "true_abstract": "We propose a new way of building portfolios of cryptocurrencies that provide\ngood diversification properties to investors. First, we seek to filter these\ndigital assets by creating some clusters based on their path signature. The\ngoal is to identify similar patterns in the behavior of these highly volatile\nassets. Once such clusters have been built, we propose \"optimal\" portfolios by\ncomparing the performances of such portfolios to a universe of unfiltered\ndigital assets. Our intuition is that clustering based on path signatures will\nmake it easier to capture the main trends and features of a group of\ncryptocurrencies, and allow parsimonious portfolios that reduce excessive\ntransaction fees. Empirically, our assumptions seem to be satisfied.",
      "generated_abstract": "We introduce the notion of ``fairness'' in asset pricing, whereby a model\nis said to be fair if its parameters are determined by the observed data. We\nformulate a novel fairness criterion for the portfolio selection problem\nwithin a model of financial markets. Our criterion is based on the\nindependence of the random returns and is more general than the traditional\nrisk-adjusted criterion. We apply our criterion to the asset pricing problem\nwith the European option model, where we show that the model is fair if and only\nif the European option payoff is independent of the European call option payoff\nand the European put option payoff. We also apply our criterion to the\nportfolio selection problem with the European option model and show that the\nmodel is fair if and only if the European option payoff is independent of the\nEuropean call option payoff and the European put option pay",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.24242424242424243,
          "f": 0.21917807723775581
        },
        "rouge-2": {
          "r": 0.018018018018018018,
          "p": 0.019417475728155338,
          "f": 0.018691583792035567
        },
        "rouge-l": {
          "r": 0.175,
          "p": 0.21212121212121213,
          "f": 0.1917808169637832
        }
      }
    },
    {
      "paper_id": "cs.SC.cs/SC/2502.11787v1",
      "true_abstract": "We propose a version of the classical shape lemma for zero-dimensional ideals\nof a commutative multivariate polynomial ring to the noncommutative setting of\nzero-dimensional ideals in an algebra of differential operators.",
      "generated_abstract": "In the context of cyber-physical systems (CPS), the interaction of CPS with\nthe physical world has been a major challenge. In this paper, we address this\nchallenge by proposing a framework to capture and model the interaction of CPS\nwith the physical world. We propose a novel framework that captures the\ninteraction of CPS with the physical world using an event-based approach. This\napproach enables the capture of the interactions between CPS and the physical\nworld in a real-time manner. We also propose a methodology to model the\ninteraction of CPS with the physical world using event-based data. This method\nallows for the simulation of the interaction between CPS and the physical\nworld using a simulation model. In this paper, we also propose a methodology to\nmodel the interaction of CPS with the physical world using event-based data.\nThis method allows for the simulation of the interaction between C",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.375,
          "p": 0.17307692307692307,
          "f": 0.23684210094182825
        },
        "rouge-2": {
          "r": 0.10344827586206896,
          "p": 0.03529411764705882,
          "f": 0.05263157515389378
        },
        "rouge-l": {
          "r": 0.375,
          "p": 0.17307692307692307,
          "f": 0.23684210094182825
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2502.12211v1",
      "true_abstract": "This study presents a comprehensive techno-economic analysis of gray, blue,\nand green hydrogen production pathways, evaluating their cost structures,\ninvestment feasibility, infrastructure challenges, and policy-driven cost\nreductions. The findings confirm that gray hydrogen (1.50-2.50/kg) remains the\nmost cost-effective today but is increasingly constrained by carbon pricing.\nBlue hydrogen (2.00-3.50/kg) offers a transitional pathway but depends on CCS\ncosts, natural gas price volatility, and regulatory support. Green hydrogen\n(3.50-6.00/kg) is currently the most expensive but benefits from declining\nrenewable electricity costs, electrolyzer efficiency improvements, and\ngovernment incentives such as the Inflation Reduction Act (IRA), which provides\ntax credits of up to 3.00/kg. The analysis shows that renewable electricity\ncosts below 20-30/MWh are essential for green hydrogen to achieve cost parity\nwith fossil-based hydrogen. The DOE's Hydrogen Shot Initiative aims to lower\ngreen hydrogen costs to 1.00/kg by 2031, emphasizing the need for CAPEX\nreductions, economies of scale, and improved electrolyzer efficiency.\nInfrastructure remains a critical challenge, with pipeline retrofitting\nreducing transport costs by 50-70%, though liquefied hydrogen and chemical\ncarriers remain costly due to energy losses and reconversion expenses.\nInvestment trends indicate a shift toward green hydrogen, with over 250 billion\nprojected by 2035, surpassing blue hydrogen's expected 100 billion. Carbon\npricing above $100/ton CO2 will likely make gray hydrogen uncompetitive by\n2030, accelerating the shift to low-carbon hydrogen. Hydrogen's long-term\nviability depends on continued cost reductions, policy incentives, and\ninfrastructure expansion, with green hydrogen positioned as a cornerstone of\nthe net-zero energy transition by 2035.",
      "generated_abstract": "This paper examines the impact of the COVID-19 pandemic on the demand for\nequity in the US stock market. We use a novel data set to investigate the\nrelationship between the stock market's daily returns and the number of\ncovid-19 deaths in the US. Our results suggest that the increase in death\ncounts was accompanied by a significant decline in the stock market's daily\nreturns. The findings suggest that stock market returns were negatively\ncorrelated with the number of deaths. We find that the decline in the stock\nmarket's daily returns was driven by a significant decline in the number of\ndeaths. We also find that the decline in the stock market's daily returns\ncorrelates with the decline in the number of deaths. We further find that the\ndecrease in the stock market's daily returns was driven by a significant\ndecline in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07428571428571429,
          "p": 0.22807017543859648,
          "f": 0.11206896181071654
        },
        "rouge-2": {
          "r": 0.00819672131147541,
          "p": 0.024096385542168676,
          "f": 0.012232412114207794
        },
        "rouge-l": {
          "r": 0.07428571428571429,
          "p": 0.22807017543859648,
          "f": 0.11206896181071654
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.08712v1",
      "true_abstract": "This study introduces the SHAP-integrated convolutional diagnostic network\n(SICDN), an interpretable feature selection method designed for limited\ndatasets, to address the challenge posed by data privacy regulations that\nrestrict access to medical datasets. The SICDN model was tested on\nclassification tasks using pneumonia and breast cancer datasets, demonstrating\nover 97% accuracy and surpassing four popular CNN models. We also integrated a\nhistorical weighted moving average technique to enhance feature selection. The\nSICDN shows potential in medical image prediction, with the code available on\nhttps://github.com/AIPMLab/SICDN.",
      "generated_abstract": "This paper introduces a novel method to detect and localize tumors in\nimages. We propose a novel multi-modal deep learning framework to identify\ntumors in lung cancer images. The proposed method uses a multi-modal deep\nlearning approach to enhance the detection and localization of tumors. The\nproposed framework utilizes a convolutional neural network (CNN) with a\nresidual block and a UNet-like architecture. The network consists of two\nresidual blocks. The first residual block is a 3D convolutional block. The\nsecond residual block consists of 3D convolutional and deconvolutional layers.\nThe second residual block is followed by a 3D UNet-like block. The\nresidual block and UNet-like block are followed by a fully connected\nlayer. The proposed method is evaluated on the LungCNN dataset, which\ncontains 300",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2328767123287671,
          "p": 0.2698412698412698,
          "f": 0.24999999502703293
        },
        "rouge-2": {
          "r": 0.012195121951219513,
          "p": 0.00980392156862745,
          "f": 0.010869560276467275
        },
        "rouge-l": {
          "r": 0.2328767123287671,
          "p": 0.2698412698412698,
          "f": 0.24999999502703293
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09508v2",
      "true_abstract": "The online randomized primal-dual method has widespread applications in\nonline algorithm design and analysis. A key challenge is identifying an\nappropriate function space, $F$, in which we search for an optimal updating\nfunction $f \\in F$ that yields the best possible lower bound on the\ncompetitiveness of a given algorithm. The choice of $F$ must balance two\ncompeting objectives: on one hand, it should impose sufficient simplifying\nconditions on $f$ to facilitate worst-case analysis and establish a valid lower\nbound; on the other hand, it should remain general enough to offer a broad\nselection of candidate functions. The tradeoff is that any additional\nconstraints on $f$ that can facilitate competitive analysis may also lead to a\nsuboptimal choice, weakening the resulting lower bound.\n  To address this challenge, we propose an auxiliary-LP-based framework capable\nof effectively approximating the best possible competitiveness achievable when\napplying the randomized primal-dual method to different function spaces.\nSpecifically, we examine the framework introduced by Huang and Zhang (STOC\n2020), which analyzes Stochastic Balance for vertex-weighted online matching\nwith stochastic rewards. Our approach yields both lower and upper bounds on the\nbest possible competitiveness attainable using the randomized primal-dual\nmethod for different choices of ${F}$. Notably, we establish that Stochastic\nBalance achieves a competitiveness of at least $0.5796$ for the problem (under\nequal vanishing probabilities), improving upon the previous bound of $0.576$ by\nHuang and Zhang (STOC 2020). Meanwhile, our analysis yields an upper bound of\n$0.5810$ for a function space strictly larger than that considered in Huang and\nZhang (STOC 2020).",
      "generated_abstract": "We investigate a class of dynamic programming problems, where the goal is to\ncompute the maximum value of a function that depends on the values of previous\ninstances. The problem is well-studied in the literature, with a number of\napproximation algorithms. In this paper, we present a polynomial-time\nalgorithm that computes the maximum value of a function for which the values of\nthe previous instances are known. The algorithm is based on an iterative\nmechanism that iteratively finds a solution for a simpler problem, and then\napplies it to the original problem. The key idea is to use the solution of the\nsimpler problem as a seed to generate a solution to the original problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.34375,
          "f": 0.20370369953360776
        },
        "rouge-2": {
          "r": 0.021834061135371178,
          "p": 0.05,
          "f": 0.030395132546817416
        },
        "rouge-l": {
          "r": 0.13815789473684212,
          "p": 0.328125,
          "f": 0.1944444402743485
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.00233v1",
      "true_abstract": "This study employs a Bayesian Probit model to empirically analyze peer\neffects and herd behavior among consumers during the \"Double 11\" shopping\nfestival, using data collected through a questionnaire survey. The results\ndemonstrate that peer effects significantly influence consumer decision-making,\nwith the probability of participation in the shopping event increasing notably\nwhen roommates are involved. Additionally, factors such as gender, online\nshopping experience, and fashion consciousness significantly impact consumers'\nherd behavior. This research not only enhances the understanding of online\nshopping behavior among college students but also provides empirical evidence\nfor e-commerce platforms to formulate targeted marketing strategies. Finally,\nthe study discusses the fragility of online consumption activities, the need\nfor adjustments in corporate marketing strategies, and the importance of\npromoting a healthy online culture.",
      "generated_abstract": "This paper examines the joint estimation and inference of structural\nmodel parameters in a multi-state dynamic stochastic general equilibrium (DSGE)\nframework. We consider a DSGE model with a single state variable, a dynamic\nintervention variable, and a set of state-dependent parameters. The state\nvariable is endogenous, and the state-dependent parameters are instrumented\nusing the dynamic intervention variable. We develop a two-stage estimation\nprocedure, which includes an initial stage of estimation of the state-dependent\nparameters and a second stage of estimation of the state variable. We use the\nsecond-stage estimation to calibrate the state-dependent parameters. We show\nthat the joint estimation and inference of the state-dependent parameters can\nbe performed without the need to make restrictions on the parameter space. We\napply our method to the CPI-Bureau of Labor Statistics (BLS) and the CPI-E\nFederal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15217391304347827,
          "p": 0.19444444444444445,
          "f": 0.17073170239143384
        },
        "rouge-2": {
          "r": 0.01680672268907563,
          "p": 0.018018018018018018,
          "f": 0.01739129935387667
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.19444444444444445,
          "f": 0.17073170239143384
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/quant-gas/2503.09275v1",
      "true_abstract": "The present paper is devoted to comprehensive theoretical studies of\nexction-polariton quantum fluids specificities in the optics of their\nutilization for quantum turbulence research. We show that a non-trivial\nimplementation of time-varying potential for excitation of quantum fluid\n(injection of quantized vortices) via the stirring procedure can be efficiently\nsubstituted with resonant excitation-based phase-imprinting techniques. The\nmost efficient phase pattern corresponds to imprinting of tiles with randomly\noriented plane waves in each. The resulting turbulent flows, spatial vortex\ndistributions, and clustering statistics resemble those for the case of a\nconventional spoon-stirring scheme. We quantify the limitations on the lifetime\nand density depletion for the development and sustainability of quantum\nturbulence. The yield is the necessity to prevent the density depletion for\nmore than one order of magnitude. Finally, we demonstrate that turbulence is\nrobust with respect to alternating gain and loss at a certain range of\nmodulation parameters, which corresponds to laser operating above and below\ncondensation threshold.",
      "generated_abstract": "We study the magnetism of the two-dimensional topological insulator\ntopo-2, which exhibits a non-trivial band topology with respect to the\ndirection of magnetic field. We demonstrate that the magnetism of topo-2\noriginates from the non-trivial phase of the Berry curvature. Our findings\ndemonstrate that the Berry curvature of the band structure can provide a\nmechanism for the emergence of magnetic moments in topological insulators.\nFurthermore, we find that the Berry curvature exhibits rich topological\nfeatures, including the formation of chiral magnetic excitations and the\nemergence of magnetic monopoles. These findings shed new light on the\nemergence of magnetic moments in topological insulators and the\nnon-trivial topology of the band structure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16363636363636364,
          "p": 0.32142857142857145,
          "f": 0.21686746540862253
        },
        "rouge-2": {
          "r": 0.039735099337748346,
          "p": 0.07317073170731707,
          "f": 0.05150214136123381
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.2857142857142857,
          "f": 0.19277107986645384
        }
      }
    },
    {
      "paper_id": "hep-th.nlin/SI/2503.05890v1",
      "true_abstract": "This review paper explores the Riccati-type pseudo-potential formulation\napplied to the quasi-integrable sine-Gordon, KdV, and NLS models. The proposed\nframework provides a unified methodology for analyzing quasi-integrability\nproperties across various integrable systems, including deformations of the\nsine-Gordon, Bullough-Dodd, Toda, KdV, pKdV, NLS and SUSY sine-Gordon models.\nKey findings include the emergence of infinite towers of anomalous conservation\nlaws within the Riccati-type approach and the identification of exact non-local\nconservation laws in the linear formulations of deformed models. As modified\nintegrable models play a crucial role in diverse fields of non-linear\nphysics-such as Bose-Einstein condensation, superconductivity, gravity models,\noptics, and soliton turbulence-these results may have far-reaching\napplications.",
      "generated_abstract": "We study the behavior of the critical point in a system with two distinct\nmasses, $m_1$ and $m_2$, in the presence of a strong attractive interaction\n$V(x)=-a|x|^{d-2}$ between the masses. We show that the critical point is\nstable against a general perturbation of the mass ratios. This result, which\nholds for any values of the interaction strength, is surprising because the\ncritical point is only stable against a general perturbation of the mass\nratios if the masses are very close to the critical point. We further\ninvestigate the criticality of the system in the limit $a\\rightarrow 0$. We\nshow that the critical point is stable against a general perturbation of the\nmass ratios when $m_1\\rightarrow m_2$ and $a\\rightarrow 0$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1,
          "p": 0.13793103448275862,
          "f": 0.11594202411258159
        },
        "rouge-2": {
          "r": 0.02912621359223301,
          "p": 0.036585365853658534,
          "f": 0.032432427496859775
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.13793103448275862,
          "f": 0.11594202411258159
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-th/2503.09686v1",
      "true_abstract": "We explore the confining pressure inside the nucleon and the related\ngravitational form factor referred to as the D-term, using the skyrmion\napproach based on the scale-invariant chiral perturbation theory, where the\nskyrmion is described as the nucleon and a scalar meson couples to the scale\nanomaly through the low energy theorem. Within this model framework, the\ncurrent quark mass and gluonic quantum contributions to the scale anomaly can\nbe described by the pion and scalar meson masses, respectively, through\nmatching with the underlying QCD. By considering the decomposition of the\nenergy momentum tensor of nucleon, we examine the role of the scale anomaly\ncontributions in the pressure inside the nucleon. As a result, the gluonic\nscale anomaly is found to dominate the confining pressure. Compared to the\nresult based on the conventional chiral perturbation theory in the chiral\nlimit, our result for the total pressure is capable of qualitatively improving\nthe alignment with lattice QCD observations. Moreover, the pressure from the\ngluonic scale anomaly is widely distributed in position space, leading to its\nsubstantial contribution to the D-term.",
      "generated_abstract": "The quark-gluon plasma (QGP) is a highly excited and strongly interacting\nmaterial that emerges in heavy-ion collisions. The QGP can be probed through\ndirect detection experiments such as the Large Hadron Collider (LHC) and the\nLarge Hadron Collider (LHCb). In this study, we investigate the production of\nthe heavy bottom quarks ($B_h$) in the QGP by using the Monte Carlo event\ngeneration program Pythia8. The bottom quarks are produced via gluon fusion and\nthen decay into bottom-antibottom-antibottom ($B_h\\bar{B}_h\\bar{B}_h$)\ndaughter-antidaughter pairs. We study the kinematics of the $B_h\\bar{B}_h\\bar{B}_h$\nsystem in the QGP and explore the impact of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16326530612244897,
          "p": 0.22535211267605634,
          "f": 0.18934910755365722
        },
        "rouge-2": {
          "r": 0.04666666666666667,
          "p": 0.07954545454545454,
          "f": 0.05882352475107727
        },
        "rouge-l": {
          "r": 0.1326530612244898,
          "p": 0.18309859154929578,
          "f": 0.1538461489737756
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/CO/2501.12837v1",
      "true_abstract": "BRBVS is a publicly available \\texttt{R} package on CRAN that implements the\nalgorithm proposed in Petti et al.(2024a). The algorithm was developed as the\nfirst proposal of variable selection for the class of Bivariate Survival Copula\nModels originally proposed in Marra & Radice (2020) and implemented in the\n\\texttt{GJRM} package. The core of the \\texttt{BRBVS} package is to implement\nand make available to practitioners variable selection algorithms for bivariate\nsurvival data affected by censoring, providing easy-to-use functions and\ngraphical outputs. The idea behind the algorithm is almost general and may also\nbe extended to different class of models.",
      "generated_abstract": "This paper introduces a novel approach to designing and conducting randomized\ncomparisons of hypothesis tests. The proposed approach is based on the\ngeneralization of the classical MMD test to the non-parametric setting. The\nproposed test is designed to address a number of challenges, including the\nnon-stationarity of the target distribution, the presence of outliers, and the\nnon-Gaussianity of the target distribution. To address these challenges, the\nproposed test is based on the empirical distribution function (EDF) and the\nempirical quantiles (EQLs). The EQLs are computed using the bootstrap. The\nproposed test is shown to achieve an asymptotic power of 0.95 under certain\nassumptions. The performance of the proposed test is compared with that of the\nCohen's d test and the Benjamini-Hochberg (BH)",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.14492753623188406,
          "f": 0.1438848870865899
        },
        "rouge-2": {
          "r": 0.010638297872340425,
          "p": 0.00980392156862745,
          "f": 0.010204076640985363
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.14492753623188406,
          "f": 0.1438848870865899
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.05025v1",
      "true_abstract": "We develop ProtComposer to generate protein structures conditioned on spatial\nprotein layouts that are specified via a set of 3D ellipsoids capturing\nsubstructure shapes and semantics. At inference time, we condition on\nellipsoids that are hand-constructed, extracted from existing proteins, or from\na statistical model, with each option unlocking new capabilities.\nHand-specifying ellipsoids enables users to control the location, size,\norientation, secondary structure, and approximate shape of protein\nsubstructures. Conditioning on ellipsoids of existing proteins enables\nredesigning their substructure's connectivity or editing substructure\nproperties. By conditioning on novel and diverse ellipsoid layouts from a\nsimple statistical model, we improve protein generation with expanded Pareto\nfrontiers between designability, novelty, and diversity. Further, this enables\nsampling designable proteins with a helix-fraction that matches PDB proteins,\nunlike existing generative models that commonly oversample conceptually simple\nhelix bundles. Code is available at https://github.com/NVlabs/protcomposer.",
      "generated_abstract": "We study the effect of non-equilibrium initial conditions on the time\ndistribution of the system's mean-field attractor. In particular, we investigate\nthe effect of the initial distribution of the mean-field attractor on the\ntime-scale of the system's transitions between different attractors. In a\nnon-equilibrium steady-state, the system is in equilibrium, and the mean-field\nattractor is the unique fixed point of the non-equilibrium dynamics. We study\nthe effect of the initial distribution of the mean-field attractor on the\ntime-scale of the system's transitions between different attractors. We\ncalculate the time-scales of the system's transitions between different\nattractors. We use this time-scale to compare the system's dynamics in\nequilibrium and in a non-equilibrium steady-state. We then use this time-scale",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10891089108910891,
          "p": 0.2558139534883721,
          "f": 0.1527777735889276
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10891089108910891,
          "p": 0.2558139534883721,
          "f": 0.1527777735889276
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/EM/2501.06270v1",
      "true_abstract": "The long-term estimation of the Marxist average rate of profit does not\nadhere to a theoretically grounded standard regarding which economic activities\nshould or should not be included for such purposes, which is relevant because\nmethodological non-uniformity can be a significant source of overestimation or\nunderestimation, generating a less accurate reflection of the capital\naccumulation dynamics. This research aims to provide a standard Marxist\ndecision criterion regarding the inclusion and exclusion of economic activities\nfor the calculation of the Marxist average profit rate for the case of United\nStates economic sectors from 1960 to 2020, based on the Marxist definition of\nproductive labor, its location in the circuit of capital, and its relationship\nwith the production of surplus value. Using wavelet-transformed Daubechies\nfilters with increased symmetry, empirical mode decomposition, Hodrick-Prescott\nfilter embedded in unobserved components model, and a wide variety of unit root\ntests the internal theoretical consistency of the presented criteria is\nevaluated. Also, the objective consistency of the theory is evaluated by a\ndynamic factor auto-regressive model, Principal Component Analysis, Singular\nValue Decomposition and Backward Elimination with Linear and Generalized Linear\nModels. The results are consistent both theoretically and econometrically with\nthe logic of Marx's political economy.",
      "generated_abstract": "This paper proposes a novel approach to calibrate econometric models by\nusing multi-objective optimization methods. The proposed approach is based on\nthe concept of multi-objective calibration, which is a way of combining\nobjectives in a single optimization problem. Multi-objective optimization\ntechniques are well-known for solving problems with nonlinear constraints,\nparticularly in the field of optimization. Multi-objective calibration uses the\nsame techniques to solve problems with linear constraints. The multi-objective\noptimization framework is used to solve two different problems: the first one\nconsiders the calibration of a single econometric model, while the second one\nconsiders the calibration of a panel data model with cross-sectional\nrelationships. The first problem is solved using a gradient-based algorithm,\nwhile the second problem is solved using an evolutionary algorithm. The\nproposed approach is used to calibrate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.22535211267605634,
          "f": 0.1592039755303088
        },
        "rouge-2": {
          "r": 0.021505376344086023,
          "p": 0.03636363636363636,
          "f": 0.027027022356647994
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.2112676056338028,
          "f": 0.1492537267740899
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2501.03658v2",
      "true_abstract": "We characterise the solutions to a continuous-time optimal liquidity\nprovision problem in a market populated by informed and uninformed traders. In\nour model, the asset price exhibits fads -- these are short-term deviations\nfrom the fundamental value of the asset. Conditional on the value of the fad,\nwe model how informed traders and uninformed traders arrive in the market. The\nmarket maker knows of the two groups of traders but only observes the anonymous\norder arrivals. We study both, the complete information and the partial\ninformation versions of the control problem faced by the market maker. In such\nframeworks, we characterise the value of information, and we find the price of\nliquidity as a function of the proportion of informed traders in the market.\nLastly, for the partial information setup, we explore how to go beyond the\nKalman-Bucy filter to extract information about the fad from the market\narrivals.",
      "generated_abstract": "This study investigates the correlation between financial markets and\nfinancial markets' sentiment. The study explores the relationship between\nfinancial markets and sentiment by employing a sentiment index of financial\nmarkets (SFSM). The study examines the relationship between financial markets\nand sentiment by employing the SFSM. The study employs a multiple regression\nmodel to assess the relationship between financial markets and sentiment. The\nresults reveal that the correlation between financial markets and sentiment is\nhigh, with a correlation coefficient of 0.97. The results also show that the\nSFSM is a reliable indicator of financial market sentiment, with a\nsignificance level of 5%. The results also suggest that the SFSM is a useful\ntool for monitoring the financial market's sentiment, providing valuable\ninsights into the financial market's future direction.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.1896551724137931,
          "f": 0.1605839367233205
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12658227848101267,
          "p": 0.1724137931034483,
          "f": 0.14598539657733514
        }
      }
    },
    {
      "paper_id": "math.MG.math/MG/2503.05435v1",
      "true_abstract": "We show that the centers of the excircles of a bicentric polygon $B$ are\nconcyclic on a circle $E$. The center of the circumscribed circle $K$ of $B$ is\nthe midpoint of the center of $E$ and the center of the inscribed circle $C$ of\n$B$. The radius of $E$ is given by a simple formula in terms of the radii of\n$C$ and $K$ and the distance between their centers.",
      "generated_abstract": "We introduce a noncommutative version of the Kashiwara-MacPherson theory,\nand prove that the resulting functor is a left adjoint to the functor from the\ncategory of modules over the path algebra of the root system of a finite root\nsystem to the category of modules over the path algebra of the root system.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19444444444444445,
          "p": 0.25925925925925924,
          "f": 0.22222221732426314
        },
        "rouge-2": {
          "r": 0.05,
          "p": 0.07894736842105263,
          "f": 0.06122448504789708
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2222222222222222,
          "f": 0.19047618557823143
        }
      }
    },
    {
      "paper_id": "q-bio.NC.q-bio/NC/2503.01226v1",
      "true_abstract": "Dementia, a progressive neurodegenerative disorder, affects memory,\nreasoning, and daily functioning, creating challenges for individuals and\nhealthcare systems. Early detection is crucial for timely interventions that\nmay slow disease progression. Large pre-trained models (LPMs) for text and\naudio, such as Generative Pre-trained Transformer (GPT), Bidirectional Encoder\nRepresentations from Transformers (BERT), and Contrastive Language-Audio\nPretraining (CLAP), have shown promise in identifying cognitive impairments.\nHowever, existing studies generally rely heavily on expert-annotated datasets\nand unimodal approaches, limiting robustness and scalability. This study\nproposes a context-based multimodal method, integrating both text and audio\ndata using the best-performing LPMs in each modality. By incorporating\ncontextual embeddings, our method improves dementia detection performance.\nAdditionally, motivated by the effectiveness of contextual embeddings, we\nfurther experimented with a context-based In-Context Learning (ICL) as a\ncomplementary technique. Results show that GPT-based embeddings, particularly\nwhen fused with CLAP audio features, achieve an F1-score of $83.33\\%$,\nsurpassing state-of-the-art dementia detection models. Furthermore, raw text\ndata outperforms expert-annotated datasets, demonstrating that LPMs can extract\nmeaningful linguistic and acoustic patterns without extensive manual labeling.\nThese findings highlight the potential for scalable, non-invasive diagnostic\ntools that reduce reliance on costly annotations while maintaining high\naccuracy. By integrating multimodal learning with contextual embeddings, this\nwork lays the foundation for future advancements in personalized dementia\ndetection and cognitive health research.",
      "generated_abstract": "A large-scale transcriptome-level analysis of the impact of the COVID-19\ncovid-19 pandemic on the global fishery has revealed an increase in the\nprevalence of infectious diseases in fish populations. The objective of this\nstudy was to investigate the impact of COVID-19 on the infectious diseases\nprevalence of the most important fish species in the Mediterranean Sea,\ncomparing data from 2019 and 2020. We applied a novel machine learning method\nto analyze the gene expression of 1,120 fish samples from 11 species, including\nsix species with commercial importance. We developed a novel model for the\nprediction of infectious diseases in fish, using the R package 'R-covid19'. We\nfound that the pandemic had a significant impact on the infectious diseases\nprevalence",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10240963855421686,
          "p": 0.2328767123287671,
          "f": 0.14225940998301864
        },
        "rouge-2": {
          "r": 0.004784688995215311,
          "p": 0.00980392156862745,
          "f": 0.006430863759062585
        },
        "rouge-l": {
          "r": 0.0963855421686747,
          "p": 0.2191780821917808,
          "f": 0.13389120914619856
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/MS/2502.03402v2",
      "true_abstract": "This paper introduces a new mathematical framework for analysis and\noptimization of tensor expressions within an enclosing loop. Tensors are\nmulti-dimensional arrays of values. They are common in high performance\ncomputing (HPC) and machine learning domains. Our framework extends Scalar\nEvolution - an important optimization pass implemented in both LLVM and GCC -\nto tensors. Scalar Evolution (SCEV) relies on the theory of `Chain of\nRecurrences' for its mathematical underpinnings. We use the same theory for\nTensor Evolution (TeV). While some concepts from SCEV map easily to TeV -- e.g.\nelement-wise operations; tensors introduce new operations such as\nconcatenation, slicing, broadcast, reduction, and reshape which have no\nequivalent in scalars and SCEV. Not all computations are amenable to TeV\nanalysis but it can play a part in the optimization and analysis parts of ML\nand HPC compilers. Also, for many mathematical/compiler ideas, applications may\ngo beyond what was initially envisioned, once others build on it and take it\nfurther. We hope for a similar trajectory for the tensor-evolution concept.",
      "generated_abstract": "We present a novel approach to solving the 3-SAT problem, called the\nnon-deterministic 3-SAT solver (3SATS). It is a deterministic finite\nautomaton (DFA) that supports a wide range of 3-SAT problems, including\nstandard 3-SAT, MiniSAT, and TinySAT. It is built on a novel non-deterministic\nDFA with two inputs, a set of 3-SAT formulas and a set of literals. This\nstructure enables the DFA to make a trade-off between the number of 3-SAT\nformulas and the number of literals. It can also be used to solve 3-SAT problems\nthat are difficult to solve by other methods. For example, we demonstrate that\n3SATS can solve the 3-SAT problem with 10000",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08196721311475409,
          "p": 0.15151515151515152,
          "f": 0.10638297416704409
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08196721311475409,
          "p": 0.15151515151515152,
          "f": 0.10638297416704409
        }
      }
    },
    {
      "paper_id": "stat.ME.q-bio/MN/2503.05448v1",
      "true_abstract": "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
      "generated_abstract": "We propose a novel method for detecting multiple outliers in multi-modal\ndata, leveraging the properties of the multivariate normal distribution. This\napproach is particularly valuable for analyzing complex data where the underlying\ndistribution is unknown, or it is not easy to identify the correct model. We\nprovide a theoretical framework to analyze the performance of our method. We\ndemonstrate its effectiveness on synthetic data and illustrate its applicability\nto real-world applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.46296296296296297,
          "f": 0.24509803532295274
        },
        "rouge-2": {
          "r": 0.021621621621621623,
          "p": 0.05714285714285714,
          "f": 0.031372545036524924
        },
        "rouge-l": {
          "r": 0.14666666666666667,
          "p": 0.4074074074074074,
          "f": 0.2156862706170704
        }
      }
    },
    {
      "paper_id": "math.OA.math/OA/2503.07398v1",
      "true_abstract": "We demonstrate that any full and faithful $*$-functor between approximable\ncategories of locally finite coarse spaces induces a coarse embedding between\nthe underlying spaces. Furthermore, we establish a general characterisation of\nsuch $*$-functors between approximable categories and prove that the functor\nassociating each locally finite coarse space with its approximable category is\nfull and faithful.",
      "generated_abstract": "In this paper, we investigate the existence and uniqueness of the limit\ncharacteristic for the solution of the linear Stokes system with the Dirichlet\nand Neumann boundary conditions. It is proved that the limit characteristic is\na smooth solution of the linear Stokes system without the Dirichlet or the\nNeumann conditions. For the Neumann condition, the limit characteristic is\ndefined by the trace of the solution of the linear Stokes system with the Neumann\nconditions, which is also a smooth solution of the linear Stokes system without\nthe Dirichlet conditions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21621621621621623,
          "p": 0.21052631578947367,
          "f": 0.21333332833422236
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.017857142857142856,
          "f": 0.01923076426035631
        },
        "rouge-l": {
          "r": 0.21621621621621623,
          "p": 0.21052631578947367,
          "f": 0.21333332833422236
        }
      }
    },
    {
      "paper_id": "cs.CY.cs/CY/2503.09276v1",
      "true_abstract": "Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes.",
      "generated_abstract": "This paper introduces a novel framework for the analysis of social media\ncommunications in a cyber threat environment. Our framework allows for the\ncomprehensive analysis of interactions between different users and the\ncommunications between them, providing a deeper understanding of the social\nnetworks that they operate within. By incorporating a large-scale social media\ndataset, we analyze how these interactions vary across different users and\ncommunication types, exploring the dynamic nature of the social networks that\nusers build. We also examine the impact of social media platforms on the\nsocial networks, highlighting the importance of tailoring communication\nstrategies to the platforms in which users operate. By providing a\ncomprehensive understanding of the social network structures and communication\npatterns, our framework provides valuable insights for cyber threat\nintelligence analysts, enabling them to better understand and respond to\nthreats within social media networks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12598425196850394,
          "p": 0.20512820512820512,
          "f": 0.1560975562612732
        },
        "rouge-2": {
          "r": 0.023391812865497075,
          "p": 0.03333333333333333,
          "f": 0.02749140408828512
        },
        "rouge-l": {
          "r": 0.10236220472440945,
          "p": 0.16666666666666666,
          "f": 0.1268292635783464
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2503.05128v1",
      "true_abstract": "We developed a theory showing that under appropriate normalizations and\nrescalings, temperature response curves show a remarkably regular behavior and\nfollow a general, universal law. The impressive universality of temperature\nresponse curves remained hidden due to various curve-fitting models not\nwell-grounded in first principles. In addition, this framework has the\npotential to explain the origin of different scaling relationships in thermal\nperformance in biology, from molecules to ecosystems. Here, we summarize the\nbackground, principles and assumptions, predictions, implications, and possible\nextensions of this theory.",
      "generated_abstract": "The identification of the genes and their regulatory mechanisms are essential\nfor understanding the mechanisms underlying the biological processes.\nGenome-wide association studies (GWAS) provide a large amount of genetic\ninformation that can be utilized for the identification of the causal variants\nand their regulatory effects. In this study, we used the GWAS data from the\nMendelian randomization Consortium (MRC) and genome-wide association study\n(GWAS) data from the KORA study to identify the causal variants that influence\nthe risk of type 2 diabetes. We used the causal effect estimation framework and\nthe Bayesian GWAS method to identify the causal variants. Using the causal\neffects, we performed the genome-wide association study (GWAS) to identify the\nassociated genetic variants. The identification of the causal variants is\nimportant for understanding",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.19402985074626866,
          "f": 0.19696969197084496
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.19402985074626866,
          "f": 0.19696969197084496
        }
      }
    },
    {
      "paper_id": "cs.OH.cs/OH/2502.14012v1",
      "true_abstract": "Considering that the physical design of printed circuit board (PCB) follows\nthe principle of modularized design, this paper proposes an automatic placement\nalgorithm for functional modules. We first model the placement problem as a\nmixed-variable optimization problem, and then, developed tailored algorithms of\nglobal placement and legalization for the top-layer centralized placement\nsubproblem and the bottom-layer pin-oriented placement subproblem. Numerical\ncomparison demonstrates that the proposed mixed-variable optimization scheme\ncan get optimized total wirelength of placement. Meanwhile, experimental\nresults on several industrial PCB cases show that the developed centralized\nstrategies can well accommodate the requirement of top-layer placement, and the\npin-oriented global placement based on bin clustering contributes to optimized\nplacement results meeting the requirement of pin-oriented design.",
      "generated_abstract": "This paper presents the design and implementation of a novel networked\ncognitive radio (NR) system based on a hierarchical architecture. The\narchitecture combines the use of a cooperative multi-hop communication\nmechanism with the integration of distributed cognitive radio (DR)\nresource-sharing mechanisms. The proposed architecture is based on the\nCooperative Multi-hop Network (Co-MN) framework, where cooperative\ncommunication links are formed between multiple base stations (BSs) using\nsatellite communication links. The BSs within the Co-MN network cooperate in\nthe transmission and reception of signals. To further enhance the network\nperformance, a DR-based mechanism is integrated into the proposed network,\nwherein the DRs collaboratively share their resources in order to maximize\nthroughput and minimize energy consumption. In this paper, we present the\ndevelopment of a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.12790697674418605,
          "f": 0.1366459577562596
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.017241379310344827,
          "f": 0.0178571378635218
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.10465116279069768,
          "f": 0.11180123725936522
        }
      }
    },
    {
      "paper_id": "cs.CY.econ/GN/2503.05754v1",
      "true_abstract": "The air transportation local share, defined as the proportion of local\npassengers relative to total passengers, serves as a critical metric reflecting\nhow economic growth, carrier strategies, and market forces jointly influence\ndemand composition. This metric is particularly useful for examining industry\nstructure changes and large-scale disruptive events such as the COVID-19\npandemic. This research offers an in-depth analysis of local share patterns on\nmore than 3900 Origin and Destination (O&D) pairs across the U.S. air\ntransportation system, revealing how economic expansion, the emergence of\nlow-cost carriers (LCCs), and strategic shifts by legacy carriers have\ncollectively elevated local share. To efficiently identify the local share\ncharacteristics of thousands of O&Ds and to categorize the O&Ds that have the\nsame behavior, a range of time series clustering methods were used. Evaluation\nusing visualization, performance metrics, and case-based examination\nhighlighted distinct patterns and trends, from magnitude-based stratification\nto trend-based groupings. The analysis also identified pattern commonalities\nwithin O&D pairs, suggesting that macro-level forces (e.g., economic cycles,\nchanging demographics, or disruptions such as COVID-19) can synchronize changes\nbetween disparate markets. These insights set the stage for predictive modeling\nof local share, guiding airline network planning and infrastructure\ninvestments. This study combines quantitative analysis with flexible clustering\nto help stakeholders anticipate market shifts, optimize resource allocation\nstrategies, and strengthen the air transportation system's resilience and\ncompetitiveness.",
      "generated_abstract": "In this paper, we propose a new method to estimate the wage gap between\neconomically different groups. We first introduce a novel approach for\nmeasuring the wage gap by using a two-step method. In the first step, we\nestimate the difference in wages between two groups, and in the second step, we\nuse a method that is based on the difference between the two groups to estimate\nthe wage gap. In the third step, we use an iterative method to estimate the\nwage gap between groups. We test the proposed method using data from the\nJapanese labor market. We also apply the method to estimate the wage gap between\nmen and women in Japan.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09259259259259259,
          "p": 0.2727272727272727,
          "f": 0.13824884414194408
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08024691358024691,
          "p": 0.23636363636363636,
          "f": 0.11981566441844178
        }
      }
    },
    {
      "paper_id": "cs.PF.cs/PF/2503.09650v1",
      "true_abstract": "With the advancement of Large Language Models (LLMs), the importance of\naccelerators that efficiently process LLM computations has been increasing.\nThis paper discusses the necessity of LLM accelerators and provides a\ncomprehensive analysis of the hardware and software characteristics of the main\ncommercial LLM accelerators. Based on this analysis, we propose considerations\nfor the development of next-generation LLM accelerators and suggest future\nresearch directions.",
      "generated_abstract": "We consider the problem of assigning jobs to a set of workers in a distributed\nsystem. We model this as a multi-agent distributed system with an unconstrained\noptimal worker assignment problem. The problem is NP-hard and we propose a\nmulti-agent algorithm that, after a first round of initialization,\nefficiently converges to an approximate solution. Our results complement the\nwork of Hodges et al. (2018) who study the problem of assigning jobs to a set of\nworkers in a centralized system. Our approach is agnostic to the underlying\ndistributed system and can be used in any distributed system with unconstrained\nworker assignment problem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.13793103448275862,
          "f": 0.15384614891272205
        },
        "rouge-2": {
          "r": 0.01694915254237288,
          "p": 0.012048192771084338,
          "f": 0.014084502185084
        },
        "rouge-l": {
          "r": 0.17391304347826086,
          "p": 0.13793103448275862,
          "f": 0.15384614891272205
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.07231v1",
      "true_abstract": "In today's globalised trade, supply chains form complex networks spanning\nmultiple organisations and even countries, making them highly vulnerable to\ndisruptions. These vulnerabilities, highlighted by recent global crises,\nunderscore the urgent need for improved visibility and resilience of the supply\nchain. However, data-sharing limitations often hinder the achievement of\ncomprehensive visibility between organisations or countries due to privacy,\nsecurity, and regulatory concerns. Moreover, most existing research studies\nfocused on individual firm- or product-level networks, overlooking the\nmultifaceted interactions among diverse entities that characterise real-world\nsupply chains, thus limiting a holistic understanding of supply chain dynamics.\nTo address these challenges, we propose a novel approach that integrates\nFederated Learning (FL) and Graph Convolutional Neural Networks (GCNs) to\nenhance supply chain visibility through relationship prediction in supply chain\nknowledge graphs. FL enables collaborative model training across countries by\nfacilitating information sharing without requiring raw data exchange, ensuring\ncompliance with privacy regulations and maintaining data security. GCNs empower\nthe framework to capture intricate relational patterns within knowledge graphs,\nenabling accurate link prediction to uncover hidden connections and provide\ncomprehensive insights into supply chain networks. Experimental results\nvalidate the effectiveness of the proposed approach, demonstrating its ability\nto accurately predict relationships within country-level supply chain knowledge\ngraphs. This enhanced visibility supports actionable insights, facilitates\nproactive risk management, and contributes to the development of resilient and\nadaptive supply chain strategies, ensuring that supply chains are better\nequipped to navigate the complexities of the global economy.",
      "generated_abstract": "The rapid growth of data has created a critical need for efficient and\naccurate data processing systems. Machine learning (ML) models have emerged as\nthe de-facto approach to solving many of these problems. However, the\ncomplexity of ML models can cause performance bottlenecks, leading to data\nprocessing bottlenecks. In this paper, we introduce a novel framework, called\nData-aware ML, that addresses these bottlenecks by identifying the data that\nshould be processed first. By grouping data into sets based on their similarity\nto the target task, Data-aware ML can intelligently prioritize data processing\ntasks and reduce the impact of bottlenecks. We validate the effectiveness of\nData-aware ML by applying it to two ML models: a neural network (NN) and a\nreinforcement learning agent. Our experiments show that Data-aware ML\nsignificantly reduces the impact of bottl",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11931818181818182,
          "p": 0.23076923076923078,
          "f": 0.15730336629325717
        },
        "rouge-2": {
          "r": 0.02631578947368421,
          "p": 0.047244094488188976,
          "f": 0.033802812306130366
        },
        "rouge-l": {
          "r": 0.10227272727272728,
          "p": 0.1978021978021978,
          "f": 0.13483145618089765
        }
      }
    },
    {
      "paper_id": "cs.NI.cs/NI/2503.08123v1",
      "true_abstract": "With the advent of 6G systems, emerging hyper-connected ecosystems\nnecessitate agile and adaptive medium access control (MAC) protocols to contend\nwith network dynamics and diverse service requirements. We propose LLM4MAC, a\nnovel framework that harnesses large language models (LLMs) within a\nreinforcement learning paradigm to drive MAC protocol emergence. By\nreformulating uplink data transmission scheduling as a semantics-generalized\npartially observable Markov game (POMG), LLM4MAC encodes network operations in\nnatural language, while proximal policy optimization (PPO) ensures continuous\nalignment with the evolving network dynamics. A structured identity embedding\n(SIE) mechanism further enables robust coordination among heterogeneous agents.\nExtensive simulations demonstrate that on top of a compact LLM, which is\npurposefully selected to balance performance with resource efficiency, the\nprotocol emerging from LLM4MAC outperforms comparative baselines in throughput\nand generalization.",
      "generated_abstract": "This paper introduces the concept of a \\textit{context-aware} service\nand describes how it can be implemented in a heterogeneous heterogeneous\nenvironment, such as an IoT-based distributed computing environment. The\ncontext-aware service is defined as a set of services that are sensitive to\nenvironmental factors, such as the current time, weather conditions, or\ntraffic conditions. The service can then adapt its behavior based on these\nfactors, such as sending a notification when it detects heavy traffic or\nsending an alert when the temperature is predicted to exceed a certain threshold.\nThe service can also be used by other services that are sensitive to the same\nenvironmental factors, such as temperature or traffic conditions, to improve\ntheir performance. The context-aware service is described as a set of\nservices, where each service has a context. The context can be any data that\nrepresents the current state of the service, such as the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.17073170731707318,
          "f": 0.14736841614626056
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.016,
          "f": 0.015936249980160612
        },
        "rouge-l": {
          "r": 0.12037037037037036,
          "p": 0.15853658536585366,
          "f": 0.13684210035678687
        }
      }
    },
    {
      "paper_id": "math.AG.math/AG/2503.09195v1",
      "true_abstract": "Refined algebraic domains are regions in the plane surrounded by finitely\nmany non-singular real algebraic curves which may intersect with normal\ncrossing. We are interested in shapes of such regions with surrounding real\nalgebraic curves. Poincar'e-Reeb Graphs of them are graphs the regions\nnaturally collapse to respecting the projection to a straight line. Such graphs\nwere first formulated by Sorea, for example, around 2020, and regions\nsurrounded by mutually disjoint non-singular real algebraic curves were mainly\nconsidered. The author has generalized the studies to several general\nsituations.\n  We find classes of such objects defined inductively by adding curves. We\nrespect characteristic finite sets in the curves. We consider regions\nsurrounded by the curves and of a new type. We investigate geometric properties\nand combinatorial ones of them and discuss important examples. We also\npreviously studied explicit classes defined inductively in this way and review\nthem.",
      "generated_abstract": "We study the theory of differential forms on the group of characters of a\nalgebraic group, called the {\\it character lattice}, in the context of\nHochschild cohomology. In particular, we define the {\\it character\nHamiltonian subbundle} and the {\\it character connection} for a character\nline bundle on a smooth algebraic group. We show that these two objects are\nequivalent to the usual notion of connection on the character lattice. We\nconstruct the {\\it character connection form} of a character line bundle on a\nsmooth algebraic group, and we show that it is a closed two-form that is\nnon-degenerate and invariant under the character group action. We show that\nthe character connection form induces a connection on the character lattice,\nand we show that the two-form is closed on the character lattice. We show that\nthe character connection form is non-degenerate and invariant under the\ncharacter group action, and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1348314606741573,
          "p": 0.2033898305084746,
          "f": 0.16216215736760423
        },
        "rouge-2": {
          "r": 0.015384615384615385,
          "p": 0.019801980198019802,
          "f": 0.017316012394821584
        },
        "rouge-l": {
          "r": 0.12359550561797752,
          "p": 0.1864406779661017,
          "f": 0.1486486438540907
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2501.08714v1",
      "true_abstract": "Neuroblastoma, is a highly heterogeneous pediatric tumour and is responsible\nfor 15% of pediatric cancer-related deaths. The clinical outcomes can vary from\nspontaneous regression to high metastatic disease. This extracranial tumour\narises from a neural crest-derived cell and can harbor different phenotypes.\nIts heterogeneity may result from variations in differentiation states\ninfluenced by genetic and epigenetic factors and individual patient\ncharacteristics. This leads downstream to disruption of homeostasis and a\nmetabolic shift in response to the tumour needs. Nutrition can play a key role\nin influencing various aspects of a tumour behaviour. This review provides an\nin-depth exploration of the aetiology of neuroblastoma and the different\navenues of disease progression, which can be targeted with individualized\nnutrition intervention strategies to improve the well-being of children and\noptimize clinical outcomes.",
      "generated_abstract": "The complexity of the cellular machinery and the dynamic interactions\nbetween different cellular components are essential for understanding cell\nfunctionality and disease progression. To study these phenomena, we present a\nnovel experimental and computational framework to study the cellular\narchitecture of single cells using confocal microscopy. We introduce a\ndynamically-adaptive algorithm to measure the cellular architecture, based on\nthe concept of the spatiotemporal complexity (STC) \\cite{furusawa2021spatiotemporal}.\nThe STC is a measure of the complexity of the cellular architecture and is\nindependent of the type of experimental methods used. We apply our framework to\nstudy the cellular architecture of human skin cells, including the\nautonomous-cells-stem-cells transition and the development of skin lesions. We\nshow that the STC of skin cells is significantly different from",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1348314606741573,
          "p": 0.17142857142857143,
          "f": 0.15094339129781273
        },
        "rouge-2": {
          "r": 0.031496062992125984,
          "p": 0.0380952380952381,
          "f": 0.034482753665651726
        },
        "rouge-l": {
          "r": 0.12359550561797752,
          "p": 0.15714285714285714,
          "f": 0.1383647749456115
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.10062v1",
      "true_abstract": "This paper addresses the one-bit consensus of controllable linear multi-agent\nsystems (MASs) with communication noises. A consensus algorithm consisting of a\ncommunication protocol and a consensus controller is designed. The\ncommunication protocol introduces a linear compression encoding function to\nachieve a one-bit data rate, thereby saving communication costs. The consensus\ncontroller with a stabilization term and a consensus term is proposed to ensure\nthe consensus of a potentially unstable but controllable MAS. Specifically, in\nthe consensus term, we adopt an estimation method to overcome the information\nloss caused by one-bit communications and a decay step to attenuate the effect\nof communication noise. Two combined Lyapunov functions are constructed to\novercome the difficulty arising from the coupling of the control and\nestimation. By establishing similar iterative structures of these two\nfunctions, this paper shows that the MAS can achieve consensus in the mean\nsquare sense at the rate of the reciprocal of the iteration number under the\ncase with a connected fixed topology. Moreover, the theoretical results are\ngeneralized to the case with jointly connected Markovian switching topologies\nby establishing a certain equivalence relationship between the Markovian\nswitching topologies and a fixed topology. Two simulation examples are given to\nvalidate the algorithm.",
      "generated_abstract": "In this paper, a novel distributed multi-agent reinforcement learning (MARL)\nsystem is developed to optimize the coordination of multi-agent systems. The\nMARL system integrates a control layer and a cooperation layer, enabling\ncoordinated communication between agents and optimal coordination of\ncommunication channels. The control layer optimizes the agents' local policies\nthrough reinforcement learning, while the cooperation layer dynamically\ndistributes the communication channels among agents to enhance the\ncoordination ability of agents. The agents are deployed on an unmanned aerial\nvehicle (UAV) platform, with each agent tasked with sensing the environment,\ncommunicating with other agents, and coordinating with other agents. To\noptimize the system, an offline optimization strategy is developed, and the\nonline learning strategy is constructed based on the obtained results. The\nresults show that the proposed MARL system achieves an average",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17391304347826086,
          "p": 0.24691358024691357,
          "f": 0.2040816278035195
        },
        "rouge-2": {
          "r": 0.022222222222222223,
          "p": 0.032520325203252036,
          "f": 0.026402635440970726
        },
        "rouge-l": {
          "r": 0.14782608695652175,
          "p": 0.20987654320987653,
          "f": 0.17346938290556035
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.14447v1",
      "true_abstract": "This paper introduces the two-way common causal covariates (CCC) assumption,\nwhich is necessary to get an unbiased estimate of the ATT when using\ntime-varying covariates in existing Difference-in-Differences methods. The\ntwo-way CCC assumption implies that the effect of the covariates remain the\nsame between groups and across time periods. This assumption has been implied\nin previous literature, but has not been explicitly addressed. Through\ntheoretical proofs and a Monte Carlo simulation study, we show that the\nstandard TWFE and the CS-DID estimators are biased when the two-way CCC\nassumption is violated. We propose a new estimator called the Intersection\nDifference-in-differences (DID-INT) which can provide an unbiased estimate of\nthe ATT under two-way CCC violations. DID-INT can also identify the ATT under\nheterogeneous treatment effects and with staggered treatment rollout. The\nestimator relies on parallel trends of the residuals of the outcome variable,\nafter appropriately adjusting for covariates. This covariate residualization\ncan recover parallel trends that are hidden with conventional estimators.",
      "generated_abstract": "This paper presents a novel approach for the analysis of longitudinal data\nlongitudinal panel data and longitudinal panel data, using a time-varying\nintercept. We propose a nonparametric estimator based on the quantile-quantile\nrelation, which is consistent and asymptotically normal. The proposed estimator\nis shown to perform consistently and asymptotically normally under conditions\non the covariate distribution and treatment assignment that are weaker than\nthose required for standard parametric methods. We also propose a\nnonparametric bootstrap method to address the finite sample issue.\n  We illustrate the proposed method through two empirical examples: the\nproportion of women in parliament in the United Kingdom and the proportion of\nmen in the workforce in the United States.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.25,
          "p": 0.35135135135135137,
          "f": 0.2921348266027018
        },
        "rouge-2": {
          "r": 0.04195804195804196,
          "p": 0.058823529411764705,
          "f": 0.048979586976760164
        },
        "rouge-l": {
          "r": 0.2403846153846154,
          "p": 0.33783783783783783,
          "f": 0.28089887154652193
        }
      }
    },
    {
      "paper_id": "q-fin.TR.q-fin/TR/2502.16246v1",
      "true_abstract": "Understanding the impact of trades on prices is a crucial question for both\nacademic research and industry practice. It is well established that impact\nfollows a square-root impact as a function of traded volume. However, the\nmicroscopic origin of such a law remains elusive: empirical studies are\nparticularly challenging due to the anonymity of orders in public data. Indeed,\nthere is ongoing debate about whether price impact has a mechanical origin or\nwhether it is primarily driven by information, as suggested by many economic\ntheories. In this paper, we revisit this question using a very detailed dataset\nprovided by the Japanese stock exchange, containing the trader IDs for all\norders sent to the exchange between 2012 and 2018. Our central result is that\nsuch a law has in fact microscopic roots and applies already at the level of\nsingle child orders, provided one waits long enough for the market to \"digest\"\nthem. The mesoscopic impact of metaorders arises from a \"double\" square-root\neffect: square-root in volume of individual impact, followed by an inverse\nsquare-root decay as a function of time. Since market orders are anonymous, we\nexpect and indeed find that these results apply to any market orders, and the\nimpact of synthetic metaorders, reconstructed by scrambling the identity of the\nissuers, is described by the very same square-root impact law. We conclude that\nprice impact is essentially mechanical, at odds with theories that emphasize\nthe information content of such trades to explain the square-root impact law.",
      "generated_abstract": "This paper investigates the application of Deep Neural Networks (DNNs) for\ncomputational finance and specifically for the quantitative evaluation of\ntrading strategies. We propose a Deep Neural Network architecture to model\nhistorical and simulated price returns and develop a practical trading\nstrategy. Our approach is based on the use of DNNs to estimate the probability\nof a future price change, which is then used as an input in a traditional\nneural network (TNN) to evaluate trading strategies. We demonstrate that our\napproach effectively models historical and simulated price returns, and that\nthe strategy achieves significant returns. Furthermore, we show that the\nstrategy is robust and can handle unseen market conditions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11486486486486487,
          "p": 0.2328767123287671,
          "f": 0.15384614942200214
        },
        "rouge-2": {
          "r": 0.004329004329004329,
          "p": 0.009900990099009901,
          "f": 0.006024092152165843
        },
        "rouge-l": {
          "r": 0.10135135135135136,
          "p": 0.2054794520547945,
          "f": 0.13574660191068993
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/CB/2408.05119v1",
      "true_abstract": "Stress generation by the actin cytoskeleton shapes cells and tissues. Despite\nimpressive progress in live imaging and quantitative physical descriptions of\ncytoskeletal network dynamics, the connection between processes at molecular\nscales and cell-scale spatio-temporal patterns is still unclear. Here we review\nstudies reporting acto-myosin clusters of micrometer size and with lifetimes of\nseveral minutes in a large number of organisms ranging from fission yeast to\nhumans. Such structures have also been found in reconstituted systems in vitro\nand in theoretical analysis of cytoskeletal dynamics. We propose that tracking\nthese clusters can serve as a simple readout for characterising living matter.\nSpatio-temporal patterns of clusters could serve as determinants of\nmorphogenetic processes that play similar roles in diverse organisms.",
      "generated_abstract": "We present the first computational analysis of the evolutionary dynamics of\nthe protein phosphatase PP1 (PP1L1), a crucial protein in regulating the\nmetabolism of nucleotides and amino acids in eukaryotic cells. PP1L1 is a\ndimeric protein that undergoes conformational changes to regulate the\nmetabolism of nucleotides and amino acids in cells. The phosphorylation of PP1L1\nis essential for its function. We use a novel mathematical framework to\ncharacterize the evolutionary dynamics of PP1L1 in response to changes in its\nphosphorylation state. Our analysis reveals a simple model that captures the\nkey features of the evolution of PP1L1. The model predicts that PP1L1's\nphosphorylation state evolves under natural selection, with a gradual\nde",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16304347826086957,
          "p": 0.23809523809523808,
          "f": 0.19354838227180032
        },
        "rouge-2": {
          "r": 0.017391304347826087,
          "p": 0.021052631578947368,
          "f": 0.019047614092971808
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.2222222222222222,
          "f": 0.1806451564653487
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.21181v1",
      "true_abstract": "It's not unreasonable to think that in-game sporting performance can be\naffected partly by what takes place off the court. We can't observe what\nhappens between games directly. Instead, we proxy for the possibility of\nathletes partying by looking at play following games in party cities. We are\ninterested to see if teams exhibit a decline in performance the day following a\ngame in a city with active nightlife; we call this a \"hangover effect\". Part of\nthe question is determining a reasonable way to measure levels of nightlife,\nand correspondingly which cities are notorious for it; we colloquially refer to\nsuch cities as \"party cities\". To carry out this study, we exploit data on\nbookmaker spreads: the expected score differential between two teams after\nconditioning on observable performance in past games and expectations about the\nupcoming game. We expect a team to meet the spread half the time, since this is\none of the easiest ways for bookmakers to guarantee a profit. We construct a\nmodel which attempts to estimate the causal effect of visiting a \"party city\"\non subsequent day performance as measured by the odds of beating the spread. In\nparticular, we only consider the hangover effect on games played back-to-back\nwithin 24 hours of each other. To the extent that odds of beating the spread\nagainst next day opponent is uncorrelated with playing in a party city the day\nbefore, which should be the case under an efficient betting market, we have\nidentification in our variable of interest. We find that visiting a city with\nactive nightlife the day prior to a game does have a statistically significant\nnegative effect on a team's likelihood of meeting bookmakers' expectations for\nboth NBA and MLB.",
      "generated_abstract": "The purpose of this paper is to estimate the parameters of a heterogeneous\nand heteroskedasticity-in-variables (HHS-IV) model under a heterogeneous\ninteraction effect of two dimensions (with two dimensions being the\ninteraction effect and the covariates). First, we propose a new method for\nidentifying the interaction effect and the covariates. Then, we propose a\nBayesian modeling approach that allows for an efficient posterior inference of\nthe estimated parameters. The proposed methodology is illustrated with an\nexample of two-way fixed-effects HHS-IV models. The simulation studies\ndemonstrate the superiority of the proposed methodology in terms of inference\naccuracy and computational efficiency.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11176470588235295,
          "p": 0.3114754098360656,
          "f": 0.16450216061543083
        },
        "rouge-2": {
          "r": 0.01845018450184502,
          "p": 0.05813953488372093,
          "f": 0.028011200824487142
        },
        "rouge-l": {
          "r": 0.10588235294117647,
          "p": 0.29508196721311475,
          "f": 0.15584415195742218
        }
      }
    },
    {
      "paper_id": "cs.CY.cs/CY/2503.10458v1",
      "true_abstract": "Social media platforms have been accused of causing a range of harms,\nresulting in dozens of lawsuits across jurisdictions. These lawsuits are\nsituated within the context of a long history of American product safety\nlitigation, suggesting opportunities for remediation outside of financial\ncompensation. Anticipating that at least some of these cases may be successful\nand/or lead to settlements, this article outlines an implementable mechanism\nfor an abatement and/or settlement plan capable of mitigating abuse. The paper\ndescribes the requirements of such a mechanism, implications for privacy and\noversight, and tradeoffs that such a procedure would entail. The mechanism is\nframed to operate at the intersection of legal procedure, standards for\ntransparent public health assessment, and the practical requirements of modern\ntechnology products.",
      "generated_abstract": "The world of digital health is growing rapidly. The increasing popularity of\ndigital health technologies is reflected in the growing number of applications\nand innovations. However, the current healthcare system suffers from several\nchallenges. For example, the lack of interoperability of digital health\nsystems, limited access to high-quality healthcare services, and the\ninadequate use of technology in the medical field. These issues have a significant\nimpact on the quality of healthcare services, the accessibility of medical\nservices, and the effectiveness of treatment. This paper explores the\npotential of the blockchain technology for improving the quality of healthcare\nservices in the digital health sector. We focus on the application of blockchain\ntechnology in the healthcare industry, including its impact on the development\nof the digital health sector, the role of blockchain in improving patient\naccessibility to healthcare services, and the role",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.19718309859154928,
          "f": 0.17283950124904757
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.008928571428571428,
          "f": 0.008658003662602906
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.18309859154929578,
          "f": 0.16049382223670186
        }
      }
    },
    {
      "paper_id": "math.QA.math/QA/2503.05553v1",
      "true_abstract": "For a simple, self-dual, strong CFT-type vertex operator algebra (VOA) of\ncentral charge $c$, we describe the Virasoro $n$-point correlation function on\na genus $g$ marked Riemann surface in the Schottky uniformisation. We show that\nthis $n$-point function determines the correlation functions for all Virasoro\nvacuum descendants. Using our recent work on genus $g$ Zhu recursion, we show\nthat the Virasoro $n$-point function is determined by a differential operator\n$\\mathcal{D}_{n}$ acting on the genus $g$ VOA partition function normalised by\nthe Heisenberg partition function to the power of $c$. We express\n$\\mathcal{D}_{n}$ as the sum of weights over certain Virasoro graphs where the\nweights explicitly depend on $c$, the classical bidifferential of the second\nkind, the projective connection, holomorphic 1-forms and derivatives with\nrespect to any $3g-3$ locally independent period matrix elements. We also\ndescribe the modular properties of $\\mathcal{D}_{n}$ under a homology base\nchange.",
      "generated_abstract": "We consider a simple example of a group action on a non-commutative\nspace. The group action is given by a shifted action on a free product of\nalgebras. This action is a shifted product action on the algebra of\ncochains. We study the dynamics of the group action. Our main result is the\nexistence of a free product of cobordisms with a prescribed winding number.\nOur proof uses the theory of cobordism classes and the theory of cochain\nmodules over a group action. We also discuss the role of the group action in\nthe dynamics of a simple group. We apply our results to the dynamics of the\ngroup of automorphisms of a surface of genus $g$ with a prescribed winding\nnumber.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18947368421052632,
          "p": 0.32727272727272727,
          "f": 0.2399999953555556
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.08888888888888889,
          "f": 0.07079645538413377
        },
        "rouge-l": {
          "r": 0.1368421052631579,
          "p": 0.23636363636363636,
          "f": 0.173333328688889
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/mes-hall/2503.09970v1",
      "true_abstract": "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
      "generated_abstract": "We propose a universal framework for the charge-density-wave (CDW) state\nin the superconducting iron pnictide Heusler compounds. Our theoretical\nframework, based on the real-space Green's function method, is applicable to\nboth conventional and twisted superconducting Heusler alloys. The proposed\nuniversal framework is based on the charge-density-wave Hamiltonian. It\nincludes a single-particle Hamiltonian for the conduction electrons and an\ninteraction Hamiltonian that accounts for the Cooper pairing between the\nconduction and valence electrons. We show that the CDW state can be realized in\na wide range of Heusler compounds, including those with twist angles ranging\nfrom 0$^\\circ$ to 45$^\\circ$. We demonstrate that the CDW state in these\ncompounds can be described by a single-particle Hamiltonian, which is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12857142857142856,
          "p": 0.1232876712328767,
          "f": 0.12587412087632666
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12857142857142856,
          "p": 0.1232876712328767,
          "f": 0.12587412087632666
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.15761v1",
      "true_abstract": "We propose a factor model and an estimator of the factors and loadings that\nare robust to weak factors. The factors can have an arbitrarily weak influence\non the mean or quantile of the outcome variable at most quantile levels; each\nfactor only needs to have a strong impact on the outcome's quantile near one\nunknown quantile level. The estimator for every factor, loading, and common\ncomponent is asymptotically normal at the $\\sqrt{N}$ or $\\sqrt{T}$ rate. It\ndoes not require the knowledge of whether the factors are weak and how weak\nthey are. We also develop a weak-factor-robust estimator of the number of\nfactors and a consistent selectors of factors of any desired strength of\ninfluence on the quantile or mean of the outcome variable. Monte Carlo\nsimulations demonstrate the effectiveness of our methods.",
      "generated_abstract": "The analysis of the data of the U.S. Census Bureau presents a unique\nchallenge for econometric models: a highly skewed distribution, with large\nnumbers of outliers. This paper proposes a novel model to address this issue,\nwhich we call the Gamma-Gamma distribution (GGD). The GGD is a\nnonparametric model, with a flexible distribution and a non-parametric\napproximation. We propose a likelihood-based approach to fit the GGD,\ncombined with an efficient algorithm for the estimation of its parameters. We\nalso propose a new method to predict the GGD, using the fitted GGD as a\nsuperposition of the Gamma and the Gamma-Gamma distributions. We demonstrate\nthe validity of our model through numerical simulations, and we use the\nGGD to analyze the data of the U.S. Census Bureau.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19480519480519481,
          "p": 0.19480519480519481,
          "f": 0.19480518980519496
        },
        "rouge-2": {
          "r": 0.05785123966942149,
          "p": 0.06140350877192982,
          "f": 0.0595744630895432
        },
        "rouge-l": {
          "r": 0.19480519480519481,
          "p": 0.19480519480519481,
          "f": 0.19480518980519496
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/LG/2503.10635v1",
      "true_abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack.",
      "generated_abstract": "In this paper, we propose a novel end-to-end framework, named Dual-Stream\nEfficient Video Cutout Generation (DSEVCG), to generate high-quality cutouts\nfrom videos. Traditional cutout methods usually rely on a large-scale dataset\nfor training, which is often expensive and time-consuming. To address this\nissue, we propose a novel approach that utilizes a small dataset for training\nthe model. To achieve this, we design two streams to process the video\nfeatures, and then fuse them into a single video-level representation.\nSpecifically, we first utilize a dense video-level feature extractor to\nextract the video features from the video, and then generate the cutout mask\nfrom the video-level features using a video-level cutout generator. Next, we\nintroduce a dual-stream strategy to fuse the video-level features. Finally, we\npropose",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11351351351351352,
          "p": 0.2625,
          "f": 0.15849056182271284
        },
        "rouge-2": {
          "r": 0.03292181069958848,
          "p": 0.07079646017699115,
          "f": 0.044943815891459835
        },
        "rouge-l": {
          "r": 0.11351351351351352,
          "p": 0.2625,
          "f": 0.15849056182271284
        }
      }
    },
    {
      "paper_id": "cs.NI.eess/SP/2503.05429v1",
      "true_abstract": "Cross-Technology Interference (CTI) poses challenges for the performance and\nrobustness of wireless networks. There are opportunities for better cooperation\nif the spectral occupation and technology of the interference can be detected.\nNamely, this information can help the Orthogonal Frequency Division Multiple\nAccess (OFDMA) scheduler in IEEE 802.11ax (Wi-Fi 6) to efficiently allocate\nresources to multiple users inthe frequency domain. This work shows that a\nsingle Channel State Information (CSI) snapshot, which is used for packet\ndemodulation in the receiver, is enough to detect and classify the type of CTI\non low-cost Wi-Fi 6 hardware. We show the classification accuracy of a small\nConvolutional Neural Network (CNN) for different Signal-to-Noise Ratio (SNR)\nand Signal-to-Interference Ratio (SIR) with simulated data, as well as using a\nwired and over-the-air test with a professional wireless connectivity tester,\nwhile running the inference on the low-cost device. Furthermore, we use\nopenwifi, a full-stack Wi-Fi transceiver running on software-defined radio\n(SDR) available in the w-iLab.t testbed, as Access Point (AP) to implement a\nCTI-aware multi-user OFDMA scheduler when the clients send CTI detection\nfeedback to the AP. We show experimentally that it can fully mitigate the 35%\nthroughput loss caused by CTI when the AP applies the appropriate scheduling.",
      "generated_abstract": "This paper presents a novel approach for estimating the minimum-phase\nsignals of interest from a single-input single-output (SISO) system. The\nmethod relies on the spectral decomposition of the system's time-domain\ntransfer function. This decomposition enables the identification of a\nnon-Hermitian phase matrix that corresponds to the minimum-phase signal. The\nphase matrix is then applied to the original system, yielding a new SISO\nsystem with the minimum-phase signal as its output. The new system is then\nestimated by means of the same spectral decomposition technique. The\napproach is illustrated through a case study on the identification of the\nminimum-phase signals of interest for the non-linear differential-algebraic\nequations (DAE) system described in [1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09523809523809523,
          "p": 0.2153846153846154,
          "f": 0.13207546744615536
        },
        "rouge-2": {
          "r": 0.03015075376884422,
          "p": 0.06382978723404255,
          "f": 0.04095562704143368
        },
        "rouge-l": {
          "r": 0.08163265306122448,
          "p": 0.18461538461538463,
          "f": 0.1132075429178535
        }
      }
    },
    {
      "paper_id": "math.OA.math/LO/2503.10505v1",
      "true_abstract": "We compute the $K_1$-group of ultraproducts of unital, simple $C^*$-algebras\nwith unique trace and strict comparison. As an application, we prove that the\nreduced free group $C^*$-algebras $C^*_r(F_m)$ and $C^*_r(F_n)$ are\nelementarily equivalent (i.e., have isomorphic ultrapowers) if and only if $m =\nn$. This settles in the negative the $C^*$-algebraic analogue of Tarski's 1945\nproblem for groups.",
      "generated_abstract": "The aim of this paper is to prove the existence of the weak solution to a\nnonlinear fluid-structure interaction model, where the structure is modeled by\na nonlinear elastic body. The fluid is described by a nonlinear p-Laplacian\nequation and the nonlinearity of the fluid is due to the interaction with the\nnonlinear elastic body. We consider the case where the nonlinear elastic body\nexhibits nonlinear elastic waves. We first prove the existence of weak solutions\nfor a general class of nonlinear elasticity equations with nonlinear elastic\nwaves and then focus on the case of linear elasticity with homogeneous\nviscosity. We provide a local-in-time existence of strong solutions for this\ncase. Necessary and sufficient conditions for the existence of weak solutions\nare obtained, and the dependence of the solution on initial data is studied.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1568627450980392,
          "p": 0.13114754098360656,
          "f": 0.14285713789700272
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.13114754098360656,
          "f": 0.14285713789700272
        }
      }
    },
    {
      "paper_id": "math.MG.math/FA/2503.07287v1",
      "true_abstract": "A functional analog of the Klain-Schneider theorem for vector-valued\nvaluations on convex functions is established, providing a classification of\ncontinuous, translation covariant, simple valuations. Under additional rotation\nequivariance assumptions, an analytic counterpart of the moment vector is\ncharacterized alongside a new epi-translation invariant valuation. The former\narises as the top-degree operator in a family of functional intrinsic moments,\nwhich are linked to functional intrinsic volumes through translations. The\nlatter represents the top-degree operator in a class of Minkowski vectors,\nwhich are introduced in this article and which lack classical counterparts on\nconvex bodies, as they vanish due to the Minkowski relations. Additional\nclassification results are obtained for homogeneous valuations of extremal\ndegrees.",
      "generated_abstract": "In this paper, we study the non-commutative cohomology of a\nnon-commutative Hopf algebra with a non-commutative $L$-operator, where $L$ is\na non-commutative central element. We obtain a formula for the non-commutative\ncohomology of the algebra with respect to the Hopf algebra. We also give a\ndescription of the non-commutative cohomology of a non-commutative Hopf algebra\nwith a non-commutative central element as a subalgebra of the algebra of\nsymmetric functions of the variables of the algebra. We construct a\nnon-commutative $L$-operator on a non-commutative Hopf algebra $H$ and apply our\nformula for the non-commutative cohomology of $H$ to obtain a formula for the\nnon-commutative $L$-operator of $H$",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14102564102564102,
          "p": 0.275,
          "f": 0.18644067348463095
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.031746031746031744,
          "f": 0.024096380832487493
        },
        "rouge-l": {
          "r": 0.11538461538461539,
          "p": 0.225,
          "f": 0.15254236839988522
        }
      }
    },
    {
      "paper_id": "math.PR.econ/TH/2411.15401v4",
      "true_abstract": "Given two random variables taking values in a bounded interval, we study\nwhether one dominates the other in higher-order stochastic dominance depends on\nthe reference interval in the model setting. We obtain two results. First, the\nstochastic dominance relations get strictly stronger when the reference\ninterval shrinks if and only if the order of stochastic dominance is larger\nthan three. Second, for mean-preserving stochastic dominance relations, the\nreference interval is irrelevant if and only if the difference between the\ndegree of the stochastic dominance and the number of moments is no larger than\nthree. These results highlight complications arising from using higher-order\nstochastic dominance in economic applications.",
      "generated_abstract": "We study the existence of a Nash equilibrium for a class of non-linear\nstochastic control problems with time-varying payoffs. The problems are\ndescribed by a continuous-time stochastic control problem with quadratic cost\nfunctions and stochastic constraints on the state. We show that the problems\npossess a unique Nash equilibrium if and only if the underlying state space is\nseparable. Additionally, we show that the existence of a unique Nash equilibrium\ncan be established if the underlying state space is countable. Finally, we\nprovide a sufficient condition for the existence of a unique Nash equilibrium\nthat does not depend on the underlying state space.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.203125,
          "p": 0.24528301886792453,
          "f": 0.22222221726641841
        },
        "rouge-2": {
          "r": 0.056179775280898875,
          "p": 0.06493506493506493,
          "f": 0.060240958881550705
        },
        "rouge-l": {
          "r": 0.203125,
          "p": 0.24528301886792453,
          "f": 0.22222221726641841
        }
      }
    },
    {
      "paper_id": "q-fin.PM.econ/EM/2410.16333v2",
      "true_abstract": "This study examines portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and a variety of\nmethods have been developed to achieve this goal. For instance, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by considering tail risk. These methods often depend on\ndistributional information estimated from historical data using predictive\nmodels, each of which carries its own uncertainty. To address this, we propose\na framework for predictive portfolio selection via conformal prediction ,\ncalled \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\nforecasts future portfolio returns, computes the corresponding prediction\nintervals, and selects the portfolio of interest based on these intervals. The\nframework is flexible and can accommodate a wide range of predictive models,\nincluding autoregressive (AR) models, random forests, and neural networks. We\ndemonstrate the effectiveness of the CPPS framework by applying it to an AR\nmodel and validate its performance through empirical studies, showing that it\ndelivers superior returns compared to simpler strategies.",
      "generated_abstract": "This study examines the impact of the Covid-19 pandemic on consumer\nbargaining behavior using a time-varying dynamic model. We find that the\npandemic reduced consumer bargaining power, particularly in the retail sector,\ncausing a decrease in the bargaining position of the consumer and an increase in\nthe bargaining position of the seller. This shift in bargaining power was most\nprevalent in the first half of 2020 and was more pronounced in the retail\nsector. The pandemic also reduced the profitability of the retail sector,\nparticularly in the second half of 2020, as consumers sought to reduce their\nsacrifices to buy essentials. This shift in profitability was most pronounced\nin the clothing sector. Additionally, the study finds that the pandemic\nsignificantly impacted the supply chain in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.24615384615384617,
          "f": 0.16753926252569842
        },
        "rouge-2": {
          "r": 0.01744186046511628,
          "p": 0.03,
          "f": 0.02205881887975876
        },
        "rouge-l": {
          "r": 0.11904761904761904,
          "p": 0.23076923076923078,
          "f": 0.15706805833721676
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/GA/2503.10415v1",
      "true_abstract": "This article focuses on NGC7538 IRS1, one of the most luminous and studied HC\nHII regions in the northern hemisphere. Our aim is to identify the young\nstellar objects (YSOs) embedded within the ionized gas and study their\nkinematic structures. This work expands on a recent survey called \"Protostellar\nOutflows at the EarliesT Stages\" (POETS), which has been devoted to studying\nyoung outflow emission on scales of 10-100 au near luminous YSOs, before they\nstart photoionizing the surrounding medium. We carried out multi-epoch Very\nLong Baseline Array observations of the 22 GHz water masers toward NGC7538 IRS1\nto measure the maser 3D velocities, which, following POETS' findings, are\nreliable tracers of the protostellar winds. Recently, we reobserved the water\nmasers in NGC7538 IRS1 with sensitive global very long baseline interferometry\n(VLBI) observations to map weaker maser emission. Our study confirms the\npresence of two embedded YSOs, IRS1a and IRS1b, at the center of the two linear\ndistributions of 6.7 GHz methanol masers observed in the southern and northern\ncores of the HC HII region, which have been previously interpreted in terms of\nedge-on rotating disks. The water masers trace an extended (~200 au) stationary\nshock front adjacent to the inner portion of the disk around IRS1a. This shock\nfront corresponds to the edge of the southern tip of the ionized core and might\nbe produced by the interaction of the disk wind ejected from IRS1a with the\ninfalling envelope. The water masers closer to IRS1b follow the same LSR\nvelocity (Vlsr) pattern of the 6.7~GHz masers rotating in the disk, but the\ndirection and amplitude of the water maser proper motions are inconsistent with\nrotation. We propose that these water masers are tracing a photo-evaporated\ndisk wind, where the maser Vlsr traces mainly the disk rotation and the proper\nmotions the poloidal velocity of the wind.",
      "generated_abstract": "We present the first 3D image of the young star cluster NGC 2264, located at\na distance of 72 pc from the Sun, using the 3D HST/STIS images obtained in\n2021. NGC 2264 is a cluster of young stars with a wide range of ages and\nluminosities, but which is also characterised by a high concentration of\nlow-mass stars. The young stars in this cluster are generally relatively\ncompact, with radii of 100-200 AU. The new 3D image reveals that NGC 2264 is\ncomposed of a large number of filamentary structures, which appear to be\nformed by the dense stellar winds of the young stars. We find that the\nfilamentary structures are connected by lobes, which are also clearly seen in\nthe 2D image. We also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12432432432432433,
          "p": 0.3108108108108108,
          "f": 0.17760617352454505
        },
        "rouge-2": {
          "r": 0.01444043321299639,
          "p": 0.03389830508474576,
          "f": 0.020253160367121516
        },
        "rouge-l": {
          "r": 0.10810810810810811,
          "p": 0.2702702702702703,
          "f": 0.1544401503585219
        }
      }
    },
    {
      "paper_id": "math.OC.math/OC/2503.10572v1",
      "true_abstract": "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
      "generated_abstract": "We present a novel method for computing the optimal control of the\ntransport of a particle in a random field. The control is chosen by solving an\noptimal control problem that incorporates the field-induced kinetic energy of\nthe particle, the field-induced entropy production, and a novel entropy\nproduction term that incorporates the transport cost. This cost includes the\ncost of moving the particle along the field, a cost that is minimized by\nmoving the particle at the optimal rate of motion. We prove that the solution\nto the optimal control problem is unique and that the control is unique. We\nalso prove that the optimal control is smooth and that the entropy production\nterm is convex. The method is illustrated through numerical examples.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.2545454545454545,
          "f": 0.1985815555253761
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.020618556701030927,
          "f": 0.019138751006617438
        },
        "rouge-l": {
          "r": 0.1511627906976744,
          "p": 0.23636363636363636,
          "f": 0.1843971583622555
        }
      }
    },
    {
      "paper_id": "q-bio.PE.econ/TH/2502.18488v1",
      "true_abstract": "Aquaculture has been the fastest growing food production sector globally due\nto its potential to improve food security, stimulate economic growth, and\nreduce poverty. Its rapid development has been linked to sustainability\nchallenges, many of which are still unresolved and poorly understood.\nSmall-scale producers account for an increasing fraction of aquacultural\noutput. At the same time, many of these producers experience poverty, food\ninsecurity, and rely on unimproved production practices. We develop a stylized\nmathematical model to explore the effects of ecological, social, and economic\nfactors on the dynamics of a small-scale pond aquaculture system. Using\nanalytical and numerical methods, we explore the stability, asymptotic\ndynamics, and bifurcations of the model. Depending on the characteristics of\nthe system, the model exhibits one of three distinct configurations:\nmonostability with a global poverty trap in a nutrient-dominated or\nfish-dominated system; bistability with poverty trap and well-being attractors;\nmultistability with poverty trap and two well-being attractors with different\ncharacteristics. The model results show that intensification can be sustainable\nonly if it takes into account the local social-ecological context. In addition,\nthe heterogeneity of small-scale aquaculture producers matters, as the effects\nof intensification can be unevenly distributed among them. Finally, more is not\nalways better because too high nutrient input or productivity can lead to a\nsuboptimal attractor or system collapse.",
      "generated_abstract": "The purpose of this paper is to present a new approach for the estimation of\nthe optimal consumption and investment policies, based on the application of\nthe Markov-Perron method. The proposed approach allows to derive the\nrepresentative vector of consumption and investment strategies by solving the\nMarkov-Perron problem with the optimal consumption and investment policies as\nthe initial condition. The methodology is applied to the case study of the\neconomic growth of a country, which is modelled by the semi-parametric\nstochastic frontier model. The results indicate that the proposed methodology\nprovides a more accurate representation of the optimal consumption and\ninvestment policies in comparison to the standard methodology, which is based\non the solution of the standard linear program. Moreover, the methodology is\napplied to the estimation of the optimal consumption and investment policies in\nthe context of the economic growth of a developing country.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13013698630136986,
          "p": 0.2753623188405797,
          "f": 0.17674418168783138
        },
        "rouge-2": {
          "r": 0.019801980198019802,
          "p": 0.0380952380952381,
          "f": 0.02605862742098137
        },
        "rouge-l": {
          "r": 0.1232876712328767,
          "p": 0.2608695652173913,
          "f": 0.16744185610643603
        }
      }
    },
    {
      "paper_id": "astro-ph.HE.astro-ph/HE/2503.10540v1",
      "true_abstract": "Context. The nearby middle-aged gamma-ray pulsar J1741-2054 and its pulsar\nwind nebula (PWN) have been studied in X-rays, and its bow-shock nebula (BSN)\nhas been investigated in the Balmer lines, but they have never been observed in\nfar ultraviolet (FUV). Aims. To further study the thermal and magnetospheric\nemission from PSR J1741-2054 and the BSN properties, we observed them in the\nFUV range with the Hubble Space Telescope (HST). Methods. We imaged the target\nin two FUV filters of the HST's ACS/SBC detector. We also re-analyzed previous\noptical observations of the pulsar and its BSN. We fit the pulsar's FUV-optical\nspectrum separately and together with its X-ray spectrum. Results. We found\nthat the pulsar's FUV-optical spectrum consists of a thermal and nonthermal\ncomponents. A joint fit of the FUV-optical and X-ray spectra with combinations\nof a nonthermal and thermal components showed a hard optical nonthermal\nspectrum with a photon index $\\Gamma_{opt} \\approx 1.0-1.2$ and a softer X-ray\ncomponent, $\\Gamma_X \\approx 2.6-2.7$. The thermal emission is dominated by the\ncold component with the temperature $kT_{cold}\\approx 40-50$ eV and emitting\nsphere radius $R_{cold}\\approx 8-15$ km, at $d=270$ pc. An additional hot\nthermal component, with $kT_{hot}\\sim 80$ eV and $R_{hot}\\sim 1$ km, is also\npossible. Such a spectrum resembles the spectra of other middle-aged pulsars,\nbut it shows a harder (softer) optical (X-ray) nonthermal spectrum. We detected\nthe FUV BSN, the first one associated with a middle-aged pulsar. Its\nclosed-shell morphology is similar to the H$\\alpha$ BSN morphology, while its\nFUV flux, $\\sim10^{-13}$ erg cm$^{-2}$ s$^{-1}$, is a factor of $\\sim 4$ higher\nthan the H$\\alpha$ flux. This FUV BSN has a higher surface brightness than the\ntwo previously known ones.",
      "generated_abstract": "We present the first detailed study of the energy spectra of the 10 MeV\nfractions of the cosmic rays detected by the Fermi-LAT in 2016 and 2017. The\nenergy spectra are consistent with a single power-law form with a spectral\nindex of 1.8$\\pm$0.2. We find no evidence for a break at the GeV energy\nscale. In addition, we present the first measurement of the 10 MeV fraction of\nthe cosmic rays detected by the Fermi-LAT in 2016 and 2017, which we measure to\nbe $<$2.5$\\times$10$^{-8}$ cm$^{-2}$ s$^{-1}$. The 10 MeV fraction is the\nhighest measured to date by the Fermi-LAT. The 10 Me",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12804878048780488,
          "p": 0.3442622950819672,
          "f": 0.18666666271446922
        },
        "rouge-2": {
          "r": 0.019011406844106463,
          "p": 0.06172839506172839,
          "f": 0.02906976384143501
        },
        "rouge-l": {
          "r": 0.10365853658536585,
          "p": 0.2786885245901639,
          "f": 0.15111110715891368
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2406.02623v2",
      "true_abstract": "Background: Limited universally-adopted data standards in veterinary medicine\nhinder data interoperability and therefore integration and comparison; this\nultimately impedes the application of existing information-based tools to\nsupport advancement in diagnostics, treatments, and precision medicine.\n  Objectives: A single, coherent, logic-based standard for documenting breed\nnames in health, production, and research-related records will improve data use\ncapabilities in veterinary and comparative medicine. Methods: The Vertebrate\nBreed Ontology (VBO) was created from breed names and related information\ncompiled from the Food and Agriculture Organization of the United Nations,\nbreed registries, communities, and experts, using manual and computational\napproaches. Each breed is represented by a VBO term that includes breed\ninformation and provenance as metadata. VBO terms are classified using\ndescription logic to allow computational applications and Artificial\nIntelligence-readiness.\n  Results: VBO is an open, community-driven ontology representing over 19,500\nlivestock and companion animal breed concepts covering 49 species. Breeds are\nclassified based on community and expert conventions (e.g., cattle breed) and\nsupported by relations to the breed's genus and species indicated by National\nCenter for Biotechnology Information (NCBI) Taxonomy terms. Relationships\nbetween VBO terms (e.g., relating breeds to their foundation stock) provide\nadditional context to support advanced data analytics. VBO term metadata\nincludes synonyms, breed identifiers/codes, and attributed cross-references to\nother databases.\n  Conclusion and clinical importance: The adoption of VBO as a source of\nstandard breed names in databases and veterinary electronic health records can\nenhance veterinary data interoperability and computability.",
      "generated_abstract": "The human brain is an exceptionally complex organ with a multitude of\nfunctions, which are controlled by a network of neurons. The brain is not\nsimply a collection of neurons; it is also a system of circuits that integrate\ninformation from multiple sources. This paper focuses on the representation of\nthe brain as a computational system, with the aim of using neural networks to\ndescribe the behavior of brain circuits. We discuss the challenges and\nopportunities associated with this task, including the need to account for\nvariability in the neural system, the necessity of modelling the brain as a\nnetwork of nodes, and the importance of considering the interactions between\nneurons. We also discuss the potential of deep learning techniques to assist in\nthe understanding of brain function, particularly by enabling the extraction of\nfeatures from complex data sets. We conclude by offering some thoughts on the\nfuture of this field, focusing on the potential",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13924050632911392,
          "p": 0.24444444444444444,
          "f": 0.17741935021462032
        },
        "rouge-2": {
          "r": 0.017543859649122806,
          "p": 0.028169014084507043,
          "f": 0.021621616891746832
        },
        "rouge-l": {
          "r": 0.13291139240506328,
          "p": 0.23333333333333334,
          "f": 0.16935483408558805
        }
      }
    },
    {
      "paper_id": "math.ST.stat/TH/2503.03567v1",
      "true_abstract": "We propose a new statistical hypothesis testing framework which decides\nvisually, using confidence intervals, whether the means of two samples are\nequal or if one is larger than the other. With our method, the user can at the\nsame time visualize the confidence region of the means and do a test to decide\nif the means of the two populations are significantly different or not by\nlooking whether the two confidence intervals overlap. To design this test we\nuse confidence intervals constructed using e-variables, which provide a measure\nof evidence in hypothesis testing. We propose both a sequential test and a\nnon-sequential test based on the overlap of confidence intervals and for each\nof these tests we give finite-time error bounds on the probabilities of error.\nWe also illustrate the practicality of our method by applying it to the\ncomparison of sequential learning algorithms.",
      "generated_abstract": "We study the statistical inference problem for estimating the average\ntemperature of a finite set of points, defined as\n$\\bar{T}=\\frac{1}{n}\\sum_{i=1}^n T_i$, where $T_i$ is the temperature at\nsite $i$ of the point cloud. The temperature is assumed to follow a\nBeta(2,2) distribution, with density\n$f_{T}(t)=\\frac{t^{\\alpha-1}}{(\\alpha-1)t^{\\alpha-2}}e^{-\\frac{t}{2}}$.\nTemperature estimation has applications in ecology, oceanography, and climate\nresearch. Existing methods for estimating the average temperature often\nrely on the central limit theorem, which requires the distribution to be\nGaussian. This approach is often inappropriate due to the non-Gaussianity of\ntemperature data. In this work, we propose a novel",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18604651162790697,
          "p": 0.22535211267605634,
          "f": 0.2038216510965963
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.044444444444444446,
          "f": 0.036036031214999424
        },
        "rouge-l": {
          "r": 0.1744186046511628,
          "p": 0.2112676056338028,
          "f": 0.1910827975934116
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.10845v1",
      "true_abstract": "Optimal experimental design (OED) is a framework that leverages a\nmathematical model of the experiment to identify optimal conditions for\nconducting the experiment. Under a Bayesian approach, the design objective\nfunction is typically chosen to be the expected information gain (EIG).\nHowever, EIG is intractable for nonlinear models and must be estimated\nnumerically. Estimating the EIG generally entails some variant of Monte Carlo\nsampling, requiring repeated data model and likelihood evaluations\n$\\unicode{x2013}$ each involving solving the governing equations of the\nexperimental physics $\\unicode{x2013}$ under different sample realizations.\nThis computation becomes impractical for high-fidelity models.\n  We introduce a novel multi-fidelity EIG (MF-EIG) estimator under the\napproximate control variate (ACV) framework. This estimator is unbiased with\nrespect to the high-fidelity mean, and minimizes variance under a given\ncomputational budget. We achieve this by first reparameterizing the EIG so that\nits expectations are independent of the data models, a requirement for\ncompatibility with ACV. We then provide specific examples under different data\nmodel forms, as well as practical enhancements of sample size optimization and\nsample reuse techniques. We demonstrate the MF-EIG estimator in two numerical\nexamples: a nonlinear benchmark and a turbulent flow problem involving the\ncalibration of shear-stress transport turbulence closure model parameters\nwithin the Reynolds-averaged Navier-Stokes model. We validate the estimator's\nunbiasedness and observe one- to two-orders-of-magnitude variance reduction\ncompared to existing single-fidelity EIG estimators.",
      "generated_abstract": "This paper presents an alternative to the classical Gaussian approximation\nof the normal distribution by leveraging the theory of the local limit\ntheory. In particular, we provide an extension of the classical Gaussian\napproximation to the case of non-Gaussian distributions. This new methodology\nis shown to be valid in the presence of a finite sample size and under the\nassumption of weak dependence between the observations. In addition, we\npresent a method for the estimation of the mean and covariance matrix of a\nnon-Gaussian distribution from its sample. The method is based on the\ninversion of a Gaussian integral. The asymptotic distribution of the estimator\nis characterized by a Weibull distribution with a non-linear parameter. Finally,\nwe discuss some empirical applications.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13013698630136986,
          "p": 0.2638888888888889,
          "f": 0.17431192218163466
        },
        "rouge-2": {
          "r": 0.027649769585253458,
          "p": 0.05454545454545454,
          "f": 0.036697243241777804
        },
        "rouge-l": {
          "r": 0.1232876712328767,
          "p": 0.25,
          "f": 0.16513761025502915
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.cond-mat/stat-mech/2503.09883v1",
      "true_abstract": "Van der Waals \"sliding\" ferroelectric bilayers, whose electric polarization\nis locked to the interlayer alignment, show promise for future non-volatile\nmemory and other nanoelectronic devices. These applications require a fuller\nunderstanding of the polarization stability and switching properties, which\npresent models have described in terms of an Ising-like binary polarization.\nHowever, it is a much larger translation symmetry that is broken in the polar\nstate. Here we introduce a discrete statistical-mechanical model that\nemphasizes the effect of this larger symmetry. Through Monte-Carlo numerics we\nshow this model possesses a richer phase diagram, including an intermediate\ncritical phase of algebraically-correlated polarization. A low energy effective\ntheory allows us to connect the ferroelectric-paraelectric transition to the\nBerezinskii-Kosterlitz-Thouless class, driven by excitations not available in\nIsing-like models. Our results indicate the need for theoretical models of this\nferroelectric system to account for the larger symmetry.",
      "generated_abstract": "We propose a simple and general method to characterize the critical phenomena of\nan ensemble of interacting systems. This method is based on the definition of\nthe so-called critical set, which is the set of states that are critical in the\nensemble. This critical set is characterized by a characteristic function,\nwhich we call the critical probability. The critical set is defined in terms\nof the critical probability and can be constructed using only the local\nproperties of the system. We then show how the critical set can be used to\ncharacterize the critical behavior of the ensemble. We show that the\ncritical set is invariant under local operations and that, in particular, the\ncritical set is invariant under transformations of the local properties of\nthe system. We also show that the critical set is invariant under local\noperations of the type of a transformation of the local properties of the\nsystem. In addition, we show that the critical set is invariant under\nlocal-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16161616161616163,
          "p": 0.24615384615384617,
          "f": 0.19512194643441416
        },
        "rouge-2": {
          "r": 0.043478260869565216,
          "p": 0.05454545454545454,
          "f": 0.04838709183792975
        },
        "rouge-l": {
          "r": 0.1414141414141414,
          "p": 0.2153846153846154,
          "f": 0.17073170253197514
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/MN/2409.19294v3",
      "true_abstract": "Agent-based models capture heterogeneity among individuals in a population\nand are widely used in studies of multi-cellular systems, disease, epidemics\nand demography to name a few. However, existing frameworks consider discrete\ntime-step simulation or assume that agents' states only change as a result of\ndiscrete events. In this note, we present AgentBasedModeling$.$jl, a Julia\npackage for simulating stochastic agent-based population models in continuous\ntime. The tool allows to easily specify and simulate agents evolving through\ngeneric continuous-time jump-diffusions and interacting via continuous-rate\nprocesses. AgentBasedModeling$.$jl provides a powerful methodology for studying\nthe effects of stochasticity on structured population dynamics.",
      "generated_abstract": "The concept of a bacterial phage is a generalization of the concept of a\nvirus, which has been the foundation of the field of microbiology. In the\npast, the concept of a bacterial phage was only restricted to bacteria. However,\nin the recent past, the concept of a bacterial phage was extended to include\nany microorganism, including bacteria, archaea, and fungi, that is capable of\ninfecting other microorganisms. This conceptual extension of the concept of a\nbacterial phage has led to the development of a large body of knowledge on\nbacterial phage. This paper provides an overview of the concept of a bacterial\nphage, and presents a review of the literature on bacterial phage. The paper\nalso discusses some of the limitations of the concept of a bacterial phage,\nand",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15853658536585366,
          "p": 0.21311475409836064,
          "f": 0.18181817692601115
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13414634146341464,
          "p": 0.18032786885245902,
          "f": 0.15384614895398327
        }
      }
    },
    {
      "paper_id": "cs.DS.stat/TH/2503.06464v1",
      "true_abstract": "Consider a pair of sparse correlated stochastic block models $\\mathcal\nS(n,\\tfrac{\\lambda}{n},\\epsilon;s)$ subsampled from a common parent stochastic\nblock model with two symmetric communities, average degree $\\lambda=O(1)$ and\ndivergence parameter $\\epsilon \\in (0,1)$. For all $\\epsilon\\in(0,1)$, we\nconstruct a statistic based on the combination of two low-degree polynomials\nand show that there exists a sufficiently small constant\n$\\delta=\\delta(\\epsilon)>0$ and a sufficiently large constant\n$\\Delta=\\Delta(\\epsilon,\\delta)$ such that when $\\lambda>\\Delta$ and\n$s>\\sqrt{\\alpha}-\\delta$ where $\\alpha\\approx 0.338$ is Otter's constant, this\nstatistic can distinguish this model and a pair of independent stochastic block\nmodels $\\mathcal S(n,\\tfrac{\\lambda s}{n},\\epsilon)$ with probability $1-o(1)$.\nWe also provide an efficient algorithm that approximates this statistic in\npolynomial time. The crux of our statistic's construction lies in a carefully\ncurated family of multigraphs called \\emph{decorated trees}, which enables\neffective aggregation of the community signal and graph correlation from the\ncounts of the same decorated tree while suppressing the undesirable\ncorrelations among counts of different decorated trees.",
      "generated_abstract": "We present a novel approach for the estimation of the distribution of the\nrandom variables of interest in the Bayesian nonparametric regression setting.\nThis approach is based on the application of the theory of random variables in\nthe Bayesian framework. It allows to efficiently estimate the parameters of the\nBayesian nonparametric regression model using the estimator of the joint\ndensity function of the random variables of interest. We prove the\nasymptotic normality of the proposed estimator and demonstrate its\ncomputational efficiency through numerical simulations.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08928571428571429,
          "p": 0.20408163265306123,
          "f": 0.12422359825006765
        },
        "rouge-2": {
          "r": 0.020833333333333332,
          "p": 0.04477611940298507,
          "f": 0.02843601462321218
        },
        "rouge-l": {
          "r": 0.08928571428571429,
          "p": 0.20408163265306123,
          "f": 0.12422359825006765
        }
      }
    },
    {
      "paper_id": "stat.CO.stat/CO/2501.07795v1",
      "true_abstract": "Black-box optimization is often encountered for decision-making in complex\nsystems management, where the knowledge of system is limited. Under these\ncircumstances, it is essential to balance the utilization of new information\nwith computational efficiency. In practice, decision-makers often face the dual\ntasks of optimization and statistical inference for the optimal performance, in\norder to achieve it with a high reliability. Our goal is to address the dual\ntasks in an online fashion. Wu et al (2022) [arXiv preprint: 2210.06737] point\nout that the sample average of performance estimates generated by the\noptimization algorithm needs not to admit a central limit theorem. We propose\nan algorithm that not only tackles this issue, but also provides an online\nconsistent estimator for the variance of the performance. Furthermore, we\ncharacterize the convergence rate of the coverage probabilities of the\nasymptotic confidence intervals.",
      "generated_abstract": "This paper studies the problem of estimating a parameter of a nonlinear\noptimization model using a small number of measurements from a large number of\ndifferent optimization problems. The goal is to minimize the total cost of\noptimization problems, which includes both the cost of the optimization and\nthe cost of data collection. We propose a Bayesian optimization algorithm\nbased on the expectation-maximization algorithm. Our algorithm is designed to\nbe data-efficient, and we show that it is able to estimate the parameter with\nhigh probability even when the number of optimization problems is very large.\nWe also show that our method is able to estimate the parameter with high\nprobability even when the number of optimization problems is very small. We\nprovide theoretical guarantees for the efficiency of our algorithm, and we\ndemonstrate its effectiveness in an empirical study.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22448979591836735,
          "p": 0.3013698630136986,
          "f": 0.25730993662733836
        },
        "rouge-2": {
          "r": 0.08270676691729323,
          "p": 0.09821428571428571,
          "f": 0.08979591340408191
        },
        "rouge-l": {
          "r": 0.22448979591836735,
          "p": 0.3013698630136986,
          "f": 0.25730993662733836
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2412.03618v3",
      "true_abstract": "In today's complex and volatile financial market environment, risk management\nof multi-asset portfolios faces significant challenges. Traditional risk\nassessment methods, due to their limited ability to capture complex\ncorrelations between assets, find it difficult to effectively cope with dynamic\nmarket changes. This paper proposes a multi-asset portfolio risk prediction\nmodel based on Convolutional Neural Networks (CNN). By utilizing image\nprocessing techniques, financial time series data are converted into\ntwo-dimensional images to extract high-order features and enhance the accuracy\nof risk prediction. Through empirical analysis of data from multiple asset\nclasses such as stocks, bonds, commodities, and foreign exchange, the results\nshow that the proposed CNN model significantly outperforms traditional models\nin terms of prediction accuracy and robustness, especially under extreme market\nconditions. This research provides a new method for financial risk management,\nwith important theoretical significance and practical value.",
      "generated_abstract": "This paper introduces a new method for estimating the expected value of\nthe log-return of a portfolio of assets, known as the log-beta risk\nmeasure. The log-beta risk measure is defined as the logarithm of the\nexpected value of the log-return of the portfolio, and has the property that\nits variance is equal to the variance of the log-return of the portfolio. We\nprove that the log-beta risk measure is a quasi-variance-free risk measure, and\nprove the consistency of this quasi-variance-free risk measure with respect to\nthe Sharpe ratio. We also show that the log-beta risk measure is a\nquasi-moment-generating function, and show how to estimate the log-beta\nrisk-neutral probability measure by solving the inverse Cram\\'er-Lundberg\nequation. Finally, we show that the log-beta",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1651376146788991,
          "p": 0.3103448275862069,
          "f": 0.2155688577417621
        },
        "rouge-2": {
          "r": 0.043795620437956206,
          "p": 0.06666666666666667,
          "f": 0.05286343133769378
        },
        "rouge-l": {
          "r": 0.1651376146788991,
          "p": 0.3103448275862069,
          "f": 0.2155688577417621
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.10838v1",
      "true_abstract": "Generalizable deepfake detection can be formulated as a detection problem\nwhere labels (bonafide and fake) are fixed but distributional drift affects the\ndeepfake set. We can always train our detector with one-selected attacks and\nbonafide data, but an attacker can generate new attacks by just retraining his\ngenerator with a different seed. One reasonable approach is to simply pool all\ndifferent attack types available in training time. Our proposed approach is to\nutilize meta-learning in combination with LoRA adapters to learn the structure\nin the training data that is common to all attack types.",
      "generated_abstract": "This paper presents a novel method for estimating the relative phase of\nmixed-signal integrated circuits (ISICs) using the finite element method (FEM).\nThe proposed method is based on a modified version of the fast Fourier\ntransform (FFT) and employs the Fast Fourier Transform (FFT) with an adaptive\nwindow size technique. The FFT window size adapts according to the size of the\nFFT FFT window size, which reduces the computational complexity of the FFT.\nThis technique reduces the computational complexity of the FFT and enables\nefficient FFT processing. Furthermore, the adaptive window size technique\nensures that the FFT window size is not reduced to zero, which is an issue\nassociated with the FFT. The adaptive window size technique also improves the\naccuracy of the phase estimate and reduces the computational complexity of the\nphase estimation. The effectiveness of the proposed method was valid",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.13043478260869565,
          "f": 0.13043477760869585
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.13043478260869565,
          "f": 0.13043477760869585
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2411.01315v1",
      "true_abstract": "Harsanyi (1955) showed that the only way to aggregate individual preferences\ninto a social preference which satisfies certain desirable properties is\n``utilitarianism'', whereby the social utility function is a weighted average\nof individual utilities. This representation forms the basis for welfare\nanalysis in most applied work. We argue, however, that welfare analysis based\non Harsanyi's version of utilitarianism may overlook important distributional\nconsiderations. We therefore introduce a notion of utilitarianism for\ndiscrete-choice settings which applies to \\textit{social choice functions},\nwhich describe the actions of society, rather than social welfare functions\nwhich describe society's preferences (as in Harsanyi). We characterize a\nrepresentation of utilitarian social choice, and show that it provides a\nfoundation for a family of \\textit{distributional welfare measures} based on\nquantiles of the distribution of individual welfare effects, rather than\naverages.",
      "generated_abstract": "This paper investigates the effect of incentives on the welfare of a group of\ndifferentiated consumers who are not willing to pay for the environmental\nbenefits of a common good. We develop a stochastic model of a market where\nconsumers can choose between two consumption patterns: one that includes the\nenvironmental benefits and one that does not. When consumers have different\npreferences for the environmental benefits, the model allows for the possibility\nof incentives to achieve a compromise between the two consumption patterns.\nThis compromise is achieved by the introduction of a welfare-neutral\nenvironmental incentive. We show that the incentive can be implemented through\na tax on carbon emissions. We provide an analytical solution for the\ndistribution of the incentive and show that the incentive is optimal if the\nconsumers have different utility functions. We also provide a simple example\nto illustrate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19767441860465115,
          "p": 0.22077922077922077,
          "f": 0.20858895207045816
        },
        "rouge-2": {
          "r": 0.048,
          "p": 0.04838709677419355,
          "f": 0.04819276608441851
        },
        "rouge-l": {
          "r": 0.1744186046511628,
          "p": 0.19480519480519481,
          "f": 0.18404907476984467
        }
      }
    },
    {
      "paper_id": "cs.SE.cs/SE/2503.09510v1",
      "true_abstract": "Code Review consists in assessing the code written by teammates with the goal\nof increasing code quality. Empirical studies documented the benefits brought\nby such a practice that, however, has its cost to pay in terms of developers'\ntime. For this reason, researchers have proposed techniques and tools to\nautomate code review tasks such as the reviewers selection (i.e., identifying\nsuitable reviewers for a given code change) or the actual review of a given\nchange (i.e., recommending improvements to the contributor as a human reviewer\nwould do). Given the substantial amount of papers recently published on the\ntopic, it may be challenging for researchers and practitioners to get a\ncomplete overview of the state-of-the-art.\n  We present a systematic literature review (SLR) featuring 119 papers\nconcerning the automation of code review tasks. We provide: (i) a\ncategorization of the code review tasks automated in the literature; (ii) an\noverview of the under-the-hood techniques used for the automation, including\nthe datasets used for training data-driven techniques; (iii) publicly available\ntechniques and datasets used for their evaluation, with a description of the\nevaluation metrics usually adopted for each task.\n  The SLR is concluded by a discussion of the current limitations of the\nstate-of-the-art, with insights for future research directions.",
      "generated_abstract": "While deep learning has become a popular tool in engineering applications,\nthe model's final output is often not as interpretable as it could be. The\ncommon practice of replacing the model output with a visualization like a\nheatmap can be misleading and leads to ambiguity in interpreting the model's\noutput. Instead, we propose a simple approach of replacing the model output\nwith the logit output from the last layer of the model. This approach is\nstraightforward and can be implemented by any machine learning model.\nAdditionally, we introduce a new metric called Cohen's D for evaluating the\naccuracy of logit-based visualizations. We show that this metric is more\naccurate than existing metrics and can be used for more effective visualization\ndesign.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15267175572519084,
          "p": 0.25316455696202533,
          "f": 0.19047618578276654
        },
        "rouge-2": {
          "r": 0.020942408376963352,
          "p": 0.03636363636363636,
          "f": 0.026578068451783902
        },
        "rouge-l": {
          "r": 0.13740458015267176,
          "p": 0.22784810126582278,
          "f": 0.17142856673514753
        }
      }
    },
    {
      "paper_id": "physics.hist-ph.physics/hist-ph/2503.05668v1",
      "true_abstract": "This paper concerns the question of which collections of general relativistic\nspacetimes are deterministic relative to which definitions. We begin by\nconsidering a series of three definitions of increasing strength due to Belot\n(1995). The strongest of these definitions is particularly interesting for\nspacetime theories because it involves an asymmetry condition called\n``rigidity'' that has been studied previously in a different context (Geroch\n1969; Halvorson and Manchak 2022; Dewar 2024). We go on to explore other\n(stronger) asymmetry conditions that give rise to other (stronger) forms of\ndeterminism. We introduce a number of definitions of this type and clarify the\nrelationships between them and the three considered by Belot. We go on to show\nthat there are collections of general relativistic spacetimes that satisfy much\nstronger forms of determinism than previously known. We also highlight a number\nof open questions.",
      "generated_abstract": "A simple model of the spread of a social phenomenon is introduced. This\nmodel can be applied to explain any phenomenon, from the spread of a disease to\nthe spread of an idea. It is based on the idea that the spread of a\nphenomenon is a consequence of the spread of the phenomenon. The model is\nsimplified by considering only two degrees of freedom: the spreading speed and\nthe phenomenon. The speed of the phenomenon is a function of its initial\nconditions. The speed of the phenomenon is a function of the initial conditions\nof the phenomenon. The speed of the phenomenon is a function of the initial\nconditions of the phenomenon. The speed of the phenomenon is a function of the\ninitial conditions of the phenomenon. The speed of the phenomenon is a\nfunction of the initial conditions of the phenomenon. The speed of the\nphenomenon is a function of the initial conditions of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.32558139534883723,
          "f": 0.20895521952216542
        },
        "rouge-2": {
          "r": 0.015873015873015872,
          "p": 0.030303030303030304,
          "f": 0.02083332882161556
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.3023255813953488,
          "f": 0.19402984638783705
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.02344v1",
      "true_abstract": "The recently emerged movable antenna (MA) shows great promise in leveraging\nspatial degrees of freedom to enhance the performance of wireless systems.\nHowever, resource allocation in MA-aided systems faces challenges due to the\nnonconvex and coupled constraints on antenna positions. This paper\nsystematically reveals the challenges posed by the minimum antenna separation\ndistance constraints. Furthermore, we propose a penalty optimization framework\nfor resource allocation under such new constraints for MA-aided systems.\nSpecifically, the proposed framework separates the non-convex and coupled\nantenna distance constraints from the movable region constraints by introducing\nauxiliary variables. Subsequently, the resulting problem is efficiently solved\nby alternating optimization, where the optimization of the original variables\nresembles that in conventional resource allocation problem while the\noptimization with respect to the auxiliary variables is achieved in closedform\nsolutions. To illustrate the effectiveness of the proposed framework, we\npresent three case studies: capacity maximization, latency minimization, and\nregularized zero-forcing precoding. Simulation results demonstrate that the\nproposed optimization framework consistently outperforms state-of-the-art\nschemes.",
      "generated_abstract": "This paper presents a novel deep learning-based method for audio source\nchallenge detection in multi-channel speech signals. Inspired by the concept of\nattention mechanisms in neural networks, we propose a hybrid attention-based\ndetection framework that uses both conventional LSTM-based and residual LSTM-based\nmodels to capture temporal contextual information in the audio signal. Our\nframework utilizes a ResNet-based model for the audio feature extraction,\nwhich is then processed by an attention mechanism to generate a weighted\nattention map. This attention map is then used as the input to a single-channel\nattention model, where the attention weights are adjusted by the residual\nconnection to generate the final detection results. In addition, we propose a\nnovel weighted attention mechanism that incorporates the attention weights of\nthe previous frame into the current frame, thereby improving the stability of\nthe model. Experimental",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17307692307692307,
          "p": 0.21428571428571427,
          "f": 0.1914893567587145
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.04878048780487805,
          "f": 0.04395603900495166
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.19047619047619047,
          "f": 0.17021276101403365
        }
      }
    },
    {
      "paper_id": "math.PR.stat/OT/2411.03955v1",
      "true_abstract": "We provide bounds on the tail probabilities for simple procedures that\ngenerate random samples _without replacement_, when the probabilities of being\nselected need not be equal.",
      "generated_abstract": "In this paper we give an elementary proof of the existence of the unique\nsolution to the system of partial differential equations\n\\begin{equation*}\n\\partial_t u_t + \\Delta u_t = 0,\n\\end{equation*}\nwith $u_t(0,x)=u(0,x)$, and the uniqueness of the solution for general $u$\n(even non-smooth). We also show that the unique solution is given by the\nsolution to the system\n\\begin{equation*}\n\\partial_t u_t + \\Delta u_t = 0,\n\\end{equation*}\nwith $u_t(0,x)=0$ for all $x\\in\\mathbb{R}^n$, and the uniqueness of the\nsolution for general $u$. This is the first proof of the uniqueness of the\nsolution to the system of partial differential equations with",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20833333333333334,
          "p": 0.10638297872340426,
          "f": 0.14084506594723284
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.0851063829787234,
          "f": 0.11267605186272583
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2412.03554v1",
      "true_abstract": "We model stochastic choices with categorization, resulting from the\npreliminary step of grouping alternatives in homogenous disjoint classes. The\nagent randomly chooses one class among those available, then randomly picks an\nitem within the selected class. We give a formal definition of a choice\ngenerated by this procedure, and provide a characterization. The characterizing\nproperties allow an external observer to elicit that categorization is applied.\nIn a more general interpretation, the model allows to describe the observed\nchoice as the composition of independent subchoices. This composition preserves\nrationalizability by random utility maximization. A generalization of the model\nsubsumes Luce model and Nested Logit.",
      "generated_abstract": "This paper develops a theoretical framework for studying the relationship\nbetween the production function and the welfare function, with a focus on\nthe classical production function model with a fixed number of output\nvariables. We show that, under certain assumptions, this model exhibits\nsublinear separability, and that its separability is stable with respect to\nchanges in the production function, welfare function, and the number of\noutputs. We then use this result to investigate the conditions under which\nseparability can be achieved with respect to changes in the production function\nand welfare function. We also consider a situation in which the production\nfunction and welfare function are not separable, but exhibit sublinear\nseparability. Finally, we use our framework to explore the role of the\nnumber of outputs in production functions with sublinear separability.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17721518987341772,
          "p": 0.2028985507246377,
          "f": 0.18918918421201622
        },
        "rouge-2": {
          "r": 0.009900990099009901,
          "p": 0.009259259259259259,
          "f": 0.009569372996042073
        },
        "rouge-l": {
          "r": 0.17721518987341772,
          "p": 0.2028985507246377,
          "f": 0.18918918421201622
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2408.01470v1",
      "true_abstract": "In order to overcome the drawbacks of assuming deterministic volatility\ncoefficients in the standard LIBOR market models to capture volatility smiles\nand skews in real markets, several extensions of LIBOR models to incorporate\nstochastic volatilities have been proposed. The efficient calibration to market\ndata of these more complex models becomes a relevant target in practice. The\nmain objective of the present work is to efficiently calibrate some recent\nSABR/LIBOR market models to real market prices of caplets and swaptions. For\nthe calibration we propose a parallelized version of the simulated annealing\nalgorithm for multi-GPUs. The numerical results clearly illustrate the\nadvantages of using the proposed multi-GPUs tools when applied to real market\ndata and popular SABR/LIBOR models.",
      "generated_abstract": "This paper presents a novel framework for modeling and calibrating\neconomic models in the presence of heterogeneous agents with varying\nperceptions of risk. We first propose a new class of stochastic volatility\nmodels, which incorporate heterogeneous beliefs about risk into a unified\nframework. To model the impact of heterogeneous beliefs on market dynamics, we\nintroduce an extension of the standard mean-variance framework that allows for\nheterogeneous risk aversion and risk-neutral probabilities. Using a\nsimulation-based approach, we demonstrate the ability of the proposed model to\ncapture the impact of heterogeneous beliefs on market dynamics. Our\nsimulation results suggest that the proposed model can capture the\nsignificant impact of heterogeneous beliefs on market dynamics, particularly in\nregards to the impact of heterogeneous beliefs on volatility and market\nconcentration. Furthermore, we show that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24,
          "p": 0.2465753424657534,
          "f": 0.2432432382441564
        },
        "rouge-2": {
          "r": 0.05504587155963303,
          "p": 0.05660377358490566,
          "f": 0.055813948489346045
        },
        "rouge-l": {
          "r": 0.22666666666666666,
          "p": 0.2328767123287671,
          "f": 0.22972972473064293
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/BM/2502.13344v1",
      "true_abstract": "Drug discovery is a complex and time-intensive process that requires\nidentifying and validating new therapeutic candidates. Computational approaches\nusing large-scale biomedical knowledge graphs (KGs) offer a promising solution\nto accelerate this process. However, extracting meaningful insights from\nlarge-scale KGs remains challenging due to the complexity of graph traversal.\nExisting subgraph-based methods are tailored to graph neural networks (GNNs),\nmaking them incompatible with other models, such as large language models\n(LLMs). We introduce K-Paths, a retrieval framework that extracts structured,\ndiverse, and biologically meaningful paths from KGs. Integrating these paths\nenables LLMs and GNNs to effectively predict unobserved drug-drug and\ndrug-disease interactions. Unlike traditional path-ranking approaches, K-Paths\nretrieves and transforms paths into a structured format that LLMs can directly\nprocess, facilitating explainable reasoning. K-Paths employs a diversity-aware\nadaptation of Yen's algorithm to retrieve the K shortest loopless paths between\nentities in an interaction query, prioritizing biologically relevant and\ndiverse relationships. Our experiments on benchmark datasets show that K-Paths\nimproves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on\ndrug repurposing and 13.42 points on interaction severity prediction. We also\nshow that Llama 70B achieves F1-score gains of 6.18 and 8.46 points,\nrespectively. K-Paths also improves the supervised training efficiency of\nEmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining\nstrong predictive performance. Beyond its scalability and efficiency, K-Paths\nuniquely bridges the gap between KGs and LLMs, providing explainable rationales\nfor predicted interactions. These capabilities show that K-Paths is a valuable\ntool for efficient data-driven drug discovery.",
      "generated_abstract": "This work introduces the concept of a coarse-grained model, where a\nmodel is decomposed into a series of sub-models that are interconnected by\nmicroscopic interactions. The concept of coarse-graining has been used in\ndifferent fields of science and engineering, including physics, chemistry, and\nbiology. The coarse-grained model is constructed by connecting sub-models and\ninterconnecting them with interactions that are broad enough to encompass\nmolecular interactions, but fine enough to capture local interactions. This\nwork aims to provide a framework for modeling coarse-grained systems in a\npractical and computationally efficient manner. We start by defining the\ncoarse-grained model, followed by a brief review of the concept of coarse-graining\nin the context of physics and chemistry. We then introduce a new concept of\ncoarse-grained modeling,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1092896174863388,
          "p": 0.26666666666666666,
          "f": 0.15503875556607186
        },
        "rouge-2": {
          "r": 0.004032258064516129,
          "p": 0.008928571428571428,
          "f": 0.00555555126913911
        },
        "rouge-l": {
          "r": 0.1092896174863388,
          "p": 0.26666666666666666,
          "f": 0.15503875556607186
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2411.09657v1",
      "true_abstract": "We study the tail asymptotics of the sum of two heavy-tailed random\nvariables. The dependence structure is modeled by copulas with the so-called\ntail order property. Examples are presented to illustrate the approach. Further\nfor each example we apply the main results to obtain the asymptotic expansions\nfor Value-at-Risk of aggregate risk.",
      "generated_abstract": "We propose a novel approach for modeling volatility of asset returns based on\nthe so-called CIR-type models, which can be interpreted as a generalization of\nthe GARCH models. In our approach, the volatility is modelled as a\nnon-linear functional of the realized volatility of the asset returns. We\nshow that the CIR-type models can be regarded as special cases of the\nautoregressive-moving-average (ARMA) models with the generalized autoregressive\ncontrol-impulse-response (GARCH-CIR) structure. We provide a detailed\nnumerical analysis of the CIR-type models. In particular, we show that the\nCIR-type models with various degrees of smoothing, the ARMA-GARCH-CIR models\nwith various degrees of smoothing, and the ARMA-GARCH-CIR-M",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23809523809523808,
          "p": 0.1724137931034483,
          "f": 0.19999999512800012
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.03614457831325301,
          "f": 0.04477611468812703
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.15517241379310345,
          "f": 0.17999999512800013
        }
      }
    },
    {
      "paper_id": "math.DS.math/DS/2503.10145v1",
      "true_abstract": "We discuss the qualitatively new properties of random walks on groups that\narise in the situation when the entropy of the step distribution is infinite.",
      "generated_abstract": "We prove the existence of a weak solution to the one-dimensional (1D)\nstratified fluid-elasticity system with nonlinear elasticity in a bounded\nhalf-space. The system includes the 1D Navier-Stokes equation, the linearized\nelasticity equation, and an additional nonlinear elasticity equation that is\nlinearized about the uniform deformation of the half-space. We use a\nduality argument to prove that the linearized elasticity equation is\nwell-posed. The proof leverages the duality between the linearized elasticity\nequation and the 1D Navier-Stokes equation in a bounded domain, and the\nwell-posedness of the linearized elasticity equation. We then establish the\nexistence of a weak solution to the original fluid-elasticity system. We\nconclude the paper with a numerical experiment.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2857142857142857,
          "p": 0.11764705882352941,
          "f": 0.16666666253472232
        },
        "rouge-2": {
          "r": 0.041666666666666664,
          "p": 0.011494252873563218,
          "f": 0.018018014628683375
        },
        "rouge-l": {
          "r": 0.23809523809523808,
          "p": 0.09803921568627451,
          "f": 0.13888888475694455
        }
      }
    },
    {
      "paper_id": "cs.LG.eess/SP/2503.04278v1",
      "true_abstract": "This study addresses the challenge of access point (AP) and user equipment\n(UE) association in cell-free massive MIMO networks. It introduces a deep\nlearning algorithm leveraging Bidirectional Long Short-Term Memory cells and a\nhybrid probabilistic methodology for weight updating. This approach enhances\nscalability by adapting to variations in the number of UEs without requiring\nretraining. Additionally, the study presents a training methodology that\nimproves scalability not only with respect to the number of UEs but also to the\nnumber of APs. Furthermore, a variant of the proposed AP-UE algorithm ensures\nrobustness against pilot contamination effects, a critical issue arising from\npilot reuse in channel estimation. Extensive numerical results validate the\neffectiveness and adaptability of the proposed methods, demonstrating their\nsuperiority over widely used heuristic alternatives.",
      "generated_abstract": "With the rapid development of deep learning, audio speech recognition has\nbecome one of the most crucial tasks in speech processing. In this paper, we\nintroduce a novel audio speech recognition model, dubbed as\n\\textbf{Wave-Transformer}, based on waveform-to-text.\n  We propose a novel Wave-Transformer model, which is composed of a Wave-Transformer\nencoder and a Text-to-Wave decoder. The encoder employs a waveform embedding\nlayer to transform the input audio signal into a sequence of waveform\ncoefficients, and the decoder converts the waveform coefficients into the\npredicted text sequence. The model is trained end-to-end with a novel\nunsupervised loss function, which is based on the waveform contrastive\nlearning (WCL) framework. The proposed model is validated on the VoxCLIP\ndataset, achieving",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10752688172043011,
          "p": 0.13513513513513514,
          "f": 0.11976047410663723
        },
        "rouge-2": {
          "r": 0.017241379310344827,
          "p": 0.018691588785046728,
          "f": 0.017937214739087236
        },
        "rouge-l": {
          "r": 0.10752688172043011,
          "p": 0.13513513513513514,
          "f": 0.11976047410663723
        }
      }
    },
    {
      "paper_id": "cs.SI.stat/ML/2503.02271v1",
      "true_abstract": "Experiments in online platforms frequently suffer from network interference,\nin which a treatment applied to a given unit affects outcomes for other units\nconnected via the platform. This SUTVA violation biases naive approaches to\nexperiment design and estimation. A common solution is to reduce interference\nby clustering connected units, and randomizing treatments at the cluster level,\ntypically followed by estimation using one of two extremes: either a simple\ndifference-in-means (DM) estimator, which ignores remaining interference; or an\nunbiased Horvitz-Thompson (HT) estimator, which eliminates interference at\ngreat cost in variance. Even combined with clustered designs, this presents a\nlimited set of achievable bias variance tradeoffs. We propose a new estimator,\ndubbed Differences-in-Neighbors (DN), designed explicitly to mitigate network\ninterference. Compared to DM estimators, DN achieves bias second order in the\nmagnitude of the interference effect, while its variance is exponentially\nsmaller than that of HT estimators. When combined with clustered designs, DN\noffers improved bias-variance tradeoffs not achievable by existing approaches.\nEmpirical evaluations on a large-scale social network and a city-level\nride-sharing simulator demonstrate the superior performance of DN in\nexperiments at practical scale.",
      "generated_abstract": "We present a new method for building multimodal graph-based models that\nutilize both graph-level and node-level information. This is achieved by\nformulating the problem as a combination of the graph completion task and\nnode prediction task. The proposed method, named GMP-GCN, is a multimodal\ngraph-based model that incorporates graph-level and node-level information to\nleverage both the structural and textual information for graph-level\ncompletion and node prediction tasks. The proposed method is evaluated on\nbenchmark datasets, including SAN, SAN+ and SAN++, and compared against the\nstate-of-the-art methods. The experimental results show that our model\nsignificantly outperforms existing methods in terms of F1-score, average\nprecision, and average recall, demonstrating its superiority in graph-level\ncompletion and node prediction tasks.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12213740458015267,
          "p": 0.2222222222222222,
          "f": 0.15763546340265489
        },
        "rouge-2": {
          "r": 0.011235955056179775,
          "p": 0.019417475728155338,
          "f": 0.01423487080102987
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.2222222222222222,
          "f": 0.15763546340265489
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.01075v1",
      "true_abstract": "Hallucinations are spurious structures not present in the ground truth,\nposing a critical challenge in medical image reconstruction, especially for\ndata-driven conditional models. We hypothesize that combining an unconditional\ndiffusion model with data consistency, trained on a diverse dataset, can reduce\nthese hallucinations. Based on this, we propose DynamicDPS, a diffusion-based\nframework that integrates conditional and unconditional diffusion models to\nenhance low-quality medical images while systematically reducing\nhallucinations. Our approach first generates an initial reconstruction using a\nconditional model, then refines it with an adaptive diffusion-based inverse\nproblem solver. DynamicDPS skips early stage in the reverse process by\nselecting an optimal starting time point per sample and applies Wolfe's line\nsearch for adaptive step sizes, improving both efficiency and image fidelity.\nUsing diffusion priors and data consistency, our method effectively reduces\nhallucinations from any conditional model output. We validate its effectiveness\nin Image Quality Transfer for low-field MRI enhancement. Extensive evaluations\non synthetic and real MR scans, including a downstream task for tissue volume\nestimation, show that DynamicDPS reduces hallucinations, improving relative\nvolume estimation by over 15% for critical tissues while using only 5% of the\nsampling steps required by baseline diffusion models. As a model-agnostic and\nfine-tuning-free approach, DynamicDPS offers a robust solution for\nhallucination reduction in medical imaging. The code will be made publicly\navailable upon publication.",
      "generated_abstract": "The increasing complexity of medical images necessitates the development of\nnew image processing methods. This study presents a novel approach based on\nmultimodal deep learning for brain tumor detection in MRI images. The proposed\nframework combines a three-stage convolutional neural network with a\nmulti-task learning strategy to enhance tumor detection accuracy. The first\nstage employs a convolutional neural network to extract features from MRI\nimages. The second stage uses a recurrent neural network to generate features\nfor subsequent stages. Finally, a multi-task learning strategy is employed to\nintegrate the tumor detection and segmentation tasks. The proposed method was\nevaluated on a large dataset of 1052 MRI images and compared with the\nstate-of-the-art method. The results show that the proposed method outperforms\nthe state-of-the-art method in terms of accuracy and sensitivity. The proposed\nmethod",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14465408805031446,
          "p": 0.3026315789473684,
          "f": 0.19574467647478508
        },
        "rouge-2": {
          "r": 0.018779342723004695,
          "p": 0.03508771929824561,
          "f": 0.024464827262576944
        },
        "rouge-l": {
          "r": 0.13836477987421383,
          "p": 0.2894736842105263,
          "f": 0.18723403817691275
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2412.03305v1",
      "true_abstract": "An investment portfolio consists of $n$ algorithmic trading strategies, which\ngenerate vectors of positions in trading assets. Sign opposite trades\n(buy/sell) cross each other as strategies are combined in a portfolio. Then\nportfolio turnover becomes a non linear function of strategies turnover. It\nrises a problem of effective (quick and precise) portfolio turnover estimation.\nKakushadze and Liew (2014) shows how to estimate turnover via covariance matrix\nof returns. We build a mathematical model for such estimations; prove a theorem\nwhich gives a necessary condition for model applicability; suggest new turnover\nestimations; check numerically the preciseness of turnover estimations for\nalgorithmic strategies on USA equity market.",
      "generated_abstract": "We propose a novel methodology for trading and risk management in cryptocurrencies\nwithin a risk-parity framework. The proposed approach is based on the\nrelationship between cryptocurrency prices and the supply of new coins,\nhighlighting the fundamental role of supply in shaping the market's direction.\nThe methodology leverages the use of the BTC-USD cryptocurrency pair as a\nbenchmark to trade and manage risk within a risk-parity framework. It aims to\nenhance the efficiency and effectiveness of trading in cryptocurrencies by\nadapting the traditional risk-parity approach to the specificities of the\ncryptocurrency market. We provide a practical guide to the implementation of\nthe methodology, including the trading strategy, risk management strategies,\nand trading and risk management tools. The methodology is based on a\ncorrelation-based trading strategy",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19736842105263158,
          "p": 0.22727272727272727,
          "f": 0.21126760065859962
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.19736842105263158,
          "p": 0.22727272727272727,
          "f": 0.21126760065859962
        }
      }
    },
    {
      "paper_id": "math.KT.math/KT/2503.04251v1",
      "true_abstract": "We prove separation and excision results in functor homology. These results\nexplain how the global Steinberg decomposition of functors proved by Djament,\nTouz{\\'e} and Vespa behaves in Ext and Tor computations.",
      "generated_abstract": "We study the generalized K\\\"ahler-Ricci flow in the setting of K\\\"ahler\nmanifolds with constant sectional curvature, when the metric is a product\nmetric. We show that for any positive number $\\epsilon$, there exists a unique\nconstant $T_{\\epsilon}$ such that the flow remains hyperbolic on the interval\n$[0,T_{\\epsilon})$. Furthermore, we show that $T_{\\epsilon}$ is a critical\npoint of the energy functional in this setting. We also provide a complete\ncharacterization of the critical points of the energy functional in this\nsetting.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14814814814814814,
          "p": 0.08163265306122448,
          "f": 0.10526315331371211
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14814814814814814,
          "p": 0.08163265306122448,
          "f": 0.10526315331371211
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2502.15822v1",
      "true_abstract": "This paper proposes a financial fraud detection system based on improved\nRandom Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the\nsystem introduces a novel model architecture called GBM-SSRF (Gradient Boosting\nMachine with Simplified and Strengthened Random Forest), which cleverly\ncombines the powerful optimization capabilities of the gradient boosting\nmachine (GBM) with improved randomization. The computational efficiency and\nfeature extraction capabilities of the Simplified and Strengthened Random\nForest (SSRF) forest significantly improve the performance of financial fraud\ndetection. Although the traditional random forest model has good classification\ncapabilities, it has high computational complexity when faced with large-scale\ndata and has certain limitations in feature selection. As a commonly used\nensemble learning method, the GBM model has significant advantages in\noptimizing performance and handling nonlinear problems. However, GBM takes a\nlong time to train and is prone to overfitting problems when data samples are\nunbalanced. In response to these limitations, this paper optimizes the random\nforest based on the structure, reducing the computational complexity and\nimproving the feature selection ability through the structural simplification\nand enhancement of the random forest. In addition, the optimized random forest\nis embedded into the GBM framework, and the model can maintain efficiency and\nstability with the help of GBM's gradient optimization capability. Experiments\nshow that the GBM-SSRF model not only has good performance, but also has good\nrobustness and generalization capabilities, providing an efficient and reliable\nsolution for financial fraud detection.",
      "generated_abstract": "The recent COVID-19 pandemic caused a major shock to the global economy,\nimpacting every country and region. The financial sector was especially\nimpacted by the pandemic. Banks and financial institutions were forced to\ndeal with the increasing number of defaults and insolvencies. This paper\nintroduces a novel framework to estimate the default probability of a loan\nportfolio. The proposed methodology, which is based on a Bayesian framework,\nis implemented on a large bank dataset. The results show that the proposed\nframework is able to estimate the default probability of a loan portfolio\nwithin 1% accuracy. The model is also able to forecast the probability of\ndefault with a 100-day forecasting horizon, which is considered as a benchmark\nin the financial industry. The results of the proposed model are highly\nsatisfactory, which shows the robustness of the framework. The paper\ncontributes",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17518248175182483,
          "p": 0.3,
          "f": 0.2211981520270128
        },
        "rouge-2": {
          "r": 0.037914691943127965,
          "p": 0.06451612903225806,
          "f": 0.04776118936707553
        },
        "rouge-l": {
          "r": 0.17518248175182483,
          "p": 0.3,
          "f": 0.2211981520270128
        }
      }
    },
    {
      "paper_id": "cs.CV.cs/MM/2503.10324v1",
      "true_abstract": "Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby utilizing complementary information from various modalities. However,\nexisting methods focus on fusing heterogeneous visual features, neglecting the\npotential benefits of text-based semantic information. To address this issue,\nwe first construct three text-enhanced multi-modal object ReID benchmarks. To\nbe specific, we propose a standardized multi-modal caption generation pipeline\nfor structured and concise text annotations with Multi-modal Large Language\nModels (MLLMs). Besides, current methods often directly aggregate multi-modal\ninformation without selecting representative local features, leading to\nredundancy and high complexity. To address the above issues, we introduce IDEA,\na novel feature learning framework comprising the Inverted Multi-modal Feature\nExtractor (IMFE) and Cooperative Deformable Aggregation (CDA). The IMFE\nutilizes Modal Prefixes and an InverseNet to integrate multi-modal information\nwith semantic guidance from inverted text. The CDA adaptively generates\nsampling positions, enabling the model to focus on the interplay between global\nfeatures and discriminative local features. With the constructed benchmarks and\nthe proposed modules, our framework can generate more robust multi-modal\nfeatures under complex scenarios. Extensive experiments on three multi-modal\nobject ReID benchmarks demonstrate the effectiveness of our proposed method.",
      "generated_abstract": "Large Language Models (LLMs) are increasingly used for image generation.\nHowever, existing methods often struggle with generating images that are\nstylistically diverse and contextually coherent. This challenge stems from the\nfact that existing generative models, such as GPT-3.5, do not provide\ninformation about the visual content and stylistic elements of the generated\nimages. To address this, we propose a new framework for image generation that\nintegrates visual content and stylistic information into the LLM's generation\nprocess. Specifically, we introduce a new LLM-based image generation model,\ncalled the Content-Guided Style Modulated (CGSM) model, that generates images\nbased on the content of the image along with stylistic information from a\npre-trained StyleGPT-3 model. By integrating content and stylistic\ninformation, our approach provides both the content of the image and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20454545454545456,
          "p": 0.32926829268292684,
          "f": 0.2523364438710805
        },
        "rouge-2": {
          "r": 0.061452513966480445,
          "p": 0.09821428571428571,
          "f": 0.07560136983550059
        },
        "rouge-l": {
          "r": 0.1893939393939394,
          "p": 0.3048780487804878,
          "f": 0.2336448550860338
        }
      }
    },
    {
      "paper_id": "q-fin.GN.econ/GN/2412.19817v1",
      "true_abstract": "Digital transformation significantly impacts firm investment, financing, and\nvalue enhancement. A systematic investigation from the corporate finance\nperspective has not yet been formed. This paper combines bibliometric and\ncontent analysis methods to systematically review the evolutionary trend,\nstatus quo, hotspots and overall structure of research in digital\ntransformation from 2011 to 2024. The study reveals an emerging and rapidly\ngrowing focus on digital transformation research, particularly in developed\ncountries. We categorize the literature into three areas according to\nbibliometric clustering: the measurements (qualitative and quantitative),\nimpact factors (internal and external), and the economic consequences\n(investment, financing, and firm value). These areas are divided into ten\nsub-branches, with a detailed literature review. We also review the existing\ntheories related to digital transformation, identify the current gaps in these\npapers, and provide directions for future research on each sub-branches.",
      "generated_abstract": "We consider the dynamic asset pricing model of Hull and White (1986) with\nthe additional assumption that the volatility is constant and independent of the\nrisk-neutral measure. We provide a generalization of the dynamic asset pricing\nresults of Hull and White (1986) by considering a more general class of\nvolatility models and derive the corresponding efficient market hypothesis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06862745098039216,
          "p": 0.1891891891891892,
          "f": 0.10071942055380172
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.058823529411764705,
          "p": 0.16216216216216217,
          "f": 0.08633093134516864
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.07332v1",
      "true_abstract": "Change-plane analysis is a pivotal tool for identifying subgroups within a\nheterogeneous population, yet it presents challenges when applied to functional\ndata. In this paper, we consider a change-plane model within the framework of\nfunctional response quantile regression, capable of identifying and testing\nsubgroups in non-Gaussian functional responses with scalar predictors. The\nproposed model naturally extends the change-plane method to account for the\nheterogeneity in functional data. To detect the existence of subgroups, we\ndevelop a weighted average of the squared score test statistic, which has a\nclosed form and thereby reduces the computational stress. An alternating\ndirection method of multipliers algorithm is formulated to estimate the\nfunctional coefficients and the grouping parameters. We establish the\nasymptotic theory for the estimates based on the reproducing kernel Hilbert\nspace and derive the asymptotic distributions of the proposed test statistic\nunder both null and alternative hypotheses. Simulation studies are conducted to\nevaluate the performance of the proposed approach in subgroup identification\nand hypothesis test. The proposed methods are also applied to two datasets, one\nfrom a study on China stocks and another from the COVID-19 pandemic.",
      "generated_abstract": "This paper investigates the problem of learning the joint posterior distribution\nof a parameter vector and its parameter perturbation. The joint posterior is\ndefined as the joint distribution of the parameter vector and its perturbation\nconditioned on the observations. It is shown that the posterior distribution is\na Gaussian mixture distribution, which can be decomposed into the Gaussian\ncomponent and the perturbation component. The perturbation component is\ncharacterized by a parameter perturbation distribution and can be well estimated\nusing the information from the observations. We propose a maximum likelihood\nestimator for the joint posterior and demonstrate that the estimator converges\nto the Gaussian mixture distribution under mild conditions. Numerical examples\nare presented to illustrate the effectiveness of the proposed method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13333333333333333,
          "p": 0.25806451612903225,
          "f": 0.17582417133196487
        },
        "rouge-2": {
          "r": 0.04,
          "p": 0.06930693069306931,
          "f": 0.05072463304059064
        },
        "rouge-l": {
          "r": 0.11666666666666667,
          "p": 0.22580645161290322,
          "f": 0.1538461493539429
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.cond-mat/dis-nn/2503.09684v1",
      "true_abstract": "When traffic is routed through a network that is susceptible to congestion,\nthe self-interested decisions made by individual users do not, in general,\nproduce the optimal flow. This discrepancy is quantified by the so-called\n\"price of anarchy.\" Here we consider whether the traffic produced by\nself-interested users is made better or worse when users have uncertain\nknowledge about the cost functions of the links in the network, and we define a\nparallel concept that we call the \"price of ignorance.\" We introduce a simple\nmodel in which fast, congestible links and slow, incongestible links are mixed\nrandomly in a large network and users plan their routes with finite uncertainty\nabout which of the two cost functions describes each link. One of our key\nfindings is that a small level of user ignorance universally improves traffic,\nregardless of the network composition. Further, there is an optimal level of\nignorance which, in our model, causes the self-interested user behavior to\ncoincide with the optimum. Many features of our model can be understood\nanalytically, including the optimal level of user ignorance and the existence\nof critical scaling near the percolation threshold for fast links, where the\npotential benefit of user ignorance is greatest.",
      "generated_abstract": "The rapid advancements in deep learning have led to a surge in the development\nof large language models (LLMs), which have demonstrated remarkable capabilities\nin various fields. However, the inherent lack of interpretability poses a\nsignificant challenge for LLMs, especially when they are employed for\nquantum-related tasks, such as simulating quantum systems. This is because the\nentanglement among quantum systems is inherently non-local, thereby making it\ndifficult to determine whether a given input is a true quantum state or not.\nThis issue can be addressed by incorporating postprocessing techniques, such\nas quantum-inspired interpretability methods, which aim to unveil the internal\nproperties of quantum systems. In this work, we propose a quantum-inspired\ninterpretability method for LLMs, referred to as quantum-inspired\ninterpretability (QII), that employs the ent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.2247191011235955,
          "f": 0.19138755491861462
        },
        "rouge-2": {
          "r": 0.010752688172043012,
          "p": 0.017094017094017096,
          "f": 0.01320131539130321
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.1797752808988764,
          "f": 0.15311004295689218
        }
      }
    },
    {
      "paper_id": "cond-mat.quant-gas.physics/space-ph/2503.09553v1",
      "true_abstract": "Cosmic rays are deemed to be generated by a process known as ``Fermi\nacceleration\", in which charged particles scatter against magnetic fluctuations\nin astrophysical plasmas. The process itself is however universal, has both\nclassical and quantum formulations, and is at the basis of dynamical systems\nwith interesting mathematical properties, such as the celebrated Fermi-Ulam\nmodel. Despite its effectiveness in accelerating particles, Fermi acceleration\nhas so far eluded unambiguous verifications in laboratory settings. Here, we\nrealize the first fully controllable Fermi accelerator by colliding ultracold\natoms against engineered movable potential barriers. We demonstrate that our\nFermi accelerator, which is only 100 um in size, can produce ultracold atomic\njets with velocities above half a meter per second. Adding dissipation, we also\nexperimentally test Bell's general argument for the ensuing energy spectra,\nwhich is at the basis of any model of cosmic ray acceleration. On the one hand,\nour work effectively opens the window to the study of high energy astrophysics\nwith cold atoms, offering new capabilities for the understanding of phenomena\nsuch as diffusive acceleration at collisionless shocks. On the other, the\nperformance of our Fermi accelerator is competitive with those of best-in-class\naccelerating methods used in quantum technology and quantum colliders, but with\nsubstantially simpler implementation and virtually no upper limit.",
      "generated_abstract": "We present a simple model of a cold atomic gas confined by a 2D optical\nplanar waveguide (PWG) and an electric field. The PWG confines the atoms\nuniformly in the $x-z$ plane. The atoms are excited by a pulsed laser field\nwith a Gaussian profile in the $z$ direction. We use the Boltzmann equation to\ndescribe the collision term. Our model shows that the atoms can be strongly\ndamped by the waveguide and the laser field. We find that the damping rate is\nindependent of the atoms' momentum. The damping rate is also independent of\nthe frequency of the laser field and of the distance between the atom and the\nwaveguide. The damping rate is proportional to the distance between the\natoms and the waveguide, and is independent of the waveguide's aspect ratio. We\nfind that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13513513513513514,
          "p": 0.2898550724637681,
          "f": 0.18433179289770446
        },
        "rouge-2": {
          "r": 0.020100502512562814,
          "p": 0.037383177570093455,
          "f": 0.026143786301636916
        },
        "rouge-l": {
          "r": 0.12837837837837837,
          "p": 0.2753623188405797,
          "f": 0.1751152030359533
        }
      }
    },
    {
      "paper_id": "cs.HC.cs/HC/2503.09849v1",
      "true_abstract": "After-action reviews (AARs) are professional discussions that help operators\nand teams enhance their task performance by analyzing completed missions with\npeers and professionals. Previous studies that compared different formats of\nAARs have mainly focused on human teams. However, the inclusion of robotic\nteammates brings along new challenges in understanding teammate intent and\ncommunication. Traditional AAR between human teammates may not be satisfactory\nfor human-robot teams. To address this limitation, we propose a new training\nreview (TR) tool, called the Virtual Spectator Interface (VSI), to enhance\nhuman-robot team performance and situational awareness (SA) in a simulated\nsearch mission. The proposed VSI primarily utilizes visual feedback to review\nsubjects' behavior. To examine the effectiveness of VSI, we took elements from\nAAR to conduct our own TR, designed a 1 x 3 between-subjects experiment with\nexperimental conditions: TR with (1) VSI, (2) screen recording, and (3)\nnon-technology (only verbal descriptions). The results of our experiments\ndemonstrated that the VSI did not result in significantly better team\nperformance than other conditions. However, the TR with VSI led to more\nimprovement in the subjects SA over the other conditions.",
      "generated_abstract": "Large language models (LLMs) have been shown to generate text that is\nappropriate for a specific context and purpose. However, the performance of\nLLMs on a specific task can vary depending on their training data. For\nexample, LLMs that are trained on news articles are likely to perform better\nthan those trained on non-news texts. This can lead to bias in decision-making\nand the creation of discriminatory policies. In this paper, we propose a\nmethodology that aims to detect such bias in LLMs, specifically, we focus on\nthe bias in the generation of news articles. To do this, we develop a\nmachine-learning model that evaluates the performance of LLMs on news articles\nand identifies the bias in their generation. Our approach is based on\ninformation-theoretic metrics that can be used to measure the reliability and\nfairness of LLMs. We evaluate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17164179104477612,
          "p": 0.26744186046511625,
          "f": 0.20909090432892571
        },
        "rouge-2": {
          "r": 0.0223463687150838,
          "p": 0.032,
          "f": 0.026315784631449988
        },
        "rouge-l": {
          "r": 0.1417910447761194,
          "p": 0.22093023255813954,
          "f": 0.17272726796528937
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.06599v1",
      "true_abstract": "The study examines the return connectedness between climate policy\nuncertainty (CPU), clean energy, fossil energy, and food markets. Using the\ntime-domain method of Diebold and Yilmaz (2012) and frequency-domain methods of\nBarun{\\'{i}}k and K{\\v{r}}hl{\\'{i}}k (2018), we find substantial spillover\neffects between these markets. Furthermore, high frequency domain is the\nprimary driver of overall connectedness. In addition, CPU is a net contributor\nof return shocks in the short term, whereas it turns to be a net recipient in\nthe medium and long terms. Across all frequencies, clean energy and oils are\nconsistent net recipients, while meat is a dominant net contributor.",
      "generated_abstract": "This study examines the role of the informal sector in the COVID-19 crisis,\nusing the COVID-19 pandemic as a case study. By using panel data from\nBrazil, the study analyzes the impact of the pandemic on the informal sector,\nhighlighting its significance in terms of employment and wages. The results\nshow that the informal sector has been significantly affected by the pandemic,\nresulting in a decline in employment and wages. The findings indicate that the\npandemic has caused significant job losses and decreased wages in the\ninformal sector, which could have severe consequences for the economy and\nsociety. The study recommends policies to help the informal sector recover and\naddress its economic challenges.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.136986301369863,
          "p": 0.14925373134328357,
          "f": 0.1428571378663267
        },
        "rouge-2": {
          "r": 0.042105263157894736,
          "p": 0.041237113402061855,
          "f": 0.0416666616672098
        },
        "rouge-l": {
          "r": 0.136986301369863,
          "p": 0.14925373134328357,
          "f": 0.1428571378663267
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/PR/2412.13172v1",
      "true_abstract": "This paper derives the expressions of correlations between prices of two\nassets, returns of two assets, and price-return correlations of two assets that\ndepend on statistical moments and correlations of the current values, past\nvalues, and volumes of their market trades. The usual frequency-based\nexpressions of correlations of time series of prices and returns describe a\npartial case of our model when all trade volumes and past trade values are\nconstant. Such an assumptions are rather far from market reality, and its use\nresults in excess losses and wrong forecasts. Traders, banks, and funds that\nperform multi-million market transactions or manage billion-valued portfolios\nshould consider the impact of large trade volumes on market prices and returns.\nThe use of the market-based correlations of prices and returns of two assets is\nmandatory for them. The development of macroeconomic models and market\nforecasts like those being created by BlackRock's Aladdin, JP Morgan, and the\nU.S. Fed., is impossible without the use of market-based correlations of prices\nand returns of two assets.",
      "generated_abstract": "This paper develops a two-sided dynamic game for agents with heterogeneous\ninterests, each with a preferential attachment rule for their offspring. Each\nagent is exposed to a random variable that determines her current welfare.\nThe welfare of a given agent depends on the welfare of her offspring, which\ndepends on the welfare of their offspring, and so on. The game is endogenous,\nwith agents' preferences and welfare functions being determined by the\nunderlying model. The model is characterized by the presence of a constant\nintergenerational transfer, which we model as a random variable. We show that\nthe equilibrium welfare is a superadditive function of the payoff of the game.\nWe characterize the equilibrium welfare for a general class of preferential\nattachment rules and show that the equilibrium welfare is a linear function of\nthe payoff",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17708333333333334,
          "p": 0.2537313432835821,
          "f": 0.20858895221348203
        },
        "rouge-2": {
          "r": 0.02097902097902098,
          "p": 0.026785714285714284,
          "f": 0.023529406838601566
        },
        "rouge-l": {
          "r": 0.17708333333333334,
          "p": 0.2537313432835821,
          "f": 0.20858895221348203
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/RM/2411.06080v1",
      "true_abstract": "Portfolio diversification, traditionally measured through asset correlations\nand volatilitybased metrics, is fundamental to managing financial risk.\nHowever, existing diversification metrics often overlook non-numerical\nrelationships between assets that can impact portfolio stability, particularly\nduring market stresses. This paper introduces the lexical ratio (LR), a novel\nmetric that leverages textual data to capture diversification dimensions absent\nin standard approaches. By treating each asset as a unique document composed of\nsectorspecific and financial keywords, the LR evaluates portfolio\ndiversification by distributing these terms across assets, incorporating\nentropy-based insights from information theory. We thoroughly analyze LR's\nproperties, including scale invariance, concavity, and maximality,\ndemonstrating its theoretical robustness and ability to enhance risk-adjusted\nportfolio returns. Using empirical tests on S&P 500 portfolios, we compare LR's\nperformance to established metrics such as Markowitz's volatility-based\nmeasures and diversification ratios. Our tests reveal LR's superiority in\noptimizing portfolio returns, especially under varied market conditions. Our\nfindings show that LR aligns with conventional metrics and captures unique\ndiversification aspects, suggesting it is a viable tool for portfolio managers.",
      "generated_abstract": "In this paper, we develop a theoretical framework for pricing and hedging\nrisky options that are priced using the Black-Scholes formula. In our framework,\nwe assume that the Black-Scholes model is incomplete and approximated using a\nnonlinear model, which is modeled as a neural network. The approximated model\ncaptures some important features of the Black-Scholes model, including the\nprice of the option, its risk-free rate, and the option's volatility. We\nintroduce the concept of an optimal neural network that approximates the\nBlack-Scholes model. We then use the optimal neural network to compute the\noption's price and risk-free rate. Next, we use this price and risk-free rate\nto price the option. Finally, we use the hedged option to hedge a riskless\nasset. We use the hedged option to hedge a riskless",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11940298507462686,
          "p": 0.23880597014925373,
          "f": 0.15920397565505817
        },
        "rouge-2": {
          "r": 0.005952380952380952,
          "p": 0.009259259259259259,
          "f": 0.00724637204789223
        },
        "rouge-l": {
          "r": 0.11194029850746269,
          "p": 0.22388059701492538,
          "f": 0.14925372689883928
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.07664v1",
      "true_abstract": "The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified\nresource derived from electronic health records (EHR) that facilitates research\ninto antimicrobial resistance (AMR). ARMD encompasses data from adult patients,\nfocusing on microbiological cultures, antibiotic susceptibilities, and\nassociated clinical and demographic features. Key attributes include organism\nidentification, susceptibility patterns for 55 antibiotics, implied\nsusceptibility rules, and de-identified patient information. This dataset\nsupports studies on antimicrobial stewardship, causal inference, and clinical\ndecision-making. ARMD is designed to be reusable and interoperable, promoting\ncollaboration and innovation in combating AMR. This paper describes the\ndataset's acquisition, structure, and utility while detailing its\nde-identification process.",
      "generated_abstract": "We introduce the concept of an \"infinite-dimensional\" state space of a\nsystem. This infinite-dimensional state space is defined by a set of dynamical\nequations, and it is not determined by the set of initial conditions. The\ninfinite-dimensional state space is defined by the system dynamics, not by\nthe system state at any point in time. The notion of an infinite-dimensional\nstate space is illustrated by the example of a quantum system.\n  We define the concept of an \"infinite-dimensional system\" as a system whose\nstate space is infinite-dimensional. We introduce a notion of \"equivalent\ninfinite-dimensional systems\". This notion is illustrated by the example of a\nquantum system.\n  We use the concept of an infinite-dimensional state space and the concept of\nan infinite-dimensional system to study the properties of an infinite-dimensional\nquantum system. We show that an infinite-dimensional quantum system can be",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11904761904761904,
          "p": 0.19607843137254902,
          "f": 0.1481481434469137
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.17647058823529413,
          "f": 0.13333332863209893
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/PR/2408.01642v2",
      "true_abstract": "The additive process generalizes the L\\'evy process by relaxing its\nassumption of time-homogeneous increments and hence covers a larger family of\nstochastic processes. Recent research in option pricing shows that modeling the\nunderlying log price with an additive process has advantages in easier\nconstruction of the risk-neural measure, an explicit option pricing formula and\ncharacteristic function, and more flexibility to fit the implied volatility\nsurface. Still, the challenge of calibrating an additive model arises from its\ntime-dependent parameterization, for which one has to prescribe parametric\nfunctions for the term structure. For this, we propose the neural term\nstructure model to utilize feedforward neural networks to represent the term\nstructure, which alleviates the difficulty of designing parametric functions\nand thus attenuates the misspecification risk. Numerical studies with S\\&P 500\noption data are conducted to evaluate the performance of the neural term\nstructure.",
      "generated_abstract": "In this paper, we consider the problem of optimal control of a multi-agent\nsystem with a dynamic nonlinear cost function, in which the agents are\ninterconnected via a network of proportional-integral (PI) controllers. The\nobjective is to minimize a performance function that takes into account both\nthe total cost and the energy consumption. We consider a network of\nproportional-integral (PI) controllers, and the agents are connected via\nnetworks of proportional-integral (PI) controllers. We assume that the\nnetworks of PI controllers are connected by a directed graph. We develop a\ndynamic optimal control framework to solve the problem. We use the Lyapunov\nfunctional approach to solve the problem. We obtain the optimal control\npolynomials. We use the Lyapunov functionals to derive the optimal control\npolynomials. We use the Lyapunov functionals to derive the optimal control\npol",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.24615384615384617,
          "f": 0.2025316407266465
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12903225806451613,
          "p": 0.18461538461538463,
          "f": 0.15189872933424145
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.17478v1",
      "true_abstract": "\"Math is not a spectator sport.\" \"Lecturing is educational malpractice.\"\nSlogans like these rally some mathematicians to teach classes that feature\n\"active learning\", where lecturing is eschewed for student participation. Yet\nas much as I believe that students must do math to learn math, I also find\nblanket statements to be more about bandwagons than considered reflection on\nteaching. In this column, published in the Fall 2021 AWM Newsletter, I urge us\nto think through the math we offer students and how we set up students to\nlearn. Although I draw primarily from my experiences teaching proofs in\nabstract algebra and real analysis, the scenarios extend to other topics in\nfirst year undergraduate education and beyond.",
      "generated_abstract": "We define and study a category of $\\beta$-Hopf algebras, where $\\beta$ is the\nsymmetric group of the dimension of the algebra. We show that the symmetric group\n$\\Sigma_n$ is a full subcategory of this category. We then study the category of\n$\\Sigma_n$-Hopf algebras with $\\Sigma_n$ being the symmetric group of the\ndimension of the algebra, and show that the symmetric group $\\Sigma_n$ is\nrepresented by the subcategory of $\\Sigma_n$-Hopf algebras with\n$\\Sigma_n$-invariants that are involutions. We then show that the category of\n$\\Sigma_n$-Hopf algebras with $\\Sigma_n$-invariants that are involutions is\nisomorphic to the category of finite dimensional $\\mathbb{Z}/n\\mathbb{Z}$-graded\nHopf algebras, where $\\mathbb{Z}/",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.21951219512195122,
          "f": 0.13432835396190704
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07526881720430108,
          "p": 0.17073170731707318,
          "f": 0.10447760769325033
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/AS/2502.03871v1",
      "true_abstract": "We consider a phase-shift mixing model for linear sensor arrays in the\ncontext of blind source extraction. We derive a blind Capon beamformer that\nseeks the direction where the output is independent of the other signals in the\nmixture. The algorithm is based on Independent Component Extraction and imposes\nan orthogonal constraint, thanks to which it optimizes only one real-valued\nparameter related to the angle of arrival. The Cram\\'er-Rao lower bound for the\nmean interference-to-signal ratio is derived. The algorithm and the bound are\ncompared with conventional blind and direction-of-arrival\nestimation+beamforming methods, showing improvements in terms of extraction\naccuracy. An application is demonstrated in frequency-domain speaker extraction\nin a low-reverberation room.",
      "generated_abstract": "Recent advancements in large language models (LLMs) have enabled their\napplication in a wide range of audio processing tasks. However, existing\nresearch has largely focused on single-modal LLMs, ignoring the integration\ncapabilities of multi-modal LLMs. This paper proposes a multi-modal audio\nprocessing framework, which integrates LLMs with audio encoders and speaker\nembedding models. Our approach enables LLMs to capture complex audio-text\nrelationships, thereby enhancing audio-text understanding. Additionally, we\ndemonstrate that combining LLMs with audio encoders can enhance performance in\naudio classification tasks. To this end, we propose a novel audio encoder\ncalled the audio-text encoder (ATE), which integrates text and audio features\nin a unified manner. Additionally, we introduce a novel speaker embedding\nmodel called speaker-embedding-based audio enc",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16455696202531644,
          "p": 0.1625,
          "f": 0.1635220075788143
        },
        "rouge-2": {
          "r": 0.009259259259259259,
          "p": 0.009259259259259259,
          "f": 0.009259254259261959
        },
        "rouge-l": {
          "r": 0.13924050632911392,
          "p": 0.1375,
          "f": 0.1383647748744118
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2502.17830v1",
      "true_abstract": "Hypothesis tests and confidence intervals are ubiquitous in empirical\nresearch, yet their connection to subsequent decision-making is often unclear.\nWe develop a theory of certified decisions that pairs recommended decisions\nwith inferential guarantees. Specifically, we attach P-certificates -- upper\nbounds on loss that hold with probability at least $1-\\alpha$ -- to recommended\nactions. We show that such certificates allow \"safe,\" risk-controlling adoption\ndecisions for ambiguity-averse downstream decision-makers. We further prove\nthat it is without loss to limit attention to P-certificates arising as minimax\ndecisions over confidence sets, or what Manski (2021) terms \"as-if decisions\nwith a set estimate.\" A parallel argument applies to E-certified decisions\nobtained from e-values in settings with unbounded loss.",
      "generated_abstract": "The application of the Expectation-Maximization (EM) algorithm in\nprobability modeling is well-known. In this paper, we examine the performance\nof the EM algorithm in the estimation of the mean and the variance of\nBayesian regression models. We consider two different Bayesian regression models:\nthe Gaussian regression model and the Student-t regression model. We apply the\nEM algorithm to estimate these two models, and we compare the performance of the\nEM algorithm with the EM algorithm for the M-estimator and the Bayesian\nestimator. Our results demonstrate that the EM algorithm is more efficient than\nthe M-estimator and Bayesian estimator in terms of convergence rates. We also\nshow that the EM algorithm can achieve higher efficiency in terms of computational\ncomplexity than the M-estimator and Bayesian estimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1590909090909091,
          "p": 0.22580645161290322,
          "f": 0.186666661816889
        },
        "rouge-2": {
          "r": 0.008928571428571428,
          "p": 0.010752688172043012,
          "f": 0.009756092603928752
        },
        "rouge-l": {
          "r": 0.1590909090909091,
          "p": 0.22580645161290322,
          "f": 0.186666661816889
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/EP/2503.10309v1",
      "true_abstract": "Observational data provided by JWST instruments continue to challenge\ntheories and models of cloud formation in sub-stellar atmospheres, requiring\nmore sophisticated approaches in an effort to understand their spatial\ncomplexity. However, to date, most cloud microphysical models using the moment\nmethod for sub-stellar atmospheres have assumed a monodisperse size\ndistribution, neglecting polydisperse properties. We aim to extend beyond the\ncommon assumption of a monodisperse size distribution and analyse cloud\nmicrophysical processes assuming an exponential distribution. We derive\nexpressions for the zeroth and first moments of condensation/evaporation and\ncollisional growth processes under the assumption of an exponential size\ndistribution. We then compare the differences between monodisperse and\nexponential distribution microphysics using a simple one-dimensional (1D)\ncolumn model applied to a Y-dwarf KCl cloud scenario. We find that adopting an\nexponential distribution modifies condensation/evaporation rates by a factor of\n$\\approx$0.9 and collisional growth rates by factors of $\\approx$1.1 (Kn $\\ll$\n1) and $\\approx$0.92 (Kn $\\gg$ 1) for Brownian coagulation and $\\approx$0.85\nfor gravitational coalescence, compared to the monodisperse case. In our\nspecific test cases, we find relative differences of a maximum 10-12% in total\nnumber density and 2-3% in mean radius of the cloud particles between the\nmonodisperse and exponential distributions. Our results offer a simple way to\ntake into account an assumed exponential size distribution for sub-stellar\natmospheric cloud microphysics using the two-moment method. In follow up\nstudies, we will examine more complex distributions, such as the log-normal and\ngamma distributions, that require more than two moments to characterise\nself-consistently.",
      "generated_abstract": "We present an extensive set of 3D hydrodynamical simulations of the\nstellar cluster NGC 2273 in the Large Magellanic Cloud. These simulations\nrepresent a step-change in resolution over previous studies, which used the\nhigh-resolution version of the CLOUDY code to model the cluster. Our simulations\ncover a 100 pc x 100 pc x 100 pc volume, with a density of 1000 cm-3, and\ninclude the effects of stellar feedback, in addition to those caused by\nclustering and mass segregation. We use a three-dimensional hydrodynamical\ncode to model the cluster's gas and stars. The model includes the effects of\nstellar feedback, which is driven by the energy released during core-collapse\nsupernovae. We present a detailed analysis of the star formation rate and\nstar-formation efficiency,",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0958904109589041,
          "p": 0.18421052631578946,
          "f": 0.12612612162324505
        },
        "rouge-2": {
          "r": 0.0043859649122807015,
          "p": 0.009259259259259259,
          "f": 0.005952376590139251
        },
        "rouge-l": {
          "r": 0.08904109589041095,
          "p": 0.17105263157894737,
          "f": 0.11711711261423603
        }
      }
    },
    {
      "paper_id": "cs.SD.cs/DL/2502.17726v1",
      "true_abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
      "generated_abstract": "In this paper, we propose a novel approach for the task of detecting\nintrusions in unstructured text. This task is of particular importance for\nemerging security threats, such as adversarial texts, where traditional\ntext-based approaches often fail to detect malicious content. To this end, we\nintroduce a novel multimodal approach that integrates textual, visual, and\naudio features, as well as a customized attention mechanism. We evaluate our\nmethod on a large corpus of adversarial text and demonstrate that our approach\nis able to detect malicious content with a significantly higher accuracy than\nstate-of-the-art methods. In addition, we perform a detailed analysis of the\neffects of various features on the classification performance, demonstrating\nthat multimodal features are crucial for robust detection. This work has\nimplications for the development of effective detection mechanisms for emerging\nsecurity threats",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10967741935483871,
          "p": 0.18888888888888888,
          "f": 0.13877550555601847
        },
        "rouge-2": {
          "r": 0.014285714285714285,
          "p": 0.023622047244094488,
          "f": 0.017804149605967656
        },
        "rouge-l": {
          "r": 0.10967741935483871,
          "p": 0.18888888888888888,
          "f": 0.13877550555601847
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10632v1",
      "true_abstract": "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of\nlearnable activation functions with the potential to capture more complex\nrelationships from data. Although KANs are useful in finding symbolic\nrepresentations and continual learning of one-dimensional functions, their\neffectiveness in diverse machine learning (ML) tasks, such as vision, remains\nquestionable. Presently, KANs are deployed by replacing multilayer perceptrons\n(MLPs) in deep network architectures, including advanced architectures such as\nvision Transformers (ViTs). In this paper, we are the first to design a general\nlearnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate\non any choice of basis. However, the computing and memory costs of training\nthem motivated us to propose a more modular version, and we designed particular\nlearnable attention, called Fourier-KArAt. Fourier-KArAt and its variants\neither outperform their ViT counterparts or show comparable performance on\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures'\nperformance and generalization capacity by analyzing their loss landscapes,\nweight distributions, optimizer path, attention visualization, and spectral\nbehavior, and contrast them with vanilla ViTs. The goal of this paper is not to\nproduce parameter- and compute-efficient attention, but to encourage the\ncommunity to explore KANs in conjunction with more advanced architectures that\nrequire a careful understanding of learnable activations. Our open-source code\nand implementation details are available on: https://subhajitmaity.me/KArAt",
      "generated_abstract": "With the rapid development of deep learning, the efficiency of neural\nnetworks has attracted significant attention. In the literature, the impact of\nmemory on neural networks has been extensively studied. However, the impact of\nmemory on neural networks for tasks with high-dimensional input data remains\nunclear. In this paper, we propose a novel neural network model, named\n\\textbf{S-NNA}, which is inspired by the memory-efficient neural architecture\nsearch (MENAS) algorithm. The proposed \\textbf{S-NNA} model is designed to\nincrease the efficiency of neural networks. In order to achieve this goal, we\nintroduce the \\textbf{S-NNA-Loss} objective function. This objective function\nis designed to improve the efficiency of neural networks, which consists of\ntwo parts: the first part is to minimize the memory consumption of the model,\nand the second part is to minimize the training",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17532467532467533,
          "p": 0.35064935064935066,
          "f": 0.2337662293217894
        },
        "rouge-2": {
          "r": 0.028846153846153848,
          "p": 0.05504587155963303,
          "f": 0.03785488507757122
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.2857142857142857,
          "f": 0.19047618603174613
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.09737v1",
      "true_abstract": "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
      "generated_abstract": "In this paper, we consider the problem of training a deep neural network (DNN)\nfrom a small number of samples, in which the training samples are\nstatistically dependent. Specifically, the training samples are drawn from a\nnon-identical distribution that is known to the network. We assume that the\ndistribution is known in the training phase, but the distribution is unknown\nduring testing. We propose a new method, called Depth-Wise Self-Attention (DSA),\nwhich is inspired by the self-attention mechanism in transformer-based neural\nnetworks. The key idea of DSA is to use the knowledge of the distribution\nduring training to learn a DNN that can generalize to the distribution in the\ntest phase. We prove that DSA has a better generalization performance than the\nstandard DNNs. We also provide theoretical guarantees for the convergence rate\nof DSA, and demonstrate that DSA ach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0958904109589041,
          "p": 0.1686746987951807,
          "f": 0.12227073773650406
        },
        "rouge-2": {
          "r": 0.005434782608695652,
          "p": 0.007936507936507936,
          "f": 0.006451608078255429
        },
        "rouge-l": {
          "r": 0.0958904109589041,
          "p": 0.1686746987951807,
          "f": 0.12227073773650406
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/RM/2501.11552v1",
      "true_abstract": "We explore the interplay between sovereign debt default/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
      "generated_abstract": "This study examines the effect of the COVID-19 pandemic on financial\nindicators of the stock market in Thailand, focusing on the impact of the\ndecrease in travel-related activity on the market. The study uses panel data\nfrom 2010 to 2025 and employs a stochastic frontier production function with\nendogenous productivity to investigate the performance of firms in the stock\nmarket. The results show that the reduction in travel-related activity had a\nsignificant negative impact on the stock market, with a 10% decrease in\nproductivity resulting in a 2.57% decline in the stock market. The study\nconcludes that the pandemic had a significant impact on the Thai stock market,\ncausing a decline in productivity and leading to a 2.57% decline in the stock\nmarket. The findings highlight the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10576923076923077,
          "p": 0.18333333333333332,
          "f": 0.1341463368233196
        },
        "rouge-2": {
          "r": 0.007042253521126761,
          "p": 0.010416666666666666,
          "f": 0.008403356531321167
        },
        "rouge-l": {
          "r": 0.10576923076923077,
          "p": 0.18333333333333332,
          "f": 0.1341463368233196
        }
      }
    },
    {
      "paper_id": "math.CO.math/AC/2503.01647v1",
      "true_abstract": "Classical results of Cauchy and Dehn imply that the 1-skeleton of a convex\npolyhedron $P$ is rigid i.e. every continuous motion of the vertices of $P$ in\n$\\mathbb R^3$ which preserves its edge lengths results in a polyhedron which is\ncongruent to $P$. This result was extended to convex poytopes in $\\mathbb R^d$\nfor all $d\\geq 3$ by Whiteley, and to generic realisations of 1-skeletons of\nsimplicial $(d-1)$-manifolds in $\\mathbb R^{d}$ by Kalai for $d\\geq 4$ and\nFogelsanger for $d\\geq 3$. We will generalise Kalai's result by showing that,\nfor all $d\\geq 4$ and any fixed $1\\leq k\\leq d-3$, every generic realisation of\nthe $k$-skeleton of a simplicial $(d-1)$-manifold in $\\mathbb R^{d}$ is volume\nrigid, i.e. every continuous motion of its vertices in $\\mathbb R^d$ which\npreserves the volumes of its $k$-faces results in a congruent realisation. In\naddition, we conjecture that our result remains true for $k=d-2$ and verify\nthis conjecture when $d=4,5,6$.",
      "generated_abstract": "In this paper, we prove the existence of a unique weak solution to a nonlinear\nprincipal eigenvalue problem for a system of differential equations with\nnon-Hamiltonian coefficients. The nonlinear eigenvalue problem is obtained by\nintegrating the differential system of equations. The system is constructed\nusing the Fourier transform and the method of characteristics. We use the\nmethod of characteristics to obtain a system of ordinary differential\nequations and then solve it to obtain a solution of the nonlinear eigenvalue\nproblem. Our approach is based on the construction of the characteristic\ntrajectories for the differential system of equations and the method of\ncharacteristics for the system of ordinary differential equations. Our\nconstruction uses the Fourier transform and the method of characteristics. We\nshow that the eigenvalue problem is equivalent to a system of ordinary\ndifferential equations and solve it to obtain a solution of the eigenvalue\nproblem. Our construction is based on the construction of the characteristic\ntra",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1566265060240964,
          "p": 0.23636363636363636,
          "f": 0.1884057923072885
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.031914893617021274,
          "f": 0.026548667707730546
        },
        "rouge-l": {
          "r": 0.14457831325301204,
          "p": 0.21818181818181817,
          "f": 0.1739130386841001
        }
      }
    },
    {
      "paper_id": "stat.AP.q-bio/OT/2503.08378v1",
      "true_abstract": "Objectives: To examine the distribution, temporal associations, and\nage/sex-specific patterns of multiple long-term conditions (MLTCs) in adults\nwith intellectual disability (ID).\n  Study Design: Observational study using longitudinal healthcare data.\nMethods: Analysis of 18144 adults with ID (10168 males and 7976 females)\nidentified in the Clinical Practice Research Datalink, linked to Hospital\nEpisode Statistics Admitted Patient Care and Outpatient data (2000-2021). We\nused temporal analysis to establish directional associations among 40 long-term\nconditions, stratified by sex and age groups (under 45, 45-64, 65 and over).\n  Results: The high prevalence of enduring mental illness across all age groups\nis an important finding unique to this population. In males, mental illness\noccurred along with upper gastrointestinal conditions (specifically reflux\ndisorders), while in females, mental illness presented alongside reflux\ndisorders, chronic pain, and endocrine conditions such as thyroid problems.\nAmong young males with intellectual disability, the combination of cerebral\npalsy with dysphagia, epilepsy, chronic constipation, and chronic pneumonia\nrepresents a distinctive pattern. In those aged 45-64, we observed early onset\nof lifestyle conditions like diabetes and hypertension, though notably these\nconditions co-occurred with mental illness and anaemia at rates exceeding those\nin the general population. The health conditions in those aged 65 and over\nlargely mirrored those seen in the general aging population.\n  Conclusions: Our findings highlight the complex patterns of MLTCs in this\npopulation, revealing sex-specific associations across age groups, and\nidentified temporal associations, thus providing insights into disease\nprogression, which can inform targeted prevention strategies and interventions\nto prevent premature mortality.",
      "generated_abstract": "In the study of genetic variation, we often focus on the population\ngenotype and phenotype, but in many biological systems the genotype and the\nenvironment are interrelated. To capture these interactions, we often use a\nprocess called genotype-phenotype correlation (GPC). The GPC is the\nrelationship between the genotype and the phenotype in the population. The\nGPC is a measure of the strength of the interaction between the genotype and\nthe environment. This paper presents a novel and general approach for\nquantifying GPC. The approach is based on the concept of the genotype-phenotype\ncorrelation coefficient (GPC-C). The GPC-C is a measure of the strength of the\ninteraction between the genotype and the phenotype in the population. The\nGPC-C is a continuous measure, and its range is in [0",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08092485549132948,
          "p": 0.23333333333333334,
          "f": 0.12017166999576354
        },
        "rouge-2": {
          "r": 0.00851063829787234,
          "p": 0.022727272727272728,
          "f": 0.01238389696441193
        },
        "rouge-l": {
          "r": 0.07514450867052024,
          "p": 0.21666666666666667,
          "f": 0.11158797900863908
        }
      }
    },
    {
      "paper_id": "math.PR.stat/TH/2502.16916v1",
      "true_abstract": "This paper establishes sharp dimension-free concentration inequalities and\nexpectation bounds for the deviation of the sum of simple random tensors from\nits expectation. As part of our analysis, we use generic chaining techniques to\nobtain a sharp high-probability upper bound on the suprema of multi-product\nempirical processes. In so doing, we generalize classical results for quadratic\nand product empirical processes to higher-order settings.",
      "generated_abstract": "We consider the problem of estimating a function $f:\\mathbb{R}^d\\to\n\\mathbb{R}$ under the assumption that $f$ is supported in a ball of radius $R$\nand we observe an independent copy of $f$. We first derive a formula for the\nestimator of $f$ from the observed data, and then derive an expression for the\nvariance of the estimator. The variance is expressed in terms of the variance\nof the original function $f$ and the distance between $f$ and the identity\nmap. This allows us to derive a variance bound for the estimator. The proof is\nbased on the idea of using the Gaussian distribution as an approximation to\n$f$ and then using the Gaussian approximation property of the Gaussian\ndistribution to derive a variance bound. This is a novel result in the\nliterature on Gaussian processes for estimating functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.24,
          "p": 0.18461538461538463,
          "f": 0.20869564725897935
        },
        "rouge-2": {
          "r": 0.04918032786885246,
          "p": 0.026785714285714284,
          "f": 0.03468207635938448
        },
        "rouge-l": {
          "r": 0.18,
          "p": 0.13846153846153847,
          "f": 0.1565217342155011
        }
      }
    },
    {
      "paper_id": "math.LO.math/RA/2503.10528v1",
      "true_abstract": "The first-order model theory of modules has been studied for decades. More\nrecently, the model theoretic study of nonelementary classes of\nmodules--especially Abstract Elementary Classes of modules--has produced\ninteresting results. This survey aims to discuss these recent results and give\nan introduction to the framework of Abstract Elementary Classes for module\ntheorists.",
      "generated_abstract": "In this paper we introduce a new way to express the algebraic\nconsequences of the Riemann-Hilbert correspondence, which is based on the\ncharacteristic classes of the associated Hilbert scheme of points. We give an\ninterpretation of the algebraic consequences of the Riemann-Hilbert correspondence\nin terms of the algebraic cohomology of the Hilbert scheme. The Riemann-Hilbert\ncorrespondence is a non-symmetric version of the Riemann-Roch theorem, which is\na symmetric version of the Weil-Petersson metric on the moduli space of stable\nmaps. We show that the Riemann-Hilbert correspondence is a special case of the\nRiemann-Roch theorem. The Riemann-Hilbert correspondence is also an\ninterpretation of the Riemann-Roch theorem in terms of the cohomology of the\nmoduli space of stable maps.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.175,
          "p": 0.1346153846153846,
          "f": 0.15217390812854456
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.02702702702702703,
          "f": 0.032520320409809685
        },
        "rouge-l": {
          "r": 0.175,
          "p": 0.1346153846153846,
          "f": 0.15217390812854456
        }
      }
    },
    {
      "paper_id": "math.OC.econ/GN/2502.12035v1",
      "true_abstract": "The transition to a low-carbon economy necessitates effective carbon capture\nand storage (CCS) solutions, particularly for hard-to-abate sectors. Herein,\npipeline networks are indispensable for cost-efficient $CO_2$ transportation\nover long distances. However, there is deep uncertainty regarding which\nindustrial sectors will participate in such systems. This poses a significant\nchallenge due to substantial investments as well as the lengthy planning and\ndevelopment timelines required for $CO_2$ pipeline projects, which are further\nconstrained by limited upgrade options for already built infrastructure. The\neconomies of scale inherent in pipeline construction exacerbate these\nchallenges, leading to potential regret over earlier decisions. While numerous\nmodels were developed to optimize the initial layout of pipeline infrastructure\nbased on known demand, a gap exists in addressing the incremental development\nof infrastructure in conjunction with deep uncertainty. Hence, this paper\nintroduces a novel optimization model for $CO_2$ pipeline infrastructure\ndevelopment, minimizing regret as its objective function and incorporating\nvarious upgrade options, such as looping and pressure increases. The model's\neffectiveness is also demonstrated by presenting a comprehensive case study of\nGermany's cement and lime industries. The developed approach quantitatively\nillustrates the trade-off between different options, which can help in deriving\neffective strategies for $CO_2$ infrastructure development.",
      "generated_abstract": "We consider the problem of optimal auction design for a set of players that\nis subject to a non-convex utility function and a non-convex bidder valuation.\nWe introduce a novel game-theoretic approach to this problem, which we refer to\nas the Weighted Game-theoretic Bidding (WGB) framework. This framework\nencompasses a broad range of existing auction formats, such as the\nWeighted-Average-Price (WAP) and the Weighted-Average-Value (WAV) auction\nformats. The WGB framework enables designing auction formats that are\ncapable of capturing the structure of the underlying valuation and the\nconstraints imposed by the players. Additionally, we derive a closed-form\nsolution to the WGB framework, enabling the efficient design of optimal\nauction formats. Moreover, we provide a computational framework to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.2535211267605634,
          "f": 0.17224879934159026
        },
        "rouge-2": {
          "r": 0.026041666666666668,
          "p": 0.045871559633027525,
          "f": 0.03322258674231033
        },
        "rouge-l": {
          "r": 0.12318840579710146,
          "p": 0.23943661971830985,
          "f": 0.16267942135115968
        }
      }
    },
    {
      "paper_id": "nucl-ex.nucl-ex/2503.07841v1",
      "true_abstract": "We present the first measurements with a new collinear laser spectroscopy\nsetup at the Argonne Tandem Linac Accelerator System utilizing its unique\ncapability to deliver neutron-rich refractory metal isotopes produced by the\nspontaneous fission of 252Cf. We measured isotope shifts from optical spectra\nfor nine radioactive ruthenium isotopes 106-114Ru, reaching deep into the\nmid-shell region. The extracted charge radii are in excellent agreement with\npredictions from the Brussels-Skyrme-on-a-Grid models that account for the\ntriaxial deformation of nuclear ground states in this region. We show that\ntriaxial deformation impacts charge radii in models that feature shell effects,\nin contrast to what could be concluded from a liquid drop analysis. This\nindicates that this exotic type of deformation should not be neglected in\nregions where it is known to occur, even if its presence cannot be\nunambiguously inferred through laser spectroscopy.",
      "generated_abstract": "The search for the $e^+e^-\\to\\Lambda\\eta^\\prime$ process, which is a good\nexperimental probe of the $\\Lambda\\eta^\\prime$ resonance, has been performed\nwith the NA64 experiment at CERN. The $\\Lambda\\eta^\\prime$ resonance is a\nparticularly important and challenging resonance to observe. In this\nrecently published paper, we have summarized the results of the NA64 experiment\nand discussed the implications for the $\\Lambda\\eta^\\prime$ resonance. We also\ndiscuss the challenges of the NA64 experiment and future experimental plans.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10784313725490197,
          "p": 0.23404255319148937,
          "f": 0.14765100239268514
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.016666666666666666,
          "f": 0.010309274078012185
        },
        "rouge-l": {
          "r": 0.08823529411764706,
          "p": 0.19148936170212766,
          "f": 0.12080536480879255
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.02609v1",
      "true_abstract": "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model.",
      "generated_abstract": "We study the problem of sequential allocation of a fixed budget among\nallocations of different budgets. We consider a monotone and strictly\nconvex allocation cost function and a linear budget allocation cost function.\nWe show that, if the budget allocation cost function is convex and the\nallocation cost function is strictly convex, then the optimal allocation\ndistribution is unique, and the optimal allocation distribution is a\nmulti-dimensional uniform distribution. We also show that, if the budget\nallocation cost function is convex and the allocation cost function is\nstrictly convex, then the optimal allocation distribution is unique, and the\noptimal allocation distribution is a multi-dimensional uniform distribution\nwith a positive density function. In this case, the optimal allocation\ndistribution is the unique minimizer of the expected utility of the optimal\nallocation distribution. We also show that, if the budget allocation cost\nfunction is strictly convex and the allocation cost function is strictly\nconvex, then the optimal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13043478260869565,
          "p": 0.2727272727272727,
          "f": 0.17647058385813158
        },
        "rouge-2": {
          "r": 0.03787878787878788,
          "p": 0.06666666666666667,
          "f": 0.04830917412308383
        },
        "rouge-l": {
          "r": 0.13043478260869565,
          "p": 0.2727272727272727,
          "f": 0.17647058385813158
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/RM/2412.04263v1",
      "true_abstract": "A simple model-free and distribution-free statistic, the functional\nrelationship between the number of \"effective\" degrees of freedom and portfolio\nsize, or N*(N), is used to discriminate between two alternative models for the\ncorrelation of daily cryptocurrency returns within a retail universe of defined\nby the list of tradable assets available to account holders at the Robinhood\nbrokerage. The average pairwise correlation between daily cryptocurrency\nreturns is found to be high (of order 60%) and the data collected supports\ndescription of the cross-section of returns by a simple isotropic correlation\nmodel distinct from a decomposition into a linear factor model with additive\nnoise with high confidence. This description appears to be relatively stable\nthrough time.",
      "generated_abstract": "In this paper, we extend the recently proposed risk-aware mean-variance (RA-MV)\nstrategy to the case of multiple risk factors, which we call RA-MV-MF. The key\ndifference between RA-MV-MF and RA-MV is that we consider the mean-variance\noptimization of the portfolio with respect to the risk factors, rather than the\nrisk-adjusted return optimization, which is the approach adopted in RA-MV. To\nexploit the joint relationship between the mean-variance and risk-adjusted\nreturn optimization, we propose a risk-adjusted portfolio value-at-risk\noptimization, which we call RAVR-MF. RAVR-MF is based on the risk-adjusted\nportfolio value-at-risk (PAVR), which is a risk-adjusted",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14102564102564102,
          "p": 0.20754716981132076,
          "f": 0.16793892647980901
        },
        "rouge-2": {
          "r": 0.02727272727272727,
          "p": 0.037037037037037035,
          "f": 0.031413607680711275
        },
        "rouge-l": {
          "r": 0.14102564102564102,
          "p": 0.20754716981132076,
          "f": 0.16793892647980901
        }
      }
    },
    {
      "paper_id": "cond-mat.dis-nn.quant-ph/2503.10462v1",
      "true_abstract": "Deep neural quantum states have recently achieved remarkable performance in\nsolving challenging quantum many-body problems. While transformer networks\nappear particularly promising due to their success in computer science, we show\nthat previously reported transformer wave functions haven't so far been capable\nto utilize their full power. Here, we introduce the convolutional transformer\nwave function (CTWF). We show that our CTWFs exhibit superior performance in\nground-state search and non-equilibrium dynamics compared to previous results,\ndemonstrating promising capacity in complex quantum problems.",
      "generated_abstract": "In this work, we investigate the possibility of using a non-equilibrium\nnon-Markovian master equation to simulate the dynamics of a one-dimensional\nspin chain in the time-dependent Hartree-Fock approximation. We find that this\napproach can be used to extract a spin-dependent energy gap, which is sensitive\nto the strength of the non-Markovianity. In addition, we investigate the\nstability of the spin-dependent gap, finding that it can be affected by\ndifferent initial conditions. These results are in agreement with the\nexperimental data of Ref. [1",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12698412698412698,
          "p": 0.13793103448275862,
          "f": 0.13223139996721553
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.1206896551724138,
          "f": 0.11570247434738086
        }
      }
    },
    {
      "paper_id": "math.NA.math/AP/2503.09580v1",
      "true_abstract": "We introduce a fast Fourier spectral method to compute linearized collision\noperators of the Boltzmann equation for variable hard-sphere gases. While the\nstate-of-the-art method provides a computational cost O(MN^4 log N), with N\nbeing the number of modes in each direction and M being the number of\nquadrature points on a hemisphere, our method reduces the cost to O(N^4 log N),\nremoving the factor M, which could be large in our numerical tests. The method\nis applied in a numerical solver for the steady-state Boltzmann equation with\nquadratic collision operators. Numerical experiments for both spatially\nhomogeneous and inhomogeneous Boltzmann equations have been carried out to test\nthe accuracy and efficiency of our method.",
      "generated_abstract": "We prove a general result for the existence of a unique solution to the\ndifferential equation\n   \\begin{equation*}\n     \\ddot{u}(t) = \\frac{\\beta^2}{4}\\frac{\\dot{u}(t)^2}{u(t)} + \\alpha u(t) +\n     \\mu u'(t) + \\lambda u(t) u'(t),\n   \\end{equation*}\n   where $\\beta, \\alpha, \\mu, \\lambda > 0$ and the coefficients $\\beta$ and\n$\\alpha$ are non-zero. Our proof is based on the method of the\n$C^1$-semigroup and the comparison theorem for the solution of the differential\nequation. We also prove that the solution is unique if the initial condition is\ngiven. This result is obtained for a class of differential equations which is\nnot necessarily the case for all differential equations. We illustrate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17567567567567569,
          "p": 0.20634920634920634,
          "f": 0.18978101693004434
        },
        "rouge-2": {
          "r": 0.01904761904761905,
          "p": 0.021052631578947368,
          "f": 0.019999995012501246
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.19047619047619047,
          "f": 0.17518247678405896
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2503.10510v1",
      "true_abstract": "Whole-slide image classification represents a key challenge in computational\npathology and medicine. Attention-based multiple instance learning (MIL) has\nemerged as an effective approach for this problem. However, the effect of\nattention mechanism architecture on model performance is not well-documented\nfor biomedical imagery. In this work, we compare different methods and\nimplementations of MIL, including deep learning variants. We introduce a new\nmethod using higher-dimensional feature spaces for deep MIL. We also develop a\nnovel algorithm for whole-slide image classification where extreme machine\nlearning is combined with attention-based MIL to improve sensitivity and reduce\ntraining complexity. We apply our algorithms to the problem of detecting\ncirculating rare cells (CRCs), such as erythroblasts, in peripheral blood. Our\nresults indicate that nonlinearities play a key role in the classification, as\nremoving them leads to a sharp decrease in stability in addition to a decrease\nin average area under the curve (AUC) of over 4%. We also demonstrate a\nconsiderable increase in robustness of the model with improvements of over 10%\nin average AUC when higher-dimensional feature spaces are leveraged. In\naddition, we show that extreme learning machines can offer clear improvements\nin terms of training efficiency by reducing the number of trained parameters by\na factor of 5 whilst still maintaining the average AUC to within 1.5% of the\ndeep MIL model. Finally, we discuss options of enriching the classical\ncomputing framework with quantum algorithms in the future. This work can thus\nhelp pave the way towards more accurate and efficient single-cell diagnostics,\none of the building blocks of precision medicine.",
      "generated_abstract": "We present a novel method to simulate gene regulatory networks (GRNs) in\ntwo-dimensional (2D) spatial domains. GRNs are dynamic systems composed of\ngene-protein interactions that regulate gene expression and play crucial roles\nin cellular processes. In this work, we develop a novel 2D GRN model that\ncombines an energy-based model (EBM) and a diffusion-based model (D-BM) to\ndescribe the dynamics of gene regulatory networks. The EBM captures the\ninformation of gene expression dynamics and the gene-gene interaction network,\nwhile the D-BM captures the information of gene expression dynamics and the\ninteractions between genes. To simulate the dynamics of the 2D GRNs, we\nintroduce a novel method called the spatial domain coupling (SDC) method. The\nSDC method combines the spatial domain coupling (",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11904761904761904,
          "p": 0.2857142857142857,
          "f": 0.16806722273850724
        },
        "rouge-2": {
          "r": 0.028688524590163935,
          "p": 0.06930693069306931,
          "f": 0.04057970600394918
        },
        "rouge-l": {
          "r": 0.10714285714285714,
          "p": 0.2571428571428571,
          "f": 0.15126050004943165
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.01687v2",
      "true_abstract": "We establish Rezk completion functors for $\\Theta_n$ spaces with respect to\neach and all of the completeness conditions. As a consequence, we obtain a\ncharacterization of Dwyer-Kan equivalences between Segal $\\Theta_n$ spaces.",
      "generated_abstract": "The aim of this paper is to prove the existence of a maximal torsion in the\ngroup of theta-functions of a genus 2 hyperelliptic curve defined over the\nfield of complex numbers. We use the theory of theta functions to study the\ngroup of theta-functions of the genus 2 hyperelliptic curve in the complex\nplane. In order to do this we need to study the group of theta-functions of the\npolarized genus 2 hyperelliptic curve. In order to do this we need to study\nthe group of theta-functions of the polarized genus 2 hyperelliptic curve in\nthe complex plane. In order to do this we need to study the group of theta-functions\nof the polarized genus 2 hyperelliptic curve in the complex plane.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21428571428571427,
          "p": 0.15789473684210525,
          "f": 0.18181817693296617
        },
        "rouge-2": {
          "r": 0.03333333333333333,
          "p": 0.017857142857142856,
          "f": 0.023255809410493045
        },
        "rouge-l": {
          "r": 0.21428571428571427,
          "p": 0.15789473684210525,
          "f": 0.18181817693296617
        }
      }
    },
    {
      "paper_id": "math.CT.math/CT/2503.03916v1",
      "true_abstract": "We study stability properties of fully faithful functors, and compute mapping\nanima in pushouts of $\\infty$-categories along fully faithful functors. We\nprovide applications of these calculations to pushouts along Dwyer functors and\nReedy categories.",
      "generated_abstract": "We prove a new result concerning the $C^*$-algebra of the modular $L^2$\nsymplectic group of an elliptic curve. The theorem generalizes a result of\nHoffstein and Vaughan, and provides a simple proof of a result of\nKr\\\"uger. Our proof relies on a new technique of analyzing the $L^2$\nsymplectic group as a module over the $C^*$-algebra of the $L^2$ symplectic\ngroup of a smooth projective curve.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12,
          "p": 0.08333333333333333,
          "f": 0.09836065090029586
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12,
          "p": 0.08333333333333333,
          "f": 0.09836065090029586
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2411.18830v1",
      "true_abstract": "We study the relationship between model complexity and out-of-sample\nperformance in the context of mean-variance portfolio optimization.\nRepresenting model complexity by the number of assets, we find that the\nperformance of low-dimensional models initially improves with complexity but\nthen declines due to overfitting. As model complexity becomes sufficiently\nhigh, the performance improves with complexity again, resulting in a double\nascent Sharpe ratio curve similar to the double descent phenomenon observed in\nartificial intelligence. The underlying mechanisms involve an intricate\ninteraction between the theoretical Sharpe ratio and estimation accuracy. In\nhigh-dimensional models, the theoretical Sharpe ratio approaches its upper\nlimit, and the overfitting problem is reduced because there are more parameters\nthan data restrictions, which allows us to choose well-behaved parameters based\non inductive bias.",
      "generated_abstract": "This study presents a novel approach for generating synthetic micro- and\nmanage-level risk measures using the pricing of credit default swaps (CDSs) and\na modified benchmarking approach. The pricing of CDSs is performed using\nequivalent risk measures (ERMs), which are defined as the averages of the\nexcess returns of the underlying and the security over a specific period of\ntime. The ERMs are then used to generate synthetic risk measures, which are\ncompared with a benchmarking approach. The synthetic measures are then used to\nevaluate the performance of the benchmarking approach, which is used to\nrebalance the synthetic portfolios. The synthetic measures are further used to\ngenerate a series of portfolios that are compared with a target portfolio,\nwhich is a set of CDSs with a fixed coupon. The results show that the\nsynthetic measures, which",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16853932584269662,
          "p": 0.22727272727272727,
          "f": 0.19354838220686799
        },
        "rouge-2": {
          "r": 0.03508771929824561,
          "p": 0.036036036036036036,
          "f": 0.03555555055644515
        },
        "rouge-l": {
          "r": 0.14606741573033707,
          "p": 0.19696969696969696,
          "f": 0.16774193059396478
        }
      }
    },
    {
      "paper_id": "math-ph.math/MP/2503.09421v1",
      "true_abstract": "We study coined Random Quantum Walks on the hexagonal lattice, where the\nstrength of disorder is monitored by the coin matrix. Each lattice site is\nequipped with an i.i.d. random variable that is uniformly distributed on the\ntorus and acts as a random phase in every step of the QW. We show dynamical\nlocalization in the regime of strong disorder, that is whenever the coin matrix\nis sufficiently close to the fully localized case, using a fractional moment\ncriterion and a finite volume method. Moreover, we adapt a topological index to\nour model and thereby obtain transport for some coin matrices.",
      "generated_abstract": "In the present work, we consider the two-dimensional (2D) quantum Hall\nexperiment in the presence of an electric field, which is generated by a\nsingle-layer graphene film sandwiched between two 2D metallic ribbons.\nConventional Hall effect is enhanced by an electric field, while the in-plane\nmagnetoresistance (IMR) is suppressed due to a strong magnetic field. We\nperform a microscopic calculation to investigate the effects of the electric\nfield on the 2D Hall effect and IMR of the graphene film. We find that the\nmagnetoresistance in the graphene film is strongly suppressed by the electric\nfield. The magnitude of the IMR is determined by the magnitude of the electric\nfield. We also present an analytical solution for the IMR in the absence of\nelectric field, which is consistent with the microscopic calculation. Our\ncalcul",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21333333333333335,
          "p": 0.23529411764705882,
          "f": 0.22377621878820494
        },
        "rouge-2": {
          "r": 0.04081632653061224,
          "p": 0.03669724770642202,
          "f": 0.03864733800928908
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.22058823529411764,
          "f": 0.20979020480219096
        }
      }
    },
    {
      "paper_id": "q-bio.PE.q-bio/PE/2503.07897v1",
      "true_abstract": "This work introduces a new markovian stochastic model that can be described\nas a non-homogeneous Pure Birth process. We propose a functional form of birth\nrate that depends on the number of individuals in the population and on the\nelapsed time, allowing us to model a contagion effect. Thus, we model the early\nstages of an epidemic. The number of individuals then becomes the infectious\ncases and the birth rate becomes the incidence rate. We obtain this way a\nprocess that depends on two competitive phenomena, infection and immunization.\nVariations in those rates allow us to monitor how effective the actions taken\nby government and health organizations are. From our model, three useful\nindicators for the epidemic evolution over time are obtained: the immunization\nrate, the infection/immunization ratio and the mean time between infections\n(MTBI). The proposed model allows either positive or negative concavities for\nthe mean value curve, provided the infection/immunization ratio is either\ngreater or less than one. We apply this model to the present SARS-CoV-2\npandemic still in its early growth stage in Latin American countries. As it is\nshown, the model accomplishes a good fit for the real number of both positive\ncases and deaths. We analyze the evolution of the three indicators for several\ncountries and perform a comparative study between them. Important conclusions\nare obtained from this analysis.",
      "generated_abstract": "The role of cellular and molecular processes in the formation of cell\ncell adhesion complexes has been studied in detail by using the adhesion\nmodel, which is based on the contact angle and the adhesion strength. This\nmodel describes the formation of a cell-cell adhesion complex by a sequence of\ncontacts between the two cells. The model is based on the concept of adhesion\nstrength, which is the product of the contact angle and the adhesion\nstrength. This concept has been developed to study the formation of adhesion\ncomplex by studying the dynamics of cell-cell adhesion complexes in a\ncontinuous media. This study focuses on the formation of cell-cell adhesion\ncomplex in the absence of the adhesion strength and in the presence of a\ndynamic force. The mathematical formulation of this model has been studied in\nthe literature. This study aims to investigate the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12408759124087591,
          "p": 0.288135593220339,
          "f": 0.1734693835469597
        },
        "rouge-2": {
          "r": 0.024271844660194174,
          "p": 0.04807692307692308,
          "f": 0.032258060057440786
        },
        "rouge-l": {
          "r": 0.11678832116788321,
          "p": 0.2711864406779661,
          "f": 0.16326530191430666
        }
      }
    },
    {
      "paper_id": "q-fin.MF.q-fin/PR/2405.12479v2",
      "true_abstract": "We present a unified, market-complete model that integrates both the\nBachelier and Black-Scholes-Merton frameworks for asset pricing. The model\nallows for the study, within a unified framework, of asset pricing in a natural\nworld that experiences the possibility of negative security prices or riskless\nrates. In contrast to classical Black-Scholes-Merton, we show that option\npricing in the unified model displays a difference depending on whether the\nreplicating, self-financing portfolio uses riskless bonds or a single riskless\nbank account. We derive option price formulas and extend our analysis to the\nterm structure of interest rates by deriving the pricing of zero-coupon bonds,\nforward contracts, and futures contracts. We identify a necessary condition for\nthe unified model to support a perpetual derivative. Discrete binomial pricing\nunder the unified model is also developed. In every scenario analyzed, we show\nthat the unified model simplifies to the standard Black-Scholes-Merton pricing\nunder specific limits and provides pricing in the Bachelier model limit. We\nnote that the Bachelier limit within the unified model allows for positive\nriskless rates. The unified model prompts us to speculate on the possibility of\na mixed multiplicative and additive deflator model for risk-neutral option\npricing.",
      "generated_abstract": "We introduce a novel approach for efficient market hypothesis (EMH)\nevaluation in financial markets using a multivariate time series modeling\napproach. The methodology is based on the EMH framework, where a model is\nconstructed to capture the joint dynamics of all variables in a market. We\ndevelop a novel approach to construct the model by decomposing the joint\ndynamics into individual components, and then employing nonlinear regression\nto estimate each component separately. We demonstrate the performance of the\nproposed methodology by applying it to a wide variety of financial datasets,\nincluding the S&P 500 index, the VIX index, and the European CryptoCurrency\nIndex. The results show that our methodology effectively captures the\ndynamic relationships among various financial variables, providing valuable\ninsights into the underlying economic and financial drivers of these markets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16346153846153846,
          "p": 0.19540229885057472,
          "f": 0.1780104662437983
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.031746031746031744,
          "f": 0.027397255367799713
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.1839080459770115,
          "f": 0.16753926205531663
        }
      }
    },
    {
      "paper_id": "q-bio.TO.q-bio/SC/2408.05119v1",
      "true_abstract": "Stress generation by the actin cytoskeleton shapes cells and tissues. Despite\nimpressive progress in live imaging and quantitative physical descriptions of\ncytoskeletal network dynamics, the connection between processes at molecular\nscales and cell-scale spatio-temporal patterns is still unclear. Here we review\nstudies reporting acto-myosin clusters of micrometer size and with lifetimes of\nseveral minutes in a large number of organisms ranging from fission yeast to\nhumans. Such structures have also been found in reconstituted systems in vitro\nand in theoretical analysis of cytoskeletal dynamics. We propose that tracking\nthese clusters can serve as a simple readout for characterising living matter.\nSpatio-temporal patterns of clusters could serve as determinants of\nmorphogenetic processes that play similar roles in diverse organisms.",
      "generated_abstract": "The discovery of the human genome in 2001 provided an unprecedented\naccess to genomic data. This genomic data has been increasingly used to\nunderstand complex biological processes. While many studies have focused on\nunderstanding the structure of genetic variants and gene-environment interactions,\na growing body of work has examined the role of transcriptome-genome data in\nunderstanding biological processes. This review summarizes the current\nliterature on the use of transcriptome-genome data to study complex biological\nprocesses, focusing on gene expression, gene networks, and gene regulation. We\nalso discuss the challenges in using transcriptome-genome data in complex\nbiological processes, including the complexity of gene expression, gene\nnetworks, and gene regulation, the lack of standardized methodologies, and the\ndifficulty in integrating multiple data types. Finally, we provide",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15217391304347827,
          "p": 0.1891891891891892,
          "f": 0.16867469385397024
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.15217391304347827,
          "p": 0.1891891891891892,
          "f": 0.16867469385397024
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2503.03503v1",
      "true_abstract": "Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research.",
      "generated_abstract": "We present a novel method for predicting the expression of genes in a cell\nspecies using only expression data from a small subset of cells. The method\nrelies on the use of a deep learning network that is trained on a\nhigh-dimensional expression dataset but is not trained on gene expression\ndata from the small subset of cells. We show that this method can perform\nwell in predicting the expression of a gene in a cell line, even in cases where\nthe small subset of cells is not representative of the cell line as a whole.\nWe also show that our method can be used to predict the expression of a gene\nfrom the expression of one of its neighboring genes in the same cell line. We\nalso show that our method can be used to predict the expression of a gene in\na cell line from the expression of a gene in a different cell line. We also\nshow that our method can be used to predict the expression of a gene in a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08484848484848485,
          "p": 0.23333333333333334,
          "f": 0.12444444053333348
        },
        "rouge-2": {
          "r": 0.004366812227074236,
          "p": 0.009900990099009901,
          "f": 0.006060601812858808
        },
        "rouge-l": {
          "r": 0.08484848484848485,
          "p": 0.23333333333333334,
          "f": 0.12444444053333348
        }
      }
    },
    {
      "paper_id": "astro-ph.EP.astro-ph/IM/2503.08854v1",
      "true_abstract": "Modern astronomical surveys detect asteroids by linking together their\nappearances across multiple images taken over time. This approach faces\nlimitations in detecting faint asteroids and handling the computational\ncomplexity of trajectory linking. We present a novel method that adapts\n``digital tracking\" - traditionally used for short-term linear asteroid motion\nacross images - to work with large-scale synoptic surveys such as the Vera\nRubin Observatory Legacy Survey of Space and Time (Rubin/LSST). Our approach\ncombines hundreds of sparse observations of individual asteroids across their\nnon-linear orbital paths to enhance detection sensitivity by several\nmagnitudes. To address the computational challenges of processing massive data\nsets and dense orbital phase spaces, we developed a specialized\nhigh-performance computing architecture. We demonstrate the effectiveness of\nour method through experiments that take advantage of the extensive\ncomputational resources at Lawrence Livermore National Laboratory. This work\nenables the detection of significantly fainter asteroids in existing and future\nsurvey data, potentially increasing the observable asteroid population by\norders of magnitude across different orbital families, from near-Earth objects\n(NEOs) to Kuiper belt objects (KBOs).",
      "generated_abstract": "We present the first systematic study of the chemical abundance variations of\nthe most common elements, C, N, O, Na, Mg, Al, Si, P, S, Cl, Ar, K, Ca, Ti,\nSc, V, Cr, Mn, Fe, Ni, Co, and Zn, in 110 nearby young stellar objects (YSOs)\nobserved with the Wide-field Infrared Survey Explorer (WISE). We performed a\nhigh-resolution spectroscopic survey of the YSOs with an average resolution of\n160,000. We identified 59 chemical elements with abundances that are\nsystematically different from solar. We show that the abundance differences\nbetween the most abundant elements, O and N, are up to 50\\% larger than the\nabundance differences between the less abundant elements, Na and Mg.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1171875,
          "p": 0.18072289156626506,
          "f": 0.14218009001415077
        },
        "rouge-2": {
          "r": 0.011560693641618497,
          "p": 0.019230769230769232,
          "f": 0.014440428523245665
        },
        "rouge-l": {
          "r": 0.1015625,
          "p": 0.1566265060240964,
          "f": 0.12322274404258683
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2407.00332v1",
      "true_abstract": "With emerging prevalence beyond traditionally endemic regions, the global\nburden of dengue disease is forecasted to be one of the fastest growing. With\nlimited direct treatment or vaccination currently available, prevention through\nvector control is widely believed to be the most effective form of managing\noutbreaks. This study examines traditional state space models (moving average,\nautoregressive, ARIMA, SARIMA), supervised learning techniques (XGBoost, SVM,\nKNN) and deep networks (LSTM, CNN, ConvLSTM) for forecasting weekly dengue\ncases in Singapore. Meteorological data and search engine trends were included\nas features for ML techniques. Forecasts using CNNs yielded lowest RMSE in\nweekly cases in 2019.",
      "generated_abstract": "The identification of genetic markers associated with disease risk is an\nhighly important goal for molecular biology. One major challenge in the\nidentification of genetic markers is to identify the genetic variants associated\nwith a particular disease. This is because many diseases are caused by multiple\ngenetic variants. For example, schizophrenia is caused by multiple genetic\nvariants, and a single genetic variant does not necessarily cause schizophrenia.\nTo address this challenge, we propose a novel method, based on the\nHierarchical Genetic Marker Network (HGMN), for the identification of\ngenetic markers associated with disease risk. This method integrates\ngenetic-marker-based models with network-based models, and it can be applied to\ndisease-associated genetic variants. First, we define the gene-disease network,\nwhich connects genes to diseases. The gen",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12941176470588237,
          "p": 0.1375,
          "f": 0.1333333283379249
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.12941176470588237,
          "p": 0.1375,
          "f": 0.1333333283379249
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.03217v1",
      "true_abstract": "Social media usage is often cited as a potential driver behind the rising\nsuicide rates. However, distinguishing the causal effect - whether social media\nincreases the risk of suicide - from reverse causality, where individuals\nalready at higher risk of suicide are more likely to use social media, remains\na significant challenge. In this paper, we use an instrumental variable\napproach to study the quasi-exogenous geographical adoption of Twitter and its\ncausal relationship with suicide rates. Our analysis first demonstrates that\nTwitter's geographical adoption was driven by the presence of certain users at\nthe 2007 SXSW festival, which led to long-term disparities in adoption rates\nacross counties in the United States. Then, using a two-stage least squares\n(2SLS) regression and controlling for a wide range of geographic, socioeconomic\nand demographic factors, we find no significant relationship between Twitter\nadoption and suicide rates.",
      "generated_abstract": "This study examines the impact of COVID-19 on the use of e-commerce platforms\nin the global South. We leverage the 2018-2022 World Input-Output (WIO) database\nto compare the use of e-commerce platforms in the global North and South,\nexamining the impact of COVID-19 on e-commerce use in China, India, and\nSouth Africa. We find that COVID-19 has had a significant impact on the use of\ne-commerce platforms in the global South, with a 12.2% decrease in e-commerce\nusage and a 6.3% increase in e-commerce sales. The impact of COVID-19 on e-\ncommerce use is more pronounced in the global South than in the global North.\nThe use of e-commerce platforms declined in the global North, with a 2.4%\ndecrease",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1485148514851485,
          "p": 0.26785714285714285,
          "f": 0.191082797958538
        },
        "rouge-2": {
          "r": 0.014705882352941176,
          "p": 0.023255813953488372,
          "f": 0.018018013271651277
        },
        "rouge-l": {
          "r": 0.13861386138613863,
          "p": 0.25,
          "f": 0.1783439444553533
        }
      }
    },
    {
      "paper_id": "cs.NE.cs/NE/2503.09340v1",
      "true_abstract": "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
      "generated_abstract": "We consider the problem of identifying an unknown point in the unit sphere of\na complex number using only the input of the point and a single scalar. We\nprovide a tight upper bound for the minimum number of queries to this problem\nfor any fixed value of the scalar. Furthermore, we show that the upper bound\ncan be achieved by any algorithm with a polynomial time runtime. In this\ncontext, we explore the complexity of a problem called the $k$-nearest point\nproblem, which is related to the problem of identifying an unknown point in the\nunit sphere using only the input of the point and a single scalar. We provide a\nlower bound for the $k$-nearest point problem with a polynomial time runtime\nwhen the point is known, and we show that this lower bound cannot be improved\nbelow $O(n^2)$ for any $n$ when the point is unknown. This result extends the\nknown complexity results for the $k$-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12295081967213115,
          "p": 0.21428571428571427,
          "f": 0.1562499953667536
        },
        "rouge-2": {
          "r": 0.027624309392265192,
          "p": 0.043478260869565216,
          "f": 0.033783779032369184
        },
        "rouge-l": {
          "r": 0.12295081967213115,
          "p": 0.21428571428571427,
          "f": 0.1562499953667536
        }
      }
    },
    {
      "paper_id": "physics.app-ph.physics/atm-clus/2502.20913v1",
      "true_abstract": "Recent research on silver nanowires prepared on DNA templates has focused on\ntwo fundamental applications: nano-scale circuits and sensors. Despite its\nbroad potential, the formation kinetics of DNA-templated silver nanowires\nremains unclear. Here, we present an experimental demonstration of the\nformation of silver nanowires with a diameter of 2.2+0.4 nm at the\nsingle-molecule level through chemical reduction. We conducted equilibrium and\nperturbation kinetic experiments to measure force spectroscopy during the\nformation of Ag+ -DNA complexes and Ag-DNA complexes, using optical tweezers\ncombined with microfluidics. The addition of AgNO3 resulted in an increase in\nforce of 5.5-7.5 pN within 2 minutes, indicating that Ag+ compacts the DNA\nstructure. In contrast, the addition of hydroquinone caused the force to\ndecrease by 4-5 pN. Morphological characterization confirmed the presence of a\ndense structure formed by silver atoms bridging the DNA strands, and revealed\nconformational differences before and after metallization. We compare our\nexperimental data with Brownian dynamics simulations using a coarse-grained\ndouble-stranded DNA (dsDNA) model that provides insights on the dependency of\nthe force on the persistence length.",
      "generated_abstract": "Theoretical studies of the energy and angular distributions of cosmic rays\nin the Earth's atmosphere are important for improving the accuracy of\ncosmic-ray physics in the Earth's atmosphere. The present study focuses on\nstudying the energy and angular distributions of cosmic rays in the Earth's\natmosphere using the data from the 2017-2019 period. The data were collected\nfrom the High Altitude Air Shower Observatory (HAAS) array, which is located in\nthe United States, and the Global Network of High Altitude Air Shower Arrays\n(GNHASA) in Australia. The analysis was carried out using the Monte Carlo\nsimulation method, and the results were compared with the experimental data.\nThe results show that the cosmic rays in the Earth's atmosphere follow the\nPoisson distribution with a mean value of 1.23 eV and",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10833333333333334,
          "p": 0.1780821917808219,
          "f": 0.13471502120325393
        },
        "rouge-2": {
          "r": 0.017857142857142856,
          "p": 0.02912621359223301,
          "f": 0.022140216689860888
        },
        "rouge-l": {
          "r": 0.1,
          "p": 0.1643835616438356,
          "f": 0.12435232690273583
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.physics/hist-ph/2502.17438v1",
      "true_abstract": "Henrietta Swan Leavitt's discovery of the relationship between the period and\nluminosity (hereafter the Leavitt Law) of 25 variable stars in the Small\nMagellanic Cloud, published in 1912, revolutionized cosmology. These variables,\neventually identified as Cepheids, became the first known \"standard candles\"\nfor measuring extragalactic distances and remain the gold standard for this\ntask today. Leavitt measured light curves, periods, and minimum and maximum\nmagnitudes from painstaking visual inspection of photographic plates. Her work\npaved the way for the first precise series of distance measurements that helped\nset the scale of the Universe, and later the discovery of its expansion by\nEdwin Hubble in 1929. Here, we re-analyze Leavitt's first Period-Luminosity\nrelation using observations of the same set of stars but with modern data and\nmethods of Cepheid analysis. Using only data from Leavitt's notebooks, we\nassess the quality of her light curves, measured periods, and the slope and\nscatter of her Period-Luminosity relations. We show that modern data and\nmethods, for the same objects, reduce the scatter of the Period-Luminosity\nrelation by a factor of two. We also find a bias brightward at the short period\nend, due to the non-linearity of the plates and environmental crowding.\nOverall, Leavitt's results are in excellent agreement with contemporary\nmeasurements, reinforcing the value of Cepheids in cosmology today, a testament\nto the enduring quality of her work.",
      "generated_abstract": "The recent discovery of a new radio source in the vicinity of the Large\nKnot of the Crab Nebula has sparked interest in this region as a possible\nnuclear black hole candidate. We present a detailed analysis of the optical\nemission from this source, focusing on its spectral energy distribution and\nX-ray emission. We find that the optical and X-ray emission are consistent\nwith the interpretation of the source as a black hole, with a mass of\n$1.27\\pm0.03\\times10^6$ M$_\\odot$. We further investigate the possibility of\na non-thermal component in the optical spectrum, which we identify as a\nthermal bremsstrahlung model, and use it to constrain the black hole's\ntemperature. We also investigate the possible existence of a second black hole\nwithin the Large Knot, and find that its mass is also consistent",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13970588235294118,
          "p": 0.24358974358974358,
          "f": 0.17757008882522504
        },
        "rouge-2": {
          "r": 0.019417475728155338,
          "p": 0.03418803418803419,
          "f": 0.024767797237202408
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.19230769230769232,
          "f": 0.1401869112551316
        }
      }
    },
    {
      "paper_id": "math.AC.math/AC/2503.09096v1",
      "true_abstract": "Consider a simple algebraic valued field extension $(L/K,v)$ and denote by\n$\\mathcal O_L$ and $\\mathcal O_K$ the corresponding valuation rings. The main\ngoal of this paper is to present, under certain assumptions, a description of\n$\\mathcal O_L$ in terms of generators and relations over $\\mathcal O_K$. The\nmain tool used here are complete sequences of key polynomials. It is known that\nif the ramification index of $(L/K,v)$ is one, then every complete set gives\nrise to a set of generators of $\\mathcal O_L$ over $\\mathcal O_K$. We show that\nwe can find a sequence of key polynomials for $(L/K,v)$ which satisfies good\nproperties (called neat). Then we present explicit ``neat\" relations that\ngenerate all the relations between the corresponding generators of $\\mathcal\nO_L$ over $\\mathcal O_K$.",
      "generated_abstract": "We study the $p$-adic analogue of the classical Riemann-Roch theorem in the\ncontext of $K$-theory. We prove that for a locally constant sheaf $\\mathcal{F}$\non a smooth projective variety $X$ of dimension $n$, the $p$-adic\nRiemann-Roch theorem is equivalent to the following statement: for any\n$p$-divisible group $A$, the complex $\\mathcal{F} \\otimes_{\\mathcal{O}_X} A$\nis torsion free of rank $n$ and has $p$-torsion divisible by $p$ of order at\nmost $n$. We also study the $p$-adic analogue of the classical Kummer\ntheorem, showing that for a locally constant sheaf $\\mathcal{F}$ on $X$, the\n$p$-adic Kummer theorem is equivalent to the following statement: for any\n$p$-divis",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14473684210526316,
          "p": 0.1864406779661017,
          "f": 0.16296295804224983
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14473684210526316,
          "p": 0.1864406779661017,
          "f": 0.16296295804224983
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.05979v1",
      "true_abstract": "Autoregressive models (ARMs) have become the workhorse for sequence\ngeneration tasks, since many problems can be modeled as next-token prediction.\nWhile there appears to be a natural ordering for text (i.e., left-to-right),\nfor many data types, such as graphs, the canonical ordering is less obvious. To\naddress this problem, we introduce a variant of ARM that generates\nhigh-dimensional data using a probabilistic ordering that is sequentially\ninferred from data. This model incorporates a trainable probability\ndistribution, referred to as an \\emph{order-policy}, that dynamically decides\nthe autoregressive order in a state-dependent manner. To train the model, we\nintroduce a variational lower bound on the exact log-likelihood, which we\noptimize with stochastic gradient estimation. We demonstrate experimentally\nthat our method can learn meaningful autoregressive orderings in image and\ngraph generation. On the challenging domain of molecular graph generation, we\nachieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated\nusing the Fr\\'{e}chet ChemNet Distance (FCD).",
      "generated_abstract": "This paper presents the first unified analysis of the two major approaches\nto sampling from high-dimensional distributions. We introduce a new framework\nfor analyzing both the Metropolis and Hamiltonian Monte Carlo samplers,\nhighlighting key differences between these two approaches. We derive\nasymptotic behavior of the Metropolis sampler and demonstrate that it achieves\nthe optimal rate of convergence in the limit of large-enough stepsize. In\ncontrast, the Hamiltonian sampler exhibits a more complicated behavior, with\nconvergence rates that depend on both the step-size and the target distribution\nof the Markov chain. We also investigate the impact of these two differences on\nthe performance of the samplers. The results provide valuable insights into\noptimizing the convergence rates of these sampling algorithms and highlighting\ntheir fundamental differences.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14912280701754385,
          "p": 0.2125,
          "f": 0.17525772711233936
        },
        "rouge-2": {
          "r": 0.013245033112582781,
          "p": 0.017391304347826087,
          "f": 0.015037589076546346
        },
        "rouge-l": {
          "r": 0.13157894736842105,
          "p": 0.1875,
          "f": 0.1546391704113085
        }
      }
    },
    {
      "paper_id": "q-fin.GN.q-fin/GN/2410.19741v1",
      "true_abstract": "Identifying client needs to provide optimal services is crucial in tourist\ndestination management. The events held in tourist destinations may help to\nmeet those needs and thus contribute to tourist satisfaction. As with product\nmanagement, the creation of hierarchical catalogs to classify those events can\naid event management. The events that can be found on the internet are listed\nin dispersed, heterogeneous sources, which makes direct classification a\ndifficult, time-consuming task. The main aim of this work is to create a novel\nprocess for automatically classifying an eclectic variety of tourist events\nusing a hierarchical taxonomy, which can be applied to support tourist\ndestination management. Leveraging data science methods such as CRISP-DM,\nsupervised machine learning, and natural language processing techniques, the\nautomatic classification process proposed here allows the creation of a\nnormalized catalog across very different geographical regions. Therefore, we\ncan build catalogs with consistent filters, allowing users to find events\nregardless of the event categories assigned at source, if any. This is very\nvaluable for companies that offer this kind of information across multiple\nregions, such as airlines, travel agencies or hotel chains. Ultimately, this\ntool has the potential to revolutionize the way companies and end users\ninteract with tourist events information.",
      "generated_abstract": "In this paper, we propose a novel methodology for the risk management of\ninterbank lending relationships, based on the analysis of the interbank\nexchange rate dynamics. We first introduce a novel framework for the pricing of\nthe risk associated with the interbank lending relationship, which combines\nthe logarithmic term with the interbank exchange rate dynamics. This framework\nis then used to develop an optimal hedging strategy that minimizes the risk\nassociated with the interbank exchange rate. Additionally, we show how this\nframework can be used to construct a risk-neutral hedging strategy, which\noffers the same level of hedging coverage as the optimal hedging strategy but\ndoes not require the knowledge of the interbank exchange rate dynamics. Our\nresults highlight the potential of this framework to enhance the risk\nmanagement capabilities of interbank lending relationships.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15555555555555556,
          "p": 0.29577464788732394,
          "f": 0.20388349062824027
        },
        "rouge-2": {
          "r": 0.03125,
          "p": 0.058823529411764705,
          "f": 0.04081632199916752
        },
        "rouge-l": {
          "r": 0.15555555555555556,
          "p": 0.29577464788732394,
          "f": 0.20388349062824027
        }
      }
    },
    {
      "paper_id": "math.CA.math/CA/2503.10190v1",
      "true_abstract": "In a famous paper published in 1904, Helge von Koch introduced the curve that\nstill serves nowadays as an iconic representation of fractal shapes. In fact,\nvon Koch's main goal was the construction of a continuous but nowhere\ndifferentiable function, very similar to the snowflake, using elementary\ngeometric procedures, and not analytical formulae. We prove that a parametrized\nfamily of functions (including and) generalizing von Koch's example enjoys a\nrich multifractal behavior, thus enriching the class of historical mathematical\nobjects having surprising regularity properties. The analysis relies on the\nstudy of the orbits of an underlying dynamical system and on the introduction\nof self-similar measures and non-trivial iterated functions systems adapted to\nthe problem.",
      "generated_abstract": "This paper is devoted to the study of the asymptotic behavior of the\ndistribution of the first positive exit time of a general Markov process in a\nbounded domain with a finite number of exit points. We prove that the\nprobability of the first positive exit time converges to a Gaussian measure on\nthe real line, as the number of exit points tends to infinity, and we give a\nnecessary and sufficient condition for the convergence of the first positive\nexit time to a Gaussian measure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14942528735632185,
          "p": 0.2765957446808511,
          "f": 0.19402984619180233
        },
        "rouge-2": {
          "r": 0.07272727272727272,
          "p": 0.11940298507462686,
          "f": 0.09039547552108294
        },
        "rouge-l": {
          "r": 0.13793103448275862,
          "p": 0.2553191489361702,
          "f": 0.17910447305747396
        }
      }
    },
    {
      "paper_id": "cs.DS.econ/TH/2501.13346v1",
      "true_abstract": "We study a general class of sequential search problems for selecting multiple\ncandidates from different societal groups under \"ex-ante constraints\" aimed at\nproducing socially desirable outcomes, such as demographic parity, diversity\nquotas, or subsidies for disadvantaged groups. Starting with the canonical\nPandora's box model [Weitzman, 1978] under a single affine constraint on\nselection and inspection probabilities, we show that the optimal constrained\npolicy retains an index-based structure similar to the unconstrained case, but\nmay randomize between two dual-based adjustments that are both easy to compute\nand economically interpretable. We then extend our results to handle multiple\naffine constraints by reducing the problem to a variant of the exact\nCarath\\'eodory problem and providing a novel polynomial-time algorithm to\ngenerate an optimal randomized dual-adjusted index-based policy that satisfies\nall constraints simultaneously. Building on these insights, we consider richer\nsearch processes (e.g., search with rejection and multistage search) modeled by\njoint Markov scheduling (JMS) [Dumitriu et al., 2003; Gittins, 1979]. By\nimposing general affine and convex ex-ante constraints, we develop a\nprimal-dual algorithm that randomizes over a polynomial number of dual-based\nadjustments to the unconstrained JMS Gittins indices, yielding a near-feasible,\nnear-optimal policy. Our approach relies on the key observation that a suitable\nrelaxation of the Lagrange dual function for these constrained problems admits\nindex-based policies akin to those in the unconstrained setting. Using a\nnumerical study, we investigate the implications of imposing various\nconstraints, in particular the utilitarian loss (price of fairness), and\nwhether these constraints induce their intended societally desirable outcomes.",
      "generated_abstract": "In this paper, we introduce a new and flexible model for the game of\ngame-theoretic fair division. The model allows for a wide range of game\ntheoretic fairness measures, including non-trivial game-theoretic fairness\nmeasures. The game-theoretic fairness measures considered in this paper\nincorporate aspects of the concept of fairness as defined in economics, and\nthus, the game-theoretic fairness measures considered in this paper are\nconsidered as fairness measures in the economics literature. The game-theoretic\nfairness measures considered in this paper include (1) the concept of fairness\nas defined in economics, (2) the concept of equal pay for equal work, (3) the\nconcept of no-show work, (4) the concept of a fair share, and (5) the\nconcept of a fair share of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05747126436781609,
          "p": 0.19230769230769232,
          "f": 0.08849557167828348
        },
        "rouge-2": {
          "r": 0.008097165991902834,
          "p": 0.022988505747126436,
          "f": 0.011976044051598642
        },
        "rouge-l": {
          "r": 0.05747126436781609,
          "p": 0.19230769230769232,
          "f": 0.08849557167828348
        }
      }
    },
    {
      "paper_id": "math.RT.math/RT/2503.10461v1",
      "true_abstract": "This article studies the compatibility of Koenig's notion of an exact Borel\nsubalgebra of a quasi-hereditary or, more generally, standardly stratified\nalgebra with taking idempotent subalgebras or quotients. As an application, we\nprovide bounds for the multiplicities of indecomposable projectives in the\nprincipal blocks of BGG category $\\mathcal{O}$ having basic regular exact Borel\nsubalgebras.",
      "generated_abstract": "In this paper, we construct a $2$-step nilpotent Lie algebroid $\\mathcal{A}$\nof type $E_6$ over $\\mathbb{C}$ satisfying the following properties:\n\\begin{enumerate}\n  \\item $\\mathcal{A}$ is obtained from the Lie algebroid $T\\mathbb{C}P^3$ by\n  twisting it along the non-repeating $2$-step nilpotent Lie bracket of the\n  Lie algebra $\\mathfrak{su}(2,2)$,\n  \\item the Lie algebroid $\\mathcal{A}$ is non-repeating,\n  \\item the Lie bracket of the nilpotent elements of $\\mathcal{A}$ is given by\n  \\[[X,Y",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11363636363636363,
          "p": 0.13157894736842105,
          "f": 0.12195121453896511
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06818181818181818,
          "p": 0.07894736842105263,
          "f": 0.07317072673408719
        }
      }
    },
    {
      "paper_id": "math.OC.eess/SY/2503.03357v1",
      "true_abstract": "Given a max-plus linear system and a semimodule, the problem of computing the\nmaximal controlled invariant subsemimodule is still open to this day. In this\npaper, we consider this problem for the specific class of fully actuated\nsystems and constraints in the form of precedence semimodules. The assumption\nof full actuation corresponds to the existence of an input for each component\nof the system state. A precedence semimodule is the set of solutions of\ninequalities typically used to represent time-window constraints. We prove\nthat, in this setting, it is possible to (i) compute the maximal controlled\ninvariant subsemimodule and (ii) decide the convergence of a fixed-point\nalgorithm introduced by R.D. Katz in strongly polynomial time.",
      "generated_abstract": "This paper addresses the problem of control synthesis for an uncertain\nunderwater vehicle (UV) with an unknown internal model and uncertain\nenvironmental dynamics. The UV is subject to environmental disturbances and\nuncertain environmental conditions, and the environment is modeled as an\ninfinite-horizon Markov decision process (MDP). The control objective is to\nminimize the mean squared error (MSE) between the estimated state and true\nstate under a given initial condition. The unknown internal model is assumed\nto be unknown at the beginning of the control process. The controller is\nobtained by solving a nonlinear optimization problem (NLP), and the MSE\noptimization problem is converted into an NLP. A linear quadratic regulator\n(LQR) controller is proposed to reduce the MSE of the final state estimation.\nThe LQR controller is obtained by solving a NLP. The stability of the\nestim",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.1794871794871795,
          "f": 0.18064515629053085
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.024793388429752067,
          "f": 0.025862063974807743
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.1794871794871795,
          "f": 0.18064515629053085
        }
      }
    },
    {
      "paper_id": "cs.PL.cs/PL/2503.05924v1",
      "true_abstract": "Techniques that rigorously bound the overall rounding error exhibited by a\nnumerical program are of significant interest for communities developing\nnumerical software. However, there are few available tools today that can be\nused to rigorously bound errors in programs that employ conditional statements\n(a basic need) as well as mixed-precision arithmetic (a direction of\nsignificant future interest) employing global optimization in error analysis.\nIn this paper, we present a new tool that fills this void while also employing\nan abstraction-guided optimization approach to allow designers to trade\nerror-bound tightness for gains in analysis time -- useful when searching for\ndesign alternatives. We first present the basic rigorous analysis framework of\nSatire and then show how to extend it to incorporate abstractions,\nconditionals, and mixed-precision arithmetic. We begin by describing Satire's\ndesign and its performance on a collection of benchmark examples. We then\ndescribe these aspects of Satire: (1) how the error-bound and tool execution\ntime vary with the abstraction level; (2) the additional machinery to handle\nconditional expression branches, including defining the concepts of instability\njumps and instability window widths and measuring these quantities; and (3) how\nthe error changes when a mix of precision values are used. To showcase how\n\\satire can add value during design, we start with a Conjugate Gradient solver\nand demonstrate how its step size and search direction are affected by\ndifferent precision settings. Satire is freely available for evaluation, and\ncan be used during the design of numerical routines to effect design tradeoffs\nguided by rigorous empirical error guarantees.",
      "generated_abstract": "We consider the problem of executing a program in a language that is\nrepresented as a directed acyclic graph. We show that, in this case,\ntranslation-based execution algorithms are not always polynomial. Instead,\nwe present a novel, polynomial-time algorithm that works for any language that\nis represented as a directed acyclic graph and for any language that can be\ntransformed to a directed acyclic graph by an algorithm that is polynomial in\nthe input size. Our algorithm has the advantage that it can be easily\ngeneralized to other graph-representable languages. We also show that, in\nthis case, a program that can be executed in polynomial time by an algorithm\nthat is polynomial in the input size can be executed in polynomial time by our\nalgorithm.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16455696202531644,
          "p": 0.4406779661016949,
          "f": 0.23963133244621893
        },
        "rouge-2": {
          "r": 0.016129032258064516,
          "p": 0.047058823529411764,
          "f": 0.02402402022202443
        },
        "rouge-l": {
          "r": 0.1518987341772152,
          "p": 0.4067796610169492,
          "f": 0.2211981527227166
        }
      }
    },
    {
      "paper_id": "q-fin.ST.q-fin/ST/2412.09631v1",
      "true_abstract": "Limit order book (LOB) is a dynamic, event-driven system that records\nreal-time market demand and supply for a financial asset in a stream flow.\nEvent stream prediction in LOB refers to forecasting both the timing and the\ntype of events. The challenge lies in modeling the time-event distribution to\ncapture the interdependence between time and event type, which has\ntraditionally relied on stochastic point processes. However, modeling complex\nmarket dynamics using stochastic processes, e.g., Hawke stochastic process, can\nbe simplistic and struggle to capture the evolution of market dynamics. In this\nstudy, we present LOBDIF (LOB event stream prediction with diffusion model),\nwhich offers a new paradigm for event stream prediction within the LOB system.\nLOBDIF learns the complex time-event distribution by leveraging a diffusion\nmodel, which decomposes the time-event distribution into sequential steps, with\neach step represented by a Gaussian distribution. Additionally, we propose a\ndenoising network and a skip-step sampling strategy. The former facilitates\neffective learning of time-event interdependence, while the latter accelerates\nthe sampling process during inference. By introducing a diffusion model, our\napproach breaks away from traditional modeling paradigms, offering novel\ninsights and providing an effective and efficient solution for learning the\ntime-event distribution in order streams within the LOB system. Extensive\nexperiments using real-world data from the limit order books of three widely\ntraded assets confirm that LOBDIF significantly outperforms current\nstate-of-the-art methods.",
      "generated_abstract": "This paper develops a novel approach to the portfolio optimization problem\nunder the convexity constraint. The proposed methodology relies on the\nconstrained optimization of the value at risk (VaR) of the portfolio. The\nproposed methodology also introduces a method for computing the expected value\nof the portfolio, which is the objective function of the problem. The\nproposed methodology is used to study the problem of portfolio optimization in\nthe context of the CVA risk. The proposed methodology is illustrated through\nnumerical examples. The results indicate that the proposed methodology offers\nsignificant advantages over the existing methods in terms of computational\nefficiency and portfolio performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1095890410958904,
          "p": 0.2711864406779661,
          "f": 0.15609755687614524
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0958904109589041,
          "p": 0.23728813559322035,
          "f": 0.13658536175419406
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2410.17503v1",
      "true_abstract": "When does a Sender, in a Sender-Receiver game, strictly value commitment? In\na setting with finite actions and finite states, we establish that,\ngenerically, Sender values commitment if and only if he values randomization.\nIn other words, commitment has no value if and only if a partitional experiment\nis optimal under commitment. Moreover, if Sender's preferred cheap-talk\nequilibrium necessarily involves randomization, then Sender values commitment.\nWe also ask: how often (i.e., for what share of preference profiles) does\ncommitment have no value? For any prior, any independent, atomless distribution\nof preferences, and any state space: if there are $\\left|A\\right|$ actions, the\nlikelihood that commitment has no value is at least\n$\\frac{1}{\\left|A\\right|^{\\left|A\\right|}}$. As the number of states grows\nlarge, this likelihood converges precisely to\n$\\frac{1}{\\left|A\\right|^{\\left|A\\right|}}$.",
      "generated_abstract": "In this paper, we introduce a new approach to the study of the interaction\nbetween private information and social structures. We focus on the case of\nnon-cooperative strategic games with two or more players. We introduce a new\nform of interaction, which we call \\textit{internal} interaction, which is\nassociated with the internal structure of the game. We study the\nevolutionary dynamics of the game with internal interaction and show that the\nevolution of the payoff function is determined by the evolution of the internal\nstructure of the game. Moreover, we provide conditions under which the\nevolution of the internal structure determines the evolution of the payoff\nfunction. We also provide conditions under which the evolution of the internal\nstructure determines the evolution of the payoff function, and we show that the\nevolution of the internal structure can be described by an evolution equation.\nFinally, we provide a simple example in which we show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17777777777777778,
          "p": 0.24242424242424243,
          "f": 0.20512820024654843
        },
        "rouge-2": {
          "r": 0.008547008547008548,
          "p": 0.00980392156862745,
          "f": 0.00913241511478347
        },
        "rouge-l": {
          "r": 0.17777777777777778,
          "p": 0.24242424242424243,
          "f": 0.20512820024654843
        }
      }
    },
    {
      "paper_id": "cs.LG.cs/LG/2503.10633v1",
      "true_abstract": "As there are now millions of publicly available neural networks, searching\nand analyzing large model repositories becomes increasingly important.\nNavigating so many models requires an atlas, but as most models are poorly\ndocumented charting such an atlas is challenging. To explore the hidden\npotential of model repositories, we chart a preliminary atlas representing the\ndocumented fraction of Hugging Face. It provides stunning visualizations of the\nmodel landscape and evolution. We demonstrate several applications of this\natlas including predicting model attributes (e.g., accuracy), and analyzing\ntrends in computer vision models. However, as the current atlas remains\nincomplete, we propose a method for charting undocumented regions.\nSpecifically, we identify high-confidence structural priors based on dominant\nreal-world model training practices. Leveraging these priors, our approach\nenables accurate mapping of previously undocumented areas of the atlas. We\npublicly release our datasets, code, and interactive atlas.",
      "generated_abstract": "We investigate the problem of estimating the mean of a general multivariate\ndistribution using only the observed samples of its marginal. This problem is\ncommonly known as the \\emph{mean estimation} problem and has been studied in\nthe literature for decades. However, the problem has recently attracted\nsignificant interest due to its relevance in data analysis and machine learning\ndue to its application in various fields, such as recommender systems,\nmulti-armed bandit, and other optimization problems. The mean estimation problem\nis NP-hard, and even solving the problem in polynomial time is a challenging\nproblem. In this paper, we propose a new algorithm for solving the mean\nestimation problem that achieves polynomial time running time. The algorithm\ntakes advantage of the fact that the mean is a linear combination of the\nmarginals. It is based on an iterative method that takes the mean of the\ncurrent estimate and the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19811320754716982,
          "p": 0.2413793103448276,
          "f": 0.21761657535933862
        },
        "rouge-2": {
          "r": 0.04285714285714286,
          "p": 0.046153846153846156,
          "f": 0.04444443945130372
        },
        "rouge-l": {
          "r": 0.1792452830188679,
          "p": 0.21839080459770116,
          "f": 0.19689118675830236
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2503.00772v1",
      "true_abstract": "With the rapid advancement of information technology and data collection\nsystems, large-scale spatial panel data presents new methodological and\ncomputational challenges. This paper introduces a dynamic spatial panel\nquantile model that incorporates unobserved heterogeneity. The proposed model\ncaptures the dynamic structure of panel data, high-dimensional cross-sectional\ndependence, and allows for heterogeneous regression coefficients. To estimate\nthe model, we propose a novel Bayesian Markov Chain Monte Carlo (MCMC)\nalgorithm. Contributions to Bayesian computation include the development of\nquantile randomization, a new Gibbs sampler for structural parameters, and\nstabilization of the tail behavior of the inverse Gaussian random generator. We\nestablish Bayesian consistency for the proposed estimation method as both the\ntime and cross-sectional dimensions of the panel approach infinity. Monte Carlo\nsimulations demonstrate the effectiveness of the method. Finally, we illustrate\nthe applicability of the approach through a case study on the quantile\nco-movement structure of the gasoline market.",
      "generated_abstract": "We study the estimation and inference of a class of heterogeneous linear\nestimands, including the mean and variance of a latent variable, in the\npresence of non-identical treatment effects. We propose a nonparametric\napproach based on the estimation of a class of doubly robust estimands, which\nis obtained by replacing the estimand of interest with a doubly robust estimand\nthat is estimated under the alternative hypothesis. We provide a general\ntheory for the consistency and asymptotic normality of the doubly robust\nestimand and derive its asymptotic distribution under mild conditions.\nFurthermore, we provide a theoretical interpretation of the doubly robust\nestimand in terms of the estimand of interest and its counterpart in the\nalternative hypothesis. We provide two simulation studies to compare the\ndoubly robust estimand with the estimand of interest and its counterpart in the\nalternative hypothesis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16494845360824742,
          "p": 0.24615384615384617,
          "f": 0.19753085939262321
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.0392156862745098,
          "f": 0.03319501586474134
        },
        "rouge-l": {
          "r": 0.12371134020618557,
          "p": 0.18461538461538463,
          "f": 0.14814814334324053
        }
      }
    },
    {
      "paper_id": "cs.DC.cs/DC/2503.09917v1",
      "true_abstract": "MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing\nCenter (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of\n314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel\nSapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory\n(HBM), organized into four partitions optimized for diverse workloads. This\ndocument evaluates MareNostrum5 through micro-benchmarks (floating-point\nperformance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL\nand HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights\nMareNostrum5's scalability, efficiency, and energy performance, utilizing the\nEAR (Energy Aware Runtime) framework to assess power consumption and the\neffects of direct liquid cooling. Additionally, HBM and DDR5 configurations are\ncompared to examine memory performance trade-offs. Designed to complement\nstandard technical documentation, this study provides insights to guide both\nnew and experienced users in optimizing their workloads and maximizing\nMareNostrum5's computational capabilities.",
      "generated_abstract": "This paper presents a comprehensive evaluation of the performance of the\nTensorFlow Lite (TfLite) model trained using the dataset from the 2018 ICDAR\nchallenge. The model was trained using the ImageNet dataset and its performance\nwas evaluated on the same dataset. The model was then converted to TfLite\nformat using the TfLiteConverter and the performance of the model was\nevaluated on the same dataset. We also compare the performance of the model\ntrained on the ImageNet dataset with the model trained on the 2018 ICDAR\ndataset. The model trained on the 2018 ICDAR dataset achieves a performance\ncomparable to the model trained on the ImageNet dataset. We also compare the\nperformance of the model trained on the 2018 ICDAR dataset with the model\ntrained on the ImageNet dataset on two different datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07142857142857142,
          "p": 0.18604651162790697,
          "f": 0.10322580244245594
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.045454545454545456,
          "f": 0.029411760328720376
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.18604651162790697,
          "f": 0.10322580244245594
        }
      }
    },
    {
      "paper_id": "nlin.PS.q-bio/CB/2409.00623v1",
      "true_abstract": "For a cell-bulk ODE-PDE model in $\\mathbb{R}^2$, a hybrid\nasymptotic-numerical theory is developed to provide a new theoretical and\ncomputationally efficient approach for studying how oscillatory dynamics\nassociated with spatially segregated dynamically active ``units\" or ``cells\"\nare regulated by a PDE bulk diffusion field that is both produced and absorbed\nby the entire cell population. The study of oscillator synchronization in a PDE\ndiffusion field was one of the initial aims of Yoshiki Kuramoto's foundational\nwork. For this cell-bulk model, strong localized perturbation theory, as\nextended to a time-dependent setting, is used to derive a new\nintegro-differential ODE system that characterizes intracellular dynamics in a\nmemory-dependent bulk-diffusion field. For this nonlocal reduced system, a\nnovel fast time-marching scheme, relying in part on the\n\\emph{sum-of-exponentials method} to numerically treat convolution integrals,\nis developed to rapidly and accurately compute numerical solutions to the\nintegro-differential system over long time intervals. For the special case of\nSel'kov reaction kinetics, a wide variety of large-scale oscillatory dynamical\nbehavior including phase synchronization, mixed-mode oscillations, and\nquorum-sensing are illustrated for various ranges of the influx and efflux\npermeability parameters, the bulk degradation rate and bulk diffusivity, and\nthe specific spatial configuration of cells. Results from our fast algorithm,\nobtained in under one minute of CPU time on a laptop, are benchmarked against\nPDE simulations of the cell-bulk model, which are performed with a commercial\nPDE solver, that have run-times that are orders of magnitude larger.",
      "generated_abstract": "We investigate the long-term evolution of a two-dimensional bacterial\nphenotypic population model, where the bacterial growth rate is a function of\nboth the population density and the environmental temperature. The model\nassumes that the population dynamics are driven by a simple kinetic model for\nthe bacterial growth rate. The model has the potential to explain the\nobservations of the bacterial density at the Earth's surface. Here we show\nthat the population dynamics are driven by a simple kinetic model, the\nGompertz-Browning model, that can be written as a differential equation. We\nperform a stochastic simulation of the model using a Monte Carlo method to\nobtain the population density at any point in time. We perform a Markov\nchain Monte Carlo simulation to estimate the probability distribution of the\npopulation density, and use this to calculate the probability distribution of\nthe time required for the population to reach",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1337579617834395,
          "p": 0.28378378378378377,
          "f": 0.1818181774636908
        },
        "rouge-2": {
          "r": 0.013215859030837005,
          "p": 0.02586206896551724,
          "f": 0.017492706893897394
        },
        "rouge-l": {
          "r": 0.12738853503184713,
          "p": 0.2702702702702703,
          "f": 0.17316016880568216
        }
      }
    },
    {
      "paper_id": "math-ph.math-ph/2503.09827v1",
      "true_abstract": "This paper defines coherent manifolds and discusses their properties and\ntheir application in quantum mechanics. Every coherent manifold with a large\ngroup of symmetries gives rise to a Hilbert space, the completed quantum space\nof $Z$, which contains a distinguished family of coherent states labeled by the\npoints of the manifold.\n  The second quantization map in quantum field theory is generalized to\nquantization operators on arbitrary coherent manifolds. It is shown how the\nSchr\\\"odinger equation on any such completed quantum space can be solved in\nterms of computations only involving the coherent product. In particular, this\napplies to a description of Bosonic Fock spaces as completed quantum spaces of\na class of coherent manifolds called Klauder spaces.",
      "generated_abstract": "We study the effect of a high-order nonlinearity on the nonlinear\ndynamics of a two-species Boussinesq fluid in an anisotropic background,\ncharacterized by a gradient of a magnetic field. We first show that the\neffect of the nonlinearity on the dynamics of the system is qualitatively\ndifferent depending on the direction of the gradient of the magnetic field. For\nthe case where the gradient of the magnetic field is parallel to the\nmagnetization of the fluids, we show that the effect of the nonlinearity on the\ndynamics of the system is to increase the frequency of the oscillations of the\nmagnetic field. For the case where the gradient of the magnetic field is\nperpendicular to the magnetization of the fluids, we show that the effect of the\nnonlinearity on the dynamics of the system is to increase the amplitude of the\nmagnetic field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11842105263157894,
          "p": 0.2,
          "f": 0.14876032590670052
        },
        "rouge-2": {
          "r": 0.018518518518518517,
          "p": 0.028985507246376812,
          "f": 0.02259886529924451
        },
        "rouge-l": {
          "r": 0.11842105263157894,
          "p": 0.2,
          "f": 0.14876032590670052
        }
      }
    },
    {
      "paper_id": "math.CO.cs/DM/2503.09525v1",
      "true_abstract": "The complexity of continuous piecewise affine (CPA) functions can be measured\nby the number of pieces $p$ or the number of distinct affine functions $n$. For\nCPA functions on $\\mathbb{R}^d$, this paper shows an upper bound of\n$p=O(n^{d+1})$ and constructs a family of functions achieving a lower bound of\n$p=\\Omega(n^{d+1-\\frac{c}{\\sqrt{\\log_2(n)}}})$.",
      "generated_abstract": "We study a class of strongly regular graphs that includes the class of\ngraphs with bounded degree and minimum degree. We prove that these graphs are\nstrongly $1$-connected, and in particular, they are strongly connected. We\nshow that these graphs have a unique vertex partition into two classes, each of\nwhich is $1$-connected. In the case of the class of strongly regular graphs,\nthis partition is determined by the graph. In the case of the class of\ngraphs with bounded degree and minimum degree, the partition is determined by\nthe graph with a vertex partition into two classes. We show that every\n$1$-connected strongly regular graph is strongly $1$-connected. Finally, we\nprovide an algorithm to determine the partition of a strongly regular graph.\n  We also show that if a strongly regular graph is strongly $1$-connected, then\nit is strongly $1$-connected by a single edge. Finally, we show that if a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1891891891891892,
          "p": 0.11864406779661017,
          "f": 0.1458333285959203
        },
        "rouge-2": {
          "r": 0.021739130434782608,
          "p": 0.00980392156862745,
          "f": 0.013513509229365858
        },
        "rouge-l": {
          "r": 0.16216216216216217,
          "p": 0.1016949152542373,
          "f": 0.12499999526258698
        }
      }
    },
    {
      "paper_id": "nlin.SI.nlin/SI/2503.06013v1",
      "true_abstract": "Hirota's discrete KdV (dKdV) equation is an integrable autonomous partial\ndifference equation on $\\mathbb{Z}^2$ that reduces to the Korteweg-de Vries\n(KdV) equation in a continuum limit. In this paper, we introduce a new\nnon-autonomous version of the dKdV equation. Furthermore, we show that the new\nequation is integrable and admits discrete Painlev\\'e transcendent solutions\ndescribed by $q$-Painlev\\'e equations of $A_J^{(1)}$-surface types\n($J=3,4,5,6$).",
      "generated_abstract": "We develop a novel method for simulating the continuous-time Markov chain\n(CTMC) model, which was introduced by P\\'eclet in 1860, and use it to study\nstochastic fluctuations of the CTMC. The model is defined by a CTMC with a\ndiscrete-time delay, where the delay is defined as a random variable. The CTMC\nis coupled with a deterministic process, which is a CTMC with a continuous-time\ndelay. We derive the CTMC transition probabilities, and derive a stochastic\ndifferential equation for the CTMC with a continuous-time delay. We use this\nequation to derive the CTMC transition probabilities, and we use this derived\nprobability matrix to derive the CTMC transition probabilities. We also derive\nthe CTMC transition probabilities from the probability matrix of the CTMC, and\nwe use this derived transition probability to derive the CT",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22916666666666666,
          "p": 0.1864406779661017,
          "f": 0.2056074716883572
        },
        "rouge-2": {
          "r": 0.016666666666666666,
          "p": 0.01020408163265306,
          "f": 0.012658223137319489
        },
        "rouge-l": {
          "r": 0.22916666666666666,
          "p": 0.1864406779661017,
          "f": 0.2056074716883572
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.08231v1",
      "true_abstract": "We discuss necessary conditions for a PAC-Bayes bound to provide a meaningful\ngeneralisation guarantee. Our analysis reveals that the optimal generalisation\nguarantee depends solely on the distribution of the risk induced by the prior\ndistribution. In particular, achieving a target generalisation level is only\nachievable if the prior places sufficient mass on high-performing predictors.\nWe relate these requirements to the prevalent practice of using data-dependent\npriors in deep learning PAC-Bayes applications, and discuss the implications\nfor the claim that PAC-Bayes ``explains'' generalisation.",
      "generated_abstract": "A number of recent advancements in the field of deep learning have been\ndriven by the development of large language models (LLMs). These models have\nachieved impressive performance across a wide range of tasks, including\ntextual summarization, question answering, and generative modeling. However,\nthese advancements have been hindered by the lack of a clear understanding of\nhow these models learn their representations. In this work, we introduce a\nframework for understanding the behavior of LLMs, known as LLM-Learning,\nthrough the lens of learning theory. By leveraging techniques from the theory\nof learning theory, we demonstrate how LLMs learn representations that are\nclosely aligned with those found in natural language processing tasks. We\ndemonstrate that this alignment is achieved through a process of iterative\nrefinement, where the LLM iteratively refines its representations until it\nachieves the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2413793103448276,
          "p": 0.15217391304347827,
          "f": 0.18666666192355566
        },
        "rouge-2": {
          "r": 0.02531645569620253,
          "p": 0.015267175572519083,
          "f": 0.019047614354196166
        },
        "rouge-l": {
          "r": 0.22413793103448276,
          "p": 0.14130434782608695,
          "f": 0.17333332859022235
        }
      }
    },
    {
      "paper_id": "stat.ML.q-bio/PE/2502.04730v1",
      "true_abstract": "Learning informative representations of phylogenetic tree structures is\nessential for analyzing evolutionary relationships. Classical distance-based\nmethods have been widely used to project phylogenetic trees into Euclidean\nspace, but they are often sensitive to the choice of distance metric and may\nlack sufficient resolution. In this paper, we introduce phylogenetic\nvariational autoencoders (PhyloVAEs), an unsupervised learning framework\ndesigned for representation learning and generative modeling of tree\ntopologies. Leveraging an efficient encoding mechanism inspired by\nautoregressive tree topology generation, we develop a deep latent-variable\ngenerative model that facilitates fast, parallelized topology generation.\nPhyloVAE combines this generative model with a collaborative inference model\nbased on learnable topological features, allowing for high-resolution\nrepresentations of phylogenetic tree samples. Extensive experiments demonstrate\nPhyloVAE's robust representation learning capabilities and fast generation of\nphylogenetic tree topologies.",
      "generated_abstract": "This paper introduces the concept of a \\textit{multidimensional Gaussian\nmultinomial distribution} (MDGMD) to address multidimensional imbalance in\nclassification. The MDGMD extends the Gaussian multinomial distribution by\nadding a new dimension, and uses a non-linear mixture model to model the\nimbalance between the classes. The mixture component models the imbalance as a\nmultinomial distribution, and is parameterized by a vector of weights. The\ndistribution is used to model the conditional probability of the classes for\neach observation. The model is applied to a range of classification tasks, and\nits performance is evaluated using benchmark datasets. The MDGMD outperforms\nother multidimensional Gaussian multinomial distributions, and also outperforms\nexisting multidimensional Gaussian mixtures, in terms of accuracy and\nperformance metrics. It is also shown that the MDGMD provides a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11458333333333333,
          "p": 0.15942028985507245,
          "f": 0.13333332846721782
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.008620689655172414,
          "f": 0.008510633298690123
        },
        "rouge-l": {
          "r": 0.11458333333333333,
          "p": 0.15942028985507245,
          "f": 0.13333332846721782
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.06821v1",
      "true_abstract": "The exploration of Bird's-Eye View (BEV) mapping technology has driven\nsignificant innovation in visual perception technology for autonomous driving.\nBEV mapping models need to be applied to the unlabeled real world, making the\nstudy of unsupervised domain adaptation models an essential path. However,\nresearch on unsupervised domain adaptation for BEV mapping remains limited and\ncannot perfectly accommodate all BEV mapping tasks. To address this gap, this\npaper proposes HierDAMap, a universal and holistic BEV domain adaptation\nframework with hierarchical perspective priors. Unlike existing research that\nsolely focuses on image-level learning using prior knowledge, this paper\nexplores the guiding role of perspective prior knowledge across three distinct\nlevels: global, sparse, and instance levels. With these priors, HierDA consists\nof three essential components, including Semantic-Guided Pseudo Supervision\n(SGPS), Dynamic-Aware Coherence Learning (DACL), and Cross-Domain Frustum\nMixing (CDFM). SGPS constrains the cross-domain consistency of perspective\nfeature distribution through pseudo labels generated by vision foundation\nmodels in 2D space. To mitigate feature distribution discrepancies caused by\nspatial variations, DACL employs uncertainty-aware predicted depth as an\nintermediary to derive dynamic BEV labels from perspective pseudo-labels,\nthereby constraining the coarse BEV features derived from corresponding\nperspective features. CDFM, on the other hand, leverages perspective masks of\nview frustum to mix multi-view perspective images from both domains, which\nguides cross-domain view transformation and encoding learning through mixed BEV\nlabels. The proposed method is verified on multiple BEV mapping tasks, such as\nBEV semantic segmentation, high-definition semantic, and vectorized mapping.\nThe source code will be made publicly available at\nhttps://github.com/lynn-yu/HierDAMap.",
      "generated_abstract": "The rapid advancement of deep learning techniques has led to the development\nof deep learning-based algorithms for computer vision tasks, including object\ndetection, segmentation, and recognition. However, these models often encounter\ncomputational challenges in real-world scenarios due to the limited GPU memory\nand limited computational power of CPUs. To address these limitations, this\npaper introduces a novel GPU-based deep learning framework, called\nGPU-DL, which employs GPU acceleration to accelerate deep learning\ncomputations. GPU-DL is built on the PyTorch framework, which is a\nhigh-performance deep learning platform. It consists of a Python-based\ninterface and a high-performance CUDA-based implementation. GPU-DL enables\nacceleration of deep learning computations on GPUs, reducing training time and\nenhancing performance. The framework is designed to be flexible and\ninteroperable, allowing users to",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16292134831460675,
          "p": 0.34523809523809523,
          "f": 0.22137404144513734
        },
        "rouge-2": {
          "r": 0.024793388429752067,
          "p": 0.05263157894736842,
          "f": 0.03370786081492293
        },
        "rouge-l": {
          "r": 0.16292134831460675,
          "p": 0.34523809523809523,
          "f": 0.22137404144513734
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ML/2503.01287v1",
      "true_abstract": "Simulation-based inference (SBI) methods typically require fully observed\ndata to infer parameters of models with intractable likelihood functions.\nHowever, datasets often contain missing values due to incomplete observations,\ndata corruptions (common in astrophysics), or instrument limitations (e.g., in\nhigh-energy physics applications). In such scenarios, missing data must be\nimputed before applying any SBI method. We formalize the problem of missing\ndata in SBI and demonstrate that naive imputation methods can introduce bias in\nthe estimation of SBI posterior. We also introduce a novel amortized method\nthat addresses this issue by jointly learning the imputation model and the\ninference network within a neural posterior estimation (NPE) framework.\nExtensive empirical results on SBI benchmarks show that our approach provides\nrobust inference outcomes compared to standard baselines for varying levels of\nmissing data. Moreover, we demonstrate the merits of our imputation model on\ntwo real-world bioactivity datasets (Adrenergic and Kinase assays). Code is\navailable at https://github.com/Aalto-QuML/RISE.",
      "generated_abstract": "The performance of neural networks (NNs) on classification tasks has been\ntested under various assumptions regarding the underlying distribution of the\ndata. However, the assumptions are often restrictive and, in many cases,\nuncertain. In this paper, we propose a novel methodology for testing the\ndistributional assumptions of NNs under uncertainty. The methodology relies on\nthe use of a Markov chain Monte Carlo (MCMC) algorithm to generate a large\nnumber of NNs, each with a specific distributional assumption, and then\ncomputes the empirical Bayes estimates of the parameters of the NNs, which\nprovide a test statistic. This statistic is then used to perform hypothesis\ntesting using the traditional method of statistical tests. We prove that the\nempirical Bayes estimates are consistent, and that the hypothesis test is\npowerful under the assumptions of the methodology. We demonstrate the\napplicability of the method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.21238938053097345,
          "p": 0.27586206896551724,
          "f": 0.2399999950845001
        },
        "rouge-2": {
          "r": 0.019867549668874173,
          "p": 0.023255813953488372,
          "f": 0.021428566459439927
        },
        "rouge-l": {
          "r": 0.18584070796460178,
          "p": 0.2413793103448276,
          "f": 0.20999999508450015
        }
      }
    },
    {
      "paper_id": "cs.CG.cs/CG/2503.09115v1",
      "true_abstract": "We prove a quasi-linear upper bound on the size of $K_{t,t}$-free polygon\nvisibility graphs. For visibility graphs of star-shaped and monotone polygons\nwe show a linear bound. In the more general setting of $n$ points on a simple\nclosed curve and visibility pseudo-segments, we provide an $O(n \\log n)$ upper\nbound and an $\\Omega(n\\alpha(n))$ lower bound.",
      "generated_abstract": "We study the optimal partition of a set of points into two disjoint subsets\nthat minimize the sum of distances to the nearest points in each of the\nsub-sets. We show that this problem is NP-hard even when the points are\nuniformly distributed on a circle. We then give an approximation algorithm that\nruns in polynomial time and uses a constant fraction of the space of all\npartitions. The approximation factor is optimal up to a constant, and the\nconstant is sharp. We prove that the approximation factor can be further\nimproved to $O(1/\\sqrt{n})$, where $n$ is the number of points. Our result\nextends the recent results of Kang et al. [2023",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.28205128205128205,
          "p": 0.1527777777777778,
          "f": 0.19819819364012672
        },
        "rouge-2": {
          "r": 0.03773584905660377,
          "p": 0.018518518518518517,
          "f": 0.024844716080398916
        },
        "rouge-l": {
          "r": 0.28205128205128205,
          "p": 0.1527777777777778,
          "f": 0.19819819364012672
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2502.17461v1",
      "true_abstract": "We prove that if a given reaction network $\\mathcal{N}$ has a weakly\nreversible deficiency zero realization for all choice of rate constants, then\nthere exists a $\\textit{unique}$ weakly reversible deficiency zero network\n$\\mathcal{N}'$ such that $\\mathcal{N}$ is realizable by $\\mathcal{N}'$.\nAdditionally, we propose an algorithm to find this weakly reversible deficiency\nzero network $\\mathcal{N}'$ when it exists.",
      "generated_abstract": "We present a computational framework for predicting protein-protein\ninteraction (PPI) networks using large language models (LLMs). Our approach\nemploys a multi-step approach that integrates LLMs, a molecular modeling\nframework, and a graph neural network (GNN) model. This approach enables us to\nmodel the PPI network structure while accounting for the structural and\nfunctional properties of the proteins, ensuring that the predicted PPI\nnetworks are accurate and realistic. We apply our method to two datasets: the\nHuman Protein Atlas (HPA) and the Drug Discovery Informatics Resource\n(DDIR) and demonstrate that our method is capable of accurately predicting\nPPI networks. Our approach offers a promising approach for biological research\nand the development of biomarkers for disease diagnosis.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.1038961038961039,
          "f": 0.13675213225217345
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.1038961038961039,
          "f": 0.13675213225217345
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.14708v1",
      "true_abstract": "We conduct an incentivized laboratory experiment to study people's perception\nof generative artificial intelligence (GenAI) alignment in the context of\neconomic decision-making. Using a panel of economic problems spanning the\ndomains of risk, time preference, social preference, and strategic\ninteractions, we ask human subjects to make choices for themselves and to\npredict the choices made by GenAI on behalf of a human user. We find that\npeople overestimate the degree of alignment between GenAI's choices and human\nchoices. In every problem, human subjects' average prediction about GenAI's\nchoice is substantially closer to the average human-subject choice than it is\nto the GenAI choice. At the individual level, different subjects' predictions\nabout GenAI's choice in a given problem are highly correlated with their own\nchoices in the same problem. We explore the implications of people\noverestimating GenAI alignment in a simple theoretical model.",
      "generated_abstract": "We investigate the implications of a trade-off between efficiency and\noptimality in a simple matching model where agents' preferences are\nconstrained to be monotone and additive. In this model, agents can only\nparticipate in matching if they satisfy their preferences and are matched\ntogether. We characterize the Nash equilibrium and the optimality conditions of\nthe model under these preferences and show that the matching is efficient if and\nonly if the set of Nash equilibria is a singleton. We further provide a\ncharacterization of the set of Nash equilibria in terms of the set of\nmonotone and additive preferences.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17777777777777778,
          "p": 0.2962962962962963,
          "f": 0.22222221753472232
        },
        "rouge-2": {
          "r": 0.03731343283582089,
          "p": 0.05747126436781609,
          "f": 0.04524886400442302
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.2777777777777778,
          "f": 0.20833332864583345
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/SC/2402.14887v4",
      "true_abstract": "Metabolic pathways are fundamental maps in biochemistry that detail how\nmolecules are transformed through various reactions. The complexity of\nmetabolic network, where a single compound can play a part in multiple\npathways, poses a challenge in inferring metabolic balance changes over time or\nafter different treatments. Isotopic labeling experiment is the standard method\nto infer metabolic flux, which is currently defined as the flow of a single\nmetabolite through a given pathway over time. However, there is still no way to\naccurately infer the metabolic balance changes after different treatments in an\nexperiment. This study introduces a different concept: molecular weight\ndistribution, which is the empirical distribution of the molecular weights of\nall metabolites of interest. By estimating the differences of the location and\nscale estimates of these distributions, it becomes possible to quantitatively\ninfer the metabolic balance changes even without requiring knowledge of the\nexact chemical structures of these compounds and their related pathways. This\nresearch article provides a mathematical framing for a classic biological\nconcept.",
      "generated_abstract": "Cell-type-specific gene expression patterns are key to understanding the\nbiological processes of cells, but existing techniques are often slow and\ncomputationally demanding. We introduce CellMap, a novel method for\nautomatically generating cell-type-specific expression maps from single-cell RNA\nsequencing data. CellMap uses a deep learning framework to learn biologically\nmeaningful patterns in expression data, enabling the generation of high-quality\ncell-type-specific expression maps from raw data. Our method provides a\ncomputationally efficient alternative to existing methods, offering a\nscalable, computationally efficient solution for automated cell-type-specific\ngene expression analysis. We validate CellMap on three publicly available\ndatasets, demonstrating its ability to generate cell-type-specific expression\nmaps with high accuracy and reproducibility.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10909090909090909,
          "p": 0.16,
          "f": 0.12972972490869267
        },
        "rouge-2": {
          "r": 0.006622516556291391,
          "p": 0.010101010101010102,
          "f": 0.007999995216322858
        },
        "rouge-l": {
          "r": 0.10909090909090909,
          "p": 0.16,
          "f": 0.12972972490869267
        }
      }
    },
    {
      "paper_id": "astro-ph.GA.astro-ph/GA/2503.09753v1",
      "true_abstract": "This thesis investigates the evolution of galaxies in diverse environments,\nutilizing Sloan Digital Sky Survey (SDSS) data to explore the impact of\nenvironmental richness on central and satellite galaxies across stellar mass\nranges, compared to isolated systems. The sample is limited to 0.03 < z < 0.1\nand apparent magnitudes brighter than 17.78, ensuring spectroscopic\ncompleteness and reliable stellar population estimates. Galaxies are\ncategorized by environment as field or cluster/group systems, with further\nseparation into satellites and centrals. By analyzing the star formation rate\n(SFR)-stellar mass plane, this work identifies systematic differences in the\nblue cloud (BC), green valley (GV), and red sequence (RS) across environments.\nMorphological and stellar population analyses reveal that T-type, metallicity,\nand stellar age transitions highlight the role of environmental quenching. A\nnewly introduced T-Type \\emph{vs.} specific SFR diagram provides evidence that\nmorphological transformation precedes full quenching. Correlating galaxy\nproperties with time since infall through projected phase space confirms the\ndelayed-then-rapid quenching model for low- and intermediate-mass galaxies,\nextending it to morphology. Time-scales for quenching and morphological\ntransitions are also derived as a function of stellar mass.",
      "generated_abstract": "We present a new method for obtaining the radial velocities of stars from\nobservations of their light curves. The method is based on the combination of\ntwo algorithms, each designed for different applications. The first algorithm\nis designed to obtain the radial velocities of stars in the low-resolution\nspectra of bright sources. The second algorithm is designed to obtain the\nradial velocities of stars in the high-resolution spectra of faint sources. In\nthe current paper we present the first application of the new algorithm for\nobtaining the radial velocities of stars in the light curves of the sources\nobserved by the GALEX satellite. We used the GALEX FUV/NUV light curves of the\nstars V1222 Lyr, V1244 Lyr, and V1257 Lyr to obtain the radial velocities of\nthese stars. We compared the obtained radial",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09090909090909091,
          "p": 0.21666666666666667,
          "f": 0.12807881356985137
        },
        "rouge-2": {
          "r": 0.005555555555555556,
          "p": 0.010309278350515464,
          "f": 0.007220212055418682
        },
        "rouge-l": {
          "r": 0.08391608391608392,
          "p": 0.2,
          "f": 0.11822659682108293
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2501.00578v1",
      "true_abstract": "I propose a model of aggregation of intervals relevant to the study of legal\nstandards of tolerance. Seven axioms: responsiveness, anonymity, continuity,\nstrategyproofness, and three variants of neutrality are then used to prove\nseveral important results about a new class of aggregation methods called\nendpoint rules. The class of endpoint rules includes extreme tolerance\n(allowing anything permitted by anyone) and a form of majoritarianism (the\nmedian rule).",
      "generated_abstract": "The standard game-theoretic model of competition in which a firm competes\nwith the market for a product is not sufficient to explain why some firms\nsucceed and others fail. We develop a model of competition in which firms\ncompete with each other over a set of resources. We show that this model\ngeneralizes the standard game-theoretic model and allows firms to capture\ninteractions between resources and their use. Our model provides a framework\nfor understanding why some firms are able to capture the market and others are\nnot. We apply our model to the US automotive industry and show that some\ncompanies are able to capture the market for new vehicles, while others are\nnot. We also show that the market is not perfectly competitive, with\nsub-optimal firms, and we describe the implications of our findings for the\nliterature on market failure.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17647058823529413,
          "p": 0.1232876712328767,
          "f": 0.14516128547996895
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.02564102564102564,
          "f": 0.033333328783333956
        },
        "rouge-l": {
          "r": 0.17647058823529413,
          "p": 0.1232876712328767,
          "f": 0.14516128547996895
        }
      }
    },
    {
      "paper_id": "cs.IT.cs/IT/2503.09174v1",
      "true_abstract": "This paper examines the number of communication modes, that is, the degrees\nof freedom (DoF), in a wireless setup comprising a small continuous linear\nintelligent antenna array in the near field of a large one. The framework\nallows for any orientations between the arrays and any positions in a\ntwo-dimensional space assuming that the transmitting array is placed at the\norigin. Therefore, apart from the length of the two continuous arrays, four key\nparameters determine the DoF and are hence considered in the analysis: the\nCartesian coordinates of the center of the receiving array and two angles that\nmodel the rotation of each array around its center. The paper starts with the\ncalculation of the deterministic DoF for a generic geometric setting, which\nextends beyond the widely studied paraxial case. Subsequently, a stochastic\ngeometry framework is proposed to study the statistical DoF, as a first step\ntowards the investigation of the system-level performance in near field\nnetworks. Numerical results applied to millimeter wave networks reveal the\nlarge number of DoF provided by near-field communications and unveiled key\nsystem-level insights.",
      "generated_abstract": "In recent years, deep learning has achieved remarkable progress in image\nprocessings, but the efficiency of deep neural networks in processing large-scale\nvideo data remains a challenging issue. To address this issue, we propose a\nnovel approach based on the fusion of video and image modalities, namely\nfusion-VIM. First, we introduce a fusion module for enhancing the\ninterpretability of the feature maps. Then, we propose a video-to-image\ntransformation module to transform the feature maps into image-like ones.\nFinally, a fusion module is designed to fuse the feature maps from the\ntransformed image-like feature maps. The experimental results show that our\nproposed fusion-VIM achieves competitive performance in terms of both\nobject-centric and scene-centric recognition, particularly in object\nrecognition.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13274336283185842,
          "p": 0.18518518518518517,
          "f": 0.15463917039377206
        },
        "rouge-2": {
          "r": 0.01764705882352941,
          "p": 0.028037383177570093,
          "f": 0.021660645078133166
        },
        "rouge-l": {
          "r": 0.11504424778761062,
          "p": 0.16049382716049382,
          "f": 0.13402061369274115
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.03151v1",
      "true_abstract": "Subset selection is central to many wireless communication problems,\nincluding link scheduling, power allocation, and spectrum management. However,\nthese problems are often NP-complete, because of which heuristic algorithms\napplied to solve these problems struggle with scalability in large-scale\nsettings. To address this, we propose a determinantal point process-based\nlearning (DPPL) framework for efficiently solving general subset selection\nproblems in massive networks. The key idea is to model the optimal subset as a\nrealization of a determinantal point process (DPP), which balances the\ntrade-off between quality (signal strength) and similarity (mutual\ninterference) by enforcing negative correlation in the selection of {\\em\nsimilar} links (those that create significant mutual interference). However,\nconventional methods for constructing similarity matrices in DPP impose\ndecomposability and symmetry constraints that often do not hold in practice. To\novercome this, we introduce a new method based on the Gershgorin Circle Theorem\nfor constructing valid similarity matrices. The effectiveness of the proposed\napproach is demonstrated by applying it to two canonical wireless network\nsettings: an ad hoc network in 2D and a cellular network serving drones in 3D.\nSimulation results show that DPPL selects near-optimal subsets that maximize\nnetwork sum-rate while significantly reducing computational complexity compared\nto traditional optimization methods, demonstrating its scalability for\nlarge-scale networks.",
      "generated_abstract": "This paper introduces a novel method for compressing the joint estimation and\ntransmission of a dual-band MIMO-OFDM system. Our approach combines the\ninverse Gaussian (IG) distribution with the block-wise log-concave (BWLC)\ndistribution, which is used to model the joint distribution of the\nmultipath-free channel and the interference channel. This distribution is\nincorporated into the generalized inverse Gaussian (GIG) distribution to\napproximate the joint distribution of the data-free channel and the interference\nchannel. We further introduce a novel method to compress the joint estimation\nand transmission of the data-free channel and the interference channel. By\ncombining the GIG distribution with the BWLC distribution, our proposed method\nachieves the same compression ratio as the BWLC distribution, which is\nsignificantly better than the GIG distribution. The performance of our method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11564625850340136,
          "p": 0.2698412698412698,
          "f": 0.161904757704762
        },
        "rouge-2": {
          "r": 0.025,
          "p": 0.05319148936170213,
          "f": 0.0340136010921381
        },
        "rouge-l": {
          "r": 0.11564625850340136,
          "p": 0.2698412698412698,
          "f": 0.161904757704762
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.09560v1",
      "true_abstract": "Solving medical imaging data scarcity through semantic image generation has\nattracted significant attention in recent years. However, existing methods\nprimarily focus on generating whole-organ or large-tissue structures, showing\nlimited effectiveness for organs with fine-grained structure. Due to stringent\ntopological consistency, fragile coronary features, and complex 3D\nmorphological heterogeneity in cardiac imaging, accurately reconstructing\nfine-grained anatomical details of the heart remains a great challenge. To\naddress this problem, in this paper, we propose the Fine-grained Cardiac image\nSynthesis(FCaS) framework, established on 3D template conditional diffusion\nmodel. FCaS achieves precise cardiac structure generation using Template-guided\nConditional Diffusion Model (TCDM) through bidirectional mechanisms, which\nprovides the fine-grained topological structure information of target image\nthrough the guidance of template. Meanwhile, we design a deformable Mask\nGeneration Module (MGM) to mitigate the scarcity of high-quality and diverse\nreference mask in the generation process. Furthermore, to alleviate the\nconfusion caused by imprecise synthetic images, we propose a Confidence-aware\nAdaptive Learning (CAL) strategy to facilitate the pre-training of downstream\nsegmentation tasks. Specifically, we introduce the Skip-Sampling Variance (SSV)\nestimation to obtain confidence maps, which are subsequently employed to\nrectify the pre-training on downstream tasks. Experimental results demonstrate\nthat images generated from FCaS achieves state-of-the-art performance in\ntopological consistency and visual quality, which significantly facilitates the\ndownstream tasks as well. Code will be released in the future.",
      "generated_abstract": "Object detection is a fundamental problem in computer vision, and it has\nbeen extensively studied in the past decades. However, due to the lack of\nground-truth annotations in the wild, the conventional supervised learning\nmethods often struggle to achieve satisfactory performance, which limits the\neffectiveness of the deep learning-based approaches in real-world scenarios.\nTo address this issue, we propose a novel supervised learning method, called\nDetection-Augmented Learning (DAL), which enhances the performance of\nobject detection models by integrating the pre-trained backbone network with\nself-supervised learning (SSL) methods. Specifically, our method first\npre-trains the backbone network on a large-scale image dataset, which is then\nfine-tuned with a small number of samples from the training set. By\ncombining the pre-trained backbone network and the SSL-based",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1518987341772152,
          "p": 0.27586206896551724,
          "f": 0.19591836276684724
        },
        "rouge-2": {
          "r": 0.028169014084507043,
          "p": 0.05172413793103448,
          "f": 0.036474159568371155
        },
        "rouge-l": {
          "r": 0.13291139240506328,
          "p": 0.2413793103448276,
          "f": 0.17142856684847993
        }
      }
    },
    {
      "paper_id": "math.GN.math/GN/2503.01241v1",
      "true_abstract": "This paper will discuss the problem of defining the new topological\ntransitivity. To do this several equivalent topological transitive and\nnon-wandering point has been discussed through this paper. This paper also\nconsider the ideal version of transitivity with the help of the amendment of\nthe result Remark $6.9(2)$ of \\cite{LL2013}. Corrected version of the Remark:\n``If $\\mathcal{\\bf I}$ is codense, then $\\mathcal{\\bf I}$-denseness,\n$*$-denseness and denseness are equivalent\" will be ``If $\\mathcal{\\bf I}$ is\ncompletely codense, then $\\mathcal{\\bf I}$-denseness, $*$-denseness and\ndenseness are equivalent\".",
      "generated_abstract": "In this paper we present a new construction of the classical Cayley-Dickson\nalgebra for any number field $F$. We prove that this algebra is isomorphic to\nthe classical Cayley-Dickson algebra for $F=\\mathbb{Q}(\\sqrt{-d})$, where $d$\nis a square-free integer. We prove that this algebra is isomorphic to the\nclassical Cayley-Dickson algebra for any number field $F$ of degree $n$ if and\nonly if $F$ is a global field and $n$ is a power of a prime number.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13725490196078433,
          "p": 0.1891891891891892,
          "f": 0.15909090421745883
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.03773584905660377,
          "f": 0.03278688033190077
        },
        "rouge-l": {
          "r": 0.11764705882352941,
          "p": 0.16216216216216217,
          "f": 0.13636363149018613
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02763v1",
      "true_abstract": "The Index of Dissimilarity (ID), widely utilized in economic literature as a\nmeasure of segregation, is inadequate for cross-country or time series studies\ndue to its failure to account for structural variations across countries' labor\nmarkets or changes over time within a single country's labor market. Building\non the works of Karmel and MacLachlan (1988) and Blackburn et al. (1993), we\npropose a new measure - the standardized ID - that isolates structural\ndifferences from true differences in segregation across space or time. A key\nadvantage of our proposed measure lies in its ease of implementation and\ninterpretation, even when working with datasets encompassing a large number of\ncountries or time periods. Moreover, our measure can be consistently applied in\nthe case of lumpy sectors or occupations that account for a large fraction of\nthe workforce. We illustrate the new measure in an analysis of the\ncross-country relationship between economic development (as measured by GDP per\ncapita) and occupational and sectoral gender segregation. Comparing the crude\nID with the standardized ID, we show that the crude ID overestimates the\npositive correlation between income and segregation, especially between low-\nand middle-income countries. This suggests that analyses relying on the crude\nID risk overestimating the importance of income differentials in explaining\ncross-country variation in gender segregation.",
      "generated_abstract": "The increasing availability of large language models (LLMs) for human\nresearch, particularly in economics, has sparked interest in using LLMs to\naddress economic research gaps. However, research on the application of LLMs\nin economics has been limited, particularly in relation to questions such as\nwhether LLMs can replace traditional econometrics methods, and whether LLMs can\nbe used to analyze large datasets. This paper explores these questions by\nanalyzing the application of LLMs to economic research in the context of three\nresearch questions. First, we explore whether LLMs can be used to analyze\nlarge datasets and identify key challenges for using LLMs in economics. Second,\nwe examine whether LLMs can be used to analyze traditional econometrics\nmethods. Third, we explore whether LLMs can be used to replace traditional\neconometrics methods. This exploration is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1450381679389313,
          "p": 0.2878787878787879,
          "f": 0.19289339655956103
        },
        "rouge-2": {
          "r": 0.015,
          "p": 0.031578947368421054,
          "f": 0.02033897868428706
        },
        "rouge-l": {
          "r": 0.12213740458015267,
          "p": 0.24242424242424243,
          "f": 0.16243654376768288
        }
      }
    },
    {
      "paper_id": "math.DG.math/DG/2503.08646v1",
      "true_abstract": "We consider isotropic and Lagrangian embeddings of coadjoint orbits of\ncompact Lie groups into products of coadjoint orbits. After reviewing the known\nfacts in the case of $\\mathrm{SU}(n)$ we initiate a similar study for\n$\\mathrm{SO}$ and $\\mathrm{Sp}$ cases. In the second part we apply this to the\nstudy of dynamical systems with $\\mathrm{SU}(n)$ symmetry, proving equivalence\nbetween systems of two types: those describing magnetic geodesic flow on flag\nmanifolds and classical `spin chains' of a special type.",
      "generated_abstract": "Let $M$ be a closed, oriented $n$-manifold. We study the existence of a\neither a smooth or a closed 1-form $\\Omega$ on $M$ such that the pair $(M,\n\\Omega)$ satisfies a weakly monotone condition. We prove that, if the pair\nsatisfies the monotonicity condition, then the pair satisfies a more restrictive\ncondition: there exists a smooth, closed 1-form $\\Omega$ on $M$ such that the\npair $(M, \\Omega)$ satisfies the weakly monotone condition. We also prove that\nthe weakly monotonic condition is equivalent to the condition that the pair\nsatisfies the monotonicity condition. We also give some applications of our\nresults.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11864406779661017,
          "p": 0.14,
          "f": 0.1284403620065653
        },
        "rouge-2": {
          "r": 0.02702702702702703,
          "p": 0.028169014084507043,
          "f": 0.027586201898692937
        },
        "rouge-l": {
          "r": 0.11864406779661017,
          "p": 0.14,
          "f": 0.1284403620065653
        }
      }
    },
    {
      "paper_id": "cs.ET.cs/ET/2503.09237v1",
      "true_abstract": "We attempt to take a comprehensive look at the challenges of representing the\nspatio-temporal structures and dynamic processes defining a city's overall\ncharacteristics. For the task of urban planning and urban operation, we take\nthe stance that even if the necessary representations of these structures and\nprocesses can be achieved, the most important representation of the relevant\nmindsets of the citizens are, unfortunately, mostly neglected.\n  After a review of major \"traditional\" urban models of structures behind urban\nscale, form, and dynamics, we turn to major recent modeling approaches\ntriggered by recent advances in AI that enable multi-modal generative models.\nSome of these models can create representations of geometries, networks and\nimages, and reason flexibly at a human-compatible semantic level. They provide\nhuge amounts of knowledge extracted from Terabytes of text and image documents\nand cover the required rich representation spectrum including geographic\nknowledge by different knowledge sources, degrees of granularity and scales.\n  We then discuss what these new opportunities mean for the modeling challenges\nposed by cities, in particular with regard to the role and impact of citizens\nand their interactions within the city infrastructure. We propose to integrate\nthese possibilities with existing approaches, such as agent-based models, which\nopens up new modeling spaces including rich citizen models which are able to\nalso represent social interactions.\n  Finally, we put forward some thoughts about a vision of a \"social AI in a\ncity ecosystem\" that adds relevant citizen models to state-of-the-art\nstructural and process models. This extended city representation will enable\nurban planners to establish citizen-oriented planning of city infrastructures\nfor human culture, city resilience and sustainability.",
      "generated_abstract": "The design of secure distributed protocols with a limited number of\nprocesses and a limited number of messages is a challenging task. We study the\ndesign of secure protocols with the property of bounded-delay. In particular, we\naddress the problem of designing secure protocols with a bounded number of\ndelays and limited number of messages. In the class of bounded-delay protocols,\nwe also study the problem of designing secure protocols with a bounded number of\ndelays and limited number of messages, and a bounded number of processes. We\nalso address the problem of designing secure protocols with a bounded number of\ndelays and limited number of processes. We study the problem of designing secure\nprotocols with a bounded number of delays and limited number of processes, and\na bounded number of messages.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06172839506172839,
          "p": 0.29411764705882354,
          "f": 0.10204081345897552
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.05555555555555555,
          "p": 0.2647058823529412,
          "f": 0.09183673182632246
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.00994v2",
      "true_abstract": "This study evaluates the performance of Vehicle-to-Vehicle Visible Light\nCommunication in dynamic environments, focusing on the effects of speed,\nhorizontal offset, and other factors on communication reliability. Using On-Off\nKeying modulation, we analyze the BER, optimal communication distance,\ncorrelation time and the maximum amount of data per communication. Our results\ndemonstrate that maintaining an optimal vehicle distance is critical for stable\ncommunication, with speed and horizontal offset significantly influencing\ncommunication. This work extends the analysis of V-VLC to real-world dynamic\nscenarios, providing insights for future research.",
      "generated_abstract": "This paper presents a novel methodology to estimate the ground reflection\ncoefficients (RK) in the frequency domain for the first time. The proposed\nmethod is based on the fast Fourier transform (FFT) and the Kalman filter (KF)\nfor estimating the RK, and its performance is evaluated by comparing it with\nthe conventional finite difference (FD) method and the RK estimation method\nbased on the Kalman filter. The proposed method has a faster convergence rate\nthan the FD method and the RK estimation method based on the Kalman filter. The\nanalysis shows that the proposed method is more accurate in estimating the RK\ncompared to the FD method and the RK estimation method based on the Kalman\nfilter. The results also show that the proposed method is more stable than the\nFD method and the RK estimation method based on the Kalman filter in terms of\nthe RMSE of the RK values",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.23529411764705882,
          "p": 0.23529411764705882,
          "f": 0.23529411264705893
        },
        "rouge-2": {
          "r": 0.023529411764705882,
          "p": 0.021052631578947368,
          "f": 0.02222221723765544
        },
        "rouge-l": {
          "r": 0.22058823529411764,
          "p": 0.22058823529411764,
          "f": 0.22058823029411775
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.econ/GN/2412.14996v1",
      "true_abstract": "In socioeconomic systems, nonequilibrium dynamics naturally stem from the\ngenerically non-reciprocal interactions between self-interested agents, whereas\nequilibrium descriptions often only apply to scenarios where individuals act\nwith the common good in mind. We bridge these two contrasting paradigms by\nstudying a Sakoda-Schelling occupation model with both individualistic and\naltruistic agents, who, in isolation, follow nonequilibrium and equilibrium\ndynamics respectively. We investigate how the relative fraction of these two\npopulations impacts the behavior of the system. In particular, we find that\nwhen fluctuations in the agents' decision-making process are small (high\nrationality), a very moderate amount of altruistic agents mitigates the\nsub-optimal concentration of individualists in dense clusters. In the regime\nwhere fluctuations carry more weight (low rationality), on the other hand,\naltruism progressively allows the agents to coordinate in a way that is\nsignificantly more robust, which we understand by reducing the model to a\nsingle effective population studied through the lens of active matter physics.\nWe highlight that localizing the altruistic intervention at the right point in\nspace may be paramount for its effectiveness.",
      "generated_abstract": "The emergence of new, highly complex and interconnected systems, driven by\nthe interplay between physical and socio-economic dynamics, has increased the\nneed for new methods for modeling, analyzing and predicting these systems.\nThis article reviews recent advances in the field of complex systems and\neconomics, focusing on models that capture the interplay between economic\nactivities and environmental and social sustainability. These models offer a\nbetter understanding of the complex dynamics underlying contemporary\nsocieties, and can be used to inform strategies for sustainable development.\nThe focus is on systems with non-linear dynamics, and on models that incorporate\nthe effects of environmental and social sustainability. The article concludes\nwith a brief discussion of the potential of these models for addressing\nchallenges in sustainable development.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13709677419354838,
          "p": 0.22666666666666666,
          "f": 0.17085426665993295
        },
        "rouge-2": {
          "r": 0.017341040462427744,
          "p": 0.026785714285714284,
          "f": 0.021052626808003544
        },
        "rouge-l": {
          "r": 0.12096774193548387,
          "p": 0.2,
          "f": 0.15075376414737018
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.04826v1",
      "true_abstract": "The reliance on large labeled datasets presents a significant challenge in\nmedical image segmentation. Few-shot learning offers a potential solution, but\nexisting methods often still require substantial training data. This paper\nproposes a novel approach that leverages the Segment Anything Model 2 (SAM2), a\nvision foundation model with strong video segmentation capabilities. We\nconceptualize 3D medical image volumes as video sequences, departing from the\ntraditional slice-by-slice paradigm. Our core innovation is a support-query\nmatching strategy: we perform extensive data augmentation on a single labeled\nsupport image and, for each frame in the query volume, algorithmically select\nthe most analogous augmented support image. This selected image, along with its\ncorresponding mask, is used as a mask prompt, driving SAM2's video\nsegmentation. This approach entirely avoids model retraining or parameter\nupdates. We demonstrate state-of-the-art performance on benchmark few-shot\nmedical image segmentation datasets, achieving significant improvements in\naccuracy and annotation efficiency. This plug-and-play method offers a powerful\nand generalizable solution for 3D medical image segmentation.",
      "generated_abstract": "In this work, we propose a novel, high-resolution method to reconstruct\nmedial axis curves (MACs) from medical images. MACs are widely used for\nsegmentation and motion estimation, but their reconstructed images often\nexhibit artifacts caused by noise and image artifacts. Traditional methods,\nsuch as gradient-based and Laplacian-based approaches, often produce\nill-conditioned and sparse images with low-quality reconstructions. To address\nthese issues, we propose a novel, high-resolution approach that uses\nhigh-frequency features to reconstruct high-resolution images. The proposed\nmethod uses a convolutional neural network (CNN) to extract high-frequency\nfeatures and a Laplacian pyramid to generate high-resolution images. We also\npresent a method for generating high-frequency features, which is essential for\nthe CNN model. This method uses a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18803418803418803,
          "p": 0.2716049382716049,
          "f": 0.22222221738751158
        },
        "rouge-2": {
          "r": 0.006578947368421052,
          "p": 0.009433962264150943,
          "f": 0.0077519331434439225
        },
        "rouge-l": {
          "r": 0.1794871794871795,
          "p": 0.25925925925925924,
          "f": 0.2121212072865015
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/CO/2501.15194v3",
      "true_abstract": "Short text clustering has gained significant attention in the data mining\ncommunity. However, the limited valuable information contained in short texts\noften leads to low-discriminative representations, increasing the difficulty of\nclustering. This paper proposes a novel short text clustering framework, called\nReliable \\textbf{P}seudo-labeling via \\textbf{O}ptimal \\textbf{T}ransport with\n\\textbf{A}ttention for Short Text Clustering (\\textbf{POTA}), that generate\nreliable pseudo-labels to aid discriminative representation learning for\nclustering. Specially, \\textbf{POTA} first implements an instance-level\nattention mechanism to capture the semantic relationships among samples, which\nare then incorporated as a semantic consistency regularization term into an\noptimal transport problem. By solving this OT problem, we can yield reliable\npseudo-labels that simultaneously account for sample-to-sample semantic\nconsistency and sample-to-cluster global structure information. Additionally,\nthe proposed OT can adaptively estimate cluster distributions, making\n\\textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we\nutilize the pseudo-labels to guide contrastive learning to generate\ndiscriminative representations and achieve efficient clustering. Extensive\nexperiments demonstrate \\textbf{POTA} outperforms state-of-the-art methods. The\ncode is available at:\n\\href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}.",
      "generated_abstract": "We study the problem of estimating the mean and variance of a continuous random\ngaussian process in a finite set of points using only a single measurement. The\ngoal is to estimate the true mean $\\mu$ and covariance matrix $\\Sigma$ with\nhigh accuracy using only a single measurement. We show that the optimal estimator\nis a convex combination of the empirical mean $\\hat{\\mu}$ and the empirical\ncovariance matrix $\\hat{\\Sigma}$, where the mixing weights are obtained by\nsolving an optimal transport problem. We further propose an efficient algorithm\nto compute the optimal transport map and show that it is an approximation to\nthe true optimal transport map. We also propose an efficient algorithm to\ncompute the optimal transport map and show that it is an approximation to the\ntrue optimal transport map. We then derive a new variance reduction method to\nimprove the efficiency of the optimal transport algorithm. We show that our\nmethod improves the convergence rate",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.152,
          "p": 0.2602739726027397,
          "f": 0.19191918726405482
        },
        "rouge-2": {
          "r": 0.018633540372670808,
          "p": 0.02586206896551724,
          "f": 0.021660644951453607
        },
        "rouge-l": {
          "r": 0.112,
          "p": 0.1917808219178082,
          "f": 0.14141413675900433
        }
      }
    },
    {
      "paper_id": "math.NA.cs/NA/2503.10199v1",
      "true_abstract": "The Bayesian inversion method demonstrates significant potential for solving\ninverse problems, enabling both point estimation and uncertainty\nquantification. However, Bayesian maximum a posteriori (MAP) estimation may\nbecome unstable when handling data from diverse distributions (e.g., solutions\nof stochastic partial differential equations (SPDEs)). Additionally, Monte\nCarlo sampling methods are computationally expensive. To address these\nchallenges, we propose a novel two-stage optimization method based on optimal\ncontrol theory and variational Bayesian methods. This method not only achieves\nstable solutions for stochastic inverse problems but also efficiently\nquantifies the uncertainty of the solutions. In the first stage, we introduce a\nnew weighting formulation to ensure the stability of the Bayesian MAP\nestimation. In the second stage, we derive the necessary condition to\nefficiently quantify the uncertainty of the solutions, by combining the new\nweighting formula with variational inference. Furthermore, we establish an\nerror estimation theorem that relates the exact solution to the optimally\nestimated solution under different amounts of observed data. Finally, the\nefficiency of the proposed method is demonstrated through numerical examples.",
      "generated_abstract": "The use of numerical quadrature methods to approximate the solution of\na differential equation is a widespread technique in numerical analysis.\nHowever, the choice of quadrature rule is often subjective and relies on the\npersonal preferences of the user. In this paper, we propose a novel approach\nthat uses a Bayesian framework to evaluate the accuracy of numerical quadrature\nrules. This approach is based on a hybrid statistical and numerical analysis.\nFirst, we conduct a rigorous statistical analysis of the accuracy of the\nexisting quadrature rules, including the quadrature rules that are\nintegrated in the existing software packages. Second, we propose a Bayesian\napproach to evaluate the accuracy of quadrature rules using Gaussian Process\nRegression. Our Bayesian approach is based on a Gaussian process model that\nis trained on the existing data. We then use this model to evaluate the\naccuracy of the existing quadrature rules and propose new rules that",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19834710743801653,
          "p": 0.34285714285714286,
          "f": 0.25130889588004723
        },
        "rouge-2": {
          "r": 0.03067484662576687,
          "p": 0.04310344827586207,
          "f": 0.0358422890487025
        },
        "rouge-l": {
          "r": 0.19834710743801653,
          "p": 0.34285714285714286,
          "f": 0.25130889588004723
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.06318v1",
      "true_abstract": "This article discusses the key principles of radio spectrum management with a\nfocus on spectrum allocation and access. We show the current regime's inherent\nrigidity and constrained possibilities for introducing new radiocommunication\nservices and applications. The article proposes how governments and spectrum\nusers could cooperate in taking spectrum management to a qualitatively new\nlevel, characterized by light touch regulation and flexible use. This could be\nachieved through the broader introduction of emerging practices such as\nSpectrum Usage Rights, liberalized spectrum trading, and full shared spectrum\naccess. We conclude by presenting a vision for a 'perfect' spectrum management\narrangement and future research directions.",
      "generated_abstract": "The growing number of sensor nodes (SNs) in the Internet of Things (IoT)\nrequires a scalable and energy-efficient network management strategy. This\npaper proposes an energy-efficient and reliable network management method for\nthe IoT using the Constrained Uniform Power Allocation (CUPA) technique. The\nmain focus of this paper is to minimize the energy consumption of the SNs\nthrough the CUPA technique, which is achieved by determining the power\ndistribution of the SNs using the CUPA method. The proposed method uses the\nminimum sum rate (MSR) metric to ensure that the energy consumption of the\nSNs is minimized while ensuring that the throughput of the network remains\nunaltered. The performance of the proposed method is evaluated through\nsimulations using a wireless sensor network (WSN) model with varying\nconfiguration and SN density. The results show that the proposed method",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22972972972972974,
          "p": 0.20987654320987653,
          "f": 0.21935483371987524
        },
        "rouge-2": {
          "r": 0.01020408163265306,
          "p": 0.008403361344537815,
          "f": 0.009216584908580036
        },
        "rouge-l": {
          "r": 0.1891891891891892,
          "p": 0.1728395061728395,
          "f": 0.18064515630052042
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2412.16850v1",
      "true_abstract": "We introduce a model for limit order book of a certain security with two main\nfeatures: First, both the limit orders and market orders for the given asset\nare allowed to appear and interact with each other. Second, the high frequency\ntrading activities are allowed and described by the scaling limit of\nnearly-unstable multi-dimensional Hawkes processes with power law decay. The\nmodel has been derived as a stochastic partial differential equation (SPDE, for\nshort), under certain intuitive identifications. Its diffusion coefficient is\ndetermined by a Volterra integral equation driven by a Hawkes process, whose\nHurst exponent is less than 1/2 (so that the relevant process is negatively\ncorrelated). As a result, the volatility path of the SPDE is rougher than that\ndriven by a (standard) Brownian motion. The well-posedness follows from a\nresult in literature. Hence, a foundation is laid down for further studies in\nthis direction.",
      "generated_abstract": "We propose a new framework for the pricing of American-style options on\nthe price of a security. The framework allows us to derive closed-form\nexpression for the option price in terms of the solution to the backward\nintegral equation, which is a nonlinear partial differential equation. We\nanalyze the properties of the solution, including the explicit dependence on\nthe volatility, and obtain closed-form expressions for the option price when\nthe volatility is known or when it is estimated based on the implied\nvolatility surface. We also derive closed-form expressions for the option\nprice when the volatility is unknown, and show that it is always lower than the\nknown volatility. We then discuss the use of the option price in portfolio\nanalysis and discuss its connection to the Black-Scholes model.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18446601941747573,
          "p": 0.2753623188405797,
          "f": 0.2209302277535155
        },
        "rouge-2": {
          "r": 0.04225352112676056,
          "p": 0.05714285714285714,
          "f": 0.048582991063614064
        },
        "rouge-l": {
          "r": 0.1650485436893204,
          "p": 0.2463768115942029,
          "f": 0.19767441380002718
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/OT/2503.02645v1",
      "true_abstract": "Mixup is a widely adopted data augmentation technique known for enhancing the\ngeneralization of machine learning models by interpolating between data points.\nDespite its success and popularity, limited attention has been given to\nunderstanding the statistical properties of the synthetic data it generates. In\nthis paper, we delve into the theoretical underpinnings of mixup, specifically\nits effects on the statistical structure of synthesized data. We demonstrate\nthat while mixup improves model performance, it can distort key statistical\nproperties such as variance, potentially leading to unintended consequences in\ndata synthesis. To address this, we propose a novel mixup method that\nincorporates a generalized and flexible weighting scheme, better preserving the\noriginal data's structure. Through theoretical developments, we provide\nconditions under which our proposed method maintains the (co)variance and\ndistributional properties of the original dataset. Numerical experiments\nconfirm that the new approach not only preserves the statistical\ncharacteristics of the original data but also sustains model performance across\nrepeated synthesis, alleviating concerns of model collapse identified in\nprevious research.",
      "generated_abstract": "In recent years, deep learning-based approaches for image segmentation\nhave made great progress. However, the existing methods often struggle with\ntexture-related challenges, such as the over-segmentation of texture regions and\nthe loss of texture information during segmentation. To address this issue, we\npropose a novel texture-aware deep learning approach, TEXTURENet, which\nenables texture segmentation through a novel attention mechanism that\ndistinguishes texture regions from other regions. Specifically, we introduce\na novel attention module that utilizes both spatial and semantic features to\naccurately capture the texture-related information. To effectively incorporate\nthe texture information, we propose a texture-aware attention module that\ncombines the attention mechanism with a texture-specific attention module to\ndistinguish texture regions from other regions. Moreover, we design a\ncontrastive learning-based framework to improve the segmentation accuracy and\nreliability. Extensive",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.24358974358974358,
          "f": 0.19095476910280054
        },
        "rouge-2": {
          "r": 0.03164556962025317,
          "p": 0.043859649122807015,
          "f": 0.03676470101319269
        },
        "rouge-l": {
          "r": 0.15702479338842976,
          "p": 0.24358974358974358,
          "f": 0.19095476910280054
        }
      }
    },
    {
      "paper_id": "physics.comp-ph.hep-ex/2503.09213v1",
      "true_abstract": "The scientific communities of nuclear, particle, and astroparticle physics\nare continuing to advance and are facing unprecedented software challenges due\nto growing data volumes, complex computing needs, and environmental\nconsiderations. As new experiments emerge, software and computing needs must be\nrecognised and integrated early in design phases. This document synthesises\ninsights from ECFA, NuPECC and APPEC, representing particle physics, nuclear\nphysics, and astroparticle physics, and presents collaborative strategies for\nimproving software, computing frameworks, infrastructure, and career\ndevelopment within these fields.",
      "generated_abstract": "We investigate the performance of the Monte Carlo (MC) method in the\nexact diagonalization (ED) of two-dimensional quantum spin chains with a\nquantum spin-1/2 model as a model Hamiltonian. Our method is based on the\nconventional MC method with the modified Hamiltonian, and we apply it to the\nspin-1/2 Heisenberg model and the Ising model. We firstly derive the exact\nequilibrium Monte Carlo (EEMC) formula and then prove the convergence of the\nEEMC method. We also discuss the effect of the spin-1/2 model on the ED\nperformance. Finally, we analyze the ED results and compare them with the\nexact diagonalization (ED) results.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0625,
          "p": 0.07142857142857142,
          "f": 0.06666666168888927
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.0625,
          "p": 0.07142857142857142,
          "f": 0.06666666168888927
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.17005v1",
      "true_abstract": "This study documents the relationship between computer skills/digital\nliteracy and influenza vaccination take-up among older adults in Europe during\nand after the COVID-19 pandemic. Using data from the Survey of Health, Aging\nand Retirement in Europe, we find a positive partial association between\ninfluenza vaccination take-up and two indicators of computer skills/digital\nliteracy, self-assessed pre-pandemic computer skills and having used a computer\nat work in any pre-pandemic job. We do not estimate significant behavioural\nchanges for individuals with better computer skills that may have been driven\nby spillover effects from the pandemic experience.",
      "generated_abstract": "The United States has the highest rate of suicide in the developed world,\nwith an estimated 10.1 suicides per 100,000 people in 2022. However, despite\nthe high rate of suicide, suicide rates among people with disabilities remain\nhigher than those without disabilities. The aim of this study is to investigate\nthe relationship between disability and suicide among adults in the United\nStates. This study used data from the 2019-2020 National Survey on Drug Use and\nHealth (NSDUH), which included 1,430 adults aged 18 years and older. The\nanalysis focused on the association between disability and suicide. The results\nof the study showed that the odds of suicide were higher among adults with\ndisabilities compared to those without disabilities. The findings also revealed\nthat there",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.23376623376623376,
          "f": 0.2465753374807657
        },
        "rouge-2": {
          "r": 0.08045977011494253,
          "p": 0.0625,
          "f": 0.07035175387288234
        },
        "rouge-l": {
          "r": 0.21739130434782608,
          "p": 0.19480519480519481,
          "f": 0.20547944706980684
        }
      }
    },
    {
      "paper_id": "physics.gen-ph.physics/gen-ph/2502.15841v1",
      "true_abstract": "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
      "generated_abstract": "We introduce a general and unified mathematical framework for\nstudying the evolution of the local density of states of a system of\ninteracting particles, within the framework of random phase-space trajectories\nand the associated Markov chains. The framework is based on the\nmulti-particle-classical (MPC) approach, which is a generalization of the\nclassical-quantum theory of interacting particles, and extends the\nclassical-quantum theory of the single-particle case. We show that the\nparticle-classical (PC) approach can be interpreted as a particular case of the\nMPC approach, where the local density of states is described as a probability\ndistribution over the phase-space trajectories. The local density of states is\nan important quantity, and its evolution is studied from different perspectives.\nWe show that the evolution of the local density of states can be expressed in\nterms of the M",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14864864864864866,
          "p": 0.16176470588235295,
          "f": 0.1549295724737157
        },
        "rouge-2": {
          "r": 0.02586206896551724,
          "p": 0.028846153846153848,
          "f": 0.027272722287604217
        },
        "rouge-l": {
          "r": 0.12162162162162163,
          "p": 0.1323529411764706,
          "f": 0.12676055838920872
        }
      }
    },
    {
      "paper_id": "math.FA.math/OA/2503.01331v1",
      "true_abstract": "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
      "generated_abstract": "In this work, we consider a $K$-theoretic problem related to the study of\nthe automorphism group of the $K$-theory of a topological space $X$. This\nproblem has been studied by several authors and it is known that the\nautomorphism group of the $K$-theory of a topological space $X$ is related to\nthe group of automorphisms of the fundamental group of $X$. However, in this\nwork we show that there are topological spaces $X$ for which the automorphism\ngroup of the $K$-theory of $X$ is isomorphic to the group of automorphisms of\nthe fundamental group of $X$. We also prove that in the case of topological\nspaces $X$ with a finite fundamental group, the automorphism group of the $K$-\ntheory of $X$ is isomorphic to the group of automorphisms of the fundamental\ngroup of $X$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16279069767441862,
          "p": 0.2692307692307692,
          "f": 0.20289854602814547
        },
        "rouge-2": {
          "r": 0.038834951456310676,
          "p": 0.05063291139240506,
          "f": 0.04395603904299052
        },
        "rouge-l": {
          "r": 0.13953488372093023,
          "p": 0.23076923076923078,
          "f": 0.1739130387817687
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.09917v1",
      "true_abstract": "Economists disagree about the factors driving the substantial increase in\nresidual wage inequality in the US over the past few decades. To identify\nchanges in the returns to unobserved skills, we make a novel assumption about\nthe dynamics of skills rather than about the stability of skill distributions\nacross cohorts, as is standard. We show that our assumption is supported by\ndata on test score dynamics for older workers in the HRS. Using survey data\nfrom the PSID and administrative data from the IRS and SSA, we estimate that\nthe returns to unobserved skills $declined$ substantially in the late-1980s and\n1990s despite an increase in residual inequality. Accounting for firm-specific\npay differences yields similar results. Extending our framework to consider\noccupational differences in returns to skill and multiple unobserved skills, we\nfurther show that skill returns display similar patterns for workers employed\nin each of cognitive, routine, and social occupations. Finally, our results\nsuggest that increasing skill dispersion, driven by rising skill volatility,\nexplains most of the growth in residual wage inequality since the 1980s.",
      "generated_abstract": "In this paper, we propose a novel approach to estimating the value of\ngovernment bonds. We develop a dynamic model that captures the dynamic\nrelationship between government bonds and interest rates, while accounting for\nthe complex interaction between government bonds and the yield curve. We\nformulate a maximum likelihood estimator for the model parameters, and we\nintroduce a new adjustment mechanism to ensure the consistency of the\nestimated parameters. We validate our model and methodology using data from\nBloomberg, and we show that the estimated parameters are consistent with the\ndata, and the adjustment mechanism provides a stable and consistent estimate\nof the parameters. The results suggest that the government bond market is\nconsistent with the market model, and the government bond market is not\ndominated by the yield curve. The model can be used to analyze the impact of\neconomic shocks on government bonds and yields",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20192307692307693,
          "p": 0.2625,
          "f": 0.22826086465028367
        },
        "rouge-2": {
          "r": 0.04516129032258064,
          "p": 0.056910569105691054,
          "f": 0.05035970729646547
        },
        "rouge-l": {
          "r": 0.18269230769230768,
          "p": 0.2375,
          "f": 0.20652173421550107
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00348v1",
      "true_abstract": "The increasing frequency of environmental hazards due to climate change\nunderscores the urgent need for effective monitoring systems. Current\napproaches either rely on expensive labelled datasets, struggle with seasonal\nvariations, or require multiple observations for confirmation (which delays\ndetection). To address these challenges, this work presents SHAZAM -\nSelf-Supervised Change Monitoring for Hazard Detection and Mapping. SHAZAM uses\na lightweight conditional UNet to generate expected images of a region of\ninterest (ROI) for any day of the year, allowing for the direct modelling of\nnormal seasonal changes and the ability to distinguish potential hazards. A\nmodified structural similarity measure compares the generated images with\nactual satellite observations to compute region-level anomaly scores and\npixel-level hazard maps. Additionally, a theoretically grounded seasonal\nthreshold eliminates the need for dataset-specific optimisation. Evaluated on\nfour diverse datasets that contain bushfires (wildfires), burned regions,\nextreme and out-of-season snowfall, floods, droughts, algal blooms, and\ndeforestation, SHAZAM achieved F1 score improvements of between 0.066 and 0.234\nover existing methods. This was achieved primarily through more effective\nhazard detection (higher recall) while using only 473K parameters. SHAZAM\ndemonstrated superior mapping capabilities through higher spatial resolution\nand improved ability to suppress background features while accentuating both\nimmediate and gradual hazards. SHAZAM has been established as an effective and\ngeneralisable solution for hazard detection and mapping across different\ngeographical regions and a diverse range of hazards. The Python code is\navailable at: https://github.com/WiseGamgee/SHAZAM",
      "generated_abstract": "The emergence of AI-driven computer vision techniques has led to the\ndevelopment of deep learning models that have achieved remarkable performance\nin various domains. However, these models often struggle with the task of\nsegmentation, wherein the objective is to identify specific objects and\nregions within a scene. In this paper, we present a novel deep learning model\nthat addresses this challenge. Our proposed model, DenseCube, is a\nmulti-resolution convolutional neural network that leverages multiple scales\nto capture fine details while maintaining efficiency. Our approach consists of\nfour key components: (1) a multi-scale encoder that captures fine details,\n(2) a multi-resolution decoder that refines the features, (3) a multi-scale\nfeature fusion layer that combines the information from multiple scales, and (4)\na multi-scale attention mechanism that adaptively selects the most relevant\nfeatures from the input image",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10857142857142857,
          "p": 0.1919191919191919,
          "f": 0.13868612677153833
        },
        "rouge-2": {
          "r": 0.004310344827586207,
          "p": 0.007692307692307693,
          "f": 0.0055248572754227785
        },
        "rouge-l": {
          "r": 0.10857142857142857,
          "p": 0.1919191919191919,
          "f": 0.13868612677153833
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06331v1",
      "true_abstract": "Models with unnormalized probability density functions are ubiquitous in\nstatistics, artificial intelligence and many other fields. However, they face\nsignificant challenges in model selection if the normalizing constants are\nintractable. Existing methods to address this issue often incur high\ncomputational costs, either due to numerical approximations of normalizing\nconstants or evaluation of bias corrections in information criteria. In this\npaper, we propose a novel and fast selection criterion, T-GIC, for nested\nmodels, allowing direct data sampling from a possibly unnormalized probability\ndensity function. T-GIC gives a consistent selection under mild regularity\nconditions and is computationally efficient, benefiting from a multiplying\nfactor that depends only on the sample size and the model complexity. Extensive\nsimulation studies and real-data applications demonstrate the efficacy of T-GIC\nin the selection of nested models with unnormalized probability densities.",
      "generated_abstract": "We propose a method for inference on the population average of a random\nvector with a known mean and covariance, and a known variance. The method is\nbased on a modified estimator of the population covariance, which is\ncomputationally more efficient than the standard one. We derive a\nsemiparametric estimation and inference formula for the population covariance\nestimator, and we show that the estimator is consistent and asymptotically\nnormal. We provide a theoretical justification of the proposed estimator,\nshowing that it is consistent and asymptotically normal when the population\ncovariance is known. We also derive the asymptotic distribution of the\npopulation covariance estimator under a mild regularity condition, which\nprovides additional theoretical support for the proposed estimator.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16326530612244897,
          "p": 0.26666666666666666,
          "f": 0.20253164085883682
        },
        "rouge-2": {
          "r": 0.031746031746031744,
          "p": 0.039603960396039604,
          "f": 0.03524228580954483
        },
        "rouge-l": {
          "r": 0.12244897959183673,
          "p": 0.2,
          "f": 0.15189872946643182
        }
      }
    },
    {
      "paper_id": "q-fin.RM.q-fin/RM/2503.01148v1",
      "true_abstract": "This paper investigates the risk spillovers among AI ETFs, AI tokens, and\ngreen markets using the R2 decomposition method. We reveal several key\ninsights. First, the overall transmission connectedness index (TCI) closely\naligns with the contemporaneous TCI, while the lagged TCI is significantly\nlower. Second, AI ETFs and clean energy act as risk transmitters, whereas AI\ntokens and green bond function as risk receivers. Third, AI tokens are\ndifficult to hedge and provide limited hedging ability compared to AI ETFs and\ngreen assets. However, multivariate portfolios effectively reduce AI tokens\ninvestment risk. Among them, the minimum correlation portfolio outperforms the\nminimum variance and minimum connectedness portfolios.",
      "generated_abstract": "We consider a heterogeneous agent model with multiple assets and a\nportfolio manager. The portfolio manager is able to trade and allocate among\nthe assets according to the portfolio objective. The model is modeled as a\nlinear-quadratic-Gaussian (LQG) game with a quadratic portfolio objective.\nWe develop a game-theoretic approach based on the Nash equilibrium and\nconstruct a dual formulation to solve the game. The dual solution provides a\nportfolio allocation, and we provide a closed-form solution for the Nash\nequilibrium. We also present a computationally efficient algorithm based on the\nHestenes-Stiefel algorithm to obtain the dual solution. We evaluate the\nconvergence of the proposed algorithm and the computational cost.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14666666666666667,
          "p": 0.1864406779661017,
          "f": 0.16417909954889745
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.13333333333333333,
          "p": 0.1694915254237288,
          "f": 0.14925372641456913
        }
      }
    },
    {
      "paper_id": "q-bio.GN.q-bio/GN/2501.04718v1",
      "true_abstract": "Gene panel selection aims to identify the most informative genomic biomarkers\nin label-free genomic datasets. Traditional approaches, which rely on domain\nexpertise, embedded machine learning models, or heuristic-based iterative\noptimization, often introduce biases and inefficiencies, potentially obscuring\ncritical biological signals. To address these challenges, we present an\niterative gene panel selection strategy that harnesses ensemble knowledge from\nexisting gene selection algorithms to establish preliminary boundaries or prior\nknowledge, which guide the initial search space. Subsequently, we incorporate\nreinforcement learning through a reward function shaped by expert behavior,\nenabling dynamic refinement and targeted selection of gene panels. This\nintegration mitigates biases stemming from initial boundaries while\ncapitalizing on RL's stochastic adaptability. Comprehensive comparative\nexperiments, case studies, and downstream analyses demonstrate the\neffectiveness of our method, highlighting its improved precision and efficiency\nfor label-free biomarker discovery. Our results underscore the potential of\nthis approach to advance single-cell genomics data analysis.",
      "generated_abstract": "The human genome has been sequenced and assembled, but the vast majority of\ngenes have been identified only in a handful of individuals. The genomic\nsequence is the basis of an individual's genetic information, but it also\ncontains clues to the individual's biological state. The sequence is the basis\nof the individual's phenotype, but it also contains clues to the individual's\nbiological state. We present a probabilistic model for the genotype-phenotype\nrelationship, and use it to predict phenotypes in populations. We compare our\nmodel to a variety of existing models, including the Markov Chain Monte\nCarlo (MCMC) approach, and show that it is more accurate. We also demonstrate\nhow the model can be used to predict the likelihood of a gene's activity from\nits genotype. Our model allows for the prediction of phenot",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15833333333333333,
          "p": 0.24050632911392406,
          "f": 0.19095476908158895
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.189873417721519,
          "f": 0.15075376405646337
        }
      }
    },
    {
      "paper_id": "eess.IV.eess/IV/2503.10260v1",
      "true_abstract": "Our study focuses on isolating swallowing dynamics from interfering patient\nmotion in videofluoroscopy, an X-ray technique that records patients swallowing\na radiopaque bolus. These recordings capture multiple motion sources, including\nhead movement, anatomical displacements, and bolus transit. To enable precise\nanalysis of swallowing physiology, we aim to eliminate distracting motion,\nparticularly head movement, while preserving essential swallowing-related\ndynamics. Optical flow methods fail due to artifacts like flickering and\ninstability, making them unreliable for distinguishing different motion groups.\nWe evaluated markerless tracking approaches (CoTracker, PIPs++, TAP-Net) and\nquantified tracking accuracy in key medical regions of interest. Our findings\nshow that even sparse tracking points generate morphing displacement fields\nthat outperform leading registration methods such as ANTs, LDDMM, and\nVoxelMorph. To compare all approaches, we assessed performance using MSE and\nSSIM metrics post-registration. We introduce a novel motion correction pipeline\nthat effectively removes disruptive motion while preserving swallowing dynamics\nand surpassing competitive registration techniques. Code will be available\nafter review.",
      "generated_abstract": "While the advent of large language models has enabled the development of\nnumerous image segmentation methods that have significantly improved performance\nover manual segmentation methods, they remain prone to various biases.\nLearning-based methods have emerged as an alternative, offering the promise of\nbetter segmentation accuracy while mitigating biases, yet they remain\ncontroversial due to their complex training and inference procedures, which\nrequires expert knowledge. In this paper, we introduce a novel method,\nSegmentation with AI (S-AI), that addresses these challenges by leveraging\ndeep learning techniques to perform segmentation in a self-supervised manner,\nwithout requiring extensive training or computational costs. We demonstrate the\neffectiveness of S-AI on the MRI dataset and highlight its advantages over\nstate-of-the-art approaches, including the use of a small dataset size and a\nlightweight architecture. These features enable S",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18548387096774194,
          "p": 0.21904761904761905,
          "f": 0.2008733574798346
        },
        "rouge-2": {
          "r": 0.01948051948051948,
          "p": 0.023076923076923078,
          "f": 0.021126755599088647
        },
        "rouge-l": {
          "r": 0.1774193548387097,
          "p": 0.20952380952380953,
          "f": 0.19213973302568618
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2412.10635v1",
      "true_abstract": "Large language models (LLMs) offer the potential to automate a large number\nof tasks that previously have not been possible to automate, including some in\nscience. There is considerable interest in whether LLMs can automate the\nprocess of causal inference by providing the information about causal links\nnecessary to build a structural model. We use the case of confounding in the\nCoronary Drug Project (CDP), for which there are several studies listing\nexpert-selected confounders that can serve as a ground truth. LLMs exhibit\nmediocre performance in identifying confounders in this setting, even though\ntext about the ground truth is in their training data. Variables that experts\nidentify as confounders are only slightly more likely to be labeled as\nconfounders by LLMs compared to variables that experts consider\nnon-confounders. Further, LLM judgment on confounder status is highly\ninconsistent across models, prompts, and irrelevant concerns like\nmultiple-choice option ordering. LLMs do not yet have the ability to automate\nthe reporting of causal links.",
      "generated_abstract": "We develop an efficient method for fitting nonparametric Bayesian\ncontemporary poverty models. Our approach combines a Dirichlet process\nhyperparameterization of the poverty distribution with a Gaussian process\nregression of the poverty status of households. The Dirichlet process\nhyperparameterization is designed to ensure the efficient sampling of\npoverty-levels and is motivated by the frequent use of the Dirichlet process\nin Bayesian inference. The Gaussian process regression captures the spatial\ndistribution of the poverty rate. We derive the posterior distribution of the\nGaussian process hyperparameter, allowing for efficient inference. We apply\nour method to a panel data set on urban poverty in China. Our results suggest\nthat the poverty distribution in China is well described by a\nDirichlet-multinomial distribution, which is the default Dirichlet process\nhyperparameterization. However, the poverty rate is non-linear in",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16964285714285715,
          "p": 0.2835820895522388,
          "f": 0.21229049810929757
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.23880597014925373,
          "f": 0.1787709450366719
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2503.02146v1",
      "true_abstract": "Reliance on stereotypes is a persistent feature of human decision-making and\nhas been extensively documented in educational settings, where it can shape\nstudents' confidence, performance, and long-term human capital accumulation.\nWhile effective techniques exist to mitigate these negative effects, a crucial\nfirst step is to establish whether teachers can recognize stereotypes in their\nprofessional environment. We introduce the Stereotype Identification Test\n(SIT), a novel survey tool that asks teachers to evaluate and comment on the\npresence of stereotypes in images randomly drawn from school textbooks. Their\nresponses are systematically linked to established measures of implicit bias\n(Implicit Association Test, IAT) and explicit bias (survey scales on teaching\nstereotypes and social values). Our findings demonstrate that the SIT is a\nvalid and reliable measure of stereotype recognition. Teachers' ability to\nrecognize stereotypes is linked to trainable traits such as implicit bias\nawareness and inclusive teaching practices. Moreover, providing personalized\nfeedback on implicit bias improves SIT scores by 0.25 standard deviations,\nreinforcing the idea that stereotype recognition is malleable and can be\nenhanced through targeted interventions.",
      "generated_abstract": "We propose a novel approach to evaluate the performance of various models\nfor evaluating the impact of a policy change on the economy. The proposed\nmethodology integrates three key steps: (1) a two-step regression model to\nestimate the impact of the policy change on economic variables, (2) a\ndiscrete choice model to evaluate the impact of the policy change on individual\nconsumer behaviors, and (3) a decision model to evaluate the impact of the\npolicy change on firms' decisions. The proposed methodology is validated using\nthe 2011-2020 dataset for India. The results show that the proposed model\noutperforms existing models in terms of predictive accuracy, explanatory\npower, and calibration.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.1935483870967742,
          "f": 0.12903225362007184
        },
        "rouge-2": {
          "r": 0.024096385542168676,
          "p": 0.047619047619047616,
          "f": 0.03199999553792063
        },
        "rouge-l": {
          "r": 0.0967741935483871,
          "p": 0.1935483870967742,
          "f": 0.12903225362007184
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.05946v1",
      "true_abstract": "Despite growing evidence that neighborhoods play a critical role in shaping\neconomic mobility and well-being, effective policies to address neighborhood\ndisadvantage remain elusive. This study evaluates the impact of the Promise\nZone program, which aims to revitalize disadvantaged neighborhoods through\nstreamlined federal support and grant incentives. I use an event study\nframework with newly obtained data on the location of failed finalist\napplications as a comparison group to estimate the effects of the program. The\nresults reveal significant improvements in poverty, household incomes, and\nemployment in Promise Zone neighborhoods, particularly in later-designated\nzones and initially low-status neighborhoods. I also find that effects are\ndriven partly by changes in residential composition, and that Promise Zones\nappear to induce positive spillovers in adjacent areas.",
      "generated_abstract": "In this paper, we analyze the impact of social distancing measures on\nincome distribution. We first conduct a general equilibrium analysis of the\neffects of social distancing measures on income distribution. We then consider\nthe impact of social distancing measures on the earnings distribution by\nmeasuring the earnings gap between two groups of workers. We find that the\nearnings gap between the highest- and lowest-paid workers increases significantly\nunder social distancing measures. The earnings gap between the highest- and\nlowest-paid workers increases more significantly under social distancing measures\nthan under other policies, such as unemployment benefits. Our analysis shows\nthat the earnings gap increases more significantly in the United States than in\nthe European Union due to the significant differences in the economic\nconditions between the two regions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17204301075268819,
          "p": 0.25396825396825395,
          "f": 0.2051282003131165
        },
        "rouge-2": {
          "r": 0.058823529411764705,
          "p": 0.07954545454545454,
          "f": 0.06763284535368422
        },
        "rouge-l": {
          "r": 0.13978494623655913,
          "p": 0.20634920634920634,
          "f": 0.16666666185157802
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2501.01084v1",
      "true_abstract": "The United States leads the world in the number of violent mass shootings\nthat occur each year, and policy making on firearms remains polarized along\nparty lines. Are legislators responsive to mass shootings? We estimate the\nlatent positions of nearly 2,000 state legislators on gun policy from their\nroll-call voting records on firearm-related bills from 2011 to 2022. Employing\na staggered difference-in-differences design, we find that mass shootings\nwithin or near a state legislator's district do not alter their voting behavior\non firearm policy, on average, for members of both parties. Our estimated\neffects of mass shootings on treated legislators' support for restrictive gun\npolicies (on a -1 to 1 scale) range from a 4.8% reduction among California\nDemocrats and a 0.9% increase among California Republicans to, across six total\nstates, a 5% (among Democrats) and 7.1% (among Republicans) increase, with 95%\nconfidence intervals spanning opposite directions. We conclude that, on\naverage, mass shootings fail to produce changes in a legislator's support\n(opposition) for restrictive (permissive) firearms bills. Our findings suggest\nthat even the most heinous acts of mass violence -- that are squarely in the\ndomain of events that state legislators might respond to -- fail to produce any\nmeasurable effects on legislators' positions on firearm-related policy.",
      "generated_abstract": "This paper studies the economic impacts of the COVID-19 pandemic on\ne-commerce logistics in Brazil. We use a dynamic panel data model to analyze\nthe effect of the pandemic on the number of packages delivered, the number of\ne-commerce logistics workers, and the logistics worker wages. Our findings\nsuggest that the pandemic had a negative impact on both the number of packages\ndelivered and the logistics worker wages. However, the impact on the number of\ne-commerce logistics workers was relatively mild. Moreover, the model suggests\nthat the impact of the pandemic on logistics worker wages was larger in\nhigh-income regions and in cities with a higher density of logistics\nworkers.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11029411764705882,
          "p": 0.2777777777777778,
          "f": 0.15789473277340732
        },
        "rouge-2": {
          "r": 0.025252525252525252,
          "p": 0.06097560975609756,
          "f": 0.03571428157244946
        },
        "rouge-l": {
          "r": 0.11029411764705882,
          "p": 0.2777777777777778,
          "f": 0.15789473277340732
        }
      }
    },
    {
      "paper_id": "cond-mat.mes-hall.cond-mat/mes-hall/2503.10359v1",
      "true_abstract": "In this study, we systematically explore the non-Hermitian skin effect (NHSE)\nand its associated complex-frequency detection in the context of a\nfrequency-dependent non-Hermitian Hamiltonian. This Hamiltonian arises from the\nself-energy correction of the subsystem and can be calculated exactly within\nour theoretical model, without the need for non-Hermitian approximations.\nAdditionally, complex frequency detection, which encompasses complex frequency\nexcitation, synthesis, and fingerprint, enables us to detect the physical\nresponses driven by complex frequency excitations. Our calculations reveal that\nboth complex frequency excitation and synthesis are sensitive to the\nnon-Hermitian approximation and are unable to characterize the presence or\nabscence of the NHSE. In contrast, the complex-frequency fingerprint\nsuccessfully detects the novel responses induced by the NHSE through the\nintroduction of a double-frequency Green's function. Our work paves the way for\na rigorous understanding of non-Hermitian physics in quantum systems and their\nexperimental verification through complex frequency-domain techniques.",
      "generated_abstract": "We report a comprehensive study of the electronic properties of YbRh2(PO4)3\n(YRP3), a rare earth-doped perovskite with a 2:2:1 asymmetric framework of\nYb3+, Rh3+ and O6- sites. We focus on the effects of Yb doping on the\nelectronic structure of YRP3, focusing on the phonons, electronic\ndynamics, and electronic and magnetic properties. We find that Yb3+ is\nelectronically more localized, and its hybridization with the O6- sites is\nstronger than that of the other rare earth ions, leading to a pronounced\nsplitting of the conduction band, and a strong hybridization of the O6-\nbands with the conduction band. The resulting band structure and its\ninteraction with phonons is found to lead",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07142857142857142,
          "p": 0.1111111111111111,
          "f": 0.0869565169754256
        },
        "rouge-2": {
          "r": 0.014492753623188406,
          "p": 0.020202020202020204,
          "f": 0.01687763226619806
        },
        "rouge-l": {
          "r": 0.07142857142857142,
          "p": 0.1111111111111111,
          "f": 0.0869565169754256
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.05512v1",
      "true_abstract": "Recently, large language model (LLM) based text-to-speech (TTS) systems have\ngradually become the mainstream in the industry due to their high naturalness\nand powerful zero-shot voice cloning capabilities.Here, we introduce the\nIndexTTS system, which is mainly based on the XTTS and Tortoise model. We add\nsome novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid\nmodeling method that combines characters and pinyin, making the pronunciations\nof polyphonic characters and long-tail characters controllable. We also\nperformed a comparative analysis of the Vector Quantization (VQ) with\nFinite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech\ntokens. To further enhance the effect and stability of voice cloning, we\nintroduce a conformer-based speech conditional encoder and replace the\nspeechcode decoder with BigVGAN2. Compared with XTTS, it has achieved\nsignificant improvements in naturalness, content consistency, and zero-shot\nvoice cloning. As for the popular TTS systems in the open-source, such as\nFish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively\nsimple training process, more controllable usage, and faster inference speed.\nMoreover, its performance surpasses that of these systems. Our demos are\navailable at https://index-tts.github.io.",
      "generated_abstract": "This paper explores the integration of deep learning-based speaker verification\n(SV) systems with multi-modal audio-visual (AV) data, leveraging natural\nlanguage-driven speaker verification (NLSV) models and audio-visual (AV)\nmodels, for enhanced speaker verification performance. We propose a\ntransfer-learning-based NLSV+AV model that leverages a pre-trained\nNLSV-based speaker verification model, and a video-level AV model for\naccelerated speaker verification. The AV-based speaker verification model is\ntrained with video-level AV data, and the NLSV-based speaker verification model\nis trained with video-level NLSV data. Additionally, we propose a\nmulti-modal-transfer-learning-based NLSV+AV model, which integrates the\ntransfer learning strategy with the multi-modal training strategy. This model",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11450381679389313,
          "p": 0.26785714285714285,
          "f": 0.1604278032909149
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.10687022900763359,
          "p": 0.25,
          "f": 0.14973261612513952
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2410.07566v1",
      "true_abstract": "Transaction Fee Mechanism Design studies auctions run by untrusted miners for\ntransaction inclusion in a blockchain. Under previously-considered desiderata,\nan auction is considered `good' if, informally-speaking, each party (i.e., the\nminer, the users, and coalitions of both miners and users) has no incentive to\ndeviate from the fixed and pre-determined protocol.\n  In this paper, we propose a novel desideratum for transaction fee mechanisms.\nWe say that a TFM is off-chain influence proof when the miner cannot achieve\nadditional revenue by running a separate auction off-chain. While the\npreviously-highlighted EIP-1559 is the gold-standard according to prior\ndesiderata, we show that it does not satisfy off-chain influence proofness.\nIntuitively, this holds because a Bayesian revenue-maximizing miner can\nstrictly increase profits by persuasively threatening to censor any bids that\ndo not transfer a tip directly to the miner off-chain.\n  On the other hand, we reconsider the Cryptographic (multi-party computation\nassisted) Second Price Auction mechanism, which is technically not `simple for\nminers' according to previous desiderata (since miners may wish to set a\nreserve by fabricating bids). We show that, in a slightly different model where\nthe miner is allowed to set the reserve directly, this auction satisfies\nsimplicity for users and miners, and off-chain influence proofness.\n  Finally, we prove a strong impossibility result: no mechanism satisfies all\npreviously-considered properties along with off-chain influence proofness, even\nwith unlimited supply, and even after soliciting input from the miner.",
      "generated_abstract": "This paper studies the learning problem for the dynamic game of Pareto\noptimization, where the payoff function depends on the state of the game. We\nintroduce a learning algorithm that uses a sequence of relaxations, each\nachieving a different trade-off between the number of iterations and the\nexpected learning gain. Our analysis reveals that the sequence of relaxations\nachieves the optimal trade-off in terms of the expected learning gain,\nachieving the optimality bound for the learning problem. Moreover, the sequence\nof relaxations can be computed in polynomial time. Our results provide a\nconstructive approach for learning the dynamic game of Pareto optimization,\nproviding a framework for the design of efficient algorithms for learning\ndynamic games.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08387096774193549,
          "p": 0.20634920634920634,
          "f": 0.11926605093636915
        },
        "rouge-2": {
          "r": 0.0045045045045045045,
          "p": 0.010309278350515464,
          "f": 0.006269588244222154
        },
        "rouge-l": {
          "r": 0.07741935483870968,
          "p": 0.19047619047619047,
          "f": 0.11009173900976366
        }
      }
    },
    {
      "paper_id": "gr-qc.quant-ph/2503.10348v1",
      "true_abstract": "The fact that quantum theory is non-differentiable, while general relativity\nis built on the assumption of differentiability sources an incompatibility\nbetween quantum theory and gravity. Higher order geometry addresses this issue\ndirectly by extending differential geometry, such that it can be applied to\ntheories that are non-differentiable, but have a certain degree of H\\\"older\nregularity. As this includes the path integral formulation of quantum theory,\nit provides a natural mathematical framework for describing the interplay\nbetween gravity and quantum theory. In this article, we review the motivation\nfor and the basic features of this framework and point towards future\ndevelopments.",
      "generated_abstract": "We study the evolution of a system of coupled free-particle oscillators in\nfinite-depth Schr\\\"odinger cat states. In contrast to the usual deep Schr\\\"odinger\ncat states, which describe a system of interacting particles, the cat states\ndescribe a system of interacting oscillators in a cat-like state. The cat states\nare not eigenstates of the total Hamiltonian but are eigenstates of a\ndifferent total Hamiltonian. We show that, as the depth of the cat states\nincreases, the state of the system approaches a cat-like state. We also show\nthat, as the depth of the cat states increases, the system becomes more and\nmore like the non-interacting system in the limit of infinitely deep cat states.\nWe investigate the evolution of the system and identify two important\nfeatures. First, as the depth of the cat states increases, the system evolves\nfrom a non-inter",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1388888888888889,
          "p": 0.1724137931034483,
          "f": 0.1538461489041422
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.15517241379310345,
          "f": 0.1384615335195268
        }
      }
    },
    {
      "paper_id": "math.NA.math/NA/2503.10552v1",
      "true_abstract": "In this paper, we propose a new workflow to analyze macrophage motion during\nwound healing. These immune cells are attracted to the wound after an injury\nand they move showing both directional and random motion. Thus, first, we\nsmooth the trajectories and we separate the random from the directional parts\nof the motion. The smoothing model is based on curve evolution where the curve\nmotion is influenced by the smoothing term and the attracting term. Once we\nobtain the random sub-trajectories, we analyze them using the mean squared\ndisplacement to characterize the type of diffusion. Finally, we compute the\nvelocities on the smoothed trajectories and use them as sparse samples to\nreconstruct the wound attractant field. To do that, we consider a minimization\nproblem for the vector components and lengths, which leads to solving the\nLaplace equation with Dirichlet conditions for the sparse samples and zero\nNeumann boundary conditions on the domain boundary.",
      "generated_abstract": "In this paper we study the asymptotic behavior of the normalized\ndistribution of the eigenvalues of a random Hermitian matrix. We show that the\nasymptotic distribution is that of a $d$-dimensional Gamma distribution, where\nthe density is given by a generalization of the classical Gamma distribution.\nFurthermore, we characterize the asymptotic distribution of the spectral\nradius of the random Hermitian matrix. Finally, we provide a numerical\nsimulation to verify our results.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13541666666666666,
          "p": 0.3170731707317073,
          "f": 0.18978101770366035
        },
        "rouge-2": {
          "r": 0.0410958904109589,
          "p": 0.1016949152542373,
          "f": 0.058536581266389345
        },
        "rouge-l": {
          "r": 0.13541666666666666,
          "p": 0.3170731707317073,
          "f": 0.18978101770366035
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2501.11983v2",
      "true_abstract": "We provide closed-form market equilibrium formula consolidating informational\nimperfections and investors beliefs. Based on Merton's model, we characterize\nthe equilibrium expected excess returns vector with incomplete information. We\nthen derive the corresponding market portfolio as the solution to a non-linear\nsystem of equations and analyze the sensitivities of extra excess returns to\nshadow-costs and market weights. We derive the market reference model for\nexcess returns under random shadow-costs. The conditional posterior\ndistribution of excess returns integrates the pick-matrix and pick-vector of\nviews and the vector of shadow-costs into a multivariate distribution with mean\nand covariance dependent on the reference model.",
      "generated_abstract": "In this paper, we propose a novel stochastic volatility model with a\nindependent and identically distributed (i.i.d.) Gaussian component to capture\nthe mean-reversion and correlation effects. The model is parameterized by a\nGaussian random field with a deterministic parameter, which is assumed to\nfollow a multivariate normal distribution. We derive a closed-form expression\nfor the conditional mean and covariance of the Gaussian random field. The\nderivation is based on a novel representation of the covariance of the Gaussian\nrandom field. This representation allows us to explicitly express the\nconditional mean and covariance of the Gaussian random field as a function of\nthe conditional mean and covariance of the i.i.d. Gaussian random field. We\ninvestigate the asymptotic properties of the proposed model, and provide a\nnumerical analysis of the model's predictive ability. The results show that the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.3770491803278688,
          "p": 0.30666666666666664,
          "f": 0.3382352891706315
        },
        "rouge-2": {
          "r": 0.0425531914893617,
          "p": 0.03669724770642202,
          "f": 0.03940886202237438
        },
        "rouge-l": {
          "r": 0.32786885245901637,
          "p": 0.26666666666666666,
          "f": 0.29411764211180796
        }
      }
    },
    {
      "paper_id": "cs.CR.econ/GN/2501.09025v2",
      "true_abstract": "The digital age, driven by the AI revolution, brings significant\nopportunities but also conceals security threats, which we refer to as cyber\nshadows. These threats pose risks at individual, organizational, and societal\nlevels. This paper examines the systemic impact of these cyber threats and\nproposes a comprehensive cybersecurity strategy that integrates AI-driven\nsolutions, such as Intrusion Detection Systems (IDS), with targeted policy\ninterventions. By combining technological and regulatory measures, we create a\nmultilevel defense capable of addressing both direct threats and indirect\nnegative externalities. We emphasize that the synergy between AI-driven\nsolutions and policy interventions is essential for neutralizing cyber threats\nand mitigating their negative impact on the digital economy. Finally, we\nunderscore the need for continuous adaptation of these strategies, especially\nin response to the rapid advancement of autonomous AI-driven attacks, to ensure\nthe creation of secure and resilient digital ecosystems.",
      "generated_abstract": "We present a framework for analyzing the impact of government subsidies on\nfossil fuel prices. Using data from the U.S. Energy Information Administration,\nwe show that subsidies for fossil fuels can increase the price of oil, natural\ngas, and coal, while subsidies for renewable energy can decrease it. This\nobservation is consistent with the idea that subsidies for fossil fuels tend to\nreduce the price of fossil fuels and subsidies for renewable energy tend to\nincrease the price of renewable energy. We also show that subsidies for\nrenewable energy tend to increase the price of fossil fuels and subsidies for\nfossil fuels tend to reduce the price of renewable energy. Our analysis\nexplains why subsidies for renewable energy tend to decrease the price of\nrenewable energy while subsidies for fossil fuels",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14423076923076922,
          "p": 0.2777777777777778,
          "f": 0.1898734132222401
        },
        "rouge-2": {
          "r": 0.007352941176470588,
          "p": 0.013157894736842105,
          "f": 0.009433957664651582
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.24074074074074073,
          "f": 0.16455695752603763
        }
      }
    },
    {
      "paper_id": "cs.LG.q-fin/PR/2407.14573v6",
      "true_abstract": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
      "generated_abstract": "This paper presents the design and implementation of the ALIB-POLYMARK, a\npolynomial model of the ALIB-MARK, a macroeconomic model that was developed\nwithin the ALIB project at the University of Oxford. The ALIB-MARK is a\ngeneralised-equilibrium model with a flexible stochastic component that\naccounts for asset prices and volatility, labour supply and demand, and the\naggregate demand for money. It is used to study a broad range of macroeconomic\nand financial phenomena, including macro-financial linkages, financial crises,\nand the transmission of monetary policy. The ALIB-POLYMARK is an extension of\nthe ALIB-MARK that adds a flexible, non-linear monetary policy component.\nThis paper describes the methodology used to design the A",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.06930693069306931,
          "p": 0.10606060606060606,
          "f": 0.08383233054896223
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.06930693069306931,
          "p": 0.10606060606060606,
          "f": 0.08383233054896223
        }
      }
    },
    {
      "paper_id": "cs.CE.cs/CE/2503.07834v1",
      "true_abstract": "The Uniswap is a Decentralized Exchange (DEX) protocol that facilitates\nautomatic token exchange without the need for traditional order books. Every\npair of tokens forms a liquidity pool on Uniswap, and each token can be paired\nwith any other token to create liquidity pools. This characteristic motivates\nus to employ a complex network approach to analyze the features of the Uniswap\nmarket. This research presents a comprehensive analysis of the Uniswap network\nusing complex network methods. The network on October 31, 2023, is built to\nobserve its recent features, showcasing both scale-free and core-periphery\nproperties. By employing node and edge-betweenness metrics, we detect the most\nimportant tokens and liquidity pools. Additionally, we construct daily networks\nspanning from the beginning of Uniswap V2 on May 5, 2020, until October 31,\n2023, and our findings demonstrate that the network becomes increasingly\nfragile over time. Furthermore, we conduct a robustness analysis by simulating\nthe deletion of nodes to estimate the impact of some extreme events such as the\nTerra collapse. The results indicate that the Uniswap network exhibits\nrobustness, yet it is notably fragile when deleting tokens with high\nbetweenness centrality. This finding highlights that, despite being a\ndecentralized exchange, Uniswap exhibits significant centralization tendencies\nin terms of token network connectivity and the distribution of TVL across nodes\n(tokens) and edges (liquidity pools).",
      "generated_abstract": "This paper introduces a framework for building customized and scalable\nmodels for data analysis. This framework, termed Modeling for Analytics\n(MoA), leverages a modularized approach to modeling. The framework includes\ntools for the initial modeling, and a refinement and fine-tuning process. We\ndemonstrate the capabilities of this framework through a case study on\npredicting the risk of financial fraud. The framework is applied to a large\ndataset of financial transactions and produces models that achieve\nstate-of-the-art performance in a real-world setting. The framework can be\napplied to many other areas of data analysis, including healthcare, security,\nand climate change, where customized and scalable models are needed.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12244897959183673,
          "p": 0.25,
          "f": 0.1643835572302497
        },
        "rouge-2": {
          "r": 0.009523809523809525,
          "p": 0.02040816326530612,
          "f": 0.012987008648171462
        },
        "rouge-l": {
          "r": 0.10204081632653061,
          "p": 0.20833333333333334,
          "f": 0.13698629695627712
        }
      }
    },
    {
      "paper_id": "cond-mat.stat-mech.q-bio/SC/2406.16569v1",
      "true_abstract": "In this article we present a comprehensive study of the totally asymmetric\nsimple exclusion process with pausing particles (pTASEP), a model initially\nintroduced to describe RNAP dynamics during transcription. We extend previous\nmean-field approaches and demonstrate that the pTASEP is equivalent to the\nexclusion process with dynamical defects (ddTASEP), thus broadening the scope\nof our investigation to a larger class of problems related to transcription and\ntranslation. We extend the mean-field theory to the open boundary case,\nrevealing the system's phase diagram and critical values of entry and exit\nrates. However, we identify a significant discrepancy between theory and\nsimulations in a region of the parameter space, indicating severe finite-size\neffects. To address this, we develop a single-cluster approximation that\ncaptures the relationship between current and lattice size, providing a more\naccurate representation of the system's dynamics. Finally, we extend our\napproach to open boundary conditions, demonstrating its applicability in\ndifferent scenarios. Our findings underscore the importance of considering\nfinite-size effects, often overlooked in the literature, when modelling\nbiological processes such as transcription and translation.",
      "generated_abstract": "We show that the stochastic heat equation can be viewed as a model of\nrandom walks with a finite-time jump, which can be interpreted as a model for\nthe dynamics of the local temperature fluctuations in biological systems.\n  We derive a formula for the mean squared displacement of a particle in a\nrandom walk with a finite-time jump and apply it to the model of the\nstochastic heat equation. This formula is shown to be valid for both the\nnon-interacting and interacting cases. We use this formula to derive a\ndynamical equation for the variance of the temperature fluctuations in\nbiological systems, which is a generalization of the Fisher-KPP equation. We\nshow that the mean squared displacement of a particle in the model of the\nstochastic heat equation and the dynamics of the variance of temperature\nfluctuations are related through a simple equation, which is valid",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15384615384615385,
          "p": 0.29508196721311475,
          "f": 0.20224718650612308
        },
        "rouge-2": {
          "r": 0.036585365853658534,
          "p": 0.057692307692307696,
          "f": 0.044776114653598195
        },
        "rouge-l": {
          "r": 0.13675213675213677,
          "p": 0.26229508196721313,
          "f": 0.17977527639376356
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2412.19754v3",
      "true_abstract": "This paper examines whether artificial intelligence (AI) acts as a substitute\nor complement to human labour, drawing on 12 million online job vacancies from\nthe United States spanning 2018-2023. We adopt a two-pronged approach: first,\nanalysing \"internal effects\" within roles explicitly requiring AI, and second,\ninvestigating \"external effects\" that arise when industries, occupations, and\nregions experience increases in AI demand. Our focus centres on whether\ncomplementary skills-such as digital literacy, teamwork, resilience, agility,\nor analytical thinking-become more prevalent and valuable as AI adoption grows.\nResults show that AI-focused roles are nearly twice as likely to require skills\nlike resilience, agility, or analytical thinking compared to non-AI roles.\nFurthermore, these skills command a significant wage premium; data scientists,\nfor instance, are offered 5-10% higher salaries if they also possess resilience\nor ethics capabilities. We observe positive spillover effects: a doubling of\nAI-specific demand across industries correlates with a 5% increase in demand\nfor complementary skills, even outside AI-related roles. Conversely, tasks\nvulnerable to AI substitution, such as basic data skills or translation,\nexhibit modest declines in demand. However, the external effect is clearly net\npositive: Complementary effects are up to 1.7x larger than substitution\neffects. These results are consistent across economies, including the United\nKingdom and Australia. Our findings highlight the necessity of reskilling\nworkers in areas where human expertise remains increasingly valuable and\nensuring workers can effectively complement and leverage emerging AI\ntechnologies.",
      "generated_abstract": "We develop a model of trade facilitation in which the government can\nchoose to make its goods more attractive to foreign buyers by lowering\ntrade-distorting barriers, while the private sector can use information about\nthese barriers to improve its own trade policy. We show that the optimal\ntrade-distortion level is zero, which corresponds to a perfect competition\nin which firms are indifferent between different trade-distortion levels. The\nmodel is then used to assess the impact of trade facilitation on the\ndistribution of benefits and costs. The results suggest that a 10% reduction\nin the trade-distortion level leads to a 50% increase in the benefits to\nfirms, and a 5% reduction in the trade-distortion level leads to a 10% increase\nin the benefits to firms.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10404624277456648,
          "p": 0.2535211267605634,
          "f": 0.14754097948031455
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.02857142857142857,
          "f": 0.01801801370018771
        },
        "rouge-l": {
          "r": 0.09826589595375723,
          "p": 0.23943661971830985,
          "f": 0.13934425816883914
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.08013v1",
      "true_abstract": "Most of the existing research on pursuit-evasion game (PEG) is conducted in a\ntwo-dimensional (2D) environment. In this paper, we investigate the PEG in a 3D\nspace. We extend the Apollonius circle (AC) to the 3D space and introduce its\ndetailed analytical form. To enhance the capture efficiency, we derive the\noptimal motion space for both the pursuer and the evader. To address the issue\narising from a discrete state space, we design a fuzzy actor-critic learning\n(FACL) algorithm to obtain the agents' strategies. To improve learning\nperformance, we devise a reward function for the agents, which enables obstacle\navoidance functionality. The effectiveness of the proposed algorithm is\nvalidated through simulation experiments.",
      "generated_abstract": "This paper proposes a novel adaptive feedback control strategy for\ncone-based multi-robot coordination using a single feedback controller for\nmulti-agent systems. The proposed strategy aims to minimize the time required\nfor the formation of a desired convex cone. The proposed controller adapts to\nthe initial state of the system and the formation of the cone. Furthermore, the\nproposed strategy considers the case when the robot is equipped with a\ncomputer. The effectiveness of the proposed adaptive feedback control strategy\nis verified by numerical simulations. The simulations demonstrate that the\nproposed adaptive feedback control strategy can efficiently reduce the time\nrequired for the formation of the desired cone.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13580246913580246,
          "p": 0.2,
          "f": 0.1617647010650953
        },
        "rouge-2": {
          "r": 0.06481481481481481,
          "p": 0.08333333333333333,
          "f": 0.072916661744792
        },
        "rouge-l": {
          "r": 0.13580246913580246,
          "p": 0.2,
          "f": 0.1617647010650953
        }
      }
    },
    {
      "paper_id": "cs.LG.math/OC/2503.10352v1",
      "true_abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
      "generated_abstract": "We study the question of which algorithms are universal quantum\ncomputers. In this paper, we show that a universal quantum computer is\nnecessarily a quantum oracle computer, which is a natural generalization of the\nclassical oracle computer. In particular, we show that all classical algorithms\nthat can be implemented using quantum circuits can be implemented using quantum\noracles. This result implies that a universal quantum computer cannot exist.\nHowever, our approach can be extended to other forms of universality, including\nuniversal quantum circuits and universal quantum circuits using quantum\ncircuits. We also provide a more direct proof of the result that a universal\nquantum computer cannot exist, showing that the necessary and sufficient\nconditions for a universal quantum computer are encoded in the\nquantum-quantum-classical complexity hierarchy. The proof uses the quantum\ncomputation complexity of the circuit model as a proxy for the classical\ncomputation complexity of the algorithm. This work extends our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.29333333333333333,
          "f": 0.2528735583135157
        },
        "rouge-2": {
          "r": 0.03816793893129771,
          "p": 0.04201680672268908,
          "f": 0.03999999501152063
        },
        "rouge-l": {
          "r": 0.21212121212121213,
          "p": 0.28,
          "f": 0.24137930543995254
        }
      }
    },
    {
      "paper_id": "cs.GT.econ/TH/2412.02435v1",
      "true_abstract": "In approval-based budget division, a budget needs to be distributed to some\ncandidates based on the voters' approval ballots over these candidates. In the\npursuit of simple, well-behaved, and approximately fair rules for this setting,\nwe introduce the class of sequential payment rules, where each voter controls a\npart of the budget and repeatedly spends his share on his approved candidates\nto determine the final distribution. We show that all sequential payment rules\nsatisfy a demanding population consistency notion and we identify two\nparticularly appealing rules within this class called the maximum payment rule\n(MP) and the $\\frac{1}{3}$-multiplicative sequential payment rule\n($\\frac{1}{3}$-MP). More specifically, we prove that (i) MP is, apart from one\nother rule, the only monotonic sequential payment rule and gives a\n$2$-approximation to a fairness notion called average fair share, and (ii)\n$\\frac{1}{3}$-MP gives a $\\frac{3}{2}$-approximation to average fair share,\nwhich is optimal among sequential payment rules.",
      "generated_abstract": "In this paper, we study the problem of allocating a collection of goods to\nmicro-markets, each with a limited number of sellers. We assume that the\nconsumers are uncertain about their demand, and the sellers have private\ninformation about their demand. We assume that the demand is a convex function\nof the sellers' private information. We first derive a tight optimal\ndistribution, which can be used as a benchmark to evaluate the performance of\nthe proposed algorithm. We then establish the convergence rate of the\nalgorithm. Furthermore, we show that the algorithm converges to a local\noptimum of the objective function. We provide a polynomial-time approximation\nscheme (PTAS) for the problem, which can be used to achieve near-optimal\nperformance in polynomial time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18085106382978725,
          "p": 0.22972972972972974,
          "f": 0.2023809474518142
        },
        "rouge-2": {
          "r": 0.02877697841726619,
          "p": 0.037037037037037035,
          "f": 0.032388659046370956
        },
        "rouge-l": {
          "r": 0.14893617021276595,
          "p": 0.1891891891891892,
          "f": 0.16666666173752848
        }
      }
    },
    {
      "paper_id": "cs.SD.eess/AS/2502.15849v1",
      "true_abstract": "Western music is an innately hierarchical system of interacting levels of\nstructure, from fine-grained melody to high-level form. In order to analyze\nmusic compositions holistically and at multiple granularities, we propose a\nunified, hierarchical meta-representation of musical structure called the\nstructural temporal graph (STG). For a single piece, the STG is a data\nstructure that defines a hierarchy of progressively finer structural musical\nfeatures and the temporal relationships between them. We use the STG to enable\na novel approach for deriving a representative structural summary of a music\ncorpus, which we formalize as a dually NP-hard combinatorial optimization\nproblem extending the Generalized Median Graph problem. Our approach first\napplies simulated annealing to develop a measure of structural distance between\ntwo music pieces rooted in graph isomorphism. Our approach then combines the\nformal guarantees of SMT solvers with nested simulated annealing over\nstructural distances to produce a structurally sound, representative centroid\nSTG for an entire corpus of STGs from individual pieces. To evaluate our\napproach, we conduct experiments verifying that structural distance accurately\ndifferentiates between music pieces, and that derived centroids accurately\nstructurally characterize their corpora.",
      "generated_abstract": "Audio-Visual Synthesis (AVS) has been a research hotspot in the past few\nyears, with significant advancements in both speech synthesis and video\ngeneration. In the speech domain, recent progress has mainly been driven by\nattention-based models and the use of large language models. However, audio\nsynthesis has traditionally been dominated by autoregressive models, which have\nbeen shown to be highly restrictive and computationally expensive. To overcome\nthese limitations, this paper proposes a novel multimodal generative model\ncalled Audio-Visual Spectral Attention (AVSA), which integrates audio and\nvisual features with attention mechanisms to better capture the multimodal\nproperties of audio-visual signals. Our model employs a novel spectral\nattention mechanism to effectively capture the temporal dependencies of audio\nand visual signals, as well as their spatial correlations. Additionally, we\nintrodu",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15702479338842976,
          "p": 0.2087912087912088,
          "f": 0.17924527811899266
        },
        "rouge-2": {
          "r": 0.016666666666666666,
          "p": 0.02459016393442623,
          "f": 0.019867544853297076
        },
        "rouge-l": {
          "r": 0.14049586776859505,
          "p": 0.18681318681318682,
          "f": 0.1603773535906908
        }
      }
    },
    {
      "paper_id": "cs.GR.cs/GR/2503.09864v1",
      "true_abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nremarkable control over various attributes, yet precise color specification\nremains a fundamental challenge. Existing approaches, such as ColorPeel, rely\non model personalization, requiring additional optimization and limiting\nflexibility in specifying arbitrary colors. In this work, we introduce\nColorWave, a novel training-free approach that achieves exact RGB-level color\ncontrol in diffusion models without fine-tuning. By systematically analyzing\nthe cross-attention mechanisms within IP-Adapter, we uncover an implicit\nbinding between textual color descriptors and reference image features.\nLeveraging this insight, our method rewires these bindings to enforce precise\ncolor attribution while preserving the generative capabilities of pretrained\nmodels. Our approach maintains generation quality and diversity, outperforming\nprior methods in accuracy and applicability across diverse object categories.\nThrough extensive evaluations, we demonstrate that ColorWave establishes a new\nparadigm for structured, color-consistent diffusion-based image synthesis.",
      "generated_abstract": "We study the problem of optimizing the design of the path of a robot in a\ngraph. We model this as a combinatorial optimization problem, where we are\ninterested in finding a path that maximizes the sum of edge weights.\nSpecifically, we study the case where the robot moves on a simple graph. We\nintroduce a novel heuristic that we call \"Clique Covering,\" which is a\ncombination of the best known heuristics for the problem of finding a\n$c$-approximate clique covering of a graph, and a $k$-approximate clique\ncovering of a graph. The approach we take for designing the heuristic is to\ncombine ideas from the combinatorial optimization literature with ideas from\ngraph theory. Our main contribution is a simple yet efficient implementation\nof our heuristic. We evaluate our algorithm on a number of benchmarks, and\nshow that our he",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1724137931034483,
          "p": 0.2631578947368421,
          "f": 0.20833332855034734
        },
        "rouge-2": {
          "r": 0.007352941176470588,
          "p": 0.008130081300813009,
          "f": 0.007722002734607655
        },
        "rouge-l": {
          "r": 0.15517241379310345,
          "p": 0.23684210526315788,
          "f": 0.187499995217014
        }
      }
    },
    {
      "paper_id": "cond-mat.other.cond-mat/other/2503.04533v1",
      "true_abstract": "In this article it is argued that synthetic axions, emergent collective\nexcitations in topological insulators or Weyl semimetals hybridize with the\ncosmological axion, a compelling dark matter candidate via a common two photon\ndecay channel since they both couple to electromagnetic fields via a\nChern-Simons term. We point out an analogy to a V-type three level system with\nthe two upper levels identified with the synthetic and cosmological axions\ndecaying into a two-photon state. The Weisskopf-Wigner theory of spontaneous\ndecay in multi level atoms is complemented and extended to describe the\ndynamics of hybridization. The final two-photon state features both kinematic\nand polarization entanglement and displays quantum beats as a consequence of\nthe interference between the decay paths. An initial population of either axion\ninduces a population of the other via hybridization. Consequently, a dark\nmatter axion condensate induces a condensate of the synthetic axion, albeit\nwith small amplitude. We obtain a momentum and polarization resolved Hanbury-\nBrown Twiss (HBT) second order coherence describing coincident correlated\ntwo-photon detection. It exhibits quantum beats with a frequency given by the\ndifference between the energies of the synthetic and cosmological axion and\n\\emph{perhaps may be harnessed} to detect either type of axion excitations. The\ncase of synthetic axions individually is obtained in the limit of vanishing\ncoupling of the cosmological axion and features similar two-photon\ncorrelations. Hence second order (HBT) two-photon coherence \\emph{may} provide\nan alternative detection mechanism for emergent condensed matter axionic\ncollective excitations. Similarities and differences with parametrically down\nconverted photons are discussed.",
      "generated_abstract": "We present a theoretical study of a class of spinless fermion systems which\nhave recently attracted considerable attention. In contrast to standard\nfermionic systems, where the spinless fermions are described by the Dirac\nequation, here the spinless fermions are described by the Majorana equation.\nThe fermions can be considered as Dirac fermions in a two-dimensional (2D)\nspin-orbit (SO) lattice. The Hamiltonian of the system is written in the\nLandau gauge, where the SO interaction is included through a complex coupling\nconstant. We present a numerical study of the system and show that,\nsufficiently close to the quantum critical point, the system exhibits a\nnon-trivial vortex-antivortex structure. In particular, we find that,\nparticularly close to the critical point, the vortex-antivortex pairing is\nstrongly suppressed, which is a",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12080536912751678,
          "p": 0.23376623376623376,
          "f": 0.15929203090570926
        },
        "rouge-2": {
          "r": 0.013157894736842105,
          "p": 0.027777777777777776,
          "f": 0.017857138494899023
        },
        "rouge-l": {
          "r": 0.10067114093959731,
          "p": 0.19480519480519481,
          "f": 0.1327433583393376
        }
      }
    },
    {
      "paper_id": "q-bio.MN.q-bio/MN/2409.06877v1",
      "true_abstract": "We present some results helpful for parameterising positive equilibria, and\nbounding the number of positive nondegenerate equilibria, in mass action\nnetworks. Any mass action network naturally gives rise to a set of polynomial\nequations whose positive solutions are precisely the positive equilibria of the\nnetwork. Here we derive alternative systems of equations, often also\npolynomial, whose solutions are in smooth, one-to-one correspondence with\npositive equilibria of the network. Often these alternative systems are simpler\nthan the original mass action equations, and allow us to infer useful bounds on\nthe number of positive equilibria. The alternative equation systems can also be\nhelpful for parameterising the equilibrium set explicitly, for deriving\ndescriptions of the parameter regions for multistationarity, and for studying\nbifurcations. We present the main construction, some bounds which follow for\nparticular classes of networks, numerous examples, and some open questions and\nconjectures.",
      "generated_abstract": "We investigate the evolution of the cooperative interactions between\nreaction networks with multiple time-varying state variables. We find that\nthe interaction of multiple reactions in a network can be expressed as an\nequilibrium point of the interaction matrix of the system. The equilibrium\npoint depends on the evolution of the state variables in the network. We also\nintroduce a model for the evolution of the state variables of the network.\nThis model incorporates the effects of the interaction between the network and\nthe state variables. The model is used to simulate the evolution of the state\nvariables of the network. Our model is further used to simulate the evolution\nof the interaction of the network and the state variables. The results of the\nsimulations are discussed.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2289156626506024,
          "p": 0.36538461538461536,
          "f": 0.28148147674513035
        },
        "rouge-2": {
          "r": 0.024,
          "p": 0.0375,
          "f": 0.02926828792385562
        },
        "rouge-l": {
          "r": 0.20481927710843373,
          "p": 0.3269230769230769,
          "f": 0.25185184711550074
        }
      }
    },
    {
      "paper_id": "astro-ph.SR.astro-ph/SR/2503.09699v1",
      "true_abstract": "We introduce a novel approach to detecting microlensing events and other\ntransients in light curves, utilising the isolation forest (iForest) algorithm\nfor anomaly detection. Focusing on the Legacy Survey of Space and Time by the\nVera C. Rubin Observatory, we show that an iForest trained on signal-less light\ncurves can efficiently identify microlensing events by different types of dark\nobjects and binaries, as well as variable stars. We further show that the\niForest has real-time applicability through a drip-feed analysis, demonstrating\nits potential as a valuable tool for LSST alert brokers to efficiently\nprioritise and classify transient candidates for follow-up observations.",
      "generated_abstract": "This study employs the Lyman-$\\alpha$ forest as a tool to search for\nsignatures of dark matter annihilation in the local Universe. We use the\nHydra-XALT survey to detect and characterize Lyman-$\\alpha$ forest absorption\nlines in 565 galaxies. Our results reveal a broad excess of absorption in\ngalaxies with a total stellar mass between 10$^{10}$ and 10$^{11.5}$\n$M_{\\odot}$. The excess is significantly higher for galaxies with stellar mass\nbelow 10$^{10}$ $M_{\\odot}$ compared to the highest stellar mass bin. This\nexcess of absorption is interpreted as a signature of dark matter annihilation.\nWe find no evidence for the presence of a dark matter halo around the highest\nstellar mass bin, which is consistent with the results of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15584415584415584,
          "p": 0.1875,
          "f": 0.17021276099994984
        },
        "rouge-2": {
          "r": 0.02040816326530612,
          "p": 0.019801980198019802,
          "f": 0.02010049751370039
        },
        "rouge-l": {
          "r": 0.15584415584415584,
          "p": 0.1875,
          "f": 0.17021276099994984
        }
      }
    },
    {
      "paper_id": "cond-mat.str-el.cond-mat/supr-con/2503.09709v1",
      "true_abstract": "We study the interplay between quantum geometry, interactions, and external\nfields in complex band systems. When Wannier obstructions preclude a\ndescription based solely on atomic-like orbitals,this complicates the\nprediction of electromagnetic responses particularly in the presence of\ndisorder and interactions. In this work, we introduce a generalized Peierls\nsubstitution framework based on Lagrange multipliers to enforce the constraints\nof the Wannier obstruction in the band of interest. Thus we obtain effective\ndescriptions of interactions and disorder in the presence of non-trivial\nquantum geometry of that band. We apply our approach to examples including the\ndiamagnetic response in flat-band superconductors and delocalization effects in\nflat-band metals caused by interactions and disorder.",
      "generated_abstract": "We present a comprehensive study of the dynamics of a weakly interacting\nmassive particle in a one-dimensional (1D) quantum wire. Our analysis is\nbased on the mean-field (MF) approach, and we compute the time-dependent\neffective mass and the effective potential in the quantum wire using the\nBethe-Salpeter equation (BSE) approach. Our results reveal that, in the\nstrong-coupling regime, the system behaves as a quantum gas of massless\nparticles. However, in the weak-coupling limit, the effective mass and the\neffective potential become non-analytic functions of time and exhibit a\ncontinuous crossover from the MF description to the Mott-insulator (MI) phase.\nMoreover, we find that the system exhibits an MI-like transition in the\nstrong-coupling regime, which is accompanied by the appearance",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2361111111111111,
          "p": 0.2236842105263158,
          "f": 0.22972972473338216
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.02857142857142857,
          "f": 0.02898550224742787
        },
        "rouge-l": {
          "r": 0.18055555555555555,
          "p": 0.17105263157894737,
          "f": 0.17567567067932813
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/AP/2503.08902v1",
      "true_abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures.",
      "generated_abstract": "We propose a novel method for constructing robust and interpretable\nprediction models for high-dimensional continuous and discrete variables\nusing a novel non-parametric mixture of experts. The method is based on the\nanalysis of the sample covariance matrix and aims to estimate the model\nparameters using only a small sample of the data. We establish the convergence\nof the proposed estimator to the true parameter in a non-asymptotic sense. We\nshow that the proposed estimator can be applied to both continuous and discrete\ndata and we demonstrate its effectiveness in simulated and real-world datasets.\nThe proposed method is particularly useful in cases where the data are noisy\nand the number of samples is limited, wherein the conventional methods fail to\nprovide a reliable estimate. We also propose a new approach to the problem of\nestimating the number of experts in the mixture and derive the asymptotic\nproperties of the proposed estimator",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16265060240963855,
          "p": 0.3253012048192771,
          "f": 0.21686746543507374
        },
        "rouge-2": {
          "r": 0.05785123966942149,
          "p": 0.10606060606060606,
          "f": 0.07486630559295404
        },
        "rouge-l": {
          "r": 0.15060240963855423,
          "p": 0.30120481927710846,
          "f": 0.2008032084069613
        }
      }
    },
    {
      "paper_id": "math.DG.math/AT/2503.08457v1",
      "true_abstract": "This paper explores foliated differential graded algebras (dga) and their\nrole in extending fundamental theorems of differential geometry to foliations.\nWe establish an $A_{\\infty}$ de Rham theorem for foliations, demonstrating that\nthe classical quasi-isomorphism between singular cochains and de Rham forms\nlifts to an $A_{\\infty}$ quasi-isomorphism in the foliated setting.\nFurthermore, we investigate the Riemann-Hilbert correspondence for foliations,\nbuilding upon the established higher Riemann-Hilbert correspondence for\nmanifolds. By constructing an integration functor, we prove a higher\nRiemann-Hilbert correspondence for foliations, revealing an equivalence between\n$\\infty$-representations of $L_{\\infty}$-algebroids and\n$\\infty$-representations of Lie $\\infty$-groupoids within the context of\nfoliations. This work generalizes the classical Riemann-Hilbert correspondence\nto foliations, providing a deeper understanding of the relationship between\nrepresentations of Lie algebroids and Lie groupoids in this framework.",
      "generated_abstract": "In this paper, we study the weakly compactness of a class of compact\noperators acting on the Hilbert space of entire functions. Our main result\nconcerns the compactness of a class of operators acting on the Hilbert space of\nBanach spaces of entire functions. This result extends the results of\nBartosik-Burda-Kalinowski-Kubica (2024) to the case of compact operators. In\naddition, we give a classification of compact operators acting on the Hilbert\nspace of entire functions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0945945945945946,
          "p": 0.2,
          "f": 0.12844036261257485
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08108108108108109,
          "p": 0.17142857142857143,
          "f": 0.11009173875936387
        }
      }
    },
    {
      "paper_id": "math.NT.cs/DM/2503.10158v1",
      "true_abstract": "Integral linear systems $Ax=b$ with matrices $A$, $b$ and solutions $x$ are\nalso required to be in integers, can be solved using invariant factors of $A$\n(by computing the Smith Canonical Form of $A$). This paper explores a new\nproblem which arises in applications, that of obtaining conditions for solving\nthe Modular Linear System $Ax=b\\rem n$ given $A,b$ in $\\zz_n$ for $x$ in\n$\\zz_n$ along with the constraint that the value of the linear function\n$\\phi(x)=<w,x>$ is coprime to $n$ for some solution $x$. In this paper we\ndevelop decomposition of the system to coprime moduli $p^{r(p)}$ which are\ndivisors of $n$ and show how such a decomposition simplifies the computation of\nSmith form. This extends the well known index calculus method of computing the\ndiscrete logarithm where the moduli over which the linear system is reduced\nwere assumed to be prime (to solve the reduced systems over prime fields) to\nthe case when the factors of the modulus are prime powers $p^{r(p)}$. It is\nshown how this problem can be addressed effciently using the invariant factors\nand Smith form of the augmented matrix $[A,-p^{r(p)}I]$ and conditions modulo\n$p$ satisfied by $w$, where $p^{r(p)}$ vary over all divisors of $n$ with $p$\nprime.",
      "generated_abstract": "We study a class of (non-admissible) discrete time quantum walk on a\nsurface of finite type. These are the quantum walk on the surface that\npreserves the topology of the surface (i.e. the discrete time version of the\nTamarkin-Zamolodchikov model). In this paper, we give a characterization of\nthe dynamics of the walk on a surface of finite type that is not simply connected.\nOur characterization relies on the notion of the quantum flow on a surface\nof finite type. The main result of the paper is the characterization of the\ndynamics of the walk on a surface of finite type that is simply connected.\nMoreover, we show that the dynamics of the walk on a surface of finite type\nthat is simply connected is the same as the dynamics of the walk on a surface\nof finite type that is simply connected with a certain additional condition.\nWe also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12280701754385964,
          "p": 0.2641509433962264,
          "f": 0.16766466632579163
        },
        "rouge-2": {
          "r": 0.021164021164021163,
          "p": 0.047619047619047616,
          "f": 0.029304025043674894
        },
        "rouge-l": {
          "r": 0.11403508771929824,
          "p": 0.24528301886792453,
          "f": 0.15568861842160003
        }
      }
    },
    {
      "paper_id": "q-bio.SC.q-bio/SC/2407.15697v1",
      "true_abstract": "Voltage distribution in sub-cellular micro-domains such as neuronal synapses,\nsmall protrusions or dendritic spines regulates the opening and closing of\nionic channels, energy production and thus cellular homeostasis and\nexcitability. Yet how voltage changes at such a small scale in vivo remains\nchallenging due to the experimental diffraction limit, large signal\nfluctuations and the still limited resolution of fast voltage indicators. Here,\nwe study the voltage distribution in nano-compartments using a computational\napproach based on the Poisson-Nernst-Planck equations for the electro-diffusion\nmotion of ions, where inward and outward fluxes are generated between channels.\nWe report a current-voltage (I-V) logarithmic relationship generalizing Nernst\nlaw that reveals how the local membrane curvature modulates the voltage. We\nfurther find that an influx current penetrating a cellular electrolyte can lead\nto perturbations from tens to hundreds of nanometers deep depending on the\nlocal channels organization. Finally, we show that the neck resistance of\ndendritic spines can be completely shunted by the transporters located on the\nhead boundary, facilitating ionic flow. To conclude, we propose that voltage is\nregulated at a subcellular level by channels organization, membrane curvature\nand narrow passages.",
      "generated_abstract": "The development of innovative diagnostic tests to improve the early\ndetermination of the onset of cardiovascular diseases is essential. Accurate\ndiagnosis of these diseases requires accurate detection of the onset of\ncardiovascular events. The aim of this study is to propose a novel algorithm\nfor the diagnosis of atherosclerosis using a deep learning approach. This\nstudy is a retrospective analysis of 1,520 patients with atherosclerosis,\ntreated for 30 years. The study focused on the development of a deep learning\nmodel that could predict the onset of cardiovascular events, such as myocardial\ninfarction (MI) and stroke, using the patient's cardiovascular history. The\nmodel was built using a deep learning algorithm, using the Convolutional\nNeural Networks (CNN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12307692307692308,
          "p": 0.2318840579710145,
          "f": 0.160804015570314
        },
        "rouge-2": {
          "r": 0.02247191011235955,
          "p": 0.04081632653061224,
          "f": 0.028985502666457354
        },
        "rouge-l": {
          "r": 0.09230769230769231,
          "p": 0.17391304347826086,
          "f": 0.12060301054518843
        }
      }
    },
    {
      "paper_id": "hep-ex.physics/ins-det/2503.09392v1",
      "true_abstract": "The muon anomalous magnetic moment, $a_\\mu=\\frac{g-2}{2}$, is a low-energy\nobservable which can be both measured and computed to high precision, making it\na sensitive test of the Standard Model and a probe for new physics. This\nanomaly was measured with a precision of $0.20$~parts per million (ppm) by the\nFermilab's Muon g-2 (E989) experiment. The final goal of the E989 experiment is\nto reach a precision of $0.14$~ppm. The experiment is based on the measurement\nof the muon spin anomalous precession frequency, $\\omega_a$, based on the\narrival time distribution of high-energy decay positrons observed by 24\nelectromagnetic calorimeters, placed around the inner circumference of a $14$~m\ndiameter storage ring, and on the precise knowledge of the storage ring\nmagnetic field and of the beam time and space distribution. Achieving this\nlevel of precision requires strict control over systematics, which is ensured\nthrough several diagnostic devices. At the accelerator level, these devices\nmonitor the quality of the injected beam (e.g., verifying that it has the\ncorrect momentum), while at the detector level, they track both the magnetic\nfield and the gain of the calorimeters. In this work the devices and techniques\nused by the E989 experiment will be presented.",
      "generated_abstract": "The FASER experiment aims to search for the Standard Model Higgs boson in the\nregion of the Higgs boson mass range of 110-140 GeV. The FASER experiment\ncombines the full momentum resolution of the Compact Muon Solenoid (CMS)\ndetector with the full energy resolution of the Inner Tracking System (ITS)\ndetector, providing unprecedented energy and momentum resolution. The\ncompilation of the FASER ITS and CMS data is presented in this work. It is\ndemonstrated that the energy and momentum resolutions of the FASER ITS\ncompared to the CMS ITS are comparable to those of other experiments in the\nsame mass region. The FASER ITS and CMS data are analyzed to search for the\nStandard Model Higgs boson in the mass range of 110",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.136,
          "p": 0.288135593220339,
          "f": 0.18478260433896512
        },
        "rouge-2": {
          "r": 0.02185792349726776,
          "p": 0.04395604395604396,
          "f": 0.029197075855666936
        },
        "rouge-l": {
          "r": 0.112,
          "p": 0.23728813559322035,
          "f": 0.15217390868679126
        }
      }
    },
    {
      "paper_id": "cs.CG.math/MG/2503.01988v1",
      "true_abstract": "Metric spaces defined within convex polygons, such as the Thompson, Funk,\nreverse Funk, and Hilbert metrics, are subjects of recent exploration and study\nin computational geometry. This paper contributes an educational piece of\nsoftware for understanding these unique geometries while also providing a tool\nto support their research. We provide dynamic software for manipulating the\nFunk, reverse Funk, and Thompson balls in convex polygonal domains.\nAdditionally, we provide a visualization program for traversing the Hilbert\npolygonal geometry.",
      "generated_abstract": "We prove the following: if $n\\geq 4$ and $d\\geq 3$, then there is a\n$d$-colored $n$-vertex graph $G$ such that every edge of $G$ has at least\n$n-1$ colors.\n  We also prove that $G$ is $(n-1)$-colorable if and only if $G$ is $d$-colorable\nfor some $d\\geq 3$.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13793103448275862,
          "p": 0.22857142857142856,
          "f": 0.17204300605850398
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.1206896551724138,
          "p": 0.2,
          "f": 0.150537629714418
        }
      }
    },
    {
      "paper_id": "q-fin.PR.q-fin/PR/2411.12375v3",
      "true_abstract": "In this paper, we introduce a novel pricing model for Uniswap V3, built upon\nstochastic processes and the Martingale Stopping Theorem. This model\ninnovatively frames the valuation of positions within Uniswap V3. We further\nconduct a numerical analysis and examine the sensitivities through Greek risk\nmeasures to elucidate the model's implications. The results underscore the\nmodel's significant academic contribution and its practical applicability for\nUniswap liquidity providers, particularly in assessing risk exposure and\nguiding hedging strategies.",
      "generated_abstract": "The development of advanced artificial intelligence (AI) and the exponential\n increase in the size of datasets have led to the emergence of a new class of\n  deep learning models, termed large language models (LLMs), which have been\n  shown to outperform traditional language models in many applications. However,\n  the development of LLMs has also raised concerns about the potential for\n  misinformation and cybersecurity risks. In response, several countries have\n  taken steps to regulate LLMs, including banning their use in government\n  applications and imposing restrictions on their use in sensitive areas. This\n  paper reviews the regulatory framework for LLMs in India and discusses the\n  potential impacts of these measures on the development and deployment of\n  LLMs in the country. The paper also explores the potential for LLMs to\n  enhance cybersecurity and recommendations for future research and\n  development",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1774193548387097,
          "p": 0.13253012048192772,
          "f": 0.1517241330359098
        },
        "rouge-2": {
          "r": 0.0136986301369863,
          "p": 0.00819672131147541,
          "f": 0.01025640557212574
        },
        "rouge-l": {
          "r": 0.16129032258064516,
          "p": 0.12048192771084337,
          "f": 0.13793102958763392
        }
      }
    },
    {
      "paper_id": "cs.LG.q-bio/QM/2502.16660v3",
      "true_abstract": "The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.",
      "generated_abstract": "We study the problem of identifying molecular signatures from protein-protein\ninteraction (PPI) networks. Traditional methods such as Gaussian process\nregression (GPR) and deep neural networks (DNNs) suffer from high-dimensional\nand non-convex problems, which often lead to suboptimal solutions. To address\nthese limitations, we propose a novel unsupervised learning framework,\nBIO-GPR, which leverages bio-molecular data to improve the predictive\nperformance of GPR. In particular, we incorporate protein-protein interaction\n(PPI) networks into the training data, enhancing the predictive capability of\nGPR. BIO-GPR leverages the latent features of the PPI networks to improve the\npredictive accuracy of GPR. To address the issue of data scarcity, we propose\na novel data augmentation strategy that utilizes a subset",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14285714285714285,
          "p": 0.21621621621621623,
          "f": 0.17204300596138294
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.04040404040404041,
          "f": 0.03333332848645905
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.21621621621621623,
          "f": 0.17204300596138294
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.06538v1",
      "true_abstract": "In two-way contingency tables under an asymmetric situation, where the row\nand column variables are defined as explanatory and response variables\nrespectively, quantifying the extent to which the explanatory variable\ncontributes to predicting the response variable is important. One\nquantification method is the association measure, which indicates the degree of\nassociation in a range from $0$ to $1$. Among various measures, those based on\nproportional reduction in error (PRE) are particularly notable for their\nsimplicity and intuitive interpretation. These measures, including\nGoodman-Kruskal's lambda proposed in 1954, are widely implemented in\nstatistical software such as R and SAS and remain extensively used. However, a\nknown limitation of PRE measures is their potential to return a value of $0$\ndespite no independence. This issue arises because the measures are constructed\nbased solely on the maximum joint and marginal probabilities, failing to\nutilize the information available in the contingency table fully. To address\nthis problem, we propose new association measures designed for the proportional\nreduction in error with multiple categories. The properties of the proposed\nmeasure are examined and their utility is demonstrated through simulations and\nreal data analyses. The results suggest their potential as practical tools in\napplied statistics.",
      "generated_abstract": "We consider a longitudinal panel data model where the time-varying\nparameter of interest is the average of the panel average of two\ninterdependent variables. We derive a consistent estimator for the average\nof the panel average of two variables, and show that it is asymptotically\nidentical to the average of the panel average of two independent variables. We\nalso derive a consistent estimator for the average of the panel average of two\nindependent variables, and show that it is asymptotically equivalent to the\naverage of the panel average of two dependent variables. The two estimators\npresented here are based on the method of moments, and do not rely on any\nasymptotic calculations. We illustrate the proposed estimators using simulated\ndata and an application to a real-world dataset.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1297709923664122,
          "p": 0.2982456140350877,
          "f": 0.18085105960445913
        },
        "rouge-2": {
          "r": 0.036458333333333336,
          "p": 0.08433734939759036,
          "f": 0.05090908669461192
        },
        "rouge-l": {
          "r": 0.10687022900763359,
          "p": 0.24561403508771928,
          "f": 0.14893616598743786
        }
      }
    },
    {
      "paper_id": "hep-ph.nucl-th/2503.10157v1",
      "true_abstract": "In this work, we perform a systematic study on the multiplicity dependence of\nhadron productions at mid-rapidity ($|y|<0.5$), ranging from the light to the\ncharm sector in pp collisions at $\\sqrt{s}=13$ TeV. This study utilizes a\nmulti-phase transport model (AMPT) coupled with PYTHIA8 initial conditions. We\nhave investigated the baryon to meson ratios as well as the strange to\nnon-strange meson ratios varying with the charged particle density. By tuning\nthe coalescence parameters, the AMPT model provides a reasonable description to\nthe experimental data for inclusive productions of both light and charm\nhadrons, comparable to the string fragmentation model calculations with color\nreconnection effects. Additionally, we have analyzed the relative production of\nhadrons by examining self-normalized particle ratios as a function of the\ncharged hadron density. Our findings suggest that parton evolution effects and\nthe coalescence hadronization process in AMPT model lead to a strong flavor\nhierarchy in the multiplicity dependence of the baryon to meson ratio.\nFurthermore, our investigation on the $p_T$ differential double ratio of baryon\nto meson fraction between high and low multiplicity events indicates distinct\nmodifications to the flavor associated baryon to meson ratio $p_T$ shape in\nhigh multiplicity events when comparing the coalescence hadronization model to\nthe color reconnection model. These observations highlight the importance of\nunderstanding the hadronization process in high-energy proton-proton collisions\nthrough a comprehensive multiplicity dependent multi-flavor analysis.",
      "generated_abstract": "The two-pronged $J/\\psi \\to \\gamma\\eta$ decay is the only decay mode\nfor $J/\\psi$ that can be measured experimentally. It is a non-perturbative\nprocess that can be treated perturbatively using QCD. The perturbative\ncalculation is valid when the decay rate is dominated by the $J/\\psi \\to\n\\gamma\\eta$ process, and the decay rate is small compared to the vacuum\noscillation rate. The perturbative calculation of the decay rate is accurate\nonly to next-to-leading order. In this paper, we calculate the decay rate for\nthe first time using the non-perturbative approximation. We obtain a result\nthat is in good agreement with the perturbative result, within a factor of two\nand within the perturbative uncertainty. We also calculate the decay rate in\nthe non-perturbative approximation using the non-factorizable perturb",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12403100775193798,
          "p": 0.24242424242424243,
          "f": 0.16410255962445772
        },
        "rouge-2": {
          "r": 0.030303030303030304,
          "p": 0.05825242718446602,
          "f": 0.03986710513261494
        },
        "rouge-l": {
          "r": 0.10077519379844961,
          "p": 0.19696969696969696,
          "f": 0.13333332885522697
        }
      }
    },
    {
      "paper_id": "econ.EM.econ/EM/2501.00800v1",
      "true_abstract": "The article examines the impact of 16 key parameters of the Georgian economy\non economic inequality, using the Perelman model and Ricci flow mathematical\nmethods. The study aims to conduct a deep analysis of the impact of\nsocio-economic challenges and technological progress on the dynamics of the\nGini coefficient. The article examines the following parameters: income\ndistribution, productivity (GDP per hour), unemployment rate, investment rate,\ninflation rate, migration (net negative), education level, social mobility,\ntrade infrastructure, capital flows, innovative activities, access to\nhealthcare, fiscal policy (budget deficit), international trade (turnover\nrelative to GDP), social protection programs, and technological access. The\nresults of the study confirm that technological innovations and social\nprotection programs have a positive impact on reducing inequality. Productivity\ngrowth, improving the quality of education, and strengthening R&D investments\nincrease the possibility of inclusive development. Sensitivity analysis shows\nthat social mobility and infrastructure are important factors that affect\neconomic stability. The accuracy of the model is confirmed by high R^2 values\n(80-90%) and the statistical reliability of the Z-statistic (<0.05). The study\nuses Ricci flow methods, which allow for a geometric analysis of the\ntransformation of economic parameters in time and space. Recommendations\ninclude the strategic introduction of technological progress, the expansion of\nsocial protection programs, improving the quality of education, and encouraging\ninternational trade, which will contribute to economic sustainability and\nreduce inequality. The article highlights multifaceted approaches that combine\ntechnological innovation and responses to socio-economic challenges to ensure\nsustainable and inclusive economic development.",
      "generated_abstract": "We propose a new method to construct a sequence of auxiliary variables for\ninference in the linear dynamic model with endogenous regressors. The\nsequence is constructed from the data and the auxiliary variables are\nidentified based on the identification theorem for the linear dynamic model. The\nsequence is constructed in a way that the conditional distribution of the\nauxiliary variable given the original variables is consistent with the\nassumptions of the linear dynamic model. The identification theorem is\nestablished based on the consistency of the auxiliary variable conditional\ndistribution. The method is applied to the data on the relationship between\nrural household income and household characteristics and the price level in\nTaiwan.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0958904109589041,
          "p": 0.25925925925925924,
          "f": 0.13999999605800012
        },
        "rouge-2": {
          "r": 0.013574660633484163,
          "p": 0.03409090909090909,
          "f": 0.01941747165446614
        },
        "rouge-l": {
          "r": 0.08904109589041095,
          "p": 0.24074074074074073,
          "f": 0.1299999960580001
        }
      }
    },
    {
      "paper_id": "stat.OT.stat/OT/2410.18062v1",
      "true_abstract": "Undergraduate graders are frequently important contributors to the teaching\nteam in post-secondary education settings. This study set out to investigate\nagreement for a team of undergraduate graders as they acquired training and\nexperience for scoring responses to open-ended tasks. Results demonstrate\ncompelling evidence that undergraduate students can develop the ability to\nestablish and sustain substantial agreement with an instructor, especially when\nequipped with proper training and a high-quality scoring rubric.",
      "generated_abstract": "This paper introduces a novel approach to the problem of estimating the\nparameters of a stochastic differential equation (SDE) when the SDE is only\nassumed to have a finite-dimensional distribution. This approach is based on a\ngeneralization of the SDE-based estimator for the parameters of a linear SDE\nintroduced by Cox and Ross. The proposed estimator has the form of a\nstochastic integral, and its estimation is performed by using a sequential\nmethod that allows to evaluate the integral only in the vicinity of the SDE\nsolution. We provide a theoretical analysis of the proposed estimator,\nestablishing its consistency and asymptotic normality. In addition, we propose\na practical algorithm that can be used to estimate the parameters of the SDE\nwithin a finite time.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2,
          "p": 0.14473684210526316,
          "f": 0.1679389264261991
        },
        "rouge-2": {
          "r": 0.014705882352941176,
          "p": 0.009009009009009009,
          "f": 0.011173179646080448
        },
        "rouge-l": {
          "r": 0.2,
          "p": 0.14473684210526316,
          "f": 0.1679389264261991
        }
      }
    },
    {
      "paper_id": "eess.SP.eess/SP/2503.09349v1",
      "true_abstract": "Correlation-based auditory attention decoding (AAD) algorithms exploit neural\ntracking mechanisms to determine listener attention among competing speech\nsources via, e.g., electroencephalography signals. The correlation coefficients\nbetween the decoded neural responses and encoded speech stimuli of the\ndifferent speakers then serve as AAD decision variables. A critical trade-off\nexists between the temporal resolution (the decision window length used to\ncompute these correlations) and the AAD accuracy. This trade-off is typically\ncharacterized by evaluating AAD accuracy across multiple window lengths,\nleading to the performance curve. We propose a novel method to model this\ntrade-off curve using labeled correlations from only a single decision window\nlength. Our approach models the (un)attended correlations with a normal\ndistribution after applying the Fisher transformation, enabling accurate AAD\naccuracy prediction across different window lengths. We validate the method on\ntwo distinct AAD implementations: a linear decoder and the non-linear VLAAI\ndeep neural network, evaluated on separate datasets. Results show consistently\nlow modeling errors of approximately 2 percent points, with 94% of true\naccuracies falling within estimated 95%-confidence intervals. The proposed\nmethod enables efficient performance curve modeling without extensive\nmulti-window length evaluation, facilitating practical applications in, e.g.,\nperformance tracking in neuro-steered hearing devices to continuously adapt the\nsystem parameters over time.",
      "generated_abstract": "This paper introduces a novel method for designing frequency-modulated\ncontinuous-wave (FMCW) radar systems. By incorporating a time-variant phase\nshift, the FMCW radar system can be made time-variant, offering the possibility\nof achieving higher sensitivity in noisy environments. However, this\nincreased sensitivity comes at the cost of reduced spatial resolution. In this\nwork, we introduce a novel time-variant phase shift design algorithm that\nincorporates both the carrier phase and frequency offset information, allowing\nfor improved sensitivity while preserving spatial resolution. The proposed\nalgorithm is evaluated through simulation and experimentation, and it is shown\nthat it offers improved sensitivity and resolution in noisy environments.\nAdditionally, the proposed algorithm is compared to other time-variant phase\nshift design methods, demonstrating its superior performance.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11409395973154363,
          "p": 0.20987654320987653,
          "f": 0.14782608239357292
        },
        "rouge-2": {
          "r": 0.015228426395939087,
          "p": 0.028037383177570093,
          "f": 0.019736837543500016
        },
        "rouge-l": {
          "r": 0.11409395973154363,
          "p": 0.20987654320987653,
          "f": 0.14782608239357292
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2502.15505v1",
      "true_abstract": "We develop a dynamic model of the Bitcoin market where users set fees\nthemselves and miners decide whether to operate and whom to validate based on\nthose fees. Our analysis reveals how, in equilibrium, users adjust their bids\nin response to short-term congestion (i.e., the amount of pending\ntransactions), how miners decide when to start operating based on the level of\ncongestion, and how the interplay between these two factors shapes the overall\nmarket dynamics. The miners hold off operating when the congestion is mild,\nwhich harms social welfare. However, we show that a block reward (a fixed\nreward paid to miners upon a block production) can mitigate these\ninefficiencies. We characterize the socially optimal block reward and\ndemonstrate that it is always positive, suggesting that Bitcoin's halving\nschedule may be suboptimal.",
      "generated_abstract": "The paper introduces a novel approach to the identification of causal effects\nin dynamic and heterogeneous-interacting population models. We develop an\napproach that utilizes an auxiliary dynamic model to identify the causal\neffects of interest. This auxiliary dynamic model is estimated using a\nnonparametric method. We show that this method yields causal identification\nresults that are optimal in the sense that it yields the optimal error bounds\nwhen the underlying model is correctly specified. Moreover, we establish\nerror bounds for the auxiliary dynamic model that are optimal in the sense that\nthey are equal to the error bounds of the best linear unbiased estimator for\nthe causal effect of interest. This result demonstrates that the auxiliary\ndynamic model is a useful tool for identifying causal effects in dynamic and\nheterogeneous-interacting population models.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19148936170212766,
          "p": 0.2857142857142857,
          "f": 0.22929935825226183
        },
        "rouge-2": {
          "r": 0.046511627906976744,
          "p": 0.0594059405940594,
          "f": 0.05217390811758081
        },
        "rouge-l": {
          "r": 0.19148936170212766,
          "p": 0.2857142857142857,
          "f": 0.22929935825226183
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/MF/2411.13792v1",
      "true_abstract": "Traditional Markowitz portfolio optimization constrains daily portfolio\nvariance to a target value, optimising returns, Sharpe or variance within this\nconstraint. However, this approach overlooks the relationship between variance\nat different time scales, typically described by $\\sigma(\\Delta t) \\propto\n(\\Delta t)^{H}$ where $H$ is the Hurst exponent, most of the time assumed to be\n\\(\\frac{1}{2}\\). This paper introduces a multifrequency optimization framework\nthat allows investors to specify target portfolio variance across a range of\nfrequencies, characterized by a target Hurst exponent $H_{target}$, or optimize\nthe portfolio at multiple time scales. By incorporating this scaling behavior,\nwe enable a more nuanced and comprehensive risk management strategy that aligns\nwith investor preferences at various time scales. This approach effectively\nmanages portfolio risk across multiple frequencies and adapts to different\nmarket conditions, providing a robust tool for dynamic asset allocation. This\novercomes some of the traditional limitations of Markowitz, when it comes to\ndealing with crashes, regime changes, volatility clustering or multifractality\nin markets. We illustrate this concept with a toy example and discuss the\npractical implementation for assets with varying scaling behaviors.",
      "generated_abstract": "The purpose of this paper is to construct a class of continuous-time\ncontracts with a single underlying asset and a single option type. The\nconstruction is based on the idea of limit order book models, and the\nconstruction is based on the classical limit order book model. This class of\ncontracts is based on a continuous-time version of the classical limit order\nbook model, and the underlying asset and option type are continuous-time\nvariables. The construction of this class of contracts is based on the\nconstruction of the continuous-time version of the classical limit order\nbook model, which is based on the classical limit order book model. The\nconstruction of this class of contracts is based on the classical limit order\nbook model, which is based on the classical limit order book model. The\nconstruction of this class of contracts is based on the classical limit order\nbook model, which is based on the classical limit order book",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08943089430894309,
          "p": 0.3055555555555556,
          "f": 0.13836477637118794
        },
        "rouge-2": {
          "r": 0.011494252873563218,
          "p": 0.03225806451612903,
          "f": 0.01694914866848695
        },
        "rouge-l": {
          "r": 0.08943089430894309,
          "p": 0.3055555555555556,
          "f": 0.13836477637118794
        }
      }
    },
    {
      "paper_id": "math.ST.stat/ME/2502.20123v1",
      "true_abstract": "We study two G-modeling strategies for estimating the signal distribution\n(the empirical Bayesian's prior) from observations corrupted with normal noise.\nFirst, we choose the signal distribution by minimizing Stein's unbiased risk\nestimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach\nmotivated by optimal empirical Bayesian shrinkage estimation of the signals.\nSecond, we select the signal distribution by minimizing Hyv\\\"arinen's score\nmatching objective for the implied score (derivative of log-marginal density),\ntargeting minimal Fisher divergence between estimated and true marginal\ndensities. While these strategies appear distinct, they are known to be\nmathematically equivalent. We provide a unified analysis of SURE and score\nmatching under both well-specified signal distribution classes and\nmisspecification. In the classical well-specified setting with homoscedastic\nnoise and compactly supported signal distribution, we establish nearly\nparametric rates of convergence of the empirical Bayes regret and the Fisher\ndivergence. In a commonly studied misspecified model, we establish fast rates\nof convergence to the oracle denoiser and corresponding oracle inequalities.\nOur empirical results demonstrate competitiveness with nonparametric maximum\nlikelihood in well-specified settings, while showing superior performance under\nmisspecification, particularly in settings involving heteroscedasticity and\nside information.",
      "generated_abstract": "We study the asymptotic distribution of the generalized Least Squares\n(GLS) estimator under the $N \\to \\infty$ limit. We prove that the\nasymptotic distribution of the GLS estimator is the same as the\nasymptotic distribution of the estimator of the $K$-th moment of the\nsample mean, where $K$ is a positive integer. We also establish the\nasymptotic normality of the GLS estimator, showing that it converges to a\nnormal distribution as $N \\to \\infty$. We provide a generalization of the\nasymptotic normality of the GLS estimator to the case where the sample\nconsists of a finite number of i.i.d. copies of a random variable. We show\nthat, under mild conditions, the asymptotic normality of the GLS estimator\nholds under the $N \\to \\infty$ limit.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.096,
          "p": 0.21428571428571427,
          "f": 0.13259668080949924
        },
        "rouge-2": {
          "r": 0.029411764705882353,
          "p": 0.05555555555555555,
          "f": 0.03846153393491177
        },
        "rouge-l": {
          "r": 0.096,
          "p": 0.21428571428571427,
          "f": 0.13259668080949924
        }
      }
    },
    {
      "paper_id": "physics.ao-ph.stat/AP/2502.19234v1",
      "true_abstract": "Arctic sea ice is in reduction and has been a key significant indicator of\nclimate change. In this paper, we explore Arctic Sea ice extent data to\nidentify teleconnection with weather change in the polar and sub-tropical jet\nstream intersection in eastern United States (US) and hence the potential\ninfluence in ground level ozone pollution. Several statistical methods\nincluding Bayesian techniques such as: spatio-temporal modelling and Bayesian\nnetwork are implemented to identify the teleconnection and also validated based\non theories in atmospheric science. We observe that the teleconnection is\nrelatively strong in autumn, winter and spring seasons compared to the summer.\nFurthermore, the sudden decremental effect of Arctic sea-ice extent in\nmid-2000s has a shifting influence in ozone pollutions compared to the previous\nyears. A similar downward shift in the Arctic sea-ice extent has been projected\nin 2030. These findings indicate to initiate further strategic policies for the\nArctic influence, ozone concentrations together the seasonal and global\nchanging patterns of climate.",
      "generated_abstract": "This paper explores the use of a non-parametric regression framework to\nanalyze the spatial and temporal variability of tropical cyclone intensity\nover the 21st century. A data set comprising 59 tropical cyclones from 1991 to\n2020 was analyzed using generalized Additive Regression Trees (GART) and\nnon-parametric multivariate regression. The GART approach was used to model\nthe spatial patterns of intensity, and the multivariate regression approach was\nused to analyze the temporal variability. The results reveal a clear\nseasonal pattern in the intensity of tropical cyclones, with the highest\nintensity occurring during the monsoon season and the lowest intensity during\nthe dry season. The GART model showed strong spatial consistency, with most\nregions exhibiting significant differences in intensity. The non-parametric\nmultivariate regression model also identified",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12962962962962962,
          "p": 0.19444444444444445,
          "f": 0.15555555075555572
        },
        "rouge-2": {
          "r": 0.013422818791946308,
          "p": 0.01834862385321101,
          "f": 0.01550387108917887
        },
        "rouge-l": {
          "r": 0.12037037037037036,
          "p": 0.18055555555555555,
          "f": 0.1444444396444446
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/GN/2503.09934v1",
      "true_abstract": "It is time to move on from attempts to make the pharmacy benefit manager\n(PBM) reseller business model more transparent. Time and time again the Big 3\nPBMs have developed opaque alternatives to piece-meal 100% pass-through\nmandates. Time and time again PBMs have demonstrated expertise in finding\nloopholes in state government disclosure laws. The purpose of this paper is to\nprovide quantitative estimates of two transparent insurance business models as\na solution to the PBM agency issue. The key parameter used is an 8% gross\nprofit margin figure disclosed by the Big 3 PBMs themselves. Based on reported\ndrug trend delivered to plans, we use a $1,200 to $1,500 per member per year\n(PMPY) as the range for this key performance indicator (KPI). We propose that\ndiscussions of PBM insurance business models start with the following figures:\n(1) a fixed premium model with medical loss ratio ranging from 92% to 85%; (2)\na fee-for-service model ranging from $96 to $180 PMPY with risk sharing of\ndeviations from a contracted PMPY delivered drug spend.",
      "generated_abstract": "The paper explores the impact of the COVID-19 pandemic on the energy\nmarket by analysing the impact on electricity demand, the price of gas, and\nthe price of renewable energy in the UK. The results show that the pandemic\nresulted in a significant reduction in electricity demand, while the price of\ngas increased significantly. This reduction in electricity demand is attributed\nto the closure of non-essential businesses, as well as the reduction in\ntravel and other non-essential activities. The increase in the price of gas\nreflects the increased demand for natural gas due to the decrease in electricity\ndemand. The increase in the price of renewable energy is attributed to the\nincrease in production costs and the decrease in the price of gas, as well as\nincreased competition for renewable energy from coal-fired power plants. The\npaper concludes by suggesting measures to minimise the",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1282051282051282,
          "p": 0.24193548387096775,
          "f": 0.16759776083518005
        },
        "rouge-2": {
          "r": 0.012345679012345678,
          "p": 0.01904761904761905,
          "f": 0.014981268636116488
        },
        "rouge-l": {
          "r": 0.1282051282051282,
          "p": 0.24193548387096775,
          "f": 0.16759776083518005
        }
      }
    },
    {
      "paper_id": "math.OC.stat/OT/2403.10987v3",
      "true_abstract": "The Fundamental Risk Quadrangle (FRQ) is a unified framework linking risk\nmanagement, statistical estimation, and optimization. Distributionally robust\noptimization (DRO) based on $\\varphi$-divergence minimizes the maximal expected\nloss, where the maximum is over a $\\varphi$-divergence ambiguity set. This\npaper introduces the \\emph{extended} $\\varphi$-divergence and the extended\n$\\varphi$-divergence quadrangle, which integrates DRO into the FRQ framework.\nWe derive the primal and dual representations of the quadrangle elements (risk,\ndeviation, regret, error, and statistic). The dual representation provides an\ninterpretation of classification, portfolio optimization, and regression as\nrobust optimization based on the extended $\\varphi$-divergence. The primal\nrepresentation offers tractable formulations of these robust optimizations as\nconvex optimization. We provide illustrative examples showing that many common\nproblems, such as least-squares regression, quantile regression, support vector\nmachines, and CVaR optimization, fall within this framework. Additionally, we\nconduct a case study to visualize the optimal solution of the inner\nmaximization in robust optimization.",
      "generated_abstract": "This paper develops a novel framework for the optimization of the\ntemperature distribution in a system of coupled partial differential equations\n(PDEs) under the assumption of a known temperature field. The proposed method\nis based on the constrained iterative method of lines (CIML) and it is\noptimized by means of a Newton algorithm. The accuracy of the method is\nverified through the analysis of a toy model, where it is shown that the\noptimized temperature profile is very close to the one obtained by the\ncorresponding numerical solution.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.29310344827586204,
          "f": 0.21249999537812508
        },
        "rouge-2": {
          "r": 0.028368794326241134,
          "p": 0.04878048780487805,
          "f": 0.035874434811881
        },
        "rouge-l": {
          "r": 0.1568627450980392,
          "p": 0.27586206896551724,
          "f": 0.1999999953781251
        }
      }
    },
    {
      "paper_id": "physics.class-ph.physics/class-ph/2503.06910v1",
      "true_abstract": "Hidden momentum is a puzzling phenomenon associated with magnetic dipoles and\nother extended relativistic systems. We point out that the origin of hidden\nmomentum lies in the effective change of individual particle masses of a\ncomposite body, during which the total momentum of the system is not equal to\nthe momentum of the center of mass. Defining the hidden momentum as the\ndifference between the total momentum and the center-of-mass momentum, we\nexplain in detail how hidden momentum arises in certain simple non-relativistic\nsystems, in typical relativistic systems due to velocity-dependent\n``relativistic mass'', and in magnetic dipoles as a special case of\nrelativistic systems.",
      "generated_abstract": "This paper presents an experimental study of the effect of the\nreaction power on the heat transfer coefficient in a thermal contact between\nan ideal gas and a solid body. The experimental data were obtained using a\npressure-gradient type contact geometry. The heat transfer coefficient was\ncalculated using the second order finite difference method. The results show\nthat the heat transfer coefficient decreases with the increase of the power of\nthe reaction. The effect of the reaction power on the heat transfer coefficient\nis similar to that of the thermal conductivity of the solid material. The\neffect of the reaction power on the heat transfer coefficient in the thermal\ncontact between an ideal gas and a solid body can be explained by the\neffect of the reaction power on the heat transfer coefficient in the contact\nbetween an ideal gas and a liquid. The effect of the reaction power on the\nheat transfer coefficient in the thermal contact between an ideal gas and a\nsolid body can be explained by the effect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1746031746031746,
          "p": 0.19642857142857142,
          "f": 0.18487394459713308
        },
        "rouge-2": {
          "r": 0.031914893617021274,
          "p": 0.03571428571428571,
          "f": 0.0337078601843209
        },
        "rouge-l": {
          "r": 0.15873015873015872,
          "p": 0.17857142857142858,
          "f": 0.16806722190805748
        }
      }
    },
    {
      "paper_id": "math.OC.q-bio/PE/2502.00509v1",
      "true_abstract": "The main aim of this study is to analyze a fractional parabolic SIR epidemic\nmodel of a reaction-diffusion, by using the nonlocal Caputo fractional\ntime-fractional derivative and employing the $p$-Laplacian operator. The\nimmunity is imposed through the vaccination program, which is regarded as a\ncontrol variable. Finding the optimal control pair that reduces the number of\nsick people, the associated vaccination, and treatment expenses across a\nconstrained time and space is our main study. The existence and uniqueness of\nthe nonnegative solution for the spatiotemporal SIR model are established. It\nis also demonstrated that an optimal control exists. In addition, we obtain a\ndescription of the optimal control in terms of state and adjoint functions.\nThen, the optimality system is resolved by a discrete iterative scheme that\nconverges after an appropriate test, similar to the forward-backward sweep\nmethod. Finally, numerical approximations are given to show the effectiveness\nof the proposed control program, which provides meaningful results using\ndifferent values of the fractional order and $p$, respectively the order of the\nCaputo derivative and the $p$-Laplacian operators.",
      "generated_abstract": "In this paper, we develop a novel method for the optimal control of\ndiscrete-time systems with stochastic perturbations. The control is\nconstructed as the solution of a stochastic optimal control problem (SOCP)\ndefined by a functional that measures the sensitivity of the state to the\nperturbations. This functional is constructed using the Lyapunov functional\nbased on the mean-field limit, which has been introduced for the control of\ndiscrete-time mean-field games in the literature. The optimal control problem\nis formulated as a semidefinite program (SDP), and the solution is obtained as\na solution of a semidefinite relaxation of the SDP. Theoretical properties of\nthe obtained optimal control are studied. Numerical examples are presented to\nillustrate the performance of the proposed control method.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2222222222222222,
          "p": 0.36923076923076925,
          "f": 0.27745664270774173
        },
        "rouge-2": {
          "r": 0.07272727272727272,
          "p": 0.11538461538461539,
          "f": 0.08921932611213249
        },
        "rouge-l": {
          "r": 0.21296296296296297,
          "p": 0.35384615384615387,
          "f": 0.26589594906612324
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2502.20788v1",
      "true_abstract": "The stock assessment model SAM contains a large number of age-dependent\nparameters that must be manually grouped together to obtain robust inference.\nThis can make the model selection process slow, non-extensive and highly\nsubjective, while producing unrealistic looking parameter estimates with\ndiscrete jumps. We propose to model age-dependent SAM parameters using\nsmoothing spline functions. This can lead to more smooth parameter estimates,\nwhile speeding up and making the model selection process more automatic and\nless subjective. We develop different spline models and compare them with\nalready existing SAM models for a selection of 17 different fish stocks, using\ncross- and forward-validation methods. The results show that our automated\nspline models overall outcompete the officially developed SAM models. We also\ndemonstrate how the developed spline models can be employed as a diagnostics\ntool for improving and better understanding properties of the officially\ndeveloped SAM models.",
      "generated_abstract": "We introduce a novel approach to the problem of evaluating the statistical\nperformance of a new binary classifier, where the goal is to determine the\nexpected performance of the classifier in the presence of an unseen class. This\napproach extends the conventional approach of evaluating a binary classifier in\nthe presence of a new class by treating the new class as a new binary classifier\nin its own right, using a previously unseen classifier to make predictions\nabout the new class. The approach is based on the assumption that the\nperformance of the new classifier can be approximated by a classifier trained\non the unseen class, and the performance of the new classifier can be evaluated\nusing a suitable classification metric. We illustrate the methodology with a\nsimple example where the performance of a binary classifier is evaluated in the\npresence of a new class, and we also consider a more general case where the\nnew class is a binary",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18681318681318682,
          "p": 0.25757575757575757,
          "f": 0.21656050468092025
        },
        "rouge-2": {
          "r": 0.022727272727272728,
          "p": 0.02586206896551724,
          "f": 0.02419354340790945
        },
        "rouge-l": {
          "r": 0.15384615384615385,
          "p": 0.21212121212121213,
          "f": 0.17834394417136612
        }
      }
    },
    {
      "paper_id": "cs.LG.stat/ME/2503.06690v1",
      "true_abstract": "Dynamic Treatment Regimes (DTRs) provide a systematic approach for making\nsequential treatment decisions that adapt to individual patient\ncharacteristics, particularly in clinical contexts where survival outcomes are\nof interest. Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL) is a\nnovel framework to address the complexities associated with censored data when\nestimating optimal DTRs. We explore ways to learn effective DTRs, from\nobservational data. By enhancing traditional tree-based reinforcement learning\nmethods with augmented inverse probability weighting (AIPW) and censoring-aware\nmodifications, CA-TRL delivers robust and interpretable treatment strategies.\nWe demonstrate its effectiveness through extensive simulations and real-world\napplications using the SANAD epilepsy dataset, where it outperformed the\nrecently proposed ASCL method in key metrics such as restricted mean survival\ntime (RMST) and decision-making accuracy. This work represents a step forward\nin advancing personalized and data-driven treatment strategies across diverse\nhealthcare settings.",
      "generated_abstract": "This paper presents a novel method for estimating the posterior of a\ndistributed hidden Markov model (HMM) from incomplete observation data. The\nproposed method combines a deep generative model with a variational inference\nframework, and is based on a novel posterior approximation of the HMM\nposterior. The variational inference is performed using a novel sampling\ntechnique for the approximate posterior, which can be viewed as a variational\napproximation of the HMM's conditional posterior distribution. In\nparticular, we propose two variational inference methods for the approximate\nposterior: a novel variational approximation of the HMM's conditional posterior\ndistribution using a modified KL divergence and a generalized version of the\nBayesian Information Criterion (BIC). In the first method, the approximate\nposterior is estimated by sampling from the KL divergence approximation and\nthen comparing the resulting posterior samples to those obtained",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15517241379310345,
          "p": 0.23684210526315788,
          "f": 0.187499995217014
        },
        "rouge-2": {
          "r": 0.007462686567164179,
          "p": 0.008695652173913044,
          "f": 0.008032123543171734
        },
        "rouge-l": {
          "r": 0.14655172413793102,
          "p": 0.2236842105263158,
          "f": 0.17708332855034734
        }
      }
    },
    {
      "paper_id": "math-ph.math/CV/2503.05690v1",
      "true_abstract": "Inspired by the duality between Jackiw-Teitelboim gravity and Schwarzian\nfield theory, we show identities between the Schwarzian action of a circle\ndiffeomorphism with (1) the hyperbolic area enclosed by the Epstein curve in\nthe hyperbolic disk $\\mathbb{D}$, (2) the asymptotic excess in the\nisoperimetric inequality for the equidistant Epstein foliation, (3) the\nvariation of the Loewner energy along its equipotential foliation, and (4) the\nasymptotic change in hyperbolic area under a conformal distortion near the\ncircle. From these geometric interpretations, we obtain two new proofs of the\nnon-negativity of the Schwarzian action for circle diffeomorphisms, one from\nthe isoperimetric inequality and the other from the monotonicity of the Loewner\nenergy. Moreover, we show that the horocycle truncation used in Epstein's\nconstruction of the Epstein curve also defines a renormalized length of\nhyperbolic geodesics in $\\mathbb{D}$, which coincides with the log of the\nbi-local observable. From this, we show that the bi-local observables on the\nedges of any ideal triangulation of $\\mathbb{D}$ determine the circle\ndiffeomorphism.",
      "generated_abstract": "We give a new proof of the existence of a weak solution to a class of\ninterfacial fluid-structure problems, based on a formulation in terms of\ndifferential forms and a generalization of the Hodge-Laplacian. The\nconstruction involves a variational formulation of the problem which allows\nfor a localization of the weak solution in the form of a weakly divergence-free\nvector field. We obtain sharp local-in-time solutions for the unsteady\ncompressible Navier-Stokes-Newton system, and provide a geometric interpretation\nof the weak solutions in terms of the geometry of the interface.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14444444444444443,
          "p": 0.25,
          "f": 0.18309858690735975
        },
        "rouge-2": {
          "r": 0.028985507246376812,
          "p": 0.05194805194805195,
          "f": 0.0372092977280698
        },
        "rouge-l": {
          "r": 0.1111111111111111,
          "p": 0.19230769230769232,
          "f": 0.14084506578059924
        }
      }
    },
    {
      "paper_id": "cs.SI.q-fin/EC/2502.21037v2",
      "true_abstract": "Recent advances in artificial intelligence have led to the proliferation of\nartificial agents in social contexts, ranging from education to online social\nmedia and financial markets, among many others. The increasing rate at which\nartificial and human agents interact makes it urgent to understand the\nconsequences of human-machine interactions for the propagation of new ideas,\nproducts, and behaviors in society. Across two distinct empirical contexts, we\nfind here that artificial agents lead to significantly faster and wider social\ncontagion. To this end, we replicate a choice experiment previously conducted\nwith human subjects by using artificial agents powered by large language models\n(LLMs). We use the experiment's results to measure the adoption thresholds of\nartificial agents and their impact on the spread of social contagion. We find\nthat artificial agents tend to exhibit lower adoption thresholds than humans,\nwhich leads to wider network-based social contagions. Our findings suggest that\nthe increased presence of artificial agents in real-world networks may\naccelerate behavioral shifts, potentially in unforeseen ways.",
      "generated_abstract": "In this paper, we propose a novel framework for the optimization of\nefficiently connected\ngraph-structured data. Specifically, we investigate the design of graph\nstructures with a high degree of connectivity to enhance the efficiency of\ngraph neural networks (GNNs), which are widely used for large-scale graph\nmodeling. To this end, we first propose a novel graph structure called\ngraph-structured data matrix (GSDM), which is a generalized version of the\ngraph structured matrix (GSM). The GSDM is designed to capture the structural\nsimilarity among graph-structured data by leveraging the matrix-vector\nproduct. We then propose a graph-structured data matrix-based\ngraph-structured GNN (GSG-GNN) model, which integrates the GSDM into the GNN\nframework. The GSG-GNN model is designed to enhance the efficiency of the GNN",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14545454545454545,
          "p": 0.22857142857142856,
          "f": 0.17777777302469147
        },
        "rouge-2": {
          "r": 0.026143790849673203,
          "p": 0.0380952380952381,
          "f": 0.031007747111051774
        },
        "rouge-l": {
          "r": 0.14545454545454545,
          "p": 0.22857142857142856,
          "f": 0.17777777302469147
        }
      }
    },
    {
      "paper_id": "q-bio.CB.q-bio/CB/2411.12123v7",
      "true_abstract": "Colorectal cancer (CRC) poses a major public health challenge due to its\nincreasing prevalence, particularly among younger populations. Microsatellite\ninstability-high (MSI-H) CRC and deficient mismatch repair (dMMR) CRC\nconstitute 15% of all CRC and exhibit remarkable responsiveness to\nimmunotherapy, especially with PD-1 inhibitors. Despite this, there is a\nsignificant need to optimise immunotherapeutic regimens to maximise clinical\nefficacy and patient quality of life whilst minimising monetary costs. To\naddress this, we employ a novel framework driven by delay integro-differential\nequations to model the interactions among cancer cells, immune cells, and\nimmune checkpoints. Several of these components are being modelled\ndeterministically for the first time in cancer, paving the way for a deeper\nunderstanding of the complex underlying immune dynamics. We consider two\ncompartments: the tumour site and the tumour-draining lymph node, incorporating\nphenomena such as dendritic cell (DC) migration, T cell proliferation, and CD8+\nT cell exhaustion and reinvigoration. Parameter values and initial conditions\nare derived from experimental data, integrating various pharmacokinetic,\nbioanalytical, and radiographic studies, along with deconvolution of bulk\nRNA-sequencing data from the TCGA COADREAD and GSE26571 datasets. We finally\noptimise neoadjuvant treatment with pembrolizumab, a widely used PD-1\ninhibitor, to balance efficacy, efficiency, and toxicity in locally advanced\nMSI-H/dMMR CRC patients. We improve upon currently FDA-approved therapeutic\nregimens for metastatic MSI-H/dMMR CRC, demonstrating that a single\nmedium-to-high dose of pembrolizumab is sufficient for effective tumour\neradication whilst being efficient, safe and practical.",
      "generated_abstract": "The molecular mechanisms underlying the transition from an undifferentiated\nstem cell population to a differentiated cell lineage are poorly understood.\nBecause of their capacity to generate all cell types in an organism,\ndifferentiated cells have long been considered crucial to the understanding of\ncellular differentiation. However, the role of undifferentiated cells in\nregulating cellular differentiation has remained poorly understood. Here we\npropose that the transition from undifferentiated to differentiated cells is\naccompanied by an increase in cellular complexity, as the proportion of\nindividual cell types within a cell population increases. We propose that this\nincrease in complexity is driven by an increase in the proportion of undifferentiated\ncells within the population. We show that this increase in complexity is\nassociated with the emergence of an undifferentiated cell lineage. Using\nchromatin immun",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.10982658959537572,
          "p": 0.2835820895522388,
          "f": 0.15833332930868066
        },
        "rouge-2": {
          "r": 0.008620689655172414,
          "p": 0.018691588785046728,
          "f": 0.011799405709314936
        },
        "rouge-l": {
          "r": 0.09826589595375723,
          "p": 0.2537313432835821,
          "f": 0.141666662642014
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.18261v2",
      "true_abstract": "Traditionally, the impact of minimum wages on employment has been studied,\nand it is generally believed to have a negative effect. Yet, some recent\nstudies have shown that the impact of minimum wages on employment can sometimes\nbe positive. In addition, certain recent proposals set a higher minimum wage\nthan the wage earned by some high-productivity workers. However, the impact of\nminimum wages on employment has been primarily studied on low-skilled workers,\nwhereas there is limited research on high-skilled workers. To address this gap\nand examine the effects of minimum wages on high-productivity workers'\nemployment, I construct a macroeconomic model incorporating productivity\nfluctuations, incomplete markets, directed search, and on-the-job search and\ncompare the steady-state distributions between the baseline model and the model\nwith a minimum wage. As a result, binding minimum wages increase the\nunemployment rate of both low and high-productivity workers.",
      "generated_abstract": "The COVID-19 pandemic led to significant economic disruptions in 2020 and\n2021. This paper examines the impact of the pandemic on the stock market and\nthe labor market in the United States. Using the difference-in-differences\nmethodology, we find that the pandemic significantly reduced investment and\nincreased unemployment. However, these effects were not persistent across\ndifferent industries. Additionally, we find that the pandemic had a neutral\neffect on the labor market, indicating that the labor market did not experience\nsignificant disruptions from the pandemic. Our findings suggest that the\npandemic had a limited impact on the stock market and labor market, which\nindicates that the economy was able to recover from the pandemic quickly.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14772727272727273,
          "p": 0.2,
          "f": 0.1699346356358667
        },
        "rouge-2": {
          "r": 0.03305785123966942,
          "p": 0.0449438202247191,
          "f": 0.038095233211338494
        },
        "rouge-l": {
          "r": 0.13636363636363635,
          "p": 0.18461538461538463,
          "f": 0.15686274021103008
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.01495v1",
      "true_abstract": "Vovk (2015) introduced cross-conformal prediction, a modification of split\nconformal designed to improve the width of prediction sets. The method, when\ntrained with a miscoverage rate equal to $\\alpha$ and $n \\gg K$, ensures a\nmarginal coverage of at least $1 - 2\\alpha - 2(1-\\alpha)(K-1)/(n+K)$, where $n$\nis the number of observations and $K$ denotes the number of folds. A simple\nmodification of the method achieves coverage of at least $1-2\\alpha$. In this\nwork, we propose new variants of both methods that yield smaller prediction\nsets without compromising the latter theoretical guarantee. The proposed\nmethods are based on recent results deriving more statistically efficient\ncombination of p-values that leverage exchangeability and randomization.\nSimulations confirm the theoretical findings and bring out some important\ntradeoffs.",
      "generated_abstract": "We study the problem of training a classifier in a setting where the classifier\n$f:X\\to Y$ is linearly separable, but the loss $\\ell:Y\\to\\mathbb{R}$ is\nnon-convex. This setting arises in a variety of applications, including\nmulti-label classification, where the classifier is only trained on positive\nexamples and the objective is to learn the decision boundary, and\nnon-convex-loss regression, where the objective is to learn the decision\nboundary. In both settings, the loss $\\ell$ is non-convex and has a smooth\npart. We consider two classes of loss functions $\\ell$ for which the smooth\npart is convex, and two classes of loss functions $\\ell$ for which the smooth\npart is non-convex. We study the problem of training a classifier $f:X\\to Y$\nusing a loss function $\\ell$ from these",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11956521739130435,
          "p": 0.1864406779661017,
          "f": 0.14569535947721604
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.09782608695652174,
          "p": 0.15254237288135594,
          "f": 0.11920529325205055
        }
      }
    },
    {
      "paper_id": "stat.ME.stat/ME/2503.09812v1",
      "true_abstract": "Selective inference aims at providing valid inference after a data-driven\nselection of models or hypotheses. It is essential to avoid overconfident\nresults and replicability issues. While significant advances have been made in\nthis area for standard regression models, relatively little attention has been\ngiven to linear mixed models (LMMs), which are widely used for analyzing\nclustered or longitudinal data. This paper reviews the existing selective\ninference approaches developed for LMMs, focusing on selection of fixed\neffects, where the random effects structure is given. We present these methods\nin detail and, through comparative simulations, assess their practical\nperformance and computational feasibility under varying data structures. In\naddition, we apply them to a real-world biological dataset to examine how\nmethod choice can impact inference in practice. Our findings highlight an\nexisting trade-off between computational complexity and statistical power and\nemphasize the scarcity of methods that perform well as the number of variables\nincreases. In such scenarios, basic sample splitting emerges as the most\nreliable approach.",
      "generated_abstract": "This study examines the effects of a \"healthy living\" intervention on\nthe health status of people living with HIV. Using data from a longitudinal\nstudy of a cohort of people living with HIV in rural Kenya, we find that\nintervention participants had a significantly better health status at six\nmonths compared to those who did not participate. The effect size was largest\nfor physical activity and dietary behaviors. The results highlight the\nsignificant benefits of implementing healthy living interventions for people\nliving with HIV, particularly those living in rural settings where behavioral\nchange is often challenging. These findings highlight the potential of\nhealthy living interventions as a promising strategy for improving health\noutcomes among people living with HIV.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16923076923076924,
          "p": 0.28205128205128205,
          "f": 0.21153845685096165
        },
        "rouge-2": {
          "r": 0.00625,
          "p": 0.00980392156862745,
          "f": 0.0076335830312948445
        },
        "rouge-l": {
          "r": 0.13076923076923078,
          "p": 0.21794871794871795,
          "f": 0.1634615337740386
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.18541v2",
      "true_abstract": "In the dynamic landscape of contemporary society, the popularity of ideas,\nopinions, and interests fluctuates rapidly. Traditional dynamical models in\nsocial sciences often fail to capture this inherent volatility, attributing\nchanges to exogenous shocks rather than intrinsic features of the system. This\npaper introduces a novel, tractable model that simulates the natural rise and\nfall of ideas' popularity, offering a more accurate representation of\nreal-world dynamics. Building upon the SIRS (Susceptible, Infectious,\nRecovered, Susceptible) epidemiological model, we incorporate a feedback\nmechanism that allows the recovery rate to vary dynamically based on the\ncurrent state of the system. This modification reflects the cyclical nature of\nidea adoption and abandonment, driven by social saturation and renewed\ninterest. Our model successfully captures the rapid and recurrent shifts in\npopularity, providing valuable insights into the mechanisms behind these\nfluctuations. This approach offers a robust framework for studying the\ndiffusion dynamics of popular ideas, with potential applications across various\nfields such as marketing, technology adoption, and political movements.",
      "generated_abstract": "This paper examines the impact of the COVID-19 pandemic on labour\nand productivity. Using a difference-in-differences (DiD) approach, we find\nthat the pandemic had a substantial negative effect on productivity growth in\nthe US. The DiD estimates suggest that, compared to the pre-pandemic period,\nproductivity growth slowed by 0.35% per year, and productivity growth was\nsignificantly lower in industries affected by the pandemic. These findings\nhighlight the persistent and significant negative impact of the pandemic on\nproductivity growth in the US.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.096,
          "p": 0.23076923076923078,
          "f": 0.13559321618947315
        },
        "rouge-2": {
          "r": 0.012578616352201259,
          "p": 0.029411764705882353,
          "f": 0.017621141177978448
        },
        "rouge-l": {
          "r": 0.088,
          "p": 0.21153846153846154,
          "f": 0.12429378116122455
        }
      }
    },
    {
      "paper_id": "cs.RO.cs/FL/2503.02719v1",
      "true_abstract": "Motion planning with simple objectives, such as collision-avoidance and\ngoal-reaching, can be solved efficiently using modern planners. However, the\ncomplexity of the allowed tasks for these planners is limited. On the other\nhand, signal temporal logic (STL) can specify complex requirements, but\nSTL-based motion planning and control algorithms often face scalability issues,\nespecially in large multi-robot systems with complex dynamics. In this paper,\nwe propose an algorithm that leverages the best of the two worlds. We first use\na single-robot motion planner to efficiently generate a set of alternative\nreference paths for each robot. Then coordination requirements are specified\nusing STL, which is defined over the assignment of paths and robots' progress\nalong those paths. We use a Mixed Integer Linear Program (MILP) to compute task\nassignments and robot progress targets over time such that the STL\nspecification is satisfied. Finally, a local controller is used to track the\ntarget progress. Simulations demonstrate that our method can handle tasks with\ncomplex constraints and scales to large multi-robot teams and intricate task\nallocation scenarios.",
      "generated_abstract": "This paper presents a framework to evaluate the performance of robotic\nassistants (RAs) in a collaborative multi-robot setting. The proposed framework\nis based on the notion of a robotic assistant-human team and consists of three\nelements: (1) a human-in-the-loop framework that simulates the human task\nassignment, (2) a robotic assistant-human team evaluation framework that\nevaluates the performance of the human-in-the-loop and robotic assistant-human\nteam, and (3) a human-in-the-loop evaluation framework that evaluates the\nperformance of the robotic assistant-human team. The framework is designed to\nenable the robotic assistant-human team to collaborate and learn from each\nother, and to evaluate the performance of the human-in-the-loop and robotic\nassistant-human team. The proposed framework is validated through experiments",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09016393442622951,
          "p": 0.21568627450980393,
          "f": 0.1271676258999634
        },
        "rouge-2": {
          "r": 0.005952380952380952,
          "p": 0.012658227848101266,
          "f": 0.008097161641071689
        },
        "rouge-l": {
          "r": 0.09016393442622951,
          "p": 0.21568627450980393,
          "f": 0.1271676258999634
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.04225v1",
      "true_abstract": "This study presents the development and validation of a digital twin for a\nsemi-autogenous grinding (SAG) mill controlled by an expert system. The digital\ntwin integrates three key components of the closed-loop operation: (1) fuzzy\nlogic for expert control, (2) a state-space model for regulatory control, and\n(3) a recurrent neural network to simulate the SAG mill process. The digital\ntwin is combined with a statistical framework for automatically detecting\nprocess disturbances (or critical operations), which triggers model retraining\nonly when deviations from expected behaviour are identified, ensuring\ncontinuous updates with new data to enhance the SAG supervision. The model was\ntrained with 68 hours of operational industrial data and validated with an\nadditional 8 hours, allowing it to predict mill behaviour within a 2.5-minute\nhorizon at 30-second intervals with errors smaller than 5%.",
      "generated_abstract": "This paper presents an innovative control design for a multi-agent system\nwith a single-cell power amplifier (PAM) that is used to transmit information\nfrom a central transmitter (CT) to a target receiver (TR). This system is\ndesigned to work in conjunction with a conventional single-cell PAM transmitter\n(CT) that is used to transmit information from a central transmitter (CT) to a\ntarget receiver (TR). The main objective of the proposed design is to maximize\nthe total power consumption of the system while ensuring the TR receives the\nmaximum amount of information. To this end, a two-level control strategy is\ndeveloped, where the first level uses a time-varying control gain to reduce the\ntotal power consumption of the system, while the second level uses a\nfeedback-control strategy to ensure the TR receives the maximum amount of\ninformation. The performance of the proposed design is evaluated",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.14432989690721648,
          "p": 0.2028985507246377,
          "f": 0.16867469393743664
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.028037383177570093,
          "f": 0.02531645074329353
        },
        "rouge-l": {
          "r": 0.13402061855670103,
          "p": 0.18840579710144928,
          "f": 0.1566265011663523
        }
      }
    },
    {
      "paper_id": "q-bio.QM.q-bio/QM/2502.20939v1",
      "true_abstract": "Neuroblastoma is a paediatric extracranial solid cancer that arises from the\ndeveloping sympathetic nervous system and is characterised by an abnormal\ndistribution of cell types in tumours compared to healthy infant tissues. In\nthis paper, we propose a new mathematical model of cell differentiation during\nsympathoadrenal development. By performing Bayesian inference of the model\nparameters using clinical data from patient samples, we show that the model\nsuccessfully accounts for the observed differences in cell type heterogeneity\namong healthy adrenal tissues and four common types of neuroblastomas. Using a\nphenotypically structured model, we show that alterations in healthy\ndifferentiation dynamics are related to cell malignancy, and tumour volume\ngrowth. We use this model to analyse the evolution of malignant traits in a\ntumour. Our findings suggest that normal development dynamics make the\nembryonic sympathetic nervous system more robust to perturbations and\naccumulation of malignancies, and that the diversity of differentiation\ndynamics found in the neuroblastoma subtypes lead to unique risk profiles for\nneuroblastoma relapse after treatment.",
      "generated_abstract": "We introduce a framework for studying the long-term evolution of\ntectonic plate tectonics, with an emphasis on the effect of plate collisions\nand their impact on plate motion. We develop a model that accounts for both\nplate tectonics and plate collisions, and show that it reproduces the main\nobservational constraints. We then show that this model predicts a\ncorrelation between the rate of plate collisions and the rate of plate tectonics\nand that this correlation is reproduced by the model. Finally, we show that the\nmodel can be used to predict the evolution of plate velocities, which is useful\nfor predicting earthquakes.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16037735849056603,
          "p": 0.2982456140350877,
          "f": 0.20858895250705717
        },
        "rouge-2": {
          "r": 0.057692307692307696,
          "p": 0.10344827586206896,
          "f": 0.07407406947721414
        },
        "rouge-l": {
          "r": 0.16037735849056603,
          "p": 0.2982456140350877,
          "f": 0.20858895250705717
        }
      }
    },
    {
      "paper_id": "math-ph.nlin/SI/2503.09502v1",
      "true_abstract": "An infinite 3-parametric family of superintegrable and exactly-solvable\nquantum models on a plane, admitting separation of variables in polar\ncoordinates, marked by integer index $k$ was introduced in Journ Phys A 42\n(2009) 242001 and was called in literature the TTW system. In this paper it is\nconjectured that the Hamiltonian and both integrals of TTW system have hidden\nalgebra $g^{(k)}$ - it was checked for $k=1,2,3,4$ - having its\nfinite-dimensional representation spaces as the invariant subspaces. It is\nchecked that for $k=1,2,3,4$ that the Hamiltonian $H$, two integrals ${\\cal\nI}_{1,2}$ and their commutator ${\\cal I}_{12} = [{\\cal I}_1,{\\cal I}_2]$ are\nfour generating elements of the polynomial algebra of integrals of the order\n$(k+1)$: $[{\\cal I}_1,{\\cal I}_{12}] = P_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, $[{\\cal I}_2,{\\cal I}_{12}] = Q_{k+1}(H, {\\cal I}_{1,2},{\\cal\nI}_{12})$, where $P_{k+1},Q_{k+1}$ are polynomials of degree $(k+1)$ written in\nterms of ordered monomials of $H, {\\cal I}_{1,2},{\\cal I}_{12}$. This implies\nthat polynomial algebra of integrals is subalgebra of $g^{(k)}$. It is\nconjectured that all is true for any integer $k$.",
      "generated_abstract": "We present a general framework for the study of finite-dimensional\ngeneralized Hermite-Bienaym\\'e polynomials of any degree. We show that the\nassociated multivariable differential operators are in one-to-one correspondence\nwith the algebra of generalized Hermite-Bienaym\\'e polynomials. We present an\nalgebraic description of the polynomials in terms of the classical Hermite\npolynomials and we derive some explicit expressions for the generalized\nHermite-Bienaym\\'e polynomials of degree $n$. We investigate the asymptotic\nbehavior of these polynomials near the singularities and we provide the\nexplicit formula for the asymptotic behavior of the generalized Hermite-Bienaym\\'e\npolynomials of degree $n$ at the singularity. We show that the generalized\nHermite-Bienaym\\'e polynomials of degree $n$ converge to the classical\nHermite-",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1308411214953271,
          "p": 0.2641509433962264,
          "f": 0.17499999556953136
        },
        "rouge-2": {
          "r": 0.0457516339869281,
          "p": 0.0875,
          "f": 0.06008583240067082
        },
        "rouge-l": {
          "r": 0.1308411214953271,
          "p": 0.2641509433962264,
          "f": 0.17499999556953136
        }
      }
    },
    {
      "paper_id": "math.OC.eess/SY/2503.07324v1",
      "true_abstract": "Distribution shifts have long been regarded as troublesome external forces\nthat a decision-maker should either counteract or conform to. An intriguing\nfeedback phenomenon termed decision dependence arises when the deployed\ndecision affects the environment and alters the data-generating distribution.\nIn the realm of performative prediction, this is encoded by distribution maps\nparameterized by decisions due to strategic behaviors. In contrast, we\nformalize an endogenous distribution shift as a feedback process featuring\nnonlinear dynamics that couple the evolving distribution with the decision.\nStochastic optimization in this dynamic regime provides a fertile ground to\nexamine the various roles played by dynamics in the composite problem\nstructure. To this end, we develop an online algorithm that achieves optimal\ndecision-making by both adapting to and shaping the dynamic distribution.\nThroughout the paper, we adopt a distributional perspective and demonstrate how\nthis view facilitates characterizations of distribution dynamics and the\noptimality and generalization performance of the proposed algorithm. We\nshowcase the theoretical results in an opinion dynamics context, where an\nopportunistic party maximizes the affinity of a dynamic polarized population,\nand in a recommender system scenario, featuring performance optimization with\ndiscrete distributions in the probability simplex.",
      "generated_abstract": "This paper addresses the design of an adaptive control system for a\nsystem with uncertainties, which can be described by the discrete-time\nMarkovian model\n  \\begin{equation}\n  \\dot{x}(k+1) = f(x(k),u(k)) + \\Delta x(k),\n  \\end{equation}\nwhere $x(k)$ is the state, $u(k)$ is the control input, $\\Delta x(k)$ is the\nnoise, and $f:\\mathbb{R}^n\\times\\mathbb{R}^m\\rightarrow\\mathbb{R}^n$ is a\ndeterministic function. The input $u(k)$ is assumed to be bounded. In\nparticular, the input is assumed to be a bounded-variance Gaussian noise. The\nstate $x(k)$ is assumed to be bounded. The system is assumed to be\nst",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.0967741935483871,
          "p": 0.21818181818181817,
          "f": 0.13407820803345727
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.08064516129032258,
          "p": 0.18181818181818182,
          "f": 0.11173183931837351
        }
      }
    },
    {
      "paper_id": "quant-ph.math-ph/2503.10400v1",
      "true_abstract": "We elucidate the requirements for quantum operations that achieve\nenvironment-assisted invariance (envariance), a symmetry of entanglement. While\nenvariance has traditionally been studied within the framework of local unitary\noperations, we extend the analysis to consider non-unitary local operations.\nFirst, we investigate the conditions imposed on operators acting on pure\nbipartite entanglement to attain envariance. We show that the local operations\nmust take a direct-sum form in their Kraus operator representations,\nestablishing decoherence-free subspaces. Furthermore, we prove that the unitary\noperation on the system's subspace uniquely determines the corresponding\nunitary operator on the environment's subspace. As an immediate consequence, we\ndemonstrate that environment-assisted shortcuts to adiabaticity cannot be\nachieved through non-unitary operations. In addition, we identify the\nrequirements that local operations must satisfy to ensure that the eternal\nblack hole states remain static in AdS/CFT.",
      "generated_abstract": "We present a new perspective on the theory of the quantum dynamics of\nsystems of interacting quantum particles. We show that the dynamics of the\nsystem is governed by the dynamics of the associated quantum field. We propose\na simple model in which the dynamics of the field is described by a single\nLangevin equation and the dynamics of the particles are described by a\nFokker-Planck equation. We derive the equations of motion for the field and\nthe particles and discuss the implications of these equations. We show that the\nsystem exhibits a rich variety of physical phenomena including chaotic\ndynamics, phase transitions, and bifurcations. We provide a unified treatment\nof these phenomena in the framework of the theory of the quantum dynamics of\nthe field.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.125,
          "p": 0.19642857142857142,
          "f": 0.15277777302469153
        },
        "rouge-2": {
          "r": 0.047619047619047616,
          "p": 0.06315789473684211,
          "f": 0.05429863763231755
        },
        "rouge-l": {
          "r": 0.125,
          "p": 0.19642857142857142,
          "f": 0.15277777302469153
        }
      }
    },
    {
      "paper_id": "stat.ML.stat/ML/2503.04358v1",
      "true_abstract": "We propose a novel approach for learning causal response representations. Our\nmethod aims to extract directions in which a multidimensional outcome is most\ndirectly caused by a treatment variable. By bridging conditional independence\ntesting with causal representation learning, we formulate an optimisation\nproblem that maximises the evidence against conditional independence between\nthe treatment and outcome, given a conditioning set. This formulation employs\nflexible regression models tailored to specific applications, creating a\nversatile framework. The problem is addressed through a generalised eigenvalue\ndecomposition. We show that, under mild assumptions, the distribution of the\nlargest eigenvalue can be bounded by a known $F$-distribution, enabling\ntestable conditional independence. We also provide theoretical guarantees for\nthe optimality of the learned representation in terms of signal-to-noise ratio\nand Fisher information maximisation. Finally, we demonstrate the empirical\neffectiveness of our approach in simulation and real-world experiments. Our\nresults underscore the utility of this framework in uncovering direct causal\neffects within complex, multivariate settings.",
      "generated_abstract": "We propose a novel approach for learning sparse regression problems from\n(possibly) noisy observations. Our method is based on the use of a\nrepresentation of the data as a sequence of Gaussian random variables, and\nemploys an unconventional gradient descent step in order to learn the\nrepresentations. We show that our method achieves optimal rate of convergence\nunder a mild assumption on the data generating process. Furthermore, we\ndemonstrate that the learned representations can be used to recover the\noriginal data, even under mild assumptions on the noise level.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.2608695652173913,
          "p": 0.44776119402985076,
          "f": 0.3296703250181138
        },
        "rouge-2": {
          "r": 0.08552631578947369,
          "p": 0.15294117647058825,
          "f": 0.10970463674980882
        },
        "rouge-l": {
          "r": 0.24347826086956523,
          "p": 0.417910447761194,
          "f": 0.3076923030400918
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.15980v1",
      "true_abstract": "This paper studies inter-firm heterogeneity in production. Unlike much of the\nexisting research, which primarily addresses heterogeneous production through\nunobserved fixed effects, our approach also focuses on differences in factors'\noutput elasticities. Using manufacturing data from Chile, Colombia, and Japan,\nwe apply an innovative Empirical Bayes methodology to estimate heterogeneous\nCobb-Douglas production functions. We uncover substantial heterogeneity in both\nfactor neutral productivity and factor elasticities, with a strong negative\ncorrelation between them. These findings are consistently observed across\ndatasets and remain robust when using CES and intensive Cobb-Douglas\nspecifications. We show that accounting for these features has significant\nimplications for issues such as markup estimation, firms' technology adoption,\nand productivity measurement.",
      "generated_abstract": "We develop a novel methodology for estimating the optimal tax rate using\ndata on the value of non-residential building stock, based on a method that\nuses the value of a building to determine its tax liability. Using data from\n313 U.S. counties, we find that the optimal tax rate is 4.25%, significantly\nlower than the current statutory rate of 3.8% and the 4.0% rate that would\nincrease tax revenue by $15.7 billion. The optimal tax rate is also lower than\nthe rate that maximizes total revenue under the U.S. Tax Cuts and Jobs Act of\n2017 (TCJA), 5.0%, while the rate that minimizes tax revenue is 4.5%. The\noptimal tax rate is also lower than the rate that maximizes total revenue under\nthe TC",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.16666666666666666,
          "p": 0.22535211267605634,
          "f": 0.19161676157911736
        },
        "rouge-2": {
          "r": 0.009174311926605505,
          "p": 0.01,
          "f": 0.00956937299970499
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.22535211267605634,
          "f": 0.19161676157911736
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.00642v1",
      "true_abstract": "Real-world low-light images captured by imaging devices suffer from poor\nvisibility and require a domain-specific enhancement to produce artifact-free\noutputs that reveal details. In this paper, we propose an unpaired low-light\nimage enhancement network leveraging novel controlled transformation-based\nself-supervision and unpaired self-conditioning strategies. The model\ndetermines the required degrees of enhancement at the input image pixels, which\nare learned from the unpaired low-lit and well-lit images without any direct\nsupervision. The self-supervision is based on a controlled transformation of\nthe input image and subsequent maintenance of its enhancement in spite of the\ntransformation. The self-conditioning performs training of the model on\nunpaired images such that it does not enhance an already-enhanced image or a\nwell-lit input image. The inherent noise in the input low-light images is\nhandled by employing low gradient magnitude suppression in a detail-preserving\nmanner. In addition, our noise handling is self-conditioned by preventing the\ndenoising of noise-free well-lit images. The training based on low-light image\nenhancement-specific attributes allows our model to avoid paired supervision\nwithout compromising significantly in performance. While our proposed\nself-supervision aids consistent enhancement, our novel self-conditioning\nfacilitates adequate enhancement. Extensive experiments on multiple standard\ndatasets demonstrate that our model, in general, outperforms the\nstate-of-the-art both quantitatively and subjectively. Ablation studies show\nthe effectiveness of our self-supervision and self-conditioning strategies, and\nthe related loss functions.",
      "generated_abstract": "Object detection is a fundamental problem in computer vision, enabling\nclassification and localization of objects in images. Recently, image-to-image\ntranslation (I2I) has been a successful technique for converting images from\none domain to another, particularly in visual-language models. However,\nobject-level translation (O2L) remains challenging due to the limited\ncapability of models to recognize and describe complex objects in images.\nExisting methods often rely on complex training datasets, which are often\ndifficult to obtain due to the high cost of collecting high-quality object\nimages. Moreover, existing methods often fail to handle complex objects, such\nas large-scale and multi-scale objects. To address these limitations, we propose\nthe first O2L model for complex objects, i.e., the Spherical Object\nTranslation (SOT), which is the first to model complex objects in spherical\nspace. Our",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13178294573643412,
          "p": 0.19101123595505617,
          "f": 0.15596329792062974
        },
        "rouge-2": {
          "r": 0.009615384615384616,
          "p": 0.01652892561983471,
          "f": 0.012158050060884438
        },
        "rouge-l": {
          "r": 0.13178294573643412,
          "p": 0.19101123595505617,
          "f": 0.15596329792062974
        }
      }
    },
    {
      "paper_id": "math.SG.math/SG/2503.00123v1",
      "true_abstract": "This survey explores the geometry of three-dimensional Anosov flows from the\nperspective of contact and symplectic geometry, following the work of\nMitsumatsu, Eliashberg-Thurston, Hozoori, and the author. We also present a few\noriginal results and discuss various open questions and conjectures.",
      "generated_abstract": "We prove a result of Chang and L\\\"{u}der concerning the existence of\nchains in the category of simplicial sets. More precisely, we show that if a\nsimplicial set is a chain complex, then it is acyclic. In this paper, we also\nprove a result of L\\\"{u}der concerning the existence of chains in the category\nof simplicial sets, which is a special case of our result.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18181818181818182,
          "p": 0.15789473684210525,
          "f": 0.16901407953183908
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.18181818181818182,
          "p": 0.15789473684210525,
          "f": 0.16901407953183908
        }
      }
    },
    {
      "paper_id": "q-bio.BM.q-bio/BM/2502.02371v1",
      "true_abstract": "Accurate identification of druggable pockets is essential for structure-based\ndrug design. However, most pocket-identification algorithms prioritize their\ngeometric properties over downstream docking performance. To address this\nlimitation, we developed RAPID-Net, a pocket-finding algorithm for seamless\nintegration with docking workflows. When guiding AutoDock Vina, RAPID-Net\noutperforms DiffBindFR on the PoseBusters benchmark and enables blind docking\non large proteins that AlphaFold 3 cannot process as a whole. Furthermore,\nRAPID-Net surpasses PUResNet and Kalasanty in docking accuracy and\npocket-ligand intersection rates across diverse datasets, including\nPoseBusters, Astex Diverse Set, BU48, and Coach420. When accuracy is evaluated\nas ``at least one correct pose in the ensemble'', RAPID-Net outperforms\nAlphaFold 3 on the PoseBusters benchmark, suggesting that our approach can be\nfurther improved with a suitable pose reweighting tool offering a\ncost-effective and competitive alternative to AlphaFold 3 for docking. Finally,\nusing several therapeutically relevant examples, we demonstrate the ability of\nRAPID-Net to identify remote functional sites, highlighting its potential to\nfacilitate the development of innovative therapeutics.",
      "generated_abstract": "Increasingly, the healthcare industry is leveraging AI to improve the\nequity of care delivery and patient outcomes. However, AI models are\nintellectually challenging, complex, and costly to train. To address these\nchallenges, we propose a novel approach to designing AI models with\nequity-focused intentions. Our framework, AI-FIT (AI for Equity-Focused\nIntention), leverages the knowledge gained from AI model training to guide the\ndevelopment of AI-enabled decision-making systems to address healthcare\nequity challenges. We demonstrate that AI-FIT can improve the equity of care\ndelivery and outcomes through improved decision-making and data utilization.\nFurthermore, we propose a methodology for AI-FIT model development using a\ntransparent, iterative process that accounts for the complexity",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.17355371900826447,
          "p": 0.2916666666666667,
          "f": 0.21761657563317147
        },
        "rouge-2": {
          "r": 0.019230769230769232,
          "p": 0.03,
          "f": 0.023437495239258782
        },
        "rouge-l": {
          "r": 0.1487603305785124,
          "p": 0.25,
          "f": 0.1865284927316171
        }
      }
    },
    {
      "paper_id": "stat.AP.stat/AP/2503.03557v1",
      "true_abstract": "Clinical practice guidelines are designed to guide clinical practice and\ninvolve causal language. Sometimes guidelines make or require stronger causal\nclaims than those in the references they rely on, a phenomenon we refer to as\n'causal language jump'. We evaluated the strength of expressed causation in\ndiabetes guidelines and the evidence they reference to assess the pattern of\njumps. We randomly sampled 300 guideline statements from four diabetes\nguidelines. We rated the causation strength in the statements and the\ndependence on causation in recommendations supported by these statements using\nexisting scales. Among the causal statements, the cited original studies were\nsimilarly assessed. We also assessed how well they report target trial\nemulation (TTE) components as a proxy for reliability. Of the sampled\nstatements, 114 (38.0%) were causal, and 76 (66.7%) expressed strong causation.\n27.2% (31/114) of causal guideline statements demonstrated a \"causal language\njump\", and 34.9% (29/83) of guideline recommendations cannot be effectively\nsupported. Of the 53 eligible studies for TTE rating, most did not report\ntreatment assignment and causal contrast in detail. Our findings suggest causal\nlanguage jumps were common among diabetes guidelines. While these jumps are\nsometimes inevitable, they should always be supported by good causal inference\npractices.",
      "generated_abstract": "We propose a new class of model-based Bayesian inference algorithms for\nmodeling multivariate time-to-event data. The proposed algorithms are based\non a novel approach for estimating the joint probability distribution of\ncumulative survival functions of a given class of random variables. The\nproposed algorithms are based on the joint modeling of the joint distribution of\ncumulative survival functions of a given class of random variables. They are\nintended for modeling survival data with a mixture of exponential and\nlog-normal distributions. The proposed methods are based on a novel approach\nfor estimating the joint probability distribution of cumulative survival\nfunctions of a given class of random variables. The proposed algorithms are\nbased on the joint modeling of the joint distribution of cumulative survival\nfunctions of a given class of random variables. They are intended for modeling\nsurvival data with a mixture of exponential and log-normal distributions. The",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07086614173228346,
          "p": 0.21428571428571427,
          "f": 0.10650887200448177
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.07086614173228346,
          "p": 0.21428571428571427,
          "f": 0.10650887200448177
        }
      }
    },
    {
      "paper_id": "cs.CV.eess/IV/2503.08455v1",
      "true_abstract": "Instead of performing text-conditioned denoising in the image domain, latent\ndiffusion models (LDMs) operate in latent space of a variational autoencoder\n(VAE), enabling more efficient processing at reduced computational costs.\nHowever, while the diffusion process has moved to the latent space, the\ncontrastive language-image pre-training (CLIP) models, as used in many image\nprocessing tasks, still operate in pixel space. Doing so requires costly\nVAE-decoding of latent images before they can be processed. In this paper, we\nintroduce Latent-CLIP, a CLIP model that operates directly in the latent space.\nWe train Latent-CLIP on 2.7B pairs of latent images and descriptive texts, and\nshow that it matches zero-shot classification performance of similarly sized\nCLIP models on both the ImageNet benchmark and a LDM-generated version of it,\ndemonstrating its effectiveness in assessing both real and generated content.\nFurthermore, we construct Latent-CLIP rewards for reward-based noise\noptimization (ReNO) and show that they match the performance of their CLIP\ncounterparts on GenEval and T2I-CompBench while cutting the cost of the total\npipeline by 21%. Finally, we use Latent-CLIP to guide generation away from\nharmful content, achieving strong performance on the inappropriate image\nprompts (I2P) benchmark and a custom evaluation, without ever requiring the\ncostly step of decoding intermediate images.",
      "generated_abstract": "In recent years, computer vision has seen significant progress in medical\napplications. However, existing methods still face challenges in capturing\nconsistent anatomical structures and accurately estimating their positions. To\naddress this issue, we propose a novel framework for anatomical structure\nreconstruction based on a multi-stage diffusion model. Specifically, our\nframework comprises a pre-training stage for modeling the anatomical structure\ndistribution, a diffusion stage for learning the distribution of anatomical\nstructure locations, and a post-training stage for refining the anatomical\nstructure locations. The pre-training stage leverages a large-scale dataset\ncomprising both natural and synthetic images. The diffusion stage leverages the\npre-trained model to learn the distribution of anatomical structure locations.\nThe post-training stage iteratively refines the anatomical structure locations\nbased on the diffusion model output. We demonstrate the effect",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15217391304347827,
          "p": 0.2727272727272727,
          "f": 0.19534883261179026
        },
        "rouge-2": {
          "r": 0.015544041450777202,
          "p": 0.02830188679245283,
          "f": 0.02006688505542548
        },
        "rouge-l": {
          "r": 0.14492753623188406,
          "p": 0.2597402597402597,
          "f": 0.1860465070303949
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2409.11569v1",
      "true_abstract": "We consider the Merton problem of optimizing expected power utility of\nterminal wealth in the case of an unobservable Markov-modulated drift. What\nmakes the model special is that the agent is allowed to purchase costly expert\nopinions of varying quality on the current state of the drift, leading to a\nmixed stochastic control problem with regular and impulse controls involving\nrandom consequences. Using ideas from filtering theory, we first embed the\noriginal problem with unobservable drift into a full information problem on a\nlarger state space. The value function of the full information problem is\ncharacterized as the unique viscosity solution of the dynamic programming PDE.\nThis characterization is achieved by a new variant of the stochastic Perron's\nmethod, which additionally allows us to show that, in between purchases of\nexpert opinions, the problem reduces to an exit time control problem which is\nknown to admit an optimal feedback control. Under the assumption of sufficient\nregularity of this feedback map, we are able to construct optimal trading and\nexpert opinion strategies.",
      "generated_abstract": "We study the effect of unobservable heterogeneity in the distribution of\nrisk premia on the optimal portfolio weights. We consider a mean-variance\noptimization problem where risk premia are modeled as a normal distribution,\nwith a non-Gaussian tail. We show that the optimal portfolio weights depend on\nthe mean and variance of the tail distribution. We derive closed-form expressions\nfor the optimal weights under various distributions of the tail. We also\nestimate the optimal portfolio weights using a kernel density estimator,\nshowing that the kernel density estimator is a consistent estimator of the\noptimal weights under non-Gaussian tail distributions.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15315315315315314,
          "p": 0.3269230769230769,
          "f": 0.2085889527103016
        },
        "rouge-2": {
          "r": 0.03067484662576687,
          "p": 0.0641025641025641,
          "f": 0.041493771555586624
        },
        "rouge-l": {
          "r": 0.15315315315315314,
          "p": 0.3269230769230769,
          "f": 0.2085889527103016
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.17597v1",
      "true_abstract": "This paper presents a model of costly information acquisition where\ndecision-makers can choose whether to elaborate information superficially or\nprecisely. The former action is costless, while the latter entails a processing\ncost. Within this framework, decision-makers' beliefs may polarize even after\nthey have access to the same evidence. From the perspective of a Bayesian\nobserver who neglects information processing constraints, the decision-makers'\noptimal behavior and belief updating may appear consistent with biases such as\ndisconfirmation, underreaction to information, and confirmation bias. However,\nthese phenomena emerge naturally within the model and are fully compatible with\nstandard Bayesian inference and rational decision-making when accounting for\nthe costs of information acquisition.",
      "generated_abstract": "This study examines the impact of the COVID-19 pandemic on the global\nfossil fuel industry. We employ a multi-stage fixed effects regression to\nquantify the impact of the pandemic on fossil fuel revenues, production, and\nemployment. Our findings reveal that the COVID-19 outbreak had a negative\nimpact on fossil fuel revenues, production, and employment, particularly in\nemerging economies. This negative impact was driven by a decline in demand,\nhigher production costs, and reduced investment. In contrast, the pandemic had\na positive impact on fossil fuel exports, which was driven by higher demand\nand increased production. These findings underscore the critical role of\ngovernment policies and external shocks in shaping the COVID-19 response.\nAdditionally, the study highlights the importance of accounting for heterogeneity\nin the impact of the pan",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.09523809523809523,
          "p": 0.1111111111111111,
          "f": 0.1025640975936886
        },
        "rouge-2": {
          "r": 0.009433962264150943,
          "p": 0.009615384615384616,
          "f": 0.009523804524265662
        },
        "rouge-l": {
          "r": 0.09523809523809523,
          "p": 0.1111111111111111,
          "f": 0.1025640975936886
        }
      }
    },
    {
      "paper_id": "cs.CL.econ/GN/2503.01870v1",
      "true_abstract": "Identifying customer needs (CNs) is important for product management, product\ndevelopment, and marketing. Applications rely on professional analysts\ninterpreting textual data (e.g., interview transcripts, online reviews) to\nunderstand the nuances of customer experience and concisely formulate \"jobs to\nbe done.\" The task is cognitively complex and time-consuming. Current practice\nfacilitates the process with keyword search and machine learning but relies on\nhuman judgment to formulate CNs. We examine whether Large Language Models\n(LLMs) can automatically extract CNs. Because evaluating CNs requires\nprofessional judgment, we partnered with a marketing consulting firm to conduct\na blind study of CNs extracted by: (1) a foundational LLM with prompt\nengineering only (Base LLM), (2) an LLM fine-tuned with professionally\nidentified CNs (SFT LLM), and (3) professional analysts. The SFT LLM performs\nas well as or better than professional analysts when extracting CNs. The\nextracted CNs are well-formulated, sufficiently specific to identify\nopportunities, and justified by source content (no hallucinations). The SFT LLM\nis efficient and provides more complete coverage of CNs. The Base LLM was not\nsufficiently accurate or specific. Organizations can rely on SFT LLMs to reduce\nmanual effort, enhance the precision of CN articulation, and provide improved\ninsight for innovation and marketing strategy.",
      "generated_abstract": "This paper introduces the concept of a \"satisfaction-driven\" approach to\nlearning, which is motivated by the observation that human beings tend to\nfocus on the satisfaction of specific concerns, such as money, food, and\nhappiness, while ignoring other concerns, such as security and knowledge. The\nsatisfaction-driven approach is conceptualized as a framework for designing\nneural networks that learn to produce satisficing answers to a given\nquestion. This approach allows us to encode both the concerns that humans\nexperience and the concerns that we think they should experience, while\nremaining faithful to the notion of satisficing. We validate the\nsatisfaction-driven approach through experiments on three tasks: (i) whether\nhuman judges are able to correctly assign satisficing answers to the\nquestion, \"How much should you pay for this house?\" (ii) whether the\nsatisfaction-driven",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1276595744680851,
          "p": 0.20930232558139536,
          "f": 0.15859030366356824
        },
        "rouge-2": {
          "r": 0.005128205128205128,
          "p": 0.008264462809917356,
          "f": 0.006329109198249003
        },
        "rouge-l": {
          "r": 0.1276595744680851,
          "p": 0.20930232558139536,
          "f": 0.15859030366356824
        }
      }
    },
    {
      "paper_id": "cs.PF.cs/PF/2503.02982v1",
      "true_abstract": "We consider a discrete-time parallel service system consisting of $n$\nheterogeneous single server queues with infinite capacity. Jobs arrive to the\nsystem as an i.i.d. process with rate proportional to $n$, and must be\nimmediately dispatched in the time slot that they arrive. The dispatcher is\nassumed to be able to exchange messages with the servers to obtain their queue\nlengths and make dispatching decisions, introducing an undesirable\ncommunication overhead.\n  In this setting, we propose a ultra-low communication overhead load balancing\npolicy dubbed $k$-Skip-the-$d$-Longest-Queues ($k$-SLQ-$d$), where queue\nlengths are only observed every $k(n-d)$ time slots and, between observations,\nincoming jobs are sent to a queue that is not one of the $d$ longest ones at\nthe time that the queues were last observed. For this policy, we establish\nconditions on $d$ for it to be throughput optimal and we show that, under that\ncondition, it is asymptotically delay-optimal in heavy-traffic for arbitrarily\nlow communication overheads (i.e., for arbitrarily large $k$).",
      "generated_abstract": "This paper presents the first results of a large-scale study on the\npotential of the Poseidon consensus algorithm for the secure distribution of\ndata in federated learning (FL) applications. We consider a scenario in which\nparticipants from a large number of institutions, each with a limited number of\ndevices, participate in a FL training process. We investigate the performance\nof the Poseidon consensus algorithm in this setting, focusing on the robustness\nand scalability of the algorithm. Our analysis reveals that the Poseidon\nalgorithm is robust under various fault scenarios, such as device failures and\ncommunication errors. Additionally, we demonstrate that the Poseidon algorithm\nis scalable to large-scale FL training, supporting large-scale training\napplications with hundreds of participants and thousands of devices.\nAdditionally, we explore the potential of the Poseidon consensus algorithm for\nscal",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1826086956521739,
          "p": 0.2727272727272727,
          "f": 0.21874999519585514
        },
        "rouge-2": {
          "r": 0.03164556962025317,
          "p": 0.044642857142857144,
          "f": 0.037037032182167993
        },
        "rouge-l": {
          "r": 0.1565217391304348,
          "p": 0.23376623376623376,
          "f": 0.18749999519585514
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/GN/2405.10917v2",
      "true_abstract": "It is a challenge to estimate fund performance by compounded returns.\nArguably, it is incorrect to use yearly returns directly for compounding, with\nreported annualized return of above 60% for Medallion for the 31 years up to\n2018. We propose an estimation based on fund sizes and trading profits and\nobtain a compounded return of 31.8% before fees. Alternatively, we suggest\nusing the manager's wealth as a proxy and arriving at a compounded growth rate\nof 25.6% for Simons for the 33 years up to 2020. We conclude that the\nannualized compounded return of Medallion before fees is probably under 35%.\nOur findings have implications for correctly estimating fund performance.",
      "generated_abstract": "We introduce a novel method for estimating the expected short-term capital\ngains tax liability for individuals with complex income structures. The method\nis based on the decomposition of the realized capital gains into three\ncomponents: (1) taxable capital gains, (2) tax-free capital gains, and (3)\nnon-capital gains. This decomposition is based on the assumption that the\ntaxpayer can be characterized by an underlying model that describes their\ncapital structure. This model can be a stochastic process or a functional\nmodel of capital structure. We show that the expected taxable capital gains\nliability is equal to the expected tax-free capital gains liability plus the\nexpected non-capital gains liability. We also show that the expected taxable\ncapital gains liability is equal to the expected tax-free capital gains liability\ntimes the tax rate, and that the expected",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.20270270270270271,
          "p": 0.24193548387096775,
          "f": 0.2205882303330451
        },
        "rouge-2": {
          "r": 0.0196078431372549,
          "p": 0.020618556701030927,
          "f": 0.02010049751572054
        },
        "rouge-l": {
          "r": 0.17567567567567569,
          "p": 0.20967741935483872,
          "f": 0.19117646562716276
        }
      }
    },
    {
      "paper_id": "cs.MA.q-fin/EC/2502.13267v1",
      "true_abstract": "BeforeIT is an open-source software for building and simulating\nstate-of-the-art macroeconomic agent-based models (macro ABMs) based on the\nrecently introduced macro ABM developed in [1] and here referred to as the base\nmodel. Written in Julia, it combines extraordinary computational efficiency\nwith user-friendliness and extensibility. We present the main structure of the\nsoftware, demonstrate its ease of use with illustrative examples, and benchmark\nits performance. Our benchmarks show that the base model built with BeforeIT is\norders of magnitude faster than a Matlab version, and significantly faster than\nMatlab-generated C code. BeforeIT is designed to facilitate reproducibility,\nextensibility, and experimentation. As the first open-source, industry-grade\nsoftware to build macro ABMs of the type of the base model, BeforeIT can\nsignificantly foster collaboration and innovation in the field of agent-based\nmacroeconomic modelling. The package, along with its documentation, is freely\navailable at https://github.com/bancaditalia/BeforeIT.jl under the AGPL-3.0.",
      "generated_abstract": "The financial sector has been a significant contributor to climate change,\nwith the impacts of the pandemic exacerbating these risks. This study examines\nthe impact of climate change on financial risks, focusing on climate-related\nfinancial disclosures and climate-related financial risk management. We find\nthat climate-related risks are significantly increased by climate change,\nespecially in the financial sector. The findings also show that climate-related\nfinancial risks have increased significantly since 2018. The study suggests\nthat firms are under-disclosing their climate-related risks and that firms\nshould increase their disclosures to be more transparent about the risks\nassociated with climate change. The findings of this study contribute to the\nunderstanding of climate change risks and their financial implications, and\nhighlight the need for firms to take action to mitigate these ris",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13592233009708737,
          "p": 0.2028985507246377,
          "f": 0.16279069286979464
        },
        "rouge-2": {
          "r": 0.02158273381294964,
          "p": 0.02608695652173913,
          "f": 0.023622042288735618
        },
        "rouge-l": {
          "r": 0.10679611650485436,
          "p": 0.15942028985507245,
          "f": 0.1279069719395621
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.07208v1",
      "true_abstract": "In this work, various analysis methods are conducted on frequency-dependent\nmethods on SED to further delve into their detailed characteristics and\nbehaviors on SED. While SED has been rapidly advancing through the adoption of\nvarious deep learning techniques from other pattern recognition fields, these\ntechniques are often not suitable for SED. To address this issue, two\nfrequency-dependent SED methods were previously proposed: FilterAugment, a data\naugmentation randomly weighting frequency bands, and frequency dynamic\nconvolution (FDY Conv), an architecture applying frequency adaptive convolution\nkernels. These methods have demonstrated superior performance in SED, and we\naim to further analyze their detailed effectiveness and characteristics in SED.\nWe compare class-wise performance to find out specific pros and cons of\nFilterAugment and FDY Conv. We apply Gradient-weighted Class Activation Mapping\n(Grad-CAM), which highlights time-frequency region that is more inferred by the\nmodel, on SED models with and without frequency masking and two types of\nFilterAugment to observe their detailed characteristics. We propose simpler\nfrequency dependent convolution methods and compare them with FDY Conv to\nfurther understand which components of FDY Conv affects SED performance.\nLastly, we apply PCA to show how FDY Conv adapts dynamic kernel across\nfrequency dimensions on different sound event classes. The results and\ndiscussions demonstrate that frequency dependency plays a significant role in\nsound event detection and further confirms the effectiveness of frequency\ndependent methods on SED.",
      "generated_abstract": "This paper presents a novel method for the detection of airstreams in\nunderwater acoustic environments. Traditional approaches to detecting airstream\nsignals rely on the assumption that the airstream is a single, unidirectional\nsignal with a known frequency. However, this assumption is incorrect in\nunderwater environments where the airstream is likely to be multi-directional\nand non-uniform. In this paper, we propose a novel method for detecting airstream\nsignals in underwater acoustic environments. Our method employs a novel\narchitecture to leverage multi-directionality and non-uniformity of airstreams\nin the acoustic environment. This architecture enables our method to accurately\ndetect airstreams. Additionally, our method is capable of simultaneously\ndetecting multiple airstreams. We validate our method on both synthetic and\nreal-world datasets.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1366906474820144,
          "p": 0.2878787878787879,
          "f": 0.18536584929256408
        },
        "rouge-2": {
          "r": 0.004739336492890996,
          "p": 0.01,
          "f": 0.006430863804140635
        },
        "rouge-l": {
          "r": 0.12949640287769784,
          "p": 0.2727272727272727,
          "f": 0.17560975173158846
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2503.07461v1",
      "true_abstract": "We study the optimal management of a photovoltaic system's battery owned by a\nself-consumption group that aims to minimize energy consumption costs. We\nassume that the photovoltaic system is composed of a photovoltaic panel and a\nbattery, where the photovoltaic panel produces energy according to a certain\nstochastic process. The management of the battery is the responsibility of a\ngroup administrator, who makes the joint decision to either store part of the\nphotovoltaic energy production and sell the remaining energy at the electricity\nspot price, or discharge part of the energy stored in the battery and sell it\nin the electricity market. Inspired by European Union and Italian legislation,\nwhich promote incentives for energy transition and renewable energy production,\nwe assume that the group receives a monetary incentive for the virtual\nself-consumed energy, defined as the minimum between the power bought from the\ngrid to satisfy the group's power demand and the energy sold to the market. In\nthis case, the energy sold by the group is a mix of part of the photovoltaic\nproduction that is not stored and part of the energy discharged from the\nbattery. We model the problem as a stochastic optimal control problem, where\nthe optimal strategy is the joint charge-discharge decision that minimizes the\ngroup's energy consumption costs. We find the solution numerically by applying\na finite difference scheme to solve the Hamilton-Jacobi-Bellman equation\nassociated with the value function of the optimal control problem.",
      "generated_abstract": "This paper investigates the impact of public sector wage transparency on\nemployees' labor supply in a model of information asymmetry. We find that\ninformation asymmetry increases the probability of quitting jobs and reduces\nthe probability of accepting wage increases. These effects are strongest for\nemployees with lower levels of education and experience. Additionally,\nwage transparency leads to a decrease in wage bargaining power among employees\nwith lower levels of education and experience, which can be offset by a\nreduction in quitting rates. These effects are also present when the wage\ntransparency law is enforced through wage surveys. These findings suggest that\nwage transparency may be a more effective policy tool than wage surveys,\nparticularly for workers with lower levels of education and experience.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1391304347826087,
          "p": 0.21621621621621623,
          "f": 0.16931216454746523
        },
        "rouge-2": {
          "r": 0.015,
          "p": 0.029411764705882353,
          "f": 0.019867545195387172
        },
        "rouge-l": {
          "r": 0.12173913043478261,
          "p": 0.1891891891891892,
          "f": 0.1481481433834441
        }
      }
    },
    {
      "paper_id": "eess.SY.eess/SY/2503.05403v2",
      "true_abstract": "We propose a decentralized framework for guaranteeing the small-signal\nstability of future power systems with grid-forming converters. Our approach\nleverages dynamic loop-shifting techniques to compensate for the lack of\npassivity in the network dynamics and establishes decentralized parametric\nstability certificates, depending on the local device-level controls and\nincorporating the effects of the network dynamics. By following practical\ntuning rules, we are able to ensure plug-and-play operation without centralized\ncoordination. Unlike prior works, our approach accommodates coupled frequency\nand voltage dynamics, incorporates network dynamics, and does not rely on\nspecific network configurations or operating points, offering a general and\nscalable solution for the integration of power-electronics-based devices into\nfuture power systems. We validate our theoretical stability results through\nnumerical case studies in a high-fidelity simulation model.",
      "generated_abstract": "This paper presents a novel approach for designing and controlling a\nsystem of unmanned aerial vehicles (UAVs) using a distributed control\narchitecture. The system consists of two UAVs that cooperate to perform a\ndynamic mission with multiple goals, such as reaching a predefined target\nlocation. The cooperative control problem is formulated as a two-stage\ndistributed optimization problem, where the first stage involves the\noptimization of the trajectory of each UAV, while the second stage aims at\nenhancing the cooperative performance of the system. To address the\ncomputational complexity of the problem, a distributed control strategy is\nproposed that employs a two-level distributed optimization to efficiently\nsolve the two-stage problem. The proposed approach is verified by solving an\nextreme-point problem in which the constraints are linear and the objective\nfunction is quadratic. The proposed distributed control strategy achieves",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.12087912087912088,
          "p": 0.13095238095238096,
          "f": 0.12571428072228594
        },
        "rouge-2": {
          "r": 0.008403361344537815,
          "p": 0.0078125,
          "f": 0.008097160998544278
        },
        "rouge-l": {
          "r": 0.10989010989010989,
          "p": 0.11904761904761904,
          "f": 0.1142857092937145
        }
      }
    },
    {
      "paper_id": "eess.AS.eess/AS/2502.20857v1",
      "true_abstract": "Sound event detection (SED) has significantly benefited from self-supervised\nlearning (SSL) approaches, particularly masked audio transformer for SED\n(MAT-SED), which leverages masked block prediction to reconstruct missing audio\nsegments. However, while effective in capturing global dependencies, masked\nblock prediction disrupts transient sound events and lacks explicit enforcement\nof temporal order, making it less suitable for fine-grained event boundary\ndetection. To address these limitations, we propose JiTTER (Jigsaw Temporal\nTransformer for Event Reconstruction), an SSL framework designed to enhance\ntemporal modeling in transformer-based SED. JiTTER introduces a hierarchical\ntemporal shuffle reconstruction strategy, where audio sequences are randomly\nshuffled at both the block-level and frame-level, forcing the model to\nreconstruct the correct temporal order. This pretraining objective encourages\nthe model to learn both global event structures and fine-grained transient\ndetails, improving its ability to detect events with sharp onset-offset\ncharacteristics. Additionally, we incorporate noise injection during block\nshuffle, providing a subtle perturbation mechanism that further regularizes\nfeature learning and enhances model robustness. Experimental results on the\nDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%\nimprovement in PSDS, highlighting the effectiveness of explicit temporal\nreasoning in SSL-based SED. Our findings suggest that structured temporal\nreconstruction tasks, rather than simple masked prediction, offer a more\neffective pretraining paradigm for sound event representation learning.",
      "generated_abstract": "This paper presents a deep learning framework for the estimation of\ndynamic impedance-based nonlinear model predictive control (NMPC) controllers\nfor an underactuated system with uncertain parameters. The proposed framework\nenables the generation of nonlinear model predictive controllers with\ndynamic impedance constraints. First, we develop a neural network architecture\nfor the dynamic impedance estimation, which consists of a set of deep\nrepresentations and a set of shallow representations. The former are used to\nestimate the dynamic impedance of the system while the latter are used to\ncapture the dynamics of the system. Then, we design a deep learning algorithm\nto estimate the dynamic impedance of the system from the neural network\nrepresentations. Finally, we use the estimated dynamic impedance for the\ngeneration of the nonlinear model predictive controller. The effectiveness of\nthe proposed framework is validated through extensive experiments on both\nsimulated and real datasets",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13245033112582782,
          "p": 0.2777777777777778,
          "f": 0.17937219293691822
        },
        "rouge-2": {
          "r": 0.0048543689320388345,
          "p": 0.008928571428571428,
          "f": 0.006289303612993302
        },
        "rouge-l": {
          "r": 0.11920529801324503,
          "p": 0.25,
          "f": 0.16143497320597652
        }
      }
    },
    {
      "paper_id": "q-bio.OT.q-bio/OT/2409.11451v1",
      "true_abstract": "Experiment 1. Rooting of quince hardwood cuttings: Rooting success was\ninfluenced by both the concentrations of IBA and the selection of rooting\nmedia. However, the control group (without IBA) notably enhanced rooting when\ncompared to the various IBA concentrations. Cuttings in the control group\n(without IBA) and those planted in river sand exhibited notably high\npercentages of successful rooting, underscoring the importance of the selected\nplanting medium. Experiment 2. Bench grafting of loquat: The success of\ngrafting loquat cutting stocks varied based on grafting dates, types of\ncuttings, and concentrations of IBA. However, IBA at different concentrations\ndid not have a significant impact. Notably, certain interactions such as\ngrafting on February 20 with loquat stock cuttings, yielded higher percentages\nof successful graft bud sprouting. Experiment 3. Performance of grafting\nloquats onto different rootstocks: Grafting success was notably influenced by\nthe selection of rootstock, with loquat rootstock demonstrating superior\nperformance compared to quince. The highest significant levels of successful\ngrafting were attained on February 20, underscoring the crucial role of\ngrafting dates. Experiment 4. Impact of tree stock types on grafting success:\nGrafting success percentage was higher in loquat tree stock when compared to\nquince. The consistency of grafting success percentages across three dates\nunderscores the significant influence of rootstock type. Experiment 5. Bench\ngrafting of loquat cutting stocks: Graft bud sprout percentages exhibited\nvariations, with loquat stock cuttings surpassing quince. Grafting success\ndemonstrated a consistent increase from February 20 to March 30, underscoring\nthe importance of selecting appropriate grafting dates.",
      "generated_abstract": "We propose an experimental framework to study the evolution of social\nnetworks. We focus on how the emergence of new nodes impacts the structure of\nthe network, focusing on the role of the initial nodes in the evolution of the\nnetwork structure. To this end, we introduce the concept of \"initial nodes\",\nwhich are nodes that influence the network structure in a deterministic way. We\nformulate an evolutionary game between the initial nodes and the network,\nwhere the initial nodes compete for the network, but they also try to influence\nit through their actions. We then study the evolution of the network structure\nunder this evolutionary game, and compare the results to the case of a\ndeterministic evolution of the network structure. We show that the initial\nnodes play a crucial role in the evolution of the network structure, and that\nthe initial nodes can influence the evolution of the network structure in a\ndeterministic way. We also",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07874015748031496,
          "p": 0.14705882352941177,
          "f": 0.10256409802182795
        },
        "rouge-2": {
          "r": 0.02843601895734597,
          "p": 0.05309734513274336,
          "f": 0.037037032494475486
        },
        "rouge-l": {
          "r": 0.07874015748031496,
          "p": 0.14705882352941177,
          "f": 0.10256409802182795
        }
      }
    },
    {
      "paper_id": "cs.NE.q-fin/PM/2501.14736v1",
      "true_abstract": "In this study, we applied the NEAT (NeuroEvolution of Augmenting Topologies)\nalgorithm to stock trading using multiple technical indicators. Our approach\nfocused on maximizing earning, avoiding risk, and outperforming the Buy & Hold\nstrategy. We used progressive training data and a multi-objective fitness\nfunction to guide the evolution of the population towards these objectives. The\nresults of our study showed that the NEAT model achieved similar returns to the\nBuy & Hold strategy, but with lower risk exposure and greater stability. We\nalso identified some challenges in the training process, including the presence\nof a large number of unused nodes and connections in the model architecture. In\nfuture work, it may be worthwhile to explore ways to improve the NEAT algorithm\nand apply it to shorter interval data in order to assess the potential impact\non performance.",
      "generated_abstract": "This paper presents a novel approach to forecasting multi-asset returns,\nusing a novel ensemble of neural networks. Neural networks have proven\neffective in many fields of artificial intelligence, including natural\nlanguage processing, computer vision, and language translation. This paper\nextends this approach to financial forecasting, using a neural network ensemble\nto improve the performance of a traditional Markov-Chain-Monte-Carlo (MCMC)\nforecasting method. The forecasting method is evaluated using real-world\nmarket data from the Australian Securities Exchange (ASX), including stock\nreturns, currency exchange rates, and bond yields. The forecasting method\nachieves an accuracy of 70% in terms of root mean squared error (RMSE),\nsignificantly higher than the traditional MCMC method, which achieves an\naccuracy of 40%. The forecasting method outperforms a variety of",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15463917525773196,
          "p": 0.1875,
          "f": 0.16949152046985236
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.027777777777777776,
          "f": 0.02521007907633739
        },
        "rouge-l": {
          "r": 0.15463917525773196,
          "p": 0.1875,
          "f": 0.16949152046985236
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.01053v1",
      "true_abstract": "Each period, two players bargain over a unit of surplus. Each player chooses\nbetween remaining flexible and committing to a take-it-or-leave-it offer at a\ncost. If players' committed demands are incompatible, then the current-period\nsurplus is destroyed in the conflict. When both players are flexible, the\nsurplus is split according to the status quo, which is the division in the last\nperiod where there was no conflict. We show that when players are patient and\nthe cost of commitment is small, there exist a class of symmetric Markov\nPerfect equilibria that are asymptotically efficient and renegotiation proof,\nin which players commit to fair demands in almost all periods.",
      "generated_abstract": "This paper provides a general framework for analyzing the impact of\nrandomized evaluations on the incidence of cooperation. We show that in\nsettings with a finite set of alternatives, the probability of cooperation\ndecreases as the number of evaluations increases. Moreover, the rate of this\ndecrease depends on the number of evaluations and the value of a parameter\nthat captures the sensitivity of the outcome to the evaluation. The framework\ncan also be applied to situations where the evaluations are randomized and\noutcomes are observed, in which case the probability of cooperation increases\nas the number of evaluations increases. The analysis is based on a framework\ninvolving a hierarchy of sets of alternatives, in which the first level\nconsiders the number of evaluations, while the second level considers the\nnumber of alternatives. We also provide a simple interpretation of our results\nin terms of the Fisher-von Mises theorem.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.18055555555555555,
          "p": 0.16883116883116883,
          "f": 0.17449663930093257
        },
        "rouge-2": {
          "r": 0.04807692307692308,
          "p": 0.04,
          "f": 0.043668117312790175
        },
        "rouge-l": {
          "r": 0.16666666666666666,
          "p": 0.15584415584415584,
          "f": 0.16107382050898622
        }
      }
    },
    {
      "paper_id": "q-fin.PM.q-fin/PM/2501.12841v1",
      "true_abstract": "This study systematically examines how several alternative approaches\nconsidered affect three aspects that determine portfolio performance (the gross\nreturn, the transaction costs and the portfolio risk). We find that it is\ndifficult to exploit the possible predictability of asset returns. However, the\npredictability of asset return volatility produces obvious economic value,\nalthough in a highly correlated cryptocurrencies market.",
      "generated_abstract": "This paper proposes a framework for portfolio allocation and risk\nportfolio allocation using a stochastic volatility model. The model is based on\nthe Black-Litterman approach to portfolio optimization. It is formulated as a\nmaximization problem with a finite set of constraints. We show that this\nframework can be extended to a general class of stochastic volatility models,\nwhich includes the Black-Litterman model, as a special case. The proposed\nframework is illustrated by a numerical example. The results demonstrate the\neffectiveness of the proposed framework for portfolio allocation and risk\nmanagement.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.22,
          "p": 0.19298245614035087,
          "f": 0.20560747165691337
        },
        "rouge-2": {
          "r": 0.0,
          "p": 0.0,
          "f": 0.0
        },
        "rouge-l": {
          "r": 0.22,
          "p": 0.19298245614035087,
          "f": 0.20560747165691337
        }
      }
    },
    {
      "paper_id": "math.OC.econ/TH/2502.11780v2",
      "true_abstract": "This paper studies distributionally robust optimization for a large class of\nrisk measures with ambiguity sets defined by $\\phi$-divergences. The risk\nmeasures are allowed to be non-linear in probabilities, are represented by a\nChoquet integral possibly induced by a probability weighting function, and\ninclude many well-known examples (for example, CVaR, Mean-Median Deviation,\nGini-type). Optimization for this class of robust risk measures is challenging\ndue to their rank-dependent nature. We show that for many types of probability\nweighting functions including concave, convex and inverse $S$-shaped, the\nrobust optimization problem can be reformulated into a rank-independent\nproblem. In the case of a concave probability weighting function, the problem\ncan be further reformulated into a convex optimization problem with finitely\nmany constraints that admits explicit conic representability for a collection\nof canonical examples. While the number of constraints in general scales\nexponentially with the dimension of the state space, we circumvent this\ndimensionality curse and provide two types of upper and lower bounds\nalgorithms. They yield tight upper and lower bounds on the exact optimal value\nand are formally shown to converge asymptotically. This is illustrated\nnumerically in two examples given by a robust newsvendor problem and a robust\nportfolio choice problem.",
      "generated_abstract": "In this paper, we propose a novel stochastic gradient algorithm for the\nnon-convex, mixed-integer non-linear (MINLP) programming problem with\nincomplete-information constraints, which includes a set of integer linear\nprogramming (ILP) constraints and an MINLP problem. The proposed algorithm\nuses the stochastic gradient descent (SGD) method to update the Lagrange\nmultiplier parameter, which is an unknown variable, while the objective\nfunction is estimated by solving an ILP problem. The algorithm is formulated\nas a stochastic control problem and can be solved by the mean-field stochastic\ncontrol method. We provide a theoretical analysis of the algorithm and show\nthat the algorithm converges to a local minimum of the objective function and\nthe objective function is bounded. To address the limited computation capability\nof conventional stochastic control algorithms, we propose a novel algorithm\nbased on the mean-field stochastic control method with stochastic gradients",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.19658119658119658,
          "p": 0.2875,
          "f": 0.23350253324744272
        },
        "rouge-2": {
          "r": 0.033707865168539325,
          "p": 0.049586776859504134,
          "f": 0.040133774445924
        },
        "rouge-l": {
          "r": 0.17094017094017094,
          "p": 0.25,
          "f": 0.20304568045556454
        }
      }
    },
    {
      "paper_id": "stat.AP.physics/ao-ph/2503.09065v1",
      "true_abstract": "Contributions from photosynthesis and other natural components of the carbon\ncycle present the largest uncertainties in our understanding of carbon dioxide\n(CO$_2$) sources and sinks. While the global spatiotemporal distribution of the\nnet flux (the sum of all contributions) can be inferred from atmospheric CO$_2$\nconcentrations through flux inversion, attributing the net flux to its\nindividual components remains challenging. The advent of solar-induced\nfluorescence (SIF) satellite observations provides an opportunity to isolate\nnatural components by anchoring gross primary productivity (GPP), the\nphotosynthetic component of the net flux. Here, we introduce a novel\nstatistical flux-inversion framework that simultaneously assimilates\nobservations of SIF and CO$_2$ concentration, extending WOMBAT v2.0 (WOllongong\nMethodology for Bayesian Assimilation of Trace-gases, version 2.0) with a\nhierarchical model of spatiotemporal dependence between GPP and SIF processes.\nWe call the new framework WOMBAT v2.S, and we apply it to SIF and CO$_2$ data\nfrom NASA's Orbiting Carbon Observatory-2 (OCO-2) satellite and other\ninstruments to estimate natural fluxes over the globe during a recent six-year\nperiod. In a simulation experiment that matches OCO-2's retrieval\ncharacteristics, the inclusion of SIF improves accuracy and uncertainty\nquantification of component flux estimates. Comparing estimates from WOMBAT\nv2.S, v2.0, and the independent FLUXCOM initiative, we observe that linking GPP\nto SIF has little effect on net flux, as expected, but leads to spatial\nredistribution and more realistic seasonal structure in natural flux\ncomponents.",
      "generated_abstract": "The global climate system is characterized by a large number of interacting\nand coupled physical and biogeochemical processes. The climate system is\ndescribed by a stochastic dynamical system, where the stochastic forcing\nprovides the forcing of the system. The stochastic dynamics of the climate system\nare characterized by a large number of stochastic parameters, which are\nunknown. In this study, we investigate the robustness of the climate system\nunder a range of stochastic forcing scenarios, including the mean and\nlong-term variations of the stochastic forcing. We use the climate model\nCAM5 to investigate the robustness of the climate system. The climate model\nCAM5 is a coupled general circulation model that can simulate the Earth's\nclimate over a wide range of timescales and spatial scales. The climate model\nCAM5 is also used to simulate the mean and long-term variations of the stochastic",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11764705882352941,
          "p": 0.29508196721311475,
          "f": 0.16822429498951882
        },
        "rouge-2": {
          "r": 0.004608294930875576,
          "p": 0.01020408163265306,
          "f": 0.00634920206278949
        },
        "rouge-l": {
          "r": 0.09803921568627451,
          "p": 0.2459016393442623,
          "f": 0.14018691181194878
        }
      }
    },
    {
      "paper_id": "econ.TH.econ/TH/2503.10132v1",
      "true_abstract": "This paper analyzes Shinohara Rock-Paper-Scissors (RPS), a variant of the\nclassic RPS game introduced by board game designer Yoshiteru Shinohara. In this\ngame, players compete against a host who always plays rock, so players choose\neither rock or paper. The catch is that if two or more players choose paper,\nthey are eliminated, creating strategic tension among the players. The last\nremaining player wins. We derive subgame perfect equilibria (SPE) of Shinohara\nRPS. A unique symmetric SPE exists, in which the probability of choosing paper\nsatisfies the equation $(1-p)^{n-1} + p^{n-1}/n = 1/n$. The game also admits a\ncontinuum of asymmetric SPE, making it unlikely that any specific SPE will be\nobserved in actual play.",
      "generated_abstract": "This paper provides a new perspective on the application of differential\ndriving incentives to promote behavioral change. We extend the model of\nDarley and Sokolowski (1970) to incorporate the effects of behavioral\ndriving on the behavioral response. The model provides a framework for\nincorporating the effect of behavioral incentives on behavioral change,\nincluding their potential to induce deviations from behavioral targets. This\napproach allows for a more comprehensive understanding of the relationship\nbetween behavioral incentives and behavioral change. We use the model to\nanalyze the effect of differential driving incentives on behavioral\nperformance and discuss the implications for the design of behavioral incentive\nsystems.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.08888888888888889,
          "p": 0.14545454545454545,
          "f": 0.11034482287752695
        },
        "rouge-2": {
          "r": 0.017699115044247787,
          "p": 0.022727272727272728,
          "f": 0.0199004925897886
        },
        "rouge-l": {
          "r": 0.08888888888888889,
          "p": 0.14545454545454545,
          "f": 0.11034482287752695
        }
      }
    },
    {
      "paper_id": "econ.GN.econ/GN/2411.16574v1",
      "true_abstract": "Algorithmic agents are used in a variety of competitive decision settings,\nnotably in making pricing decisions in contexts that range from online retail\nto residential home rentals. Business managers, algorithm designers, legal\nscholars, and regulators alike are all starting to consider the ramifications\nof \"algorithmic collusion.\" We study the emergent behavior of multi-armed\nbandit machine learning algorithms used in situations where agents are\ncompeting, but they have no information about the strategic interaction they\nare engaged in. Using a general-form repeated Prisoner's Dilemma game, agents\nengage in online learning with no prior model of game structure and no\nknowledge of competitors' states or actions (e.g., no observation of competing\nprices). We show that these context-free bandits, with no knowledge of\nopponents' choices or outcomes, still will consistently learn collusive\nbehavior - what we call \"naive collusion.\" We primarily study this system\nthrough an analytical model and examine perturbations to the model through\nsimulations.\n  Our findings have several notable implications for regulators. First, calls\nto limit algorithms from conditioning on competitors' prices are insufficient\nto prevent algorithmic collusion. This is a direct result of collusion arising\neven in the naive setting. Second, symmetry in algorithms can increase\ncollusion potential. This highlights a new, simple mechanism for\n\"hub-and-spoke\" algorithmic collusion. A central distributor need not imbue its\nalgorithm with supra-competitive tendencies for apparent collusion to arise; it\ncan simply arise by using certain (common) machine learning algorithms.\nFinally, we highlight that collusive outcomes depend starkly on the specific\nalgorithm being used, and we highlight market and algorithmic conditions under\nwhich it will be unknown a priori whether collusion occurs.",
      "generated_abstract": "The use of digital technologies in the field of agriculture has been\nprominent in recent years, especially in the context of the COVID-19\npandemic. Digital technologies allow the monitoring of the growth and\ndevelopment of crops, the identification of pests and diseases, and the\nanalysis of the quality of the harvests. This review summarizes the main\ntechnologies used in the agricultural sector, including sensors, software, and\ndigital systems, as well as their applications in the context of the COVID-19\npandemic. The review also analyzes the impact of the use of digital technologies\nin the agricultural sector on food production, food safety, and food\ndistribution. The review highlights the potential of digital technologies in\nimproving food security and food safety, as well as their contribution to\nenhancing the resilience of agricultural systems to climate change and\ndisasters.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.05555555555555555,
          "p": 0.14925373134328357,
          "f": 0.08097165596551348
        },
        "rouge-2": {
          "r": 0.007722007722007722,
          "p": 0.018518518518518517,
          "f": 0.010899178407740892
        },
        "rouge-l": {
          "r": 0.05555555555555555,
          "p": 0.14925373134328357,
          "f": 0.08097165596551348
        }
      }
    },
    {
      "paper_id": "cs.SD.cs/SD/2503.06984v1",
      "true_abstract": "Video-to-audio generation is essential for synthesizing realistic audio\ntracks that synchronize effectively with silent videos. Following the\nperspective of extracting essential signals from videos that can precisely\ncontrol the mature text-to-audio generative diffusion models, this paper\npresents how to balance the representation of mel-spectrograms in terms of\ncompleteness and complexity through a new approach called Mel\nQuantization-Continuum Decomposition (Mel-QCD). We decompose the\nmel-spectrogram into three distinct types of signals, employing quantization or\ncontinuity to them, we can effectively predict them from video by a devised\nvideo-to-all (V2X) predictor. Then, the predicted signals are recomposed and\nfed into a ControlNet, along with a textual inversion design, to control the\naudio generation process. Our proposed Mel-QCD method demonstrates\nstate-of-the-art performance across eight metrics, evaluating dimensions such\nas quality, synchronization, and semantic consistency. Our codes and demos will\nbe released at \\href{Website}{https://wjc2830.github.io/MelQCD/}.",
      "generated_abstract": "We study the problem of learning a policy that maximizes a certain type of\ndiversity in a finite number of games. We show that the optimal policy is\ndeterministic, and that a deterministic policy with a certain type of\ndiversity can be approximated by a policy with another type of diversity. We\nalso show that the optimal policy cannot be approximated by a deterministic\npolicy with a type of diversity that is less than a given threshold. We\nidentify a class of games in which the optimal policy is a deterministic\npolicy with a type of diversity that is greater than a given threshold. We show\nthat this class of games can be approximated by a class of games in which the\noptimal policy is a deterministic policy with a type of diversity that is less\nthan a given threshold, and by a class of games in which the optimal policy is\na deterministic policy with a type of diversity that is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.11607142857142858,
          "p": 0.3170731707317073,
          "f": 0.16993463659959857
        },
        "rouge-2": {
          "r": 0.014388489208633094,
          "p": 0.027777777777777776,
          "f": 0.01895734147570916
        },
        "rouge-l": {
          "r": 0.11607142857142858,
          "p": 0.3170731707317073,
          "f": 0.16993463659959857
        }
      }
    },
    {
      "paper_id": "cs.DS.cs/DS/2503.09762v1",
      "true_abstract": "We study a centralized discrete-time dynamic two-way matching model with\nfinitely many agent types. Agents arrive stochastically over time and join\ntheir type-dedicated queues waiting to be matched. We focus on\nstate-independent greedy policies that achieve constant regret at all times by\nmaking matching decisions based solely on agent availability across types,\nrather than requiring complete queue-length information. Such policies are\nparticularly appealing for life-saving applications such as kidney exchange, as\nthey require less information and provide more transparency compared to\nstate-dependent policies.\n  First, for acyclic matching networks, we analyze a deterministic priority\npolicy proposed by Kerimov et al. [2023] that follows a static priority order\nover matches. We derive the first explicit regret bound in terms of the general\nposition gap (GPG) parameter $\\epsilon$, which measures the distance of the\nfluid relaxation from degeneracy. Second, for general two-way matching\nnetworks, we design a randomized state-independent greedy policy that achieves\nconstant regret with optimal scaling $O(\\epsilon^{-1})$, matching the existing\nlower bound established by Kerimov et al. [2024].",
      "generated_abstract": "The classic problem of finding a longest subsequence in a binary sequence\nis known to be NP-hard in the worst case. In this paper, we introduce the\nproblem of finding a longest subsequence in a binary sequence, called a\npseudo-binary sequence, where the subsequence is guaranteed to be a pseudo-binary\nsequence, i.e., each subsequence is a binary sequence of length at most 3. We\nobtain a polynomial-time algorithm for this problem, which we call\nSubsequence-Pseudo-Binary-Sequence (SPBS). The SPBS algorithm can be seen as a\ngeneralization of the known algorithms for the classic problem. The SPBS\nalgorithm runs in polynomial time and outputs a subsequence with length at most\n3, which is a pseudo-binary sequence. The SPBS algorithm can be viewed as an\nimproved version of the pseudo-binary sequence construction algorithm by\nPapadimitriou",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.13114754098360656,
          "p": 0.23880597014925373,
          "f": 0.16931216473558983
        },
        "rouge-2": {
          "r": 0.01910828025477707,
          "p": 0.02830188679245283,
          "f": 0.022813683400946524
        },
        "rouge-l": {
          "r": 0.13114754098360656,
          "p": 0.23880597014925373,
          "f": 0.16931216473558983
        }
      }
    },
    {
      "paper_id": "math.AT.math/AT/2503.01614v1",
      "true_abstract": "Recently, bipath persistent homology has been proposed as an extension of\nstandard persistent homology, along with its visualization (bipath persistence\ndiagram) and computational methods. In the setting of standard persistent\nhomology, the stability theorem with respect to real-valued functions on a\ntopological space is one of the fundamental results, which gives a mathematical\njustification for using persistent homology to noisy data. In proving the\nstability theorem, the algebraic stability theorem/the isometry theorem for\npersistence modules plays a central role. In this point of view, the stability\nproperty for bipath persistent homology is desired for analyzing data. In this\npaper, we prove the stability theorem of bipath persistent homology with\nrespect to bipath functions on a topological space. This theorem suggests a\nstability of bipath persistence diagrams: small changes in a bipath function\n(except at their ends) result in only small changes in the bipath persistence\ndiagram. Similar to the stability theorem of standard persistent homology, we\ndeduce the stability theorem of bipath persistent homology by using the\nalgebraic stability theorem/the isometry theorem of bipath persistence modules.",
      "generated_abstract": "We present a systematic study of the large class of linear systems that can\nbe solved by the method of Lyapunov exponents. We introduce a novel notion of\n\\emph{co-spectral structure} for a system of linear differential equations,\nwhich is used to define a Lyapunov spectrum, which captures the dynamics of the\nsystem at infinity. We prove that a large class of systems can be solved by\nthe method of Lyapunov exponents in the sense that the Lyapunov spectrum of the\nsystem is contained in the co-spectral spectrum. We further provide conditions\non the Lyapunov spectrum that guarantee that the associated system of\nlinear equations admits a solution that is exponentially close to the\nsolution of a system of ordinary differential equations. We establish a\nconnection between the Lyapunov spectrum and the spectrum of a matrix associated\nwith the system of equations. Our results contribute to the understanding",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1686746987951807,
          "p": 0.21212121212121213,
          "f": 0.18791945815233557
        },
        "rouge-2": {
          "r": 0.023076923076923078,
          "p": 0.02564102564102564,
          "f": 0.02429149298955994
        },
        "rouge-l": {
          "r": 0.13253012048192772,
          "p": 0.16666666666666666,
          "f": 0.14765100177649673
        }
      }
    },
    {
      "paper_id": "q-fin.CP.q-fin/TR/2412.19245v1",
      "true_abstract": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis.",
      "generated_abstract": "We introduce a novel approach to the market neutral strategy, leveraging\ndynamic factor models to capture the relationship between stock prices and\nmarket factors. We develop a methodology that combines a factor model with\nMarkov-Switching Dynamic Factor Models (MS-DFM) to derive a joint market\nneutral strategy. This approach leverages the latent dynamics of the factors\nincorporating an information flow between them. Our methodology is\nautomated, and it provides a simple and practical approach for constructing\nmarket neutral strategies in a no-trading-day setting. We apply our methodology\nto the US equity market, generating market neutral strategies with low\nrisk-adjusted return and sharpe-ratio. The results demonstrate the efficacy of\nour approach in generating market neutral strategies with superior performance\ncompared to traditional strategies, such as the traditional market neutral\nstrategy and the traditional market",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.1686746987951807,
          "p": 0.35443037974683544,
          "f": 0.22857142420191592
        },
        "rouge-2": {
          "r": 0.027450980392156862,
          "p": 0.059322033898305086,
          "f": 0.03753350773886156
        },
        "rouge-l": {
          "r": 0.13855421686746988,
          "p": 0.2911392405063291,
          "f": 0.18775509767130374
        }
      }
    },
    {
      "paper_id": "econ.GN.q-fin/EC/2502.12116v1",
      "true_abstract": "Do home prices incorporate flood risk in the immediate aftermath of specific\nflood events, or is it the repeated exposure over the years that plays a more\nsignificant role? We address this question through the first systematic study\nof the Italian housing market, which is an ideal case study because it is\nhighly exposed to floods, though unevenly distributed across the national\nterritory. Using a novel dataset containing about 550,000 mortgage-financed\ntransactions between 2016 and 2024, as well as hedonic regressions and a\ndifference-in-difference design, we find that: (i) specific floods do not\ndecrease home prices in areas at risk; (ii) the repeated exposure to floods in\nflood-prone areas leads to a price decline, up to 4\\% in the most frequently\nflooded regions; (iii) responses are heterogeneous by buyers' income and age.\nYoung buyers (with limited exposure to prior floods) do not obtain any price\nreduction for settling in risky areas, while experienced buyers do. At the same\ntime, buyers who settle in risky areas have lower incomes than buyers in safe\nareas in the most affected regions. Our results emphasize the importance of\ncultural and institutional factors in understanding how flood risk affects the\nhousing market and socioeconomic outcomes.",
      "generated_abstract": "The global pandemic has affected all aspects of economic activity, including\nthe housing market. The impact of the coronavirus pandemic on home sales has\nbeen substantial, with some economists estimating that the pandemic caused\nmore than $1 trillion in home sales losses. This paper analyzes the impact of\nthe pandemic on home sales by using an empirical model based on time-varying\ndemand for housing. The model is based on a simple formula that accounts for\nthe impact of the pandemic on the demand for housing. The results show that the\npandemic led to a decrease in the number of housing sales and a decrease in the\nprice of housing, but did not cause a decline in the number of housing\noccupancy. This suggests that the pandemic had a minimal impact on housing\nsales. The model also shows that the pandemic has a positive impact on the\nnumber of housing occupancy, which is",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.15714285714285714,
          "p": 0.3013698630136986,
          "f": 0.2065727654477728
        },
        "rouge-2": {
          "r": 0.037037037037037035,
          "p": 0.06140350877192982,
          "f": 0.0462046157683892
        },
        "rouge-l": {
          "r": 0.14285714285714285,
          "p": 0.273972602739726,
          "f": 0.18779342272476812
        }
      }
    },
    {
      "paper_id": "math.HO.math/HO/2502.17533v1",
      "true_abstract": "The constant $\\pi$ has fascinated scholars for centuries, inspiring the\nderivation of countless formulas rooted in profound mathematical insight. This\nabundance of formulas raises a question: Are they interconnected, and can a\nunifying structure explain their relationships?\n  We propose a systematic methodology for discovering and proving formula\nequivalences, leveraging modern large language models, large-scale data\nprocessing, and novel mathematical algorithms. Analyzing 457,145 arXiv papers,\nover a third of the validated formulas for $\\pi$ were proven to be derivable\nfrom a single mathematical object - including formulas by Euler, Gauss, Lord\nBrouncker, and newer ones from algorithmic discoveries by the Ramanujan\nMachine.\n  Our approach extends to other constants, such as $e$, $\\zeta(3)$, and\nCatalan's constant, proving its broad applicability. This work represents a\nstep toward the automatic unification of mathematical knowledge, laying a\nfoundation for AI-driven discoveries of connections across scientific domains.",
      "generated_abstract": "We prove a new formula for the number of solutions to the system of linear\ndifferential equations with a given set of coefficients. We derive this formula\nusing the notion of the generalized Hahn-Banach theorem for systems of linear\ndifferential equations with a given set of coefficients. The main result is a\nnew formula for the number of solutions to the system of linear differential\nequations with a given set of coefficients. We derive this formula using the\nnotion of the generalized Hahn-Banach theorem for systems of linear differential\nequations with a given set of coefficients.",
      "rouge_scores": {
        "rouge-1": {
          "r": 0.07407407407407407,
          "p": 0.25806451612903225,
          "f": 0.11510791020340573
        },
        "rouge-2": {
          "r": 0.007142857142857143,
          "p": 0.023809523809523808,
          "f": 0.010989007438716277
        },
        "rouge-l": {
          "r": 0.07407407407407407,
          "p": 0.25806451612903225,
          "f": 0.11510791020340573
        }
      }
    }
  ]
}